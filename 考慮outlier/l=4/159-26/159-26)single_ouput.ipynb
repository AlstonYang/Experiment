{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import tensorflow package for modeling\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "## Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## Plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Initializing module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "## Copy module\n",
    "import copy\n",
    "\n",
    "## Used to calculate the training time\n",
    "import time\n",
    "\n",
    "## Set the GUP environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the display\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control memory usage space for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 查詢有無可用 GPU\n",
    "torch.cuda.is_available()\n",
    "## 查詢可用 GPU 的數量\n",
    "torch.cuda.device_count()\n",
    "##目前設備\n",
    "print(\"目前設備：\",torch.cuda.current_device())\n",
    "## 目前設備名\n",
    "print(\"目前設備名：\",torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cacl(pred_value, actual_value):\n",
    "    \n",
    "#     yo, loss, tape = network.forward()\n",
    "    performance = []\n",
    "    performance.append(torch.mean(torch.abs(pred_value - actual_value)))\n",
    "    performance.append(torch.mean(torch.abs((pred_value - actual_value) / actual_value))) \n",
    "    performance.append(torch.sqrt(torch.mean((pred_value - actual_value)**2)))\n",
    "    \n",
    "    for i in range(2000,3001,1000):\n",
    "        correct_times = torch.nonzero(torch.abs(pred_value - actual_value) <= i)\n",
    "        accuracy = correct_times.shape[0]/pred_value.shape[0]\n",
    "        performance.append(accuracy)\n",
    "                       \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(name, pred_value, actual_value,block_index):\n",
    "    \n",
    "#     fig, ax = plt.subplots(2,2,figsize=(20,10), sharex=True, sharey=True)\n",
    "    fig, ax = plt.subplots(1,figsize=(20,10), sharex=True, sharey=True)\n",
    "#     ax.set_xlim(0,pred_value.shape[0])  \n",
    "    \n",
    "    \n",
    "    \n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.plot(pred_value, label=\"LLAAT\")\n",
    "    ax.plot(actual_value, label=\"Actual\")\n",
    "    ax.set_title(\"Forecasted performance for l=%d\" %(1))\n",
    "    ax.legend()\n",
    "        \n",
    "    #fig.text(0.5, 0, \"Stage of training\", ha='center', fontsize=20)\n",
    "    #fig.text(0, 0.5, \"Copper price value\", va='center', rotation='vertical')\n",
    "\n",
    "    fig.suptitle(\"In the %s process in the M=%d window\"%(name, block_index))\n",
    "    fig.tight_layout()\n",
    "#     fig.savefig(\"In the %s process in the M=%d window.png\"%(name, block_index),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adopted_node(network,block_index):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "#     ax.set_xticklabels([i for i in range(network.nb_node_acceptable.shape[0]+5)])\n",
    "    \n",
    "    ax.set_title(\"Total amount of adopted hidden nodes in the training process in the M=%d window\"%(block_index))\n",
    "    ax.plot(network.nb_node_acceptable,\"-o\")\n",
    "\n",
    "    ax.set_xlabel(\"Stage of training\")\n",
    "    ax.set_ylabel(\"Hidden nodes\")\n",
    "    \n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "#     fig.savefig(\"hidden nodes in the training process in the M=%d window\"%(block_index),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_table(evaluation_results, block_index, name, performance, nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node):\n",
    "\n",
    "    print(performance[3])\n",
    "    print(type(performance[3]))\n",
    "    \n",
    "    new_result = pd.DataFrame({\n",
    "\n",
    "        \"Window_index\":block_index,\n",
    "        \"Stage\":name,\n",
    "        \"MAE\" : round(performance[0].item(),2),\n",
    "        \"MAPE\" : \"%.2f\"%(performance[1]*100).item(),\n",
    "        \"RMSE\" : round(performance[2].item(),2),\n",
    "        \"Accuracy(2000)\" : [round(performance[3]*100,2)],\n",
    "        \"Accuracy(3000)\" : [round(performance[4]*100,2)],\n",
    "        \"Step4\":nb_step4,\n",
    "        \"Step6.1\":nb_step6_1,\n",
    "        \"Step6.2\":nb_step6_2,\n",
    "        \"Time\":time,\n",
    "        \"Adopted_hidden_node\":adopted_hidden_node\n",
    "    })\n",
    "\n",
    "    evaluation_results = evaluation_results.append(new_result, ignore_index=True)\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(network, nb_step4, nb_step6_1, nb_step6_2, x_train_scaled, y_train_scaled, x_test, y_test, start, end, block_index, evaluation_results_train, evaluation_results_outlier, evaluation_results_test):\n",
    "\n",
    "    ## Training_Step\n",
    "    print(\"<<Training step>>\")\n",
    "    print(\"The training time(s):\",end - start)\n",
    "    time = end - start\n",
    "    yo, loss= network.forward()\n",
    "    \n",
    "    ## N - outlier\n",
    "    pre_train = yo.data.cpu()\n",
    "    true_train = network.y.data.cpu()\n",
    "    \n",
    "    pred_value_train = torch.FloatTensor(sc.inverse_transform(pre_train))\n",
    "    actual_value_train = torch.FloatTensor(sc.inverse_transform(true_train))\n",
    "    accuracy_train = accuracy_cacl(pred_value_train,actual_value_train)\n",
    "\n",
    "    ## Outlier\n",
    "    pre_outlier = torch.FloatTensor(sc.inverse_transform(network.forecast(x_train_scaled).data.cpu()))\n",
    "    actual_outlier = torch.FloatTensor(sc.inverse_transform(y_train_scaled))\n",
    "    accuracy_outlier = accuracy_cacl(pre_outlier,actual_outlier)\n",
    "    \n",
    "    ## B\n",
    "    pred_value_test = torch.FloatTensor(sc.inverse_transform(network.forecast(x_test).data.cpu()))\n",
    "    accuracy_test = accuracy_cacl(pred_value_test, y_test)\n",
    "    \n",
    "    total_time = nb_step4 + nb_step6_1 + nb_step6_2\n",
    "    print(\"<<The percentage of each step>>\")\n",
    "    print(\"Step 4: %.2f%%\"%((nb_step4/total_time)*100))\n",
    "    print(\"Step 6.1: %.2f%%\"%((nb_step6_1/total_time)*100))\n",
    "    print(\"Step 6.2: %.2f%%\"%((nb_step6_2/total_time)*100))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"Total frequency of cramming occurrences:\",nb_step6_2)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"The amount of hidden node that be pruned:\",network.nb_node_pruned)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    adopted_hidden_node = network.nb_node_acceptable[-1].item()\n",
    "    print(\"The amount of adopted hidden nodes:\",network.nb_node_acceptable[-1].item())\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"<<Accuracy in training step>>\")\n",
    "    print(\"The MAE for l = 1: %.2f\" %(accuracy_train[0]))\n",
    "    print(\"The MAPE for l = 1: %.2f%%\" %(accuracy_train[1]))\n",
    "    print(\"The RMSE for l = 1: %.2f\" %(accuracy_train[2]))\n",
    "    print(\"The accuracy(2000) for l = 1: %.2f%%\" %(accuracy_train[3]*100))\n",
    "    print(\"The accuracy(3000) for l = 1: %.2f%%\" %(accuracy_train[4]*100))\n",
    "#     print(\"The accuracy for l = 2: %.1f%%\" %(accuracy_train[1]*100))\n",
    "#     print(\"The accuracy for l = 3: %.1f%%\" %(accuracy_train[2]*100))\n",
    "#     print(\"The accuracy for l = 4: %.1f%%\" %(accuracy_train[3]*100))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"<<Accuracy in toutlier>>\")\n",
    "    print(\"The MAE for l = 1: %.2f\" %(accuracy_outlier[0]))\n",
    "    print(\"The MAPE for l = 1: %.2f%%\" %(accuracy_outlier[1]))\n",
    "    print(\"The RMSE for l = 1: %.2f\" %(accuracy_outlier[2]))\n",
    "    print(\"The accuracy(2000) for l = 1: %.2f%%\" %(accuracy_outlier[3]*100))\n",
    "    print(\"The accuracy(3000) for l = 1: %.2f%%\" %(accuracy_outlier[4]*100))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"<<Accuracy in inferencing step>>\")\n",
    "    print(\"The MAE for l = 1: %.1f\" %(accuracy_test[0]))\n",
    "    print(\"The MAPE for l = 1: %.1f%%\" %(accuracy_test[1]))\n",
    "    print(\"The RMSE for l = 1: %.1f\" %(accuracy_test[2]))\n",
    "    print(\"The accuracy(2000) for l = 1: %.1f%%\" %(accuracy_test[3]*100))\n",
    "    print(\"The accuracy(3000) for l = 1: %.1f%%\" %(accuracy_test[4]*100))\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    \n",
    "    evaluation_table_train = evaluation_table(evaluation_results_train, block_index, \"Training\", accuracy_train,nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node)\n",
    "    evaluation_table_outlier = evaluation_table(evaluation_results_outlier, block_index, \"Outlier\", accuracy_outlier,nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node)\n",
    "    evaluation_table_test = evaluation_table(evaluation_results_test, block_index, \"Inferencing\", accuracy_test,nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node)\n",
    "    pre_LDSS = sc.inverse_transform(network.forecast(x_test).data.cpu())\n",
    "#     pd.DataFrame(pre_LDSS).to_csv(\"pre_LDSS_%d.csv\"%(block_index), index=False)\n",
    "    \n",
    "#     if block_index%5==0:\n",
    "    plot_result(\"training\",pred_value_train, actual_value_train,block_index)\n",
    "    plot_result(\"inferencing\",pred_value_test, y_test,block_index)\n",
    "    plot_adopted_node(network,block_index)\n",
    "    \n",
    "    return(evaluation_table_train,evaluation_table_outlier, evaluation_table_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek=4, futureWeek=4, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek):\n",
    "        X = np.array(train.iloc[i:i+defaultWeek])\n",
    "        X = np.append(X,train[\"CCSP\"].iloc[i+defaultWeek:i+pastWeek])\n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use min-max normalization to scale the data to the range from 1 to 0\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design get_data() to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(futureWeek):\n",
    "    \n",
    "    ## Read weekly copper price data\n",
    "    path = \"WeeklyFinalData.csv\"\n",
    "    data = read(path)\n",
    "    \n",
    "    date = data[\"Date\"]\n",
    "    data.drop(\"Date\", axis=1, inplace=True)\n",
    "    \n",
    "    ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "    x_data, y_data = buildTrain(data, futureWeek=futureWeek)\n",
    "\n",
    "\n",
    "    return (x_data, y_data)\n",
    "\n",
    "#     return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, nb_neuro, x_train_scaled, y_train_scaled):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(x_train_scaled.shape[1], nb_neuro).cuda()\n",
    "        self.linear2 = torch.nn.Linear(nb_neuro, 1).cuda()\n",
    "        \n",
    "        \n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 0.12\n",
    "        self.threshold_for_lr = 1e-4\n",
    "        \n",
    "        # Input data \n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "        \n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-3\n",
    "        \n",
    "        # Whether the network is acceptable, default as False\n",
    "        self.acceptable = False\n",
    "        \n",
    "        # Some record for experiment\n",
    "        self.nb_node_pruned = 0\n",
    "        self.nb_node_acceptable=torch.IntTensor([nb_neuro])\n",
    "        \n",
    "        self.limit = nb_neuro\n",
    "        \n",
    "    ## Forecast the test data\n",
    "    def forecast(self, x_test):\n",
    "    \n",
    "        x_test = torch.FloatTensor(x_test).cuda()\n",
    "        activation_value = self.linear1(x_test).clamp(min=0)\n",
    "        forecast_value = self.linear2(activation_value)\n",
    "       \n",
    "        return forecast_value\n",
    "\n",
    "    ## Reset the x and y data\n",
    "    def setData(self, x_train_scaled, y_train_scaled):\n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "    \n",
    "    ## Add the new data to the x and y data\n",
    "    def addData(self, new_x_train, new_y_train):\n",
    "\n",
    "        self.x = torch.cat([self.x, new_x_train.reshape(1,-1).cuda()],0)\n",
    "        self.y = torch.cat([self.y, new_y_train.reshape(-1,1).cuda()],0)\n",
    "    \n",
    "    ## forward operation\n",
    "    def forward(self, reg_strength=0):\n",
    "       \n",
    "        y1 = self.linear1(self.x).clamp(min=0)\n",
    "        yo = self.linear2(y1)\n",
    "\n",
    "        # performance measure\n",
    "        param_val= torch.sum(torch.pow(self.linear2.bias.data,2))+torch.sum(torch.pow(self.linear2.weight.data,2))+torch.sum(torch.pow(self.linear1.bias.data,2))+torch.sum(torch.pow(self.linear1.weight.data,2))\n",
    "        reg_term= reg_strength/((self.linear2.bias.data.shape[0]*(self.linear2.weight.data.shape[1]+1)) +(self.linear1.bias.data.shape[0]*(self.linear1.weight.data.shape[1]+1)))*param_val\n",
    "        loss = torch.nn.functional.mse_loss(yo,self.y)+reg_term\n",
    "        loss = loss.cuda()\n",
    "        return(yo, loss)\n",
    "\n",
    "    # backward operation\n",
    "    def backward_Adadelta(self,loss):    \n",
    "\n",
    "        optimizer = optim.Adadelta(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializing(network, initial_x, initial_y):\n",
    "    print(\"Initializing module\")\n",
    "    ## Find each minimum output value y\n",
    "    min_y = torch.min(initial_y, axis=0)\n",
    "    ## Subtract min_y from each y\n",
    "    res_y = initial_y-min_y.values\n",
    "    \n",
    "    ## Use linear regression to find the initial W1,b1,Wo,bo\n",
    "    reg = LinearRegression().fit(initial_x, res_y)\n",
    "    \n",
    "    ## Set up the initial parameter of the network\n",
    "    network.linear1.weight = torch.nn.Parameter(torch.FloatTensor(reg.coef_).cuda())\n",
    "    network.linear1.bias = torch.nn.Parameter(torch.FloatTensor(reg.intercept_).cuda())\n",
    "    network.linear2.weight=torch.nn.Parameter(torch.FloatTensor([[1]]).cuda())\n",
    "    network.linear2.bias = torch.nn.Parameter(torch.FloatTensor(min_y.values).cuda())\n",
    "    \n",
    "#     print(reg.coef_)\n",
    "#     print(reg.intercept_)\n",
    "\n",
    "    ## Set up the acceptable of the initial network as True\n",
    "    network.acceptable =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting(network, x_train_scaled, y_train_scaled):\n",
    "    \n",
    "    print(\"<<Selecting module>>\")\n",
    "    loss = []\n",
    "    temp_network = copy.deepcopy(network)\n",
    "    \n",
    "    ## Put each data into network to calculate the loss value\n",
    "    for i in range(x_train_scaled.shape[0]):\n",
    "        temp_network.setData(x_train_scaled[i].reshape(1,-1), y_train_scaled[i].reshape(-1,1))\n",
    "        loss.append((temp_network.forward()[1].item(),i))\n",
    "#         print(network.state_dict())\n",
    "#         print(temp_network.y)\n",
    "#         print(\"-\"*20)\n",
    "#         print(temp_network.forward()[1])\n",
    "#         print(\"-\"*20)\n",
    "#     ## Sort the data according to the loss value from smallest to largest, and save the data index in sorted_index\n",
    "    sorted_index = [sorted_data[1] for sorted_data in sorted(loss, key = lambda x:x[0])]\n",
    "    \n",
    "    \n",
    "    ## Print out some info for debug\n",
    "    print(\"The loss value of k:\",loss[sorted_index[0]])\n",
    "    print(\"The second_loss value of k:\",loss[sorted_index[1]])\n",
    "    print(\"Selecting module finish!\")\n",
    "#     print(\"Loss\",loss)\n",
    "#     print(network.state_dict())\n",
    "    \n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def matching(network):\n",
    "\n",
    "#     times_enlarge=0\n",
    "#     times_shrink=0\n",
    "    \n",
    "#     print(\"<<Matching module>>\")\n",
    "#     print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "#     ## Set up the learning rate of the network\n",
    "#     network.learning_rate = 1e-3\n",
    "#     network.acceptable = False\n",
    "#     initial_network = copy.deepcopy(network)\n",
    "\n",
    "#     yo, loss = network.forward()\n",
    "    \n",
    "#     if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "#         print(\"Matching finished (firstly) - the network is acceptable\")\n",
    "#         network.acceptable = True\n",
    "# #         print(\"Matching firstly finished - the network is acceptable\")\n",
    "#         print(\"Number of enlarge:\",times_enlarge)\n",
    "#         print(\"Number of shrink:\",times_shrink)\n",
    "#         return(network)\n",
    "    \n",
    "#     else:\n",
    "    \n",
    "#         while True:\n",
    "\n",
    "#             yo, loss = network.forward()\n",
    "#             network_pre = copy.deepcopy(network)\n",
    "#             loss_pre = loss\n",
    "            \n",
    "#             # Backward and check the loss performance of the network with new learning rate\n",
    "#             network.backward_Adadelta(loss)\n",
    "#             yo, loss = network.forward()\n",
    "\n",
    "#             # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "#             if loss <= loss_pre and torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "       \n",
    "#                 network.acceptable = True\n",
    "#                 print(\"Matching finished - the network is acceptable\")\n",
    "#                 print(\"Number of enlarge:\",times_enlarge)\n",
    "#                 print(\"Number of shrink:\",times_shrink)\n",
    "#                 return(network)\n",
    "\n",
    "#             elif loss <= loss_pre:\n",
    "                \n",
    "#                 times_enlarge+=1\n",
    "#                 network.learning_rate *= 1.2\n",
    "\n",
    "#             else:         \n",
    "\n",
    "#                 # Identify whether the current learning rate is less than the threshold\n",
    "#                 if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "#                     # If true, set the acceptable of the network as false and return it\n",
    "#                     network.acceptable = False\n",
    "#                     print(\"Matching finished - the network is Unacceptable\")\n",
    "#                     print(\"Number of enlarge:\",times_enlarge)\n",
    "#                     print(\"Number of shrink:\",times_shrink)\n",
    "#                     return(initial_network)\n",
    "\n",
    "#                 # On the contrary, restore w and adjust the learning rate\n",
    "#                 else:\n",
    "                    \n",
    "#                     # Restore the papameter of the network\n",
    "#                     network = copy.deepcopy(network_pre)\n",
    "#                     times_shrink+=1\n",
    "#                     network.learning_rate *= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(network):\n",
    "\n",
    "    times_enlarge=0\n",
    "    times_shrink=0\n",
    "    \n",
    "    print(\"<<Matching module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    network.acceptable = False\n",
    "    initial_network = copy.deepcopy(network)\n",
    "    yo, loss = network.forward()\n",
    "    \n",
    "    if torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "        network.acceptable = True\n",
    "        print(\"Matching(o) first finished - the network is acceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(10000):\n",
    "            \n",
    "            yo, loss = network.forward()\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            loss_pre = loss\n",
    "#             print(\"<前Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adadelta(loss)\n",
    "            yo, loss = network.forward()\n",
    "#             print(\"<後Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss <= loss_pre and torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.acceptable = True\n",
    "                print(\"Matching finished - the network is acceptable\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "            elif loss <= loss_pre:\n",
    "                \n",
    "#                 print(\"*1.2\")\n",
    "                times_enlarge+=1\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "\n",
    "            else:         \n",
    "\n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "                    print(\"Matching finished - the network is Unacceptable\")\n",
    "                    print(\"Number of enlarge:\",times_enlarge)\n",
    "                    print(\"Number of shrink:\",times_shrink)\n",
    "                    return(initial_network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "#                     print(\"*0.7\")\n",
    "                    # Restore the papameter of the network\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    times_shrink+=1\n",
    "                    network.learning_rate *= 0.7\n",
    "                \n",
    "        network.acceptable = False\n",
    "        print(\"Matching的第%d回合\"%(i+1))\n",
    "        print(\"Matching finished - the network is Unacceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(initial_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching for reorganizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_for_reorganizing(network):\n",
    "\n",
    "    times_enlarge=0\n",
    "    times_shrink=0\n",
    "    \n",
    "    print(\"<<Matching module for reorganizing>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    network.acceptable = False\n",
    "    initial_network = copy.deepcopy(network)\n",
    "    yo, loss = network.forward()\n",
    "    \n",
    "    if torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "        network.acceptable = True\n",
    "        print(\"Matching(o) first finished - the network is acceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(500):\n",
    "            \n",
    "            yo, loss = network.forward()\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            loss_pre = loss\n",
    "#             print(\"<前Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adadelta(loss)\n",
    "            yo, loss = network.forward()\n",
    "#             print(\"<後Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss <= loss_pre and torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.acceptable = True\n",
    "                print(\"Matching finished(o) - the network is acceptable\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "            elif loss <= loss_pre:\n",
    "                \n",
    "#                 print(\"*1.2\")\n",
    "                times_enlarge+=1\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "\n",
    "            else:         \n",
    "\n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "                    print(\"Matching finished(o) - the network is Unacceptable\")\n",
    "                    print(\"Number of enlarge:\",times_enlarge)\n",
    "                    print(\"Number of shrink:\",times_shrink)\n",
    "                    return(initial_network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "#                     print(\"*0.7\")\n",
    "                    # Restore the papameter of the network\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    times_shrink+=1\n",
    "                    network.learning_rate *= 0.7\n",
    "                \n",
    "        network.acceptable = False\n",
    "        print(\"Matching的第%d回合\"%(i+1))\n",
    "        print(\"Matching finished - the network is Unacceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(initial_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cramming module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramming(network):\n",
    "    \n",
    "    torch.random.manual_seed(0)\n",
    "    print(\"<<Cramming module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Find unsatisfied data:K\n",
    "    yo, loss = network.forward()\n",
    "    undesired_index = torch.nonzero(torch.abs(yo-network.y) > network.threshold_for_error+0.001, as_tuple =False)\n",
    "\n",
    "    ## Print out the undesired_index for debug\n",
    "    print(\"不滿足個數：\",undesired_index.shape[0])\n",
    "    print(\"The index of the undesired data:\",undesired_index)\n",
    "\n",
    "    \n",
    "    if undesired_index.shape[0] == 1:\n",
    "        \n",
    "        # Unsatisfied situation\n",
    "        ## Find the index of the unsatisfied data\n",
    "        k_data_num = undesired_index[0][0]\n",
    "\n",
    "        undesired_data = torch.reshape(network.x[k_data_num,:], [1,-1])\n",
    "\n",
    "        ## Remove the data that does not meet the error term\n",
    "        left_data = network.x[:k_data_num,:]\n",
    "        right_data = network.x[k_data_num+1:,:]\n",
    "        remain_tensor = torch.cat([left_data, right_data], 0)\n",
    "\n",
    "\n",
    "        ## Use the random method to find out the gamma and zeta\n",
    "        while True:\n",
    "\n",
    "            ## Find m-vector gamma: r\n",
    "            ## Use the random method to generate the gamma that can make the conditions met\n",
    "            gamma = torch.rand(size=[1,network.x.shape[1]]).cuda()\n",
    "            subtract_undesired_data = torch.sub(remain_tensor, undesired_data)\n",
    "            matmul_value = torch.mm(gamma,torch.t(subtract_undesired_data))\n",
    "\n",
    "            if torch.all(matmul_value != 0):\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "\n",
    "            ## Find the tiny value: zeta\n",
    "            ## Use the random method to generate the zeta that can make the conditions met\n",
    "            zeta = torch.rand(size=[1]).cuda()\n",
    "\n",
    "            if torch.all(torch.mul(torch.add(zeta,matmul_value),torch.sub(zeta,matmul_value))<0):\n",
    "                break\n",
    "\n",
    "       \n",
    "\n",
    "        k_l = undesired_index[0][1]\n",
    "        \n",
    "        ## The weight of input layer to hidden layer I\n",
    "        w10 = gamma\n",
    "        w11 = gamma\n",
    "        w12 = gamma\n",
    "\n",
    "        W1_new = torch.cat([w10,w11,w12],0)\n",
    "        \n",
    "\n",
    "        ## The bias of input layer to hidden layer I\n",
    "        matual_value = torch.mm(gamma,torch.t(undesired_data))\n",
    "       \n",
    "        \n",
    "        b10 = torch.sub(zeta,matual_value)\n",
    "        b11 = -1*matual_value\n",
    "        b12 = torch.sub(-1*zeta,matual_value)\n",
    "\n",
    "        b1_new = torch.reshape(torch.cat([b10,b11,b12],0),[3])\n",
    "        \n",
    "#         print(\"b1_new\",b1_new)\n",
    "\n",
    "\n",
    "        ## The weight of hidden layer I to output layer\n",
    "        gap = network.y[k_data_num, k_l]-yo[k_data_num, k_l]\n",
    "#         print(\"gap:\",gap)\n",
    "\n",
    "        wo0_value = gap/zeta\n",
    "        wo1_value = (-2*gap)/zeta\n",
    "        wo2_value = gap/zeta\n",
    "\n",
    "        Wo_new = torch.reshape(torch.cat([wo0_value,wo1_value,wo2_value],0),[1,-1])\n",
    "\n",
    "        ## Add new neuroes to the network\n",
    "        network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight.data, W1_new]))\n",
    "        network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias.data, b1_new]))\n",
    "        network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight.data, Wo_new],1))\n",
    "\n",
    "\n",
    "        yo, loss = network.forward()\n",
    "        \n",
    "        ## Determine if cramming is successful and print out the corresponding information\n",
    "        if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "            network.acceptable = True \n",
    "            print(\"Cramming success!\")\n",
    "\n",
    "        else:\n",
    "            print(\"Cramming failed!\")\n",
    "    \n",
    "    else:\n",
    "        print(\"條件不合，不能Cramming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizing(network):\n",
    "\n",
    "    print(\"<<Regularizing module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    ## Record the number of executions\n",
    "    times_enlarge = 0\n",
    "    times_shrink = 0\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "\n",
    "    ## Set epoch to 100\n",
    "    for i in range(100):\n",
    "\n",
    "        ## Store the parameter of the network\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "        loss_pre = loss\n",
    "\n",
    "#         print(\"調整前的network\")\n",
    "#         print(\"<<變數>>\")\n",
    "#         print(network.state_dict())\n",
    "#         print(\"<<Loss值>>\")\n",
    "#         print(loss)\n",
    "#         print(\"差異\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "        \n",
    "        ## Backward operation to obtain w'\n",
    "        network.backward_Adadelta(loss)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "#         print(\"調整後的network\")\n",
    "#         print(\"<<變數>>\")\n",
    "#         print(network.state_dict())\n",
    "#         print(\"<<Loss值>>\")\n",
    "#         print(loss)\n",
    "#         print(\"差異\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "         # Confirm whether the adjusted loss value is smaller than the current one\n",
    "        if loss <= loss_pre:\n",
    "            \n",
    "            ## Identify that all forecast value has met the error term\n",
    "            if torch.all(torch.abs(yo-network.y) < network.threshold_for_error):\n",
    "                \n",
    "                ## If true, multiply the learning rate by 1.2\n",
    "#                 print(\"*1.2\")\n",
    "                network.learning_rate *= 1.2\n",
    "                times_enlarge += 1\n",
    "#                 print(\"Regularizing %d process - Enlarge\"%i)\n",
    "#                 print(\"第\\\"%d\\\"回合是成功執行regularizing\"%(i+1))\n",
    "#                 print(\"差異\")\n",
    "#                 print(torch.abs(yo-network.y))\n",
    "\n",
    "            else:\n",
    "\n",
    "                ## Else, restore w and end the process\n",
    "                network = copy.deepcopy(network_pre)\n",
    "                print(\"Regularizing結束-因為沒有顧好預測誤差\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "#                 print(\"Regularizing result: Unable to meet the error term\")\n",
    "                return(network)\n",
    "\n",
    "        # If the adjusted loss value is not smaller than the current one\n",
    "        else:\n",
    "\n",
    "            ## If the learning rate is greater than the threshold for learning rate\n",
    "            if network.learning_rate > network.threshold_for_lr:\n",
    "                \n",
    "                ## Restore the w and multiply the learning rate by 0.7\n",
    "                network = copy.deepcopy(network_pre)\n",
    "#                 print(\"*0.7\")\n",
    "                network.learning_rate *= 0.7\n",
    "                times_shrink += 1\n",
    "#                 print(\"把Learning rate變小\")\n",
    "#                 print(\"Regularizing %d process - Shrink\"%i)\n",
    "             ## If the learning rate is smaller than the threshold for learning rate\n",
    "            else:\n",
    "                \n",
    "                ## Restore the w\n",
    "                network = copy.deepcopy(network_pre)\n",
    "                print(\"Regularizing結束-Learning不能這麼小\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "#                 print(\"Regularizing result: Less than the epsilon for the learning rate\")\n",
    "                return(network)\n",
    "\n",
    "    print(\"第\\\"%d\\\"回合Regularizing module完畢\"%(i+1))\n",
    "    print(\"Number of enlarge:\",times_enlarge)\n",
    "    print(\"Number of shrink:\",times_shrink)\n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganizing(network):\n",
    "    print(\"<<Reorganizing module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    limit = 4\n",
    "    if network.linear1.bias.shape[0] <= limit:\n",
    "        network = regularizing(network)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ## Set up the k = 1, and p = the number of hidden node\n",
    "        k = 1\n",
    "    #     p = network.W1.shape[1]\n",
    "        p = network.linear1.weight.data.shape[0]\n",
    "\n",
    "        while True:\n",
    "\n",
    "            ## If k > p, end of Process\n",
    "            if k > p or p<=limit:\n",
    "\n",
    "                print(\"Reorganizing result: The final number of neuro is \",p)\n",
    "                return(network)\n",
    "\n",
    "            ## Else, Process is ongoing\n",
    "            else:\n",
    "\n",
    "                ## Using the regularizing module to adjust the network\n",
    "                network = regularizing(network)\n",
    "\n",
    "                ## Store the network and w\n",
    "                network_pre = copy.deepcopy(network)\n",
    "\n",
    "                ## Set up the acceptable of the network as false\n",
    "                network.acceptable = False\n",
    "                \n",
    "            \n",
    "                ## Ignore the K hidden node\n",
    "                network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight[:k-1],network.linear1.weight[k:]],0))\n",
    "                network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias[:k-1],network.linear1.bias[k:]]))\n",
    "                network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight[:,:k-1],network.linear2.weight[:,k:]],1))\n",
    "\n",
    "                \n",
    "                ## Using the matching module to adjust the network\n",
    "                network = matching_for_reorganizing(network)\n",
    "\n",
    "                print(\"是不是可以不要這個hidden node:\",network.acceptable)\n",
    "\n",
    "                ## If the resulting network is acceptable, this means that the k hidden node can be removed\n",
    "                if network.acceptable:\n",
    "\n",
    "                    print(\"Drop out the nero number: %d / %d\" %(k, p))\n",
    "                    network.nb_node_pruned += 1\n",
    "                    ## p--\n",
    "                    p-=1\n",
    "\n",
    "                ## Else, it means that the k hidden node cannot be removed\n",
    "                else:\n",
    "\n",
    "                    ## Restore the network and w\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    print(\"Cannot drop out the nero number: %d / %d\" %(k, p))\n",
    "\n",
    "                    ## k++\n",
    "                    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <<1>> Block\n",
      "Initializing module\n",
      "<<Initializing後看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "現在訓練到第幾筆資料: 20\n",
      "剩餘X 資料 torch.Size([140, 18])\n",
      "剩餘Y 資料 torch.Size([140, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.507336148482864e-06, 1)\n",
      "The second_loss value of k: (9.45589054026641e-05, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.9821])\n",
      "目前模型的Data狀態 torch.Size([20, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8040],\n",
      "        [0.8136],\n",
      "        [0.8421],\n",
      "        [0.8447],\n",
      "        [0.8890],\n",
      "        [0.8533],\n",
      "        [0.8548],\n",
      "        [0.8454],\n",
      "        [0.7929],\n",
      "        [0.8065],\n",
      "        [0.8315],\n",
      "        [0.8036],\n",
      "        [0.8492],\n",
      "        [0.8295],\n",
      "        [0.8496],\n",
      "        [0.8043],\n",
      "        [0.8094],\n",
      "        [0.8200],\n",
      "        [0.8145],\n",
      "        [0.9805]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0000],\n",
      "        [    0.0016]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0015]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.29161906242370605\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 21\n",
      "剩餘X 資料 torch.Size([139, 18])\n",
      "剩餘Y 資料 torch.Size([139, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (9.62483900366351e-05, 0)\n",
      "The second_loss value of k: (0.0004444399964995682, 13)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.9290])\n",
      "目前模型的Data狀態 torch.Size([21, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8040],\n",
      "        [0.8136],\n",
      "        [0.8422],\n",
      "        [0.8448],\n",
      "        [0.8890],\n",
      "        [0.8534],\n",
      "        [0.8548],\n",
      "        [0.8455],\n",
      "        [0.7930],\n",
      "        [0.8066],\n",
      "        [0.8316],\n",
      "        [0.8037],\n",
      "        [0.8493],\n",
      "        [0.8296],\n",
      "        [0.8497],\n",
      "        [0.8043],\n",
      "        [0.8095],\n",
      "        [0.8200],\n",
      "        [0.8145],\n",
      "        [0.9806],\n",
      "        [0.9388]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0015],\n",
      "        [    0.0098]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0009],\n",
      "        [    0.0010],\n",
      "        [    0.0005],\n",
      "        [    0.0004],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0000],\n",
      "        [    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0025],\n",
      "        [    0.0087]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.552842378616333\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 22\n",
      "剩餘X 資料 torch.Size([138, 18])\n",
      "剩餘Y 資料 torch.Size([138, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.00046535348519682884, 12)\n",
      "The second_loss value of k: (0.0011976729147136211, 20)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引12，y= tensor([0.9367])\n",
      "目前模型的Data狀態 torch.Size([22, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8030],\n",
      "        [0.8126],\n",
      "        [0.8412],\n",
      "        [0.8437],\n",
      "        [0.8885],\n",
      "        [0.8529],\n",
      "        [0.8545],\n",
      "        [0.8451],\n",
      "        [0.7929],\n",
      "        [0.8062],\n",
      "        [0.8313],\n",
      "        [0.8035],\n",
      "        [0.8490],\n",
      "        [0.8293],\n",
      "        [0.8494],\n",
      "        [0.8041],\n",
      "        [0.8092],\n",
      "        [0.8198],\n",
      "        [0.8143],\n",
      "        [0.9796],\n",
      "        [0.9377],\n",
      "        [0.9151]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0010],\n",
      "        [    0.0010],\n",
      "        [    0.0009],\n",
      "        [    0.0010],\n",
      "        [    0.0005],\n",
      "        [    0.0004],\n",
      "        [    0.0003],\n",
      "        [    0.0003],\n",
      "        [    0.0000],\n",
      "        [    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0001],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0002],\n",
      "        [    0.0025],\n",
      "        [    0.0087],\n",
      "        [    0.0216]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0000],\n",
      "        [    0.0004],\n",
      "        [    0.0017],\n",
      "        [    0.0014],\n",
      "        [    0.0016],\n",
      "        [    0.0014],\n",
      "        [    0.0013],\n",
      "        [    0.0013],\n",
      "        [    0.0014],\n",
      "        [    0.0014],\n",
      "        [    0.0012],\n",
      "        [    0.0011],\n",
      "        [    0.0010],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0007],\n",
      "        [    0.0015],\n",
      "        [    0.0095],\n",
      "        [    0.0194]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.8423190116882324\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 23\n",
      "剩餘X 資料 torch.Size([137, 18])\n",
      "剩餘Y 資料 torch.Size([137, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0012810667976737022, 19)\n",
      "The second_loss value of k: (0.00141897180583328, 25)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引19，y= tensor([0.9682])\n",
      "目前模型的Data狀態 torch.Size([23, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8040],\n",
      "        [0.8135],\n",
      "        [0.8421],\n",
      "        [0.8443],\n",
      "        [0.8907],\n",
      "        [0.8548],\n",
      "        [0.8563],\n",
      "        [0.8469],\n",
      "        [0.7942],\n",
      "        [0.8078],\n",
      "        [0.8329],\n",
      "        [0.8050],\n",
      "        [0.8504],\n",
      "        [0.8306],\n",
      "        [0.8506],\n",
      "        [0.8051],\n",
      "        [0.8103],\n",
      "        [0.8207],\n",
      "        [0.8152],\n",
      "        [0.9806],\n",
      "        [0.9385],\n",
      "        [0.9173],\n",
      "        [1.0040]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0000],\n",
      "        [    0.0004],\n",
      "        [    0.0017],\n",
      "        [    0.0014],\n",
      "        [    0.0016],\n",
      "        [    0.0014],\n",
      "        [    0.0013],\n",
      "        [    0.0013],\n",
      "        [    0.0014],\n",
      "        [    0.0014],\n",
      "        [    0.0012],\n",
      "        [    0.0011],\n",
      "        [    0.0010],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0008],\n",
      "        [    0.0007],\n",
      "        [    0.0015],\n",
      "        [    0.0095],\n",
      "        [    0.0194],\n",
      "        [    0.0358]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(    0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0018],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0026],\n",
      "        [0.0029],\n",
      "        [0.0027],\n",
      "        [0.0082],\n",
      "        [0.0208],\n",
      "        [0.0310]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.1153082847595215\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 24\n",
      "剩餘X 資料 torch.Size([136, 18])\n",
      "剩餘Y 資料 torch.Size([136, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0016099591739475727, 0)\n",
      "The second_loss value of k: (0.001621368108317256, 24)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.9747])\n",
      "目前模型的Data狀態 torch.Size([24, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8036],\n",
      "        [0.8127],\n",
      "        [0.8412],\n",
      "        [0.8429],\n",
      "        [0.8898],\n",
      "        [0.8531],\n",
      "        [0.8544],\n",
      "        [0.8448],\n",
      "        [0.7921],\n",
      "        [0.8059],\n",
      "        [0.8309],\n",
      "        [0.8028],\n",
      "        [0.8478],\n",
      "        [0.8280],\n",
      "        [0.8477],\n",
      "        [0.8021],\n",
      "        [0.8072],\n",
      "        [0.8174],\n",
      "        [0.8115],\n",
      "        [0.9794],\n",
      "        [0.9371],\n",
      "        [0.9159],\n",
      "        [0.9992],\n",
      "        [0.9346]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0004],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0018],\n",
      "        [0.0008],\n",
      "        [0.0002],\n",
      "        [0.0004],\n",
      "        [0.0006],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0009],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0019],\n",
      "        [0.0022],\n",
      "        [0.0023],\n",
      "        [0.0026],\n",
      "        [0.0029],\n",
      "        [0.0027],\n",
      "        [0.0082],\n",
      "        [0.0208],\n",
      "        [0.0310],\n",
      "        [0.0401]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0048],\n",
      "        [    0.0039],\n",
      "        [    0.0038],\n",
      "        [    0.0028],\n",
      "        [    0.0040],\n",
      "        [    0.0021],\n",
      "        [    0.0017],\n",
      "        [    0.0015],\n",
      "        [    0.0013],\n",
      "        [    0.0015],\n",
      "        [    0.0008],\n",
      "        [    0.0005],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0006],\n",
      "        [    0.0014],\n",
      "        [    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0023],\n",
      "        [    0.0017],\n",
      "        [    0.0125],\n",
      "        [    0.0179],\n",
      "        [    0.0305],\n",
      "        [    0.0356]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.3774192333221436\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 25\n",
      "剩餘X 資料 torch.Size([135, 18])\n",
      "剩餘Y 資料 torch.Size([135, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0016884282231330872, 23)\n",
      "The second_loss value of k: (0.0017768704565241933, 11)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引23，y= tensor([0.8297])\n",
      "目前模型的Data狀態 torch.Size([25, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8088],\n",
      "        [0.8174],\n",
      "        [0.8459],\n",
      "        [0.8475],\n",
      "        [0.8930],\n",
      "        [0.8555],\n",
      "        [0.8565],\n",
      "        [0.8469],\n",
      "        [0.7941],\n",
      "        [0.8080],\n",
      "        [0.8323],\n",
      "        [0.8041],\n",
      "        [0.8492],\n",
      "        [0.8295],\n",
      "        [0.8490],\n",
      "        [0.8029],\n",
      "        [0.8081],\n",
      "        [0.8182],\n",
      "        [0.8122],\n",
      "        [0.9838],\n",
      "        [0.9414],\n",
      "        [0.9188],\n",
      "        [0.9986],\n",
      "        [0.9391],\n",
      "        [0.7886]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0048],\n",
      "        [    0.0039],\n",
      "        [    0.0038],\n",
      "        [    0.0028],\n",
      "        [    0.0040],\n",
      "        [    0.0021],\n",
      "        [    0.0017],\n",
      "        [    0.0015],\n",
      "        [    0.0013],\n",
      "        [    0.0015],\n",
      "        [    0.0008],\n",
      "        [    0.0005],\n",
      "        [    0.0000],\n",
      "        [    0.0001],\n",
      "        [    0.0006],\n",
      "        [    0.0014],\n",
      "        [    0.0013],\n",
      "        [    0.0018],\n",
      "        [    0.0023],\n",
      "        [    0.0017],\n",
      "        [    0.0125],\n",
      "        [    0.0179],\n",
      "        [    0.0305],\n",
      "        [    0.0356],\n",
      "        [    0.0411]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0042],\n",
      "        [    0.0031],\n",
      "        [    0.0033],\n",
      "        [    0.0026],\n",
      "        [    0.0018],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0000],\n",
      "        [    0.0079],\n",
      "        [    0.0000],\n",
      "        [    0.0006],\n",
      "        [    0.0004],\n",
      "        [    0.0002],\n",
      "        [    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0015],\n",
      "        [    0.0016],\n",
      "        [    0.0020],\n",
      "        [    0.0025],\n",
      "        [    0.0015],\n",
      "        [    0.0119],\n",
      "        [    0.0198],\n",
      "        [    0.0291],\n",
      "        [    0.0348],\n",
      "        [    0.0289]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.6365747451782227\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 26\n",
      "剩餘X 資料 torch.Size([134, 18])\n",
      "剩餘Y 資料 torch.Size([134, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0017331585986539721, 1)\n",
      "The second_loss value of k: (0.0018054613610729575, 22)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.9767])\n",
      "目前模型的Data狀態 torch.Size([26, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8082],\n",
      "        [0.8166],\n",
      "        [0.8454],\n",
      "        [0.8473],\n",
      "        [0.8907],\n",
      "        [0.8535],\n",
      "        [0.8547],\n",
      "        [0.8454],\n",
      "        [0.8008],\n",
      "        [0.8066],\n",
      "        [0.8309],\n",
      "        [0.8033],\n",
      "        [0.8490],\n",
      "        [0.8294],\n",
      "        [0.8490],\n",
      "        [0.8028],\n",
      "        [0.8078],\n",
      "        [0.8180],\n",
      "        [0.8119],\n",
      "        [0.9836],\n",
      "        [0.9409],\n",
      "        [0.9168],\n",
      "        [0.9973],\n",
      "        [0.9399],\n",
      "        [0.8008],\n",
      "        [0.9351]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0042],\n",
      "        [    0.0031],\n",
      "        [    0.0033],\n",
      "        [    0.0026],\n",
      "        [    0.0018],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0000],\n",
      "        [    0.0079],\n",
      "        [    0.0000],\n",
      "        [    0.0006],\n",
      "        [    0.0004],\n",
      "        [    0.0002],\n",
      "        [    0.0001],\n",
      "        [    0.0005],\n",
      "        [    0.0015],\n",
      "        [    0.0016],\n",
      "        [    0.0020],\n",
      "        [    0.0025],\n",
      "        [    0.0015],\n",
      "        [    0.0119],\n",
      "        [    0.0198],\n",
      "        [    0.0291],\n",
      "        [    0.0348],\n",
      "        [    0.0289],\n",
      "        [    0.0416]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0070],\n",
      "        [    0.0054],\n",
      "        [    0.0061],\n",
      "        [    0.0058],\n",
      "        [    0.0020],\n",
      "        [    0.0003],\n",
      "        [    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0134],\n",
      "        [    0.0003],\n",
      "        [    0.0010],\n",
      "        [    0.0026],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0004],\n",
      "        [    0.0020],\n",
      "        [    0.0014],\n",
      "        [    0.0017],\n",
      "        [    0.0020],\n",
      "        [    0.0043],\n",
      "        [    0.0144],\n",
      "        [    0.0191],\n",
      "        [    0.0285],\n",
      "        [    0.0308],\n",
      "        [    0.0234],\n",
      "        [    0.0371]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.8952701091766357\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 27\n",
      "剩餘X 資料 torch.Size([133, 18])\n",
      "剩餘Y 資料 torch.Size([133, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.00137273408472538, 21)\n",
      "The second_loss value of k: (0.001602186355739832, 22)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引21，y= tensor([0.8433])\n",
      "目前模型的Data狀態 torch.Size([27, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8109],\n",
      "        [0.8189],\n",
      "        [0.8482],\n",
      "        [0.8505],\n",
      "        [0.8910],\n",
      "        [0.8536],\n",
      "        [0.8548],\n",
      "        [0.8459],\n",
      "        [0.8062],\n",
      "        [0.8068],\n",
      "        [0.8305],\n",
      "        [0.8062],\n",
      "        [0.8497],\n",
      "        [0.8303],\n",
      "        [0.8500],\n",
      "        [0.8062],\n",
      "        [0.8080],\n",
      "        [0.8183],\n",
      "        [0.8124],\n",
      "        [0.9864],\n",
      "        [0.9434],\n",
      "        [0.9176],\n",
      "        [0.9967],\n",
      "        [0.9440],\n",
      "        [0.8062],\n",
      "        [0.9396],\n",
      "        [0.8062]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0070],\n",
      "        [    0.0054],\n",
      "        [    0.0061],\n",
      "        [    0.0058],\n",
      "        [    0.0020],\n",
      "        [    0.0003],\n",
      "        [    0.0001],\n",
      "        [    0.0004],\n",
      "        [    0.0134],\n",
      "        [    0.0003],\n",
      "        [    0.0010],\n",
      "        [    0.0026],\n",
      "        [    0.0005],\n",
      "        [    0.0007],\n",
      "        [    0.0004],\n",
      "        [    0.0020],\n",
      "        [    0.0014],\n",
      "        [    0.0017],\n",
      "        [    0.0020],\n",
      "        [    0.0043],\n",
      "        [    0.0144],\n",
      "        [    0.0191],\n",
      "        [    0.0285],\n",
      "        [    0.0308],\n",
      "        [    0.0234],\n",
      "        [    0.0371],\n",
      "        [    0.0371]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 74\n",
      "Number of shrink: 26\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0085],\n",
      "        [0.0066],\n",
      "        [0.0077],\n",
      "        [0.0077],\n",
      "        [0.0023],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0011],\n",
      "        [0.0192],\n",
      "        [0.0056],\n",
      "        [0.0008],\n",
      "        [0.0085],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0017],\n",
      "        [0.0078],\n",
      "        [0.0027],\n",
      "        [0.0009],\n",
      "        [0.0011],\n",
      "        [0.0060],\n",
      "        [0.0158],\n",
      "        [0.0182],\n",
      "        [0.0288],\n",
      "        [0.0280],\n",
      "        [0.0175],\n",
      "        [0.0339],\n",
      "        [0.0312]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 2.153900623321533\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 28\n",
      "剩餘X 資料 torch.Size([132, 18])\n",
      "剩餘Y 資料 torch.Size([132, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0011665200581774116, 21)\n",
      "The second_loss value of k: (0.0016244128346443176, 22)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引21，y= tensor([0.8463])\n",
      "目前模型的Data狀態 torch.Size([28, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8124],\n",
      "        [0.8201],\n",
      "        [0.8498],\n",
      "        [0.8524],\n",
      "        [0.8913],\n",
      "        [0.8540],\n",
      "        [0.8554],\n",
      "        [0.8466],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8308],\n",
      "        [0.8121],\n",
      "        [0.8509],\n",
      "        [0.8315],\n",
      "        [0.8512],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8191],\n",
      "        [0.8134],\n",
      "        [0.9881],\n",
      "        [0.9448],\n",
      "        [0.9185],\n",
      "        [0.9970],\n",
      "        [0.9467],\n",
      "        [0.8121],\n",
      "        [0.9428],\n",
      "        [0.8121],\n",
      "        [0.8121]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0085],\n",
      "        [0.0066],\n",
      "        [0.0077],\n",
      "        [0.0077],\n",
      "        [0.0023],\n",
      "        [0.0007],\n",
      "        [0.0006],\n",
      "        [0.0011],\n",
      "        [0.0192],\n",
      "        [0.0056],\n",
      "        [0.0008],\n",
      "        [0.0085],\n",
      "        [0.0017],\n",
      "        [0.0020],\n",
      "        [0.0017],\n",
      "        [0.0078],\n",
      "        [0.0027],\n",
      "        [0.0009],\n",
      "        [0.0011],\n",
      "        [0.0060],\n",
      "        [0.0158],\n",
      "        [0.0182],\n",
      "        [0.0288],\n",
      "        [0.0280],\n",
      "        [0.0175],\n",
      "        [0.0339],\n",
      "        [0.0312],\n",
      "        [0.0342]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0113],\n",
      "        [0.0075],\n",
      "        [0.0090],\n",
      "        [0.0091],\n",
      "        [0.0024],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0013],\n",
      "        [0.0224],\n",
      "        [0.0087],\n",
      "        [0.0013],\n",
      "        [0.0116],\n",
      "        [0.0019],\n",
      "        [0.0023],\n",
      "        [0.0019],\n",
      "        [0.0110],\n",
      "        [0.0058],\n",
      "        [0.0013],\n",
      "        [0.0008],\n",
      "        [0.0074],\n",
      "        [0.0168],\n",
      "        [0.0176],\n",
      "        [0.0275],\n",
      "        [0.0256],\n",
      "        [0.0144],\n",
      "        [0.0310],\n",
      "        [0.0280],\n",
      "        [0.0310]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 2.4112956523895264\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 29\n",
      "剩餘X 資料 torch.Size([131, 18])\n",
      "剩餘Y 資料 torch.Size([131, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.001378627959638834, 21)\n",
      "The second_loss value of k: (0.0017615671968087554, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引21，y= tensor([0.8524])\n",
      "目前模型的Data狀態 torch.Size([29, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8153],\n",
      "        [0.8211],\n",
      "        [0.8511],\n",
      "        [0.8538],\n",
      "        [0.8914],\n",
      "        [0.8540],\n",
      "        [0.8554],\n",
      "        [0.8467],\n",
      "        [0.8153],\n",
      "        [0.8153],\n",
      "        [0.8303],\n",
      "        [0.8153],\n",
      "        [0.8511],\n",
      "        [0.8318],\n",
      "        [0.8515],\n",
      "        [0.8153],\n",
      "        [0.8153],\n",
      "        [0.8187],\n",
      "        [0.8153],\n",
      "        [0.9895],\n",
      "        [0.9457],\n",
      "        [0.9190],\n",
      "        [0.9957],\n",
      "        [0.9491],\n",
      "        [0.8153],\n",
      "        [0.9456],\n",
      "        [0.8153],\n",
      "        [0.8153],\n",
      "        [0.8153]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0113],\n",
      "        [0.0075],\n",
      "        [0.0090],\n",
      "        [0.0091],\n",
      "        [0.0024],\n",
      "        [0.0006],\n",
      "        [0.0006],\n",
      "        [0.0013],\n",
      "        [0.0224],\n",
      "        [0.0087],\n",
      "        [0.0013],\n",
      "        [0.0116],\n",
      "        [0.0019],\n",
      "        [0.0023],\n",
      "        [0.0019],\n",
      "        [0.0110],\n",
      "        [0.0058],\n",
      "        [0.0013],\n",
      "        [0.0008],\n",
      "        [0.0074],\n",
      "        [0.0168],\n",
      "        [0.0176],\n",
      "        [0.0275],\n",
      "        [0.0256],\n",
      "        [0.0144],\n",
      "        [0.0310],\n",
      "        [0.0280],\n",
      "        [0.0310],\n",
      "        [0.0371]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0147],\n",
      "        [0.0060],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0006],\n",
      "        [0.0012],\n",
      "        [0.0010],\n",
      "        [0.0003],\n",
      "        [0.0258],\n",
      "        [0.0121],\n",
      "        [0.0033],\n",
      "        [0.0150],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        [0.0006],\n",
      "        [0.0144],\n",
      "        [0.0092],\n",
      "        [0.0013],\n",
      "        [0.0042],\n",
      "        [0.0066],\n",
      "        [0.0154],\n",
      "        [0.0188],\n",
      "        [0.0247],\n",
      "        [0.0252],\n",
      "        [0.0110],\n",
      "        [0.0300],\n",
      "        [0.0246],\n",
      "        [0.0276],\n",
      "        [0.0337]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 2.6719987392425537\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 30\n",
      "剩餘X 資料 torch.Size([130, 18])\n",
      "剩餘Y 資料 torch.Size([130, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0018632299033924937, 10)\n",
      "The second_loss value of k: (0.002376473741605878, 24)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引10，y= tensor([0.9159])\n",
      "目前模型的Data狀態 torch.Size([30, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8187],\n",
      "        [0.8196],\n",
      "        [0.8501],\n",
      "        [0.8529],\n",
      "        [0.8896],\n",
      "        [0.8521],\n",
      "        [0.8537],\n",
      "        [0.8452],\n",
      "        [0.8187],\n",
      "        [0.8187],\n",
      "        [0.8282],\n",
      "        [0.8187],\n",
      "        [0.8500],\n",
      "        [0.8307],\n",
      "        [0.8502],\n",
      "        [0.8187],\n",
      "        [0.8187],\n",
      "        [0.8187],\n",
      "        [0.8187],\n",
      "        [0.9887],\n",
      "        [0.9444],\n",
      "        [0.9179],\n",
      "        [0.9929],\n",
      "        [0.9495],\n",
      "        [0.8187],\n",
      "        [0.9467],\n",
      "        [0.8187],\n",
      "        [0.8187],\n",
      "        [0.8187],\n",
      "        [0.8727]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0147],\n",
      "        [0.0060],\n",
      "        [0.0080],\n",
      "        [0.0082],\n",
      "        [0.0006],\n",
      "        [0.0012],\n",
      "        [0.0010],\n",
      "        [0.0003],\n",
      "        [0.0258],\n",
      "        [0.0121],\n",
      "        [0.0033],\n",
      "        [0.0150],\n",
      "        [0.0008],\n",
      "        [0.0011],\n",
      "        [0.0006],\n",
      "        [0.0144],\n",
      "        [0.0092],\n",
      "        [0.0013],\n",
      "        [0.0042],\n",
      "        [0.0066],\n",
      "        [0.0154],\n",
      "        [0.0188],\n",
      "        [0.0247],\n",
      "        [0.0252],\n",
      "        [0.0110],\n",
      "        [0.0300],\n",
      "        [0.0246],\n",
      "        [0.0276],\n",
      "        [0.0337],\n",
      "        [0.0432]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0145],\n",
      "        [0.0075],\n",
      "        [0.0097],\n",
      "        [0.0096],\n",
      "        [0.0046],\n",
      "        [0.0023],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.0255],\n",
      "        [0.0119],\n",
      "        [0.0005],\n",
      "        [0.0148],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0029],\n",
      "        [0.0142],\n",
      "        [0.0090],\n",
      "        [0.0016],\n",
      "        [0.0040],\n",
      "        [0.0086],\n",
      "        [0.0168],\n",
      "        [0.0142],\n",
      "        [0.0255],\n",
      "        [0.0227],\n",
      "        [0.0112],\n",
      "        [0.0265],\n",
      "        [0.0249],\n",
      "        [0.0278],\n",
      "        [0.0340],\n",
      "        [0.0388]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 2.9323368072509766\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 31\n",
      "剩餘X 資料 torch.Size([129, 18])\n",
      "剩餘Y 資料 torch.Size([129, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0024001263082027435, 23)\n",
      "The second_loss value of k: (0.0026371099520474672, 28)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引23，y= tensor([0.8674])\n",
      "目前模型的Data狀態 torch.Size([31, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8184],\n",
      "        [0.8210],\n",
      "        [0.8519],\n",
      "        [0.8543],\n",
      "        [0.8935],\n",
      "        [0.8557],\n",
      "        [0.8573],\n",
      "        [0.8485],\n",
      "        [0.8184],\n",
      "        [0.8184],\n",
      "        [0.8310],\n",
      "        [0.8184],\n",
      "        [0.8528],\n",
      "        [0.8333],\n",
      "        [0.8525],\n",
      "        [0.8184],\n",
      "        [0.8184],\n",
      "        [0.8184],\n",
      "        [0.8184],\n",
      "        [0.9907],\n",
      "        [0.9458],\n",
      "        [0.9224],\n",
      "        [0.9937],\n",
      "        [0.9520],\n",
      "        [0.8184],\n",
      "        [0.9502],\n",
      "        [0.8184],\n",
      "        [0.8184],\n",
      "        [0.8184],\n",
      "        [0.8771],\n",
      "        [0.8184]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0145],\n",
      "        [0.0075],\n",
      "        [0.0097],\n",
      "        [0.0096],\n",
      "        [0.0046],\n",
      "        [0.0023],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.0255],\n",
      "        [0.0119],\n",
      "        [0.0005],\n",
      "        [0.0148],\n",
      "        [0.0036],\n",
      "        [0.0038],\n",
      "        [0.0029],\n",
      "        [0.0142],\n",
      "        [0.0090],\n",
      "        [0.0016],\n",
      "        [0.0040],\n",
      "        [0.0086],\n",
      "        [0.0168],\n",
      "        [0.0142],\n",
      "        [0.0255],\n",
      "        [0.0227],\n",
      "        [0.0112],\n",
      "        [0.0265],\n",
      "        [0.0249],\n",
      "        [0.0278],\n",
      "        [0.0340],\n",
      "        [0.0388],\n",
      "        [0.0490]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0177],\n",
      "        [0.0082],\n",
      "        [0.0107],\n",
      "        [0.0104],\n",
      "        [0.0063],\n",
      "        [0.0039],\n",
      "        [0.0041],\n",
      "        [0.0045],\n",
      "        [0.0288],\n",
      "        [0.0152],\n",
      "        [0.0005],\n",
      "        [0.0181],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0039],\n",
      "        [0.0174],\n",
      "        [0.0123],\n",
      "        [0.0017],\n",
      "        [0.0072],\n",
      "        [0.0097],\n",
      "        [0.0175],\n",
      "        [0.0121],\n",
      "        [0.0252],\n",
      "        [0.0209],\n",
      "        [0.0080],\n",
      "        [0.0240],\n",
      "        [0.0216],\n",
      "        [0.0246],\n",
      "        [0.0307],\n",
      "        [0.0367],\n",
      "        [0.0457]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 3.193112373352051\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 32\n",
      "剩餘X 資料 torch.Size([128, 18])\n",
      "剩餘Y 資料 torch.Size([128, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0026134049985557795, 27)\n",
      "The second_loss value of k: (0.002801307011395693, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引27，y= tensor([0.9354])\n",
      "目前模型的Data狀態 torch.Size([32, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8217],\n",
      "        [0.8217],\n",
      "        [0.8528],\n",
      "        [0.8551],\n",
      "        [0.8952],\n",
      "        [0.8572],\n",
      "        [0.8589],\n",
      "        [0.8499],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8320],\n",
      "        [0.8217],\n",
      "        [0.8541],\n",
      "        [0.8346],\n",
      "        [0.8535],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.9918],\n",
      "        [0.9465],\n",
      "        [0.9246],\n",
      "        [0.9934],\n",
      "        [0.9538],\n",
      "        [0.8217],\n",
      "        [0.9527],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8792],\n",
      "        [0.8217],\n",
      "        [0.8843]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0177],\n",
      "        [0.0082],\n",
      "        [0.0107],\n",
      "        [0.0104],\n",
      "        [0.0063],\n",
      "        [0.0039],\n",
      "        [0.0041],\n",
      "        [0.0045],\n",
      "        [0.0288],\n",
      "        [0.0152],\n",
      "        [0.0005],\n",
      "        [0.0181],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0039],\n",
      "        [0.0174],\n",
      "        [0.0123],\n",
      "        [0.0017],\n",
      "        [0.0072],\n",
      "        [0.0097],\n",
      "        [0.0175],\n",
      "        [0.0121],\n",
      "        [0.0252],\n",
      "        [0.0209],\n",
      "        [0.0080],\n",
      "        [0.0240],\n",
      "        [0.0216],\n",
      "        [0.0246],\n",
      "        [0.0307],\n",
      "        [0.0367],\n",
      "        [0.0457],\n",
      "        [0.0511]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 74\n",
      "Number of shrink: 26\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0182],\n",
      "        [0.0087],\n",
      "        [0.0111],\n",
      "        [0.0102],\n",
      "        [0.0096],\n",
      "        [0.0060],\n",
      "        [0.0067],\n",
      "        [0.0068],\n",
      "        [0.0293],\n",
      "        [0.0157],\n",
      "        [0.0034],\n",
      "        [0.0186],\n",
      "        [0.0072],\n",
      "        [0.0071],\n",
      "        [0.0056],\n",
      "        [0.0179],\n",
      "        [0.0128],\n",
      "        [0.0022],\n",
      "        [0.0077],\n",
      "        [0.0106],\n",
      "        [0.0177],\n",
      "        [0.0090],\n",
      "        [0.0263],\n",
      "        [0.0204],\n",
      "        [0.0074],\n",
      "        [0.0231],\n",
      "        [0.0211],\n",
      "        [0.0241],\n",
      "        [0.0302],\n",
      "        [0.0339],\n",
      "        [0.0452],\n",
      "        [0.0476]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 3.4532861709594727\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 33\n",
      "剩餘X 資料 torch.Size([127, 18])\n",
      "剩餘Y 資料 torch.Size([127, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0029469053260982037, 14)\n",
      "The second_loss value of k: (0.0037537352181971073, 22)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([1.])\n",
      "目前模型的Data狀態 torch.Size([33, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8222],\n",
      "        [0.8222],\n",
      "        [0.8533],\n",
      "        [0.8548],\n",
      "        [0.8985],\n",
      "        [0.8594],\n",
      "        [0.8614],\n",
      "        [0.8522],\n",
      "        [0.8222],\n",
      "        [0.8222],\n",
      "        [0.8349],\n",
      "        [0.8222],\n",
      "        [0.8564],\n",
      "        [0.8366],\n",
      "        [0.8552],\n",
      "        [0.8222],\n",
      "        [0.8222],\n",
      "        [0.8222],\n",
      "        [0.8222],\n",
      "        [0.9927],\n",
      "        [0.9467],\n",
      "        [0.9276],\n",
      "        [0.9945],\n",
      "        [0.9543],\n",
      "        [0.8222],\n",
      "        [0.9536],\n",
      "        [0.8222],\n",
      "        [0.8222],\n",
      "        [0.8222],\n",
      "        [0.8820],\n",
      "        [0.8222],\n",
      "        [0.8878],\n",
      "        [1.0543]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0182],\n",
      "        [0.0087],\n",
      "        [0.0111],\n",
      "        [0.0102],\n",
      "        [0.0096],\n",
      "        [0.0060],\n",
      "        [0.0067],\n",
      "        [0.0068],\n",
      "        [0.0293],\n",
      "        [0.0157],\n",
      "        [0.0034],\n",
      "        [0.0186],\n",
      "        [0.0072],\n",
      "        [0.0071],\n",
      "        [0.0056],\n",
      "        [0.0179],\n",
      "        [0.0128],\n",
      "        [0.0022],\n",
      "        [0.0077],\n",
      "        [0.0106],\n",
      "        [0.0177],\n",
      "        [0.0090],\n",
      "        [0.0263],\n",
      "        [0.0204],\n",
      "        [0.0074],\n",
      "        [0.0231],\n",
      "        [0.0211],\n",
      "        [0.0241],\n",
      "        [0.0302],\n",
      "        [0.0339],\n",
      "        [0.0452],\n",
      "        [0.0476],\n",
      "        [0.0543]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0178],\n",
      "        [0.0088],\n",
      "        [0.0110],\n",
      "        [0.0090],\n",
      "        [0.0101],\n",
      "        [0.0049],\n",
      "        [0.0053],\n",
      "        [0.0052],\n",
      "        [0.0288],\n",
      "        [0.0152],\n",
      "        [0.0021],\n",
      "        [0.0181],\n",
      "        [0.0047],\n",
      "        [0.0046],\n",
      "        [0.0025],\n",
      "        [0.0174],\n",
      "        [0.0123],\n",
      "        [0.0017],\n",
      "        [0.0073],\n",
      "        [0.0100],\n",
      "        [0.0167],\n",
      "        [0.0096],\n",
      "        [0.0198],\n",
      "        [0.0217],\n",
      "        [0.0079],\n",
      "        [0.0245],\n",
      "        [0.0216],\n",
      "        [0.0246],\n",
      "        [0.0307],\n",
      "        [0.0346],\n",
      "        [0.0457],\n",
      "        [0.0482],\n",
      "        [0.0480]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 3.7146053314208984\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 34\n",
      "剩餘X 資料 torch.Size([126, 18])\n",
      "剩餘Y 資料 torch.Size([126, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0038148213643580675, 21)\n",
      "The second_loss value of k: (0.004559243097901344, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引21，y= tensor([0.8835])\n",
      "目前模型的Data狀態 torch.Size([34, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8217],\n",
      "        [0.8223],\n",
      "        [0.8532],\n",
      "        [0.8537],\n",
      "        [0.8990],\n",
      "        [0.8582],\n",
      "        [0.8601],\n",
      "        [0.8506],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8337],\n",
      "        [0.8217],\n",
      "        [0.8539],\n",
      "        [0.8341],\n",
      "        [0.8521],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.9921],\n",
      "        [0.9456],\n",
      "        [0.9271],\n",
      "        [0.9880],\n",
      "        [0.9530],\n",
      "        [0.8217],\n",
      "        [0.9522],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8217],\n",
      "        [0.8813],\n",
      "        [0.8217],\n",
      "        [0.8872],\n",
      "        [1.0480],\n",
      "        [0.8217]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0178],\n",
      "        [0.0088],\n",
      "        [0.0110],\n",
      "        [0.0090],\n",
      "        [0.0101],\n",
      "        [0.0049],\n",
      "        [0.0053],\n",
      "        [0.0052],\n",
      "        [0.0288],\n",
      "        [0.0152],\n",
      "        [0.0021],\n",
      "        [0.0181],\n",
      "        [0.0047],\n",
      "        [0.0046],\n",
      "        [0.0025],\n",
      "        [0.0174],\n",
      "        [0.0123],\n",
      "        [0.0017],\n",
      "        [0.0073],\n",
      "        [0.0100],\n",
      "        [0.0167],\n",
      "        [0.0096],\n",
      "        [0.0198],\n",
      "        [0.0217],\n",
      "        [0.0079],\n",
      "        [0.0245],\n",
      "        [0.0216],\n",
      "        [0.0246],\n",
      "        [0.0307],\n",
      "        [0.0346],\n",
      "        [0.0457],\n",
      "        [0.0482],\n",
      "        [0.0480],\n",
      "        [0.0618]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0216],\n",
      "        [0.0120],\n",
      "        [0.0111],\n",
      "        [0.0082],\n",
      "        [0.0109],\n",
      "        [0.0041],\n",
      "        [0.0046],\n",
      "        [0.0042],\n",
      "        [0.0327],\n",
      "        [0.0190],\n",
      "        [0.0017],\n",
      "        [0.0219],\n",
      "        [0.0035],\n",
      "        [0.0032],\n",
      "        [0.0005],\n",
      "        [0.0213],\n",
      "        [0.0161],\n",
      "        [0.0056],\n",
      "        [0.0111],\n",
      "        [0.0098],\n",
      "        [0.0159],\n",
      "        [0.0097],\n",
      "        [0.0145],\n",
      "        [0.0224],\n",
      "        [0.0041],\n",
      "        [0.0251],\n",
      "        [0.0177],\n",
      "        [0.0207],\n",
      "        [0.0268],\n",
      "        [0.0347],\n",
      "        [0.0418],\n",
      "        [0.0479],\n",
      "        [0.0430],\n",
      "        [0.0579]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 3.975248336791992\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 35\n",
      "剩餘X 資料 torch.Size([125, 18])\n",
      "剩餘Y 資料 torch.Size([125, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.004051991738379002, 19)\n",
      "The second_loss value of k: (0.004489571321755648, 21)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引19，y= tensor([0.8892])\n",
      "目前模型的Data狀態 torch.Size([35, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8256],\n",
      "        [0.8256],\n",
      "        [0.8533],\n",
      "        [0.8529],\n",
      "        [0.8998],\n",
      "        [0.8575],\n",
      "        [0.8593],\n",
      "        [0.8496],\n",
      "        [0.8256],\n",
      "        [0.8256],\n",
      "        [0.8332],\n",
      "        [0.8256],\n",
      "        [0.8527],\n",
      "        [0.8328],\n",
      "        [0.8501],\n",
      "        [0.8256],\n",
      "        [0.8256],\n",
      "        [0.8256],\n",
      "        [0.8256],\n",
      "        [0.9919],\n",
      "        [0.9448],\n",
      "        [0.9269],\n",
      "        [0.9827],\n",
      "        [0.9524],\n",
      "        [0.8256],\n",
      "        [0.9516],\n",
      "        [0.8256],\n",
      "        [0.8256],\n",
      "        [0.8256],\n",
      "        [0.8812],\n",
      "        [0.8256],\n",
      "        [0.8876],\n",
      "        [1.0430],\n",
      "        [0.8256],\n",
      "        [0.8256]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0216],\n",
      "        [0.0120],\n",
      "        [0.0111],\n",
      "        [0.0082],\n",
      "        [0.0109],\n",
      "        [0.0041],\n",
      "        [0.0046],\n",
      "        [0.0042],\n",
      "        [0.0327],\n",
      "        [0.0190],\n",
      "        [0.0017],\n",
      "        [0.0219],\n",
      "        [0.0035],\n",
      "        [0.0032],\n",
      "        [0.0005],\n",
      "        [0.0213],\n",
      "        [0.0161],\n",
      "        [0.0056],\n",
      "        [0.0111],\n",
      "        [0.0098],\n",
      "        [0.0159],\n",
      "        [0.0097],\n",
      "        [0.0145],\n",
      "        [0.0224],\n",
      "        [0.0041],\n",
      "        [0.0251],\n",
      "        [0.0177],\n",
      "        [0.0207],\n",
      "        [0.0268],\n",
      "        [0.0347],\n",
      "        [0.0418],\n",
      "        [0.0479],\n",
      "        [0.0430],\n",
      "        [0.0579],\n",
      "        [0.0637]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0258],\n",
      "        [0.0162],\n",
      "        [0.0132],\n",
      "        [0.0094],\n",
      "        [0.0140],\n",
      "        [0.0058],\n",
      "        [0.0063],\n",
      "        [0.0058],\n",
      "        [0.0369],\n",
      "        [0.0232],\n",
      "        [0.0038],\n",
      "        [0.0261],\n",
      "        [0.0047],\n",
      "        [0.0044],\n",
      "        [0.0011],\n",
      "        [0.0255],\n",
      "        [0.0203],\n",
      "        [0.0098],\n",
      "        [0.0153],\n",
      "        [0.0116],\n",
      "        [0.0171],\n",
      "        [0.0074],\n",
      "        [0.0124],\n",
      "        [0.0209],\n",
      "        [0.0001],\n",
      "        [0.0235],\n",
      "        [0.0135],\n",
      "        [0.0165],\n",
      "        [0.0226],\n",
      "        [0.0324],\n",
      "        [0.0376],\n",
      "        [0.0452],\n",
      "        [0.0411],\n",
      "        [0.0537],\n",
      "        [0.0594]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 4.232506990432739\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 36\n",
      "剩餘X 資料 torch.Size([124, 18])\n",
      "剩餘Y 資料 torch.Size([124, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0039428058080375195, 20)\n",
      "The second_loss value of k: (0.004632177297025919, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引20，y= tensor([0.8926])\n",
      "目前模型的Data狀態 torch.Size([36, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8298],\n",
      "        [0.8298],\n",
      "        [0.8553],\n",
      "        [0.8541],\n",
      "        [0.9030],\n",
      "        [0.8591],\n",
      "        [0.8611],\n",
      "        [0.8512],\n",
      "        [0.8298],\n",
      "        [0.8298],\n",
      "        [0.8353],\n",
      "        [0.8298],\n",
      "        [0.8539],\n",
      "        [0.8339],\n",
      "        [0.8507],\n",
      "        [0.8298],\n",
      "        [0.8298],\n",
      "        [0.8298],\n",
      "        [0.8298],\n",
      "        [0.9937],\n",
      "        [0.9461],\n",
      "        [0.9293],\n",
      "        [0.9805],\n",
      "        [0.9538],\n",
      "        [0.8298],\n",
      "        [0.9532],\n",
      "        [0.8298],\n",
      "        [0.8298],\n",
      "        [0.8298],\n",
      "        [0.8835],\n",
      "        [0.8298],\n",
      "        [0.8903],\n",
      "        [1.0411],\n",
      "        [0.8298],\n",
      "        [0.8298],\n",
      "        [0.8298]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0258],\n",
      "        [0.0162],\n",
      "        [0.0132],\n",
      "        [0.0094],\n",
      "        [0.0140],\n",
      "        [0.0058],\n",
      "        [0.0063],\n",
      "        [0.0058],\n",
      "        [0.0369],\n",
      "        [0.0232],\n",
      "        [0.0038],\n",
      "        [0.0261],\n",
      "        [0.0047],\n",
      "        [0.0044],\n",
      "        [0.0011],\n",
      "        [0.0255],\n",
      "        [0.0203],\n",
      "        [0.0098],\n",
      "        [0.0153],\n",
      "        [0.0116],\n",
      "        [0.0171],\n",
      "        [0.0074],\n",
      "        [0.0124],\n",
      "        [0.0209],\n",
      "        [0.0001],\n",
      "        [0.0235],\n",
      "        [0.0135],\n",
      "        [0.0165],\n",
      "        [0.0226],\n",
      "        [0.0324],\n",
      "        [0.0376],\n",
      "        [0.0452],\n",
      "        [0.0411],\n",
      "        [0.0537],\n",
      "        [0.0594],\n",
      "        [0.0628]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0293],\n",
      "        [0.0197],\n",
      "        [0.0119],\n",
      "        [0.0075],\n",
      "        [0.0135],\n",
      "        [0.0043],\n",
      "        [0.0051],\n",
      "        [0.0044],\n",
      "        [0.0404],\n",
      "        [0.0267],\n",
      "        [0.0029],\n",
      "        [0.0296],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0005],\n",
      "        [0.0290],\n",
      "        [0.0238],\n",
      "        [0.0133],\n",
      "        [0.0188],\n",
      "        [0.0103],\n",
      "        [0.0152],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0224],\n",
      "        [0.0036],\n",
      "        [0.0247],\n",
      "        [0.0100],\n",
      "        [0.0130],\n",
      "        [0.0191],\n",
      "        [0.0334],\n",
      "        [0.0341],\n",
      "        [0.0455],\n",
      "        [0.0375],\n",
      "        [0.0502],\n",
      "        [0.0560],\n",
      "        [0.0593]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 4.489722013473511\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 37\n",
      "剩餘X 資料 torch.Size([123, 18])\n",
      "剩餘Y 資料 torch.Size([123, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.00416954793035984, 19)\n",
      "The second_loss value of k: (0.004701479338109493, 17)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引19，y= tensor([0.8978])\n",
      "目前模型的Data狀態 torch.Size([37, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8333],\n",
      "        [0.8333],\n",
      "        [0.8540],\n",
      "        [0.8521],\n",
      "        [0.9025],\n",
      "        [0.8577],\n",
      "        [0.8598],\n",
      "        [0.8498],\n",
      "        [0.8333],\n",
      "        [0.8333],\n",
      "        [0.8344],\n",
      "        [0.8333],\n",
      "        [0.8527],\n",
      "        [0.8333],\n",
      "        [0.8490],\n",
      "        [0.8333],\n",
      "        [0.8333],\n",
      "        [0.8333],\n",
      "        [0.8333],\n",
      "        [0.9924],\n",
      "        [0.9442],\n",
      "        [0.9283],\n",
      "        [0.9766],\n",
      "        [0.9524],\n",
      "        [0.8333],\n",
      "        [0.9520],\n",
      "        [0.8333],\n",
      "        [0.8333],\n",
      "        [0.8333],\n",
      "        [0.8825],\n",
      "        [0.8333],\n",
      "        [0.8900],\n",
      "        [1.0375],\n",
      "        [0.8333],\n",
      "        [0.8333],\n",
      "        [0.8333],\n",
      "        [0.8333]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0293],\n",
      "        [0.0197],\n",
      "        [0.0119],\n",
      "        [0.0075],\n",
      "        [0.0135],\n",
      "        [0.0043],\n",
      "        [0.0051],\n",
      "        [0.0044],\n",
      "        [0.0404],\n",
      "        [0.0267],\n",
      "        [0.0029],\n",
      "        [0.0296],\n",
      "        [0.0035],\n",
      "        [0.0037],\n",
      "        [0.0005],\n",
      "        [0.0290],\n",
      "        [0.0238],\n",
      "        [0.0133],\n",
      "        [0.0188],\n",
      "        [0.0103],\n",
      "        [0.0152],\n",
      "        [0.0083],\n",
      "        [0.0085],\n",
      "        [0.0224],\n",
      "        [0.0036],\n",
      "        [0.0247],\n",
      "        [0.0100],\n",
      "        [0.0130],\n",
      "        [0.0191],\n",
      "        [0.0334],\n",
      "        [0.0341],\n",
      "        [0.0455],\n",
      "        [0.0375],\n",
      "        [0.0502],\n",
      "        [0.0560],\n",
      "        [0.0593],\n",
      "        [0.0646]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0325],\n",
      "        [0.0230],\n",
      "        [0.0131],\n",
      "        [0.0081],\n",
      "        [0.0159],\n",
      "        [0.0057],\n",
      "        [0.0067],\n",
      "        [0.0059],\n",
      "        [0.0436],\n",
      "        [0.0300],\n",
      "        [0.0050],\n",
      "        [0.0329],\n",
      "        [0.0049],\n",
      "        [0.0070],\n",
      "        [0.0005],\n",
      "        [0.0322],\n",
      "        [0.0271],\n",
      "        [0.0165],\n",
      "        [0.0221],\n",
      "        [0.0114],\n",
      "        [0.0159],\n",
      "        [0.0064],\n",
      "        [0.0077],\n",
      "        [0.0213],\n",
      "        [0.0069],\n",
      "        [0.0234],\n",
      "        [0.0068],\n",
      "        [0.0098],\n",
      "        [0.0159],\n",
      "        [0.0315],\n",
      "        [0.0309],\n",
      "        [0.0432],\n",
      "        [0.0369],\n",
      "        [0.0470],\n",
      "        [0.0527],\n",
      "        [0.0561],\n",
      "        [0.0613]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 4.74585747718811\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 38\n",
      "剩餘X 資料 torch.Size([122, 18])\n",
      "剩餘Y 資料 torch.Size([122, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.004268643446266651, 17)\n",
      "The second_loss value of k: (0.004817536566406488, 20)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引17，y= tensor([0.9018])\n",
      "目前模型的Data狀態 torch.Size([38, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8365],\n",
      "        [0.8365],\n",
      "        [0.8553],\n",
      "        [0.8528],\n",
      "        [0.9048],\n",
      "        [0.8591],\n",
      "        [0.8615],\n",
      "        [0.8513],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8541],\n",
      "        [0.8365],\n",
      "        [0.8501],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.9936],\n",
      "        [0.9449],\n",
      "        [0.9303],\n",
      "        [0.9758],\n",
      "        [0.9535],\n",
      "        [0.8365],\n",
      "        [0.9533],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8844],\n",
      "        [0.8365],\n",
      "        [0.8923],\n",
      "        [1.0369],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0325],\n",
      "        [0.0230],\n",
      "        [0.0131],\n",
      "        [0.0081],\n",
      "        [0.0159],\n",
      "        [0.0057],\n",
      "        [0.0067],\n",
      "        [0.0059],\n",
      "        [0.0436],\n",
      "        [0.0300],\n",
      "        [0.0050],\n",
      "        [0.0329],\n",
      "        [0.0049],\n",
      "        [0.0070],\n",
      "        [0.0005],\n",
      "        [0.0322],\n",
      "        [0.0271],\n",
      "        [0.0165],\n",
      "        [0.0221],\n",
      "        [0.0114],\n",
      "        [0.0159],\n",
      "        [0.0064],\n",
      "        [0.0077],\n",
      "        [0.0213],\n",
      "        [0.0069],\n",
      "        [0.0234],\n",
      "        [0.0068],\n",
      "        [0.0098],\n",
      "        [0.0159],\n",
      "        [0.0315],\n",
      "        [0.0309],\n",
      "        [0.0432],\n",
      "        [0.0369],\n",
      "        [0.0470],\n",
      "        [0.0527],\n",
      "        [0.0561],\n",
      "        [0.0613],\n",
      "        [0.0653]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0357],\n",
      "        [0.0261],\n",
      "        [0.0133],\n",
      "        [0.0077],\n",
      "        [0.0173],\n",
      "        [0.0062],\n",
      "        [0.0074],\n",
      "        [0.0065],\n",
      "        [0.0468],\n",
      "        [0.0331],\n",
      "        [0.0081],\n",
      "        [0.0360],\n",
      "        [0.0056],\n",
      "        [0.0101],\n",
      "        [0.0007],\n",
      "        [0.0354],\n",
      "        [0.0302],\n",
      "        [0.0197],\n",
      "        [0.0252],\n",
      "        [0.0117],\n",
      "        [0.0156],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0211],\n",
      "        [0.0100],\n",
      "        [0.0229],\n",
      "        [0.0036],\n",
      "        [0.0066],\n",
      "        [0.0128],\n",
      "        [0.0305],\n",
      "        [0.0278],\n",
      "        [0.0416],\n",
      "        [0.0352],\n",
      "        [0.0438],\n",
      "        [0.0496],\n",
      "        [0.0529],\n",
      "        [0.0582],\n",
      "        [0.0622]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 5.002408027648926\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 39\n",
      "剩餘X 資料 torch.Size([121, 18])\n",
      "剩餘Y 資料 torch.Size([121, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0043915146961808205, 19)\n",
      "The second_loss value of k: (0.004893066827207804, 18)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引19，y= tensor([0.9059])\n",
      "目前模型的Data狀態 torch.Size([39, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8396],\n",
      "        [0.8396],\n",
      "        [0.8555],\n",
      "        [0.8524],\n",
      "        [0.9062],\n",
      "        [0.8595],\n",
      "        [0.8622],\n",
      "        [0.8519],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8548],\n",
      "        [0.8396],\n",
      "        [0.8503],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.9938],\n",
      "        [0.9445],\n",
      "        [0.9314],\n",
      "        [0.9740],\n",
      "        [0.9537],\n",
      "        [0.8396],\n",
      "        [0.9538],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8854],\n",
      "        [0.8396],\n",
      "        [0.8938],\n",
      "        [1.0352],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8396],\n",
      "        [0.8396]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0357],\n",
      "        [0.0261],\n",
      "        [0.0133],\n",
      "        [0.0077],\n",
      "        [0.0173],\n",
      "        [0.0062],\n",
      "        [0.0074],\n",
      "        [0.0065],\n",
      "        [0.0468],\n",
      "        [0.0331],\n",
      "        [0.0081],\n",
      "        [0.0360],\n",
      "        [0.0056],\n",
      "        [0.0101],\n",
      "        [0.0007],\n",
      "        [0.0354],\n",
      "        [0.0302],\n",
      "        [0.0197],\n",
      "        [0.0252],\n",
      "        [0.0117],\n",
      "        [0.0156],\n",
      "        [0.0053],\n",
      "        [0.0058],\n",
      "        [0.0211],\n",
      "        [0.0100],\n",
      "        [0.0229],\n",
      "        [0.0036],\n",
      "        [0.0066],\n",
      "        [0.0128],\n",
      "        [0.0305],\n",
      "        [0.0278],\n",
      "        [0.0416],\n",
      "        [0.0352],\n",
      "        [0.0438],\n",
      "        [0.0496],\n",
      "        [0.0529],\n",
      "        [0.0582],\n",
      "        [0.0622],\n",
      "        [0.0663]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0387],\n",
      "        [0.0291],\n",
      "        [0.0133],\n",
      "        [0.0072],\n",
      "        [0.0182],\n",
      "        [0.0062],\n",
      "        [0.0078],\n",
      "        [0.0067],\n",
      "        [0.0498],\n",
      "        [0.0362],\n",
      "        [0.0112],\n",
      "        [0.0391],\n",
      "        [0.0061],\n",
      "        [0.0132],\n",
      "        [0.0007],\n",
      "        [0.0384],\n",
      "        [0.0333],\n",
      "        [0.0227],\n",
      "        [0.0282],\n",
      "        [0.0116],\n",
      "        [0.0150],\n",
      "        [0.0046],\n",
      "        [0.0040],\n",
      "        [0.0210],\n",
      "        [0.0130],\n",
      "        [0.0225],\n",
      "        [0.0006],\n",
      "        [0.0036],\n",
      "        [0.0097],\n",
      "        [0.0299],\n",
      "        [0.0247],\n",
      "        [0.0404],\n",
      "        [0.0336],\n",
      "        [0.0408],\n",
      "        [0.0465],\n",
      "        [0.0499],\n",
      "        [0.0552],\n",
      "        [0.0591],\n",
      "        [0.0632]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 5.261462211608887\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 40\n",
      "剩餘X 資料 torch.Size([120, 18])\n",
      "剩餘Y 資料 torch.Size([120, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.004476042464375496, 18)\n",
      "The second_loss value of k: (0.0050990646705031395, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引18，y= tensor([0.9096])\n",
      "目前模型的Data狀態 torch.Size([40, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8427],\n",
      "        [0.8427],\n",
      "        [0.8554],\n",
      "        [0.8519],\n",
      "        [0.9072],\n",
      "        [0.8596],\n",
      "        [0.8625],\n",
      "        [0.8522],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8553],\n",
      "        [0.8427],\n",
      "        [0.8503],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.9937],\n",
      "        [0.9440],\n",
      "        [0.9320],\n",
      "        [0.9722],\n",
      "        [0.9537],\n",
      "        [0.8427],\n",
      "        [0.9542],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8860],\n",
      "        [0.8427],\n",
      "        [0.8950],\n",
      "        [1.0336],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427],\n",
      "        [0.8427]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0387],\n",
      "        [0.0291],\n",
      "        [0.0133],\n",
      "        [0.0072],\n",
      "        [0.0182],\n",
      "        [0.0062],\n",
      "        [0.0078],\n",
      "        [0.0067],\n",
      "        [0.0498],\n",
      "        [0.0362],\n",
      "        [0.0112],\n",
      "        [0.0391],\n",
      "        [0.0061],\n",
      "        [0.0132],\n",
      "        [0.0007],\n",
      "        [0.0384],\n",
      "        [0.0333],\n",
      "        [0.0227],\n",
      "        [0.0282],\n",
      "        [0.0116],\n",
      "        [0.0150],\n",
      "        [0.0046],\n",
      "        [0.0040],\n",
      "        [0.0210],\n",
      "        [0.0130],\n",
      "        [0.0225],\n",
      "        [0.0006],\n",
      "        [0.0036],\n",
      "        [0.0097],\n",
      "        [0.0299],\n",
      "        [0.0247],\n",
      "        [0.0404],\n",
      "        [0.0336],\n",
      "        [0.0408],\n",
      "        [0.0465],\n",
      "        [0.0499],\n",
      "        [0.0552],\n",
      "        [0.0591],\n",
      "        [0.0632],\n",
      "        [0.0669]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0417],\n",
      "        [0.0321],\n",
      "        [0.0131],\n",
      "        [0.0066],\n",
      "        [0.0188],\n",
      "        [0.0061],\n",
      "        [0.0080],\n",
      "        [0.0069],\n",
      "        [0.0527],\n",
      "        [0.0391],\n",
      "        [0.0141],\n",
      "        [0.0420],\n",
      "        [0.0064],\n",
      "        [0.0161],\n",
      "        [0.0007],\n",
      "        [0.0414],\n",
      "        [0.0362],\n",
      "        [0.0256],\n",
      "        [0.0312],\n",
      "        [0.0115],\n",
      "        [0.0143],\n",
      "        [0.0042],\n",
      "        [0.0024],\n",
      "        [0.0210],\n",
      "        [0.0160],\n",
      "        [0.0221],\n",
      "        [0.0023],\n",
      "        [0.0006],\n",
      "        [0.0068],\n",
      "        [0.0295],\n",
      "        [0.0218],\n",
      "        [0.0394],\n",
      "        [0.0322],\n",
      "        [0.0378],\n",
      "        [0.0436],\n",
      "        [0.0470],\n",
      "        [0.0522],\n",
      "        [0.0562],\n",
      "        [0.0603],\n",
      "        [0.0640]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 5.520406246185303\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 41\n",
      "剩餘X 資料 torch.Size([119, 18])\n",
      "剩餘Y 資料 torch.Size([119, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0046895695850253105, 9)\n",
      "The second_loss value of k: (0.0055287908762693405, 18)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.9141])\n",
      "目前模型的Data狀態 torch.Size([41, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8456],\n",
      "        [0.8456],\n",
      "        [0.8553],\n",
      "        [0.8513],\n",
      "        [0.9078],\n",
      "        [0.8594],\n",
      "        [0.8627],\n",
      "        [0.8523],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8556],\n",
      "        [0.8456],\n",
      "        [0.8503],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.9936],\n",
      "        [0.9433],\n",
      "        [0.9325],\n",
      "        [0.9705],\n",
      "        [0.9537],\n",
      "        [0.8456],\n",
      "        [0.9546],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8864],\n",
      "        [0.8456],\n",
      "        [0.8960],\n",
      "        [1.0322],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456],\n",
      "        [0.8456]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0417],\n",
      "        [0.0321],\n",
      "        [0.0131],\n",
      "        [0.0066],\n",
      "        [0.0188],\n",
      "        [0.0061],\n",
      "        [0.0080],\n",
      "        [0.0069],\n",
      "        [0.0527],\n",
      "        [0.0391],\n",
      "        [0.0141],\n",
      "        [0.0420],\n",
      "        [0.0064],\n",
      "        [0.0161],\n",
      "        [0.0007],\n",
      "        [0.0414],\n",
      "        [0.0362],\n",
      "        [0.0256],\n",
      "        [0.0312],\n",
      "        [0.0115],\n",
      "        [0.0143],\n",
      "        [0.0042],\n",
      "        [0.0024],\n",
      "        [0.0210],\n",
      "        [0.0160],\n",
      "        [0.0221],\n",
      "        [0.0023],\n",
      "        [0.0006],\n",
      "        [0.0068],\n",
      "        [0.0295],\n",
      "        [0.0218],\n",
      "        [0.0394],\n",
      "        [0.0322],\n",
      "        [0.0378],\n",
      "        [0.0436],\n",
      "        [0.0470],\n",
      "        [0.0522],\n",
      "        [0.0562],\n",
      "        [0.0603],\n",
      "        [0.0640],\n",
      "        [0.0685]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0445],\n",
      "        [0.0349],\n",
      "        [0.0130],\n",
      "        [0.0062],\n",
      "        [0.0194],\n",
      "        [0.0060],\n",
      "        [0.0082],\n",
      "        [0.0070],\n",
      "        [0.0556],\n",
      "        [0.0420],\n",
      "        [0.0170],\n",
      "        [0.0449],\n",
      "        [0.0069],\n",
      "        [0.0190],\n",
      "        [0.0008],\n",
      "        [0.0442],\n",
      "        [0.0391],\n",
      "        [0.0285],\n",
      "        [0.0340],\n",
      "        [0.0115],\n",
      "        [0.0138],\n",
      "        [0.0037],\n",
      "        [0.0010],\n",
      "        [0.0209],\n",
      "        [0.0188],\n",
      "        [0.0216],\n",
      "        [0.0052],\n",
      "        [0.0022],\n",
      "        [0.0039],\n",
      "        [0.0291],\n",
      "        [0.0189],\n",
      "        [0.0384],\n",
      "        [0.0311],\n",
      "        [0.0350],\n",
      "        [0.0407],\n",
      "        [0.0441],\n",
      "        [0.0493],\n",
      "        [0.0533],\n",
      "        [0.0574],\n",
      "        [0.0611],\n",
      "        [0.0656]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 5.779399871826172\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 42\n",
      "剩餘X 資料 torch.Size([118, 18])\n",
      "剩餘Y 資料 torch.Size([118, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.00510913971811533, 17)\n",
      "The second_loss value of k: (0.008141138590872288, 13)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引17，y= tensor([0.9200])\n",
      "目前模型的Data狀態 torch.Size([42, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8485],\n",
      "        [0.8485],\n",
      "        [0.8552],\n",
      "        [0.8508],\n",
      "        [0.9084],\n",
      "        [0.8593],\n",
      "        [0.8630],\n",
      "        [0.8525],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8561],\n",
      "        [0.8485],\n",
      "        [0.8504],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.9936],\n",
      "        [0.9428],\n",
      "        [0.9329],\n",
      "        [0.9692],\n",
      "        [0.9539],\n",
      "        [0.8485],\n",
      "        [0.9551],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8868],\n",
      "        [0.8485],\n",
      "        [0.8970],\n",
      "        [1.0311],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485],\n",
      "        [0.8485]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0445],\n",
      "        [0.0349],\n",
      "        [0.0130],\n",
      "        [0.0062],\n",
      "        [0.0194],\n",
      "        [0.0060],\n",
      "        [0.0082],\n",
      "        [0.0070],\n",
      "        [0.0556],\n",
      "        [0.0420],\n",
      "        [0.0170],\n",
      "        [0.0449],\n",
      "        [0.0069],\n",
      "        [0.0190],\n",
      "        [0.0008],\n",
      "        [0.0442],\n",
      "        [0.0391],\n",
      "        [0.0285],\n",
      "        [0.0340],\n",
      "        [0.0115],\n",
      "        [0.0138],\n",
      "        [0.0037],\n",
      "        [0.0010],\n",
      "        [0.0209],\n",
      "        [0.0188],\n",
      "        [0.0216],\n",
      "        [0.0052],\n",
      "        [0.0022],\n",
      "        [0.0039],\n",
      "        [0.0291],\n",
      "        [0.0189],\n",
      "        [0.0384],\n",
      "        [0.0311],\n",
      "        [0.0350],\n",
      "        [0.0407],\n",
      "        [0.0441],\n",
      "        [0.0493],\n",
      "        [0.0533],\n",
      "        [0.0574],\n",
      "        [0.0611],\n",
      "        [0.0656],\n",
      "        [0.0715]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0471],\n",
      "        [0.0375],\n",
      "        [0.0135],\n",
      "        [0.0064],\n",
      "        [0.0200],\n",
      "        [0.0060],\n",
      "        [0.0084],\n",
      "        [0.0072],\n",
      "        [0.0582],\n",
      "        [0.0445],\n",
      "        [0.0195],\n",
      "        [0.0474],\n",
      "        [0.0073],\n",
      "        [0.0215],\n",
      "        [0.0015],\n",
      "        [0.0468],\n",
      "        [0.0416],\n",
      "        [0.0311],\n",
      "        [0.0366],\n",
      "        [0.0118],\n",
      "        [0.0138],\n",
      "        [0.0033],\n",
      "        [0.0001],\n",
      "        [0.0202],\n",
      "        [0.0214],\n",
      "        [0.0207],\n",
      "        [0.0078],\n",
      "        [0.0048],\n",
      "        [0.0014],\n",
      "        [0.0287],\n",
      "        [0.0164],\n",
      "        [0.0376],\n",
      "        [0.0301],\n",
      "        [0.0324],\n",
      "        [0.0382],\n",
      "        [0.0415],\n",
      "        [0.0468],\n",
      "        [0.0508],\n",
      "        [0.0549],\n",
      "        [0.0585],\n",
      "        [0.0630],\n",
      "        [0.0689]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.036893129348755\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 43\n",
      "剩餘X 資料 torch.Size([117, 18])\n",
      "剩餘Y 資料 torch.Size([117, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.007954442873597145, 13)\n",
      "The second_loss value of k: (0.008014178834855556, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引13，y= tensor([0.9685])\n",
      "目前模型的Data狀態 torch.Size([43, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8511],\n",
      "        [0.8511],\n",
      "        [0.8556],\n",
      "        [0.8511],\n",
      "        [0.9089],\n",
      "        [0.8593],\n",
      "        [0.8631],\n",
      "        [0.8527],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8565],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.9939],\n",
      "        [0.9428],\n",
      "        [0.9333],\n",
      "        [0.9681],\n",
      "        [0.9545],\n",
      "        [0.8511],\n",
      "        [0.9560],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8872],\n",
      "        [0.8511],\n",
      "        [0.8978],\n",
      "        [1.0301],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [0.8511],\n",
      "        [1.0577]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0471],\n",
      "        [0.0375],\n",
      "        [0.0135],\n",
      "        [0.0064],\n",
      "        [0.0200],\n",
      "        [0.0060],\n",
      "        [0.0084],\n",
      "        [0.0072],\n",
      "        [0.0582],\n",
      "        [0.0445],\n",
      "        [0.0195],\n",
      "        [0.0474],\n",
      "        [0.0073],\n",
      "        [0.0215],\n",
      "        [0.0015],\n",
      "        [0.0468],\n",
      "        [0.0416],\n",
      "        [0.0311],\n",
      "        [0.0366],\n",
      "        [0.0118],\n",
      "        [0.0138],\n",
      "        [0.0033],\n",
      "        [0.0001],\n",
      "        [0.0202],\n",
      "        [0.0214],\n",
      "        [0.0207],\n",
      "        [0.0078],\n",
      "        [0.0048],\n",
      "        [0.0014],\n",
      "        [0.0287],\n",
      "        [0.0164],\n",
      "        [0.0376],\n",
      "        [0.0301],\n",
      "        [0.0324],\n",
      "        [0.0382],\n",
      "        [0.0415],\n",
      "        [0.0468],\n",
      "        [0.0508],\n",
      "        [0.0549],\n",
      "        [0.0585],\n",
      "        [0.0630],\n",
      "        [0.0689],\n",
      "        [0.0892]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0466],\n",
      "        [0.0370],\n",
      "        [0.0124],\n",
      "        [0.0058],\n",
      "        [0.0161],\n",
      "        [0.0008],\n",
      "        [0.0026],\n",
      "        [0.0051],\n",
      "        [0.0576],\n",
      "        [0.0440],\n",
      "        [0.0190],\n",
      "        [0.0469],\n",
      "        [0.0013],\n",
      "        [0.0210],\n",
      "        [0.0009],\n",
      "        [0.0462],\n",
      "        [0.0411],\n",
      "        [0.0305],\n",
      "        [0.0361],\n",
      "        [0.0092],\n",
      "        [0.0113],\n",
      "        [0.0085],\n",
      "        [0.0132],\n",
      "        [0.0230],\n",
      "        [0.0209],\n",
      "        [0.0237],\n",
      "        [0.0072],\n",
      "        [0.0043],\n",
      "        [0.0019],\n",
      "        [0.0335],\n",
      "        [0.0169],\n",
      "        [0.0432],\n",
      "        [0.0171],\n",
      "        [0.0330],\n",
      "        [0.0387],\n",
      "        [0.0421],\n",
      "        [0.0473],\n",
      "        [0.0513],\n",
      "        [0.0554],\n",
      "        [0.0591],\n",
      "        [0.0636],\n",
      "        [0.0695],\n",
      "        [0.0760]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.293842077255249\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 44\n",
      "剩餘X 資料 torch.Size([116, 18])\n",
      "剩餘Y 資料 torch.Size([116, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.008413375355303288, 0)\n",
      "The second_loss value of k: (0.008701183833181858, 8)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.9994])\n",
      "目前模型的Data狀態 torch.Size([44, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8505],\n",
      "        [0.8505],\n",
      "        [0.8546],\n",
      "        [0.8505],\n",
      "        [0.9051],\n",
      "        [0.8541],\n",
      "        [0.8574],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.9913],\n",
      "        [0.9403],\n",
      "        [0.9282],\n",
      "        [0.9550],\n",
      "        [0.9518],\n",
      "        [0.8505],\n",
      "        [0.9530],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8824],\n",
      "        [0.8505],\n",
      "        [0.8922],\n",
      "        [1.0171],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [0.8505],\n",
      "        [1.0445],\n",
      "        [0.9077]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0466],\n",
      "        [0.0370],\n",
      "        [0.0124],\n",
      "        [0.0058],\n",
      "        [0.0161],\n",
      "        [0.0008],\n",
      "        [0.0026],\n",
      "        [0.0051],\n",
      "        [0.0576],\n",
      "        [0.0440],\n",
      "        [0.0190],\n",
      "        [0.0469],\n",
      "        [0.0013],\n",
      "        [0.0210],\n",
      "        [0.0009],\n",
      "        [0.0462],\n",
      "        [0.0411],\n",
      "        [0.0305],\n",
      "        [0.0361],\n",
      "        [0.0092],\n",
      "        [0.0113],\n",
      "        [0.0085],\n",
      "        [0.0132],\n",
      "        [0.0230],\n",
      "        [0.0209],\n",
      "        [0.0237],\n",
      "        [0.0072],\n",
      "        [0.0043],\n",
      "        [0.0019],\n",
      "        [0.0335],\n",
      "        [0.0169],\n",
      "        [0.0432],\n",
      "        [0.0171],\n",
      "        [0.0330],\n",
      "        [0.0387],\n",
      "        [0.0421],\n",
      "        [0.0473],\n",
      "        [0.0513],\n",
      "        [0.0554],\n",
      "        [0.0591],\n",
      "        [0.0636],\n",
      "        [0.0695],\n",
      "        [0.0760],\n",
      "        [0.0917]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0473],\n",
      "        [0.0377],\n",
      "        [0.0243],\n",
      "        [0.0166],\n",
      "        [0.0231],\n",
      "        [0.0066],\n",
      "        [0.0079],\n",
      "        [0.0070],\n",
      "        [0.0584],\n",
      "        [0.0447],\n",
      "        [0.0197],\n",
      "        [0.0476],\n",
      "        [0.0048],\n",
      "        [0.0217],\n",
      "        [0.0017],\n",
      "        [0.0470],\n",
      "        [0.0418],\n",
      "        [0.0313],\n",
      "        [0.0368],\n",
      "        [0.0193],\n",
      "        [0.0214],\n",
      "        [0.0020],\n",
      "        [0.0142],\n",
      "        [0.0118],\n",
      "        [0.0216],\n",
      "        [0.0122],\n",
      "        [0.0080],\n",
      "        [0.0050],\n",
      "        [0.0012],\n",
      "        [0.0269],\n",
      "        [0.0162],\n",
      "        [0.0395],\n",
      "        [0.0160],\n",
      "        [0.0322],\n",
      "        [0.0380],\n",
      "        [0.0413],\n",
      "        [0.0466],\n",
      "        [0.0506],\n",
      "        [0.0547],\n",
      "        [0.0583],\n",
      "        [0.0628],\n",
      "        [0.0687],\n",
      "        [0.0747],\n",
      "        [0.0798]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.5519773960113525\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 45\n",
      "剩餘X 資料 torch.Size([115, 18])\n",
      "剩餘Y 資料 torch.Size([115, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.008562861941754818, 7)\n",
      "The second_loss value of k: (0.010176539421081543, 13)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引7，y= tensor([0.9438])\n",
      "目前模型的Data狀態 torch.Size([45, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8513],\n",
      "        [0.8513],\n",
      "        [0.8664],\n",
      "        [0.8613],\n",
      "        [0.9121],\n",
      "        [0.8599],\n",
      "        [0.8627],\n",
      "        [0.8524],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8540],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [1.0014],\n",
      "        [0.9504],\n",
      "        [0.9347],\n",
      "        [0.9540],\n",
      "        [0.9629],\n",
      "        [0.8513],\n",
      "        [0.9644],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8890],\n",
      "        [0.8513],\n",
      "        [0.8960],\n",
      "        [1.0160],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [0.8513],\n",
      "        [1.0432],\n",
      "        [0.9196],\n",
      "        [0.8513]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0473],\n",
      "        [0.0377],\n",
      "        [0.0243],\n",
      "        [0.0166],\n",
      "        [0.0231],\n",
      "        [0.0066],\n",
      "        [0.0079],\n",
      "        [0.0070],\n",
      "        [0.0584],\n",
      "        [0.0447],\n",
      "        [0.0197],\n",
      "        [0.0476],\n",
      "        [0.0048],\n",
      "        [0.0217],\n",
      "        [0.0017],\n",
      "        [0.0470],\n",
      "        [0.0418],\n",
      "        [0.0313],\n",
      "        [0.0368],\n",
      "        [0.0193],\n",
      "        [0.0214],\n",
      "        [0.0020],\n",
      "        [0.0142],\n",
      "        [0.0118],\n",
      "        [0.0216],\n",
      "        [0.0122],\n",
      "        [0.0080],\n",
      "        [0.0050],\n",
      "        [0.0012],\n",
      "        [0.0269],\n",
      "        [0.0162],\n",
      "        [0.0395],\n",
      "        [0.0160],\n",
      "        [0.0322],\n",
      "        [0.0380],\n",
      "        [0.0413],\n",
      "        [0.0466],\n",
      "        [0.0506],\n",
      "        [0.0547],\n",
      "        [0.0583],\n",
      "        [0.0628],\n",
      "        [0.0687],\n",
      "        [0.0747],\n",
      "        [0.0798],\n",
      "        [0.0925]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0501],\n",
      "        [0.0405],\n",
      "        [0.0233],\n",
      "        [0.0159],\n",
      "        [0.0199],\n",
      "        [0.0031],\n",
      "        [0.0048],\n",
      "        [0.0086],\n",
      "        [0.0611],\n",
      "        [0.0475],\n",
      "        [0.0225],\n",
      "        [0.0504],\n",
      "        [0.0048],\n",
      "        [0.0245],\n",
      "        [0.0044],\n",
      "        [0.0498],\n",
      "        [0.0446],\n",
      "        [0.0340],\n",
      "        [0.0396],\n",
      "        [0.0176],\n",
      "        [0.0194],\n",
      "        [0.0050],\n",
      "        [0.0203],\n",
      "        [0.0119],\n",
      "        [0.0244],\n",
      "        [0.0118],\n",
      "        [0.0107],\n",
      "        [0.0078],\n",
      "        [0.0016],\n",
      "        [0.0297],\n",
      "        [0.0134],\n",
      "        [0.0432],\n",
      "        [0.0099],\n",
      "        [0.0295],\n",
      "        [0.0352],\n",
      "        [0.0386],\n",
      "        [0.0438],\n",
      "        [0.0478],\n",
      "        [0.0519],\n",
      "        [0.0556],\n",
      "        [0.0601],\n",
      "        [0.0660],\n",
      "        [0.0682],\n",
      "        [0.0791],\n",
      "        [0.0898]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.808177471160889\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 46\n",
      "剩餘X 資料 torch.Size([114, 18])\n",
      "剩餘Y 資料 torch.Size([114, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.009626534767448902, 12)\n",
      "The second_loss value of k: (0.01275116577744484, 11)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引12，y= tensor([0.9521])\n",
      "目前模型的Data狀態 torch.Size([46, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8540],\n",
      "        [0.8540],\n",
      "        [0.8654],\n",
      "        [0.8606],\n",
      "        [0.9089],\n",
      "        [0.8565],\n",
      "        [0.8595],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.9997],\n",
      "        [0.9484],\n",
      "        [0.9317],\n",
      "        [0.9479],\n",
      "        [0.9629],\n",
      "        [0.8540],\n",
      "        [0.9649],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8862],\n",
      "        [0.8540],\n",
      "        [0.8923],\n",
      "        [1.0099],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [0.8540],\n",
      "        [1.0366],\n",
      "        [0.9204],\n",
      "        [0.8540],\n",
      "        [0.8540]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0501],\n",
      "        [0.0405],\n",
      "        [0.0233],\n",
      "        [0.0159],\n",
      "        [0.0199],\n",
      "        [0.0031],\n",
      "        [0.0048],\n",
      "        [0.0086],\n",
      "        [0.0611],\n",
      "        [0.0475],\n",
      "        [0.0225],\n",
      "        [0.0504],\n",
      "        [0.0048],\n",
      "        [0.0245],\n",
      "        [0.0044],\n",
      "        [0.0498],\n",
      "        [0.0446],\n",
      "        [0.0340],\n",
      "        [0.0396],\n",
      "        [0.0176],\n",
      "        [0.0194],\n",
      "        [0.0050],\n",
      "        [0.0203],\n",
      "        [0.0119],\n",
      "        [0.0244],\n",
      "        [0.0118],\n",
      "        [0.0107],\n",
      "        [0.0078],\n",
      "        [0.0016],\n",
      "        [0.0297],\n",
      "        [0.0134],\n",
      "        [0.0432],\n",
      "        [0.0099],\n",
      "        [0.0295],\n",
      "        [0.0352],\n",
      "        [0.0386],\n",
      "        [0.0438],\n",
      "        [0.0478],\n",
      "        [0.0519],\n",
      "        [0.0556],\n",
      "        [0.0601],\n",
      "        [0.0660],\n",
      "        [0.0682],\n",
      "        [0.0791],\n",
      "        [0.0898],\n",
      "        [0.0981]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0536],\n",
      "        [0.0440],\n",
      "        [0.0263],\n",
      "        [0.0189],\n",
      "        [0.0224],\n",
      "        [0.0053],\n",
      "        [0.0075],\n",
      "        [0.0121],\n",
      "        [0.0647],\n",
      "        [0.0510],\n",
      "        [0.0260],\n",
      "        [0.0539],\n",
      "        [0.0083],\n",
      "        [0.0280],\n",
      "        [0.0079],\n",
      "        [0.0533],\n",
      "        [0.0481],\n",
      "        [0.0375],\n",
      "        [0.0431],\n",
      "        [0.0201],\n",
      "        [0.0214],\n",
      "        [0.0021],\n",
      "        [0.0208],\n",
      "        [0.0080],\n",
      "        [0.0279],\n",
      "        [0.0071],\n",
      "        [0.0142],\n",
      "        [0.0113],\n",
      "        [0.0051],\n",
      "        [0.0267],\n",
      "        [0.0099],\n",
      "        [0.0412],\n",
      "        [0.0093],\n",
      "        [0.0259],\n",
      "        [0.0317],\n",
      "        [0.0351],\n",
      "        [0.0403],\n",
      "        [0.0443],\n",
      "        [0.0484],\n",
      "        [0.0521],\n",
      "        [0.0566],\n",
      "        [0.0625],\n",
      "        [0.0672],\n",
      "        [0.0743],\n",
      "        [0.0863],\n",
      "        [0.0946]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 7.063636064529419\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 47\n",
      "剩餘X 資料 torch.Size([113, 18])\n",
      "剩餘Y 資料 torch.Size([113, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.011972540989518166, 11)\n",
      "The second_loss value of k: (0.012058405205607414, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引11，y= tensor([0.9669])\n",
      "目前模型的Data狀態 torch.Size([47, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8575],\n",
      "        [0.8575],\n",
      "        [0.8685],\n",
      "        [0.8636],\n",
      "        [0.9114],\n",
      "        [0.8587],\n",
      "        [0.8622],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [1.0023],\n",
      "        [0.9503],\n",
      "        [0.9345],\n",
      "        [0.9474],\n",
      "        [0.9667],\n",
      "        [0.8575],\n",
      "        [0.9696],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8892],\n",
      "        [0.8575],\n",
      "        [0.8942],\n",
      "        [1.0093],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [1.0357],\n",
      "        [0.9251],\n",
      "        [0.8575],\n",
      "        [0.8575],\n",
      "        [0.8575]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0536],\n",
      "        [0.0440],\n",
      "        [0.0263],\n",
      "        [0.0189],\n",
      "        [0.0224],\n",
      "        [0.0053],\n",
      "        [0.0075],\n",
      "        [0.0121],\n",
      "        [0.0647],\n",
      "        [0.0510],\n",
      "        [0.0260],\n",
      "        [0.0539],\n",
      "        [0.0083],\n",
      "        [0.0280],\n",
      "        [0.0079],\n",
      "        [0.0533],\n",
      "        [0.0481],\n",
      "        [0.0375],\n",
      "        [0.0431],\n",
      "        [0.0201],\n",
      "        [0.0214],\n",
      "        [0.0021],\n",
      "        [0.0208],\n",
      "        [0.0080],\n",
      "        [0.0279],\n",
      "        [0.0071],\n",
      "        [0.0142],\n",
      "        [0.0113],\n",
      "        [0.0051],\n",
      "        [0.0267],\n",
      "        [0.0099],\n",
      "        [0.0412],\n",
      "        [0.0093],\n",
      "        [0.0259],\n",
      "        [0.0317],\n",
      "        [0.0351],\n",
      "        [0.0403],\n",
      "        [0.0443],\n",
      "        [0.0484],\n",
      "        [0.0521],\n",
      "        [0.0566],\n",
      "        [0.0625],\n",
      "        [0.0672],\n",
      "        [0.0743],\n",
      "        [0.0863],\n",
      "        [0.0946],\n",
      "        [0.1094]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0569],\n",
      "        [0.0473],\n",
      "        [0.0260],\n",
      "        [0.0187],\n",
      "        [0.0224],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0154],\n",
      "        [0.0680],\n",
      "        [0.0543],\n",
      "        [0.0293],\n",
      "        [0.0572],\n",
      "        [0.0117],\n",
      "        [0.0313],\n",
      "        [0.0113],\n",
      "        [0.0566],\n",
      "        [0.0514],\n",
      "        [0.0409],\n",
      "        [0.0464],\n",
      "        [0.0195],\n",
      "        [0.0201],\n",
      "        [0.0017],\n",
      "        [0.0225],\n",
      "        [0.0072],\n",
      "        [0.0312],\n",
      "        [0.0054],\n",
      "        [0.0176],\n",
      "        [0.0146],\n",
      "        [0.0085],\n",
      "        [0.0260],\n",
      "        [0.0065],\n",
      "        [0.0412],\n",
      "        [0.0074],\n",
      "        [0.0226],\n",
      "        [0.0284],\n",
      "        [0.0317],\n",
      "        [0.0370],\n",
      "        [0.0410],\n",
      "        [0.0450],\n",
      "        [0.0487],\n",
      "        [0.0532],\n",
      "        [0.0591],\n",
      "        [0.0649],\n",
      "        [0.0725],\n",
      "        [0.0829],\n",
      "        [0.0913],\n",
      "        [0.1061]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 7.318256616592407\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 48\n",
      "剩餘X 資料 torch.Size([112, 18])\n",
      "剩餘Y 資料 torch.Size([112, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.011332723312079906, 6)\n",
      "The second_loss value of k: (0.013142725452780724, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.9673])\n",
      "目前模型的Data狀態 torch.Size([48, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8609],\n",
      "        [0.8609],\n",
      "        [0.8681],\n",
      "        [0.8634],\n",
      "        [0.9113],\n",
      "        [0.8609],\n",
      "        [0.8628],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [1.0016],\n",
      "        [0.9491],\n",
      "        [0.9350],\n",
      "        [0.9456],\n",
      "        [0.9676],\n",
      "        [0.8609],\n",
      "        [0.9713],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8899],\n",
      "        [0.8609],\n",
      "        [0.8943],\n",
      "        [1.0074],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [1.0334],\n",
      "        [0.9270],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609],\n",
      "        [0.8609]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0569],\n",
      "        [0.0473],\n",
      "        [0.0260],\n",
      "        [0.0187],\n",
      "        [0.0224],\n",
      "        [0.0075],\n",
      "        [0.0080],\n",
      "        [0.0154],\n",
      "        [0.0680],\n",
      "        [0.0543],\n",
      "        [0.0293],\n",
      "        [0.0572],\n",
      "        [0.0117],\n",
      "        [0.0313],\n",
      "        [0.0113],\n",
      "        [0.0566],\n",
      "        [0.0514],\n",
      "        [0.0409],\n",
      "        [0.0464],\n",
      "        [0.0195],\n",
      "        [0.0201],\n",
      "        [0.0017],\n",
      "        [0.0225],\n",
      "        [0.0072],\n",
      "        [0.0312],\n",
      "        [0.0054],\n",
      "        [0.0176],\n",
      "        [0.0146],\n",
      "        [0.0085],\n",
      "        [0.0260],\n",
      "        [0.0065],\n",
      "        [0.0412],\n",
      "        [0.0074],\n",
      "        [0.0226],\n",
      "        [0.0284],\n",
      "        [0.0317],\n",
      "        [0.0370],\n",
      "        [0.0410],\n",
      "        [0.0450],\n",
      "        [0.0487],\n",
      "        [0.0532],\n",
      "        [0.0591],\n",
      "        [0.0649],\n",
      "        [0.0725],\n",
      "        [0.0829],\n",
      "        [0.0913],\n",
      "        [0.1061],\n",
      "        [0.1065]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 73\n",
      "Number of shrink: 27\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0596],\n",
      "        [0.0500],\n",
      "        [0.0260],\n",
      "        [0.0188],\n",
      "        [0.0224],\n",
      "        [0.0102],\n",
      "        [0.0088],\n",
      "        [0.0181],\n",
      "        [0.0706],\n",
      "        [0.0570],\n",
      "        [0.0320],\n",
      "        [0.0599],\n",
      "        [0.0143],\n",
      "        [0.0340],\n",
      "        [0.0139],\n",
      "        [0.0593],\n",
      "        [0.0541],\n",
      "        [0.0435],\n",
      "        [0.0491],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0015],\n",
      "        [0.0223],\n",
      "        [0.0068],\n",
      "        [0.0339],\n",
      "        [0.0048],\n",
      "        [0.0202],\n",
      "        [0.0173],\n",
      "        [0.0111],\n",
      "        [0.0258],\n",
      "        [0.0039],\n",
      "        [0.0409],\n",
      "        [0.0076],\n",
      "        [0.0200],\n",
      "        [0.0257],\n",
      "        [0.0291],\n",
      "        [0.0343],\n",
      "        [0.0383],\n",
      "        [0.0424],\n",
      "        [0.0461],\n",
      "        [0.0506],\n",
      "        [0.0565],\n",
      "        [0.0650],\n",
      "        [0.0719],\n",
      "        [0.0803],\n",
      "        [0.0886],\n",
      "        [0.1034],\n",
      "        [0.1038]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 7.5748021602630615\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 49\n",
      "剩餘X 資料 torch.Size([111, 18])\n",
      "剩餘Y 資料 torch.Size([111, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.01254374347627163, 0)\n",
      "The second_loss value of k: (0.01334582082927227, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.9755])\n",
      "目前模型的Data狀態 torch.Size([49, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8635],\n",
      "        [0.8635],\n",
      "        [0.8682],\n",
      "        [0.8635],\n",
      "        [0.9113],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [1.0017],\n",
      "        [0.9491],\n",
      "        [0.9351],\n",
      "        [0.9459],\n",
      "        [0.9680],\n",
      "        [0.8635],\n",
      "        [0.9719],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8901],\n",
      "        [0.8635],\n",
      "        [0.8946],\n",
      "        [1.0076],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [1.0334],\n",
      "        [0.9275],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635],\n",
      "        [0.8635]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0596],\n",
      "        [0.0500],\n",
      "        [0.0260],\n",
      "        [0.0188],\n",
      "        [0.0224],\n",
      "        [0.0102],\n",
      "        [0.0088],\n",
      "        [0.0181],\n",
      "        [0.0706],\n",
      "        [0.0570],\n",
      "        [0.0320],\n",
      "        [0.0599],\n",
      "        [0.0143],\n",
      "        [0.0340],\n",
      "        [0.0139],\n",
      "        [0.0593],\n",
      "        [0.0541],\n",
      "        [0.0435],\n",
      "        [0.0491],\n",
      "        [0.0196],\n",
      "        [0.0201],\n",
      "        [0.0015],\n",
      "        [0.0223],\n",
      "        [0.0068],\n",
      "        [0.0339],\n",
      "        [0.0048],\n",
      "        [0.0202],\n",
      "        [0.0173],\n",
      "        [0.0111],\n",
      "        [0.0258],\n",
      "        [0.0039],\n",
      "        [0.0409],\n",
      "        [0.0076],\n",
      "        [0.0200],\n",
      "        [0.0257],\n",
      "        [0.0291],\n",
      "        [0.0343],\n",
      "        [0.0383],\n",
      "        [0.0424],\n",
      "        [0.0461],\n",
      "        [0.0506],\n",
      "        [0.0565],\n",
      "        [0.0650],\n",
      "        [0.0719],\n",
      "        [0.0803],\n",
      "        [0.0886],\n",
      "        [0.1034],\n",
      "        [0.1038],\n",
      "        [0.1120]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0623],\n",
      "        [0.0527],\n",
      "        [0.0271],\n",
      "        [0.0215],\n",
      "        [0.0229],\n",
      "        [0.0129],\n",
      "        [0.0115],\n",
      "        [0.0208],\n",
      "        [0.0734],\n",
      "        [0.0597],\n",
      "        [0.0347],\n",
      "        [0.0626],\n",
      "        [0.0170],\n",
      "        [0.0367],\n",
      "        [0.0166],\n",
      "        [0.0620],\n",
      "        [0.0568],\n",
      "        [0.0463],\n",
      "        [0.0518],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0005],\n",
      "        [0.0240],\n",
      "        [0.0048],\n",
      "        [0.0366],\n",
      "        [0.0019],\n",
      "        [0.0230],\n",
      "        [0.0200],\n",
      "        [0.0138],\n",
      "        [0.0244],\n",
      "        [0.0012],\n",
      "        [0.0405],\n",
      "        [0.0055],\n",
      "        [0.0172],\n",
      "        [0.0230],\n",
      "        [0.0263],\n",
      "        [0.0316],\n",
      "        [0.0356],\n",
      "        [0.0397],\n",
      "        [0.0434],\n",
      "        [0.0479],\n",
      "        [0.0537],\n",
      "        [0.0624],\n",
      "        [0.0687],\n",
      "        [0.0776],\n",
      "        [0.0859],\n",
      "        [0.1007],\n",
      "        [0.1011],\n",
      "        [0.1093]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 7.8666956424713135\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 50\n",
      "剩餘X 資料 torch.Size([110, 18])\n",
      "剩餘Y 資料 torch.Size([110, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.012725144624710083, 1)\n",
      "The second_loss value of k: (0.012947033159434795, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.9790])\n",
      "目前模型的Data狀態 torch.Size([50, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8662],\n",
      "        [0.8662],\n",
      "        [0.8693],\n",
      "        [0.8662],\n",
      "        [0.9118],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [1.0019],\n",
      "        [0.9490],\n",
      "        [0.9361],\n",
      "        [0.9442],\n",
      "        [0.9699],\n",
      "        [0.8662],\n",
      "        [0.9748],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8915],\n",
      "        [0.8662],\n",
      "        [0.8950],\n",
      "        [1.0055],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [1.0308],\n",
      "        [0.9307],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662],\n",
      "        [0.8662]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0623],\n",
      "        [0.0527],\n",
      "        [0.0271],\n",
      "        [0.0215],\n",
      "        [0.0229],\n",
      "        [0.0129],\n",
      "        [0.0115],\n",
      "        [0.0208],\n",
      "        [0.0734],\n",
      "        [0.0597],\n",
      "        [0.0347],\n",
      "        [0.0626],\n",
      "        [0.0170],\n",
      "        [0.0367],\n",
      "        [0.0166],\n",
      "        [0.0620],\n",
      "        [0.0568],\n",
      "        [0.0463],\n",
      "        [0.0518],\n",
      "        [0.0197],\n",
      "        [0.0200],\n",
      "        [0.0005],\n",
      "        [0.0240],\n",
      "        [0.0048],\n",
      "        [0.0366],\n",
      "        [0.0019],\n",
      "        [0.0230],\n",
      "        [0.0200],\n",
      "        [0.0138],\n",
      "        [0.0244],\n",
      "        [0.0012],\n",
      "        [0.0405],\n",
      "        [0.0055],\n",
      "        [0.0172],\n",
      "        [0.0230],\n",
      "        [0.0263],\n",
      "        [0.0316],\n",
      "        [0.0356],\n",
      "        [0.0397],\n",
      "        [0.0434],\n",
      "        [0.0479],\n",
      "        [0.0537],\n",
      "        [0.0624],\n",
      "        [0.0687],\n",
      "        [0.0776],\n",
      "        [0.0859],\n",
      "        [0.1007],\n",
      "        [0.1011],\n",
      "        [0.1093],\n",
      "        [0.1128]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0654],\n",
      "        [0.0558],\n",
      "        [0.0275],\n",
      "        [0.0246],\n",
      "        [0.0233],\n",
      "        [0.0160],\n",
      "        [0.0146],\n",
      "        [0.0239],\n",
      "        [0.0765],\n",
      "        [0.0628],\n",
      "        [0.0378],\n",
      "        [0.0657],\n",
      "        [0.0201],\n",
      "        [0.0398],\n",
      "        [0.0197],\n",
      "        [0.0651],\n",
      "        [0.0599],\n",
      "        [0.0493],\n",
      "        [0.0549],\n",
      "        [0.0194],\n",
      "        [0.0194],\n",
      "        [0.0005],\n",
      "        [0.0247],\n",
      "        [0.0035],\n",
      "        [0.0397],\n",
      "        [0.0004],\n",
      "        [0.0260],\n",
      "        [0.0231],\n",
      "        [0.0169],\n",
      "        [0.0230],\n",
      "        [0.0019],\n",
      "        [0.0397],\n",
      "        [0.0044],\n",
      "        [0.0141],\n",
      "        [0.0199],\n",
      "        [0.0233],\n",
      "        [0.0285],\n",
      "        [0.0325],\n",
      "        [0.0366],\n",
      "        [0.0403],\n",
      "        [0.0448],\n",
      "        [0.0507],\n",
      "        [0.0608],\n",
      "        [0.0663],\n",
      "        [0.0745],\n",
      "        [0.0828],\n",
      "        [0.0976],\n",
      "        [0.0980],\n",
      "        [0.1062],\n",
      "        [0.1097]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 8.141321897506714\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 51\n",
      "剩餘X 資料 torch.Size([109, 18])\n",
      "剩餘Y 資料 torch.Size([109, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.012253833003342152, 3)\n",
      "The second_loss value of k: (0.01300202589482069, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.9800])\n",
      "目前模型的Data狀態 torch.Size([51, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8693],\n",
      "        [0.8693],\n",
      "        [0.8696],\n",
      "        [0.8693],\n",
      "        [0.9123],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [1.0015],\n",
      "        [0.9483],\n",
      "        [0.9372],\n",
      "        [0.9435],\n",
      "        [0.9713],\n",
      "        [0.8693],\n",
      "        [0.9771],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8929],\n",
      "        [0.8693],\n",
      "        [0.8958],\n",
      "        [1.0044],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [1.0293],\n",
      "        [0.9331],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693],\n",
      "        [0.8693]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0654],\n",
      "        [0.0558],\n",
      "        [0.0275],\n",
      "        [0.0246],\n",
      "        [0.0233],\n",
      "        [0.0160],\n",
      "        [0.0146],\n",
      "        [0.0239],\n",
      "        [0.0765],\n",
      "        [0.0628],\n",
      "        [0.0378],\n",
      "        [0.0657],\n",
      "        [0.0201],\n",
      "        [0.0398],\n",
      "        [0.0197],\n",
      "        [0.0651],\n",
      "        [0.0599],\n",
      "        [0.0493],\n",
      "        [0.0549],\n",
      "        [0.0194],\n",
      "        [0.0194],\n",
      "        [0.0005],\n",
      "        [0.0247],\n",
      "        [0.0035],\n",
      "        [0.0397],\n",
      "        [0.0004],\n",
      "        [0.0260],\n",
      "        [0.0231],\n",
      "        [0.0169],\n",
      "        [0.0230],\n",
      "        [0.0019],\n",
      "        [0.0397],\n",
      "        [0.0044],\n",
      "        [0.0141],\n",
      "        [0.0199],\n",
      "        [0.0233],\n",
      "        [0.0285],\n",
      "        [0.0325],\n",
      "        [0.0366],\n",
      "        [0.0403],\n",
      "        [0.0448],\n",
      "        [0.0507],\n",
      "        [0.0608],\n",
      "        [0.0663],\n",
      "        [0.0745],\n",
      "        [0.0828],\n",
      "        [0.0976],\n",
      "        [0.0980],\n",
      "        [0.1062],\n",
      "        [0.1097],\n",
      "        [0.1107]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 65\n",
      "Number of shrink: 35\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0676],\n",
      "        [0.0580],\n",
      "        [0.0294],\n",
      "        [0.0268],\n",
      "        [0.0251],\n",
      "        [0.0182],\n",
      "        [0.0168],\n",
      "        [0.0261],\n",
      "        [0.0787],\n",
      "        [0.0650],\n",
      "        [0.0400],\n",
      "        [0.0679],\n",
      "        [0.0223],\n",
      "        [0.0420],\n",
      "        [0.0219],\n",
      "        [0.0673],\n",
      "        [0.0621],\n",
      "        [0.0516],\n",
      "        [0.0571],\n",
      "        [0.0212],\n",
      "        [0.0212],\n",
      "        [0.0023],\n",
      "        [0.0229],\n",
      "        [0.0015],\n",
      "        [0.0419],\n",
      "        [0.0024],\n",
      "        [0.0282],\n",
      "        [0.0253],\n",
      "        [0.0191],\n",
      "        [0.0212],\n",
      "        [0.0041],\n",
      "        [0.0378],\n",
      "        [0.0062],\n",
      "        [0.0119],\n",
      "        [0.0177],\n",
      "        [0.0211],\n",
      "        [0.0263],\n",
      "        [0.0303],\n",
      "        [0.0344],\n",
      "        [0.0381],\n",
      "        [0.0426],\n",
      "        [0.0484],\n",
      "        [0.0625],\n",
      "        [0.0643],\n",
      "        [0.0723],\n",
      "        [0.0806],\n",
      "        [0.0954],\n",
      "        [0.0958],\n",
      "        [0.1040],\n",
      "        [0.1075],\n",
      "        [0.1085]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 8.397133111953735\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 52\n",
      "剩餘X 資料 torch.Size([108, 18])\n",
      "剩餘Y 資料 torch.Size([108, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.012504961341619492, 2)\n",
      "The second_loss value of k: (0.014695818535983562, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.9834])\n",
      "目前模型的Data狀態 torch.Size([52, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.9141],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [1.0034],\n",
      "        [0.9502],\n",
      "        [0.9390],\n",
      "        [0.9453],\n",
      "        [0.9732],\n",
      "        [0.8715],\n",
      "        [0.9791],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8947],\n",
      "        [0.8715],\n",
      "        [0.8976],\n",
      "        [1.0062],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [1.0310],\n",
      "        [0.9351],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715],\n",
      "        [0.8715]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0676],\n",
      "        [0.0580],\n",
      "        [0.0294],\n",
      "        [0.0268],\n",
      "        [0.0251],\n",
      "        [0.0182],\n",
      "        [0.0168],\n",
      "        [0.0261],\n",
      "        [0.0787],\n",
      "        [0.0650],\n",
      "        [0.0400],\n",
      "        [0.0679],\n",
      "        [0.0223],\n",
      "        [0.0420],\n",
      "        [0.0219],\n",
      "        [0.0673],\n",
      "        [0.0621],\n",
      "        [0.0516],\n",
      "        [0.0571],\n",
      "        [0.0212],\n",
      "        [0.0212],\n",
      "        [0.0023],\n",
      "        [0.0229],\n",
      "        [0.0015],\n",
      "        [0.0419],\n",
      "        [0.0024],\n",
      "        [0.0282],\n",
      "        [0.0253],\n",
      "        [0.0191],\n",
      "        [0.0212],\n",
      "        [0.0041],\n",
      "        [0.0378],\n",
      "        [0.0062],\n",
      "        [0.0119],\n",
      "        [0.0177],\n",
      "        [0.0211],\n",
      "        [0.0263],\n",
      "        [0.0303],\n",
      "        [0.0344],\n",
      "        [0.0381],\n",
      "        [0.0426],\n",
      "        [0.0484],\n",
      "        [0.0625],\n",
      "        [0.0643],\n",
      "        [0.0723],\n",
      "        [0.0806],\n",
      "        [0.0954],\n",
      "        [0.0958],\n",
      "        [0.1040],\n",
      "        [0.1075],\n",
      "        [0.1085],\n",
      "        [0.1118]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 74\n",
      "Number of shrink: 26\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0703],\n",
      "        [0.0607],\n",
      "        [0.0322],\n",
      "        [0.0296],\n",
      "        [0.0252],\n",
      "        [0.0209],\n",
      "        [0.0195],\n",
      "        [0.0289],\n",
      "        [0.0814],\n",
      "        [0.0678],\n",
      "        [0.0428],\n",
      "        [0.0707],\n",
      "        [0.0251],\n",
      "        [0.0448],\n",
      "        [0.0247],\n",
      "        [0.0700],\n",
      "        [0.0649],\n",
      "        [0.0543],\n",
      "        [0.0598],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0022],\n",
      "        [0.0251],\n",
      "        [0.0006],\n",
      "        [0.0446],\n",
      "        [0.0049],\n",
      "        [0.0310],\n",
      "        [0.0280],\n",
      "        [0.0219],\n",
      "        [0.0208],\n",
      "        [0.0069],\n",
      "        [0.0377],\n",
      "        [0.0036],\n",
      "        [0.0092],\n",
      "        [0.0149],\n",
      "        [0.0183],\n",
      "        [0.0235],\n",
      "        [0.0275],\n",
      "        [0.0316],\n",
      "        [0.0353],\n",
      "        [0.0398],\n",
      "        [0.0457],\n",
      "        [0.0594],\n",
      "        [0.0613],\n",
      "        [0.0695],\n",
      "        [0.0778],\n",
      "        [0.0926],\n",
      "        [0.0930],\n",
      "        [0.1012],\n",
      "        [0.1047],\n",
      "        [0.1057],\n",
      "        [0.1091]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 8.649002075195312\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 53\n",
      "剩餘X 資料 torch.Size([107, 18])\n",
      "剩餘Y 資料 torch.Size([107, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.014031571336090565, 0)\n",
      "The second_loss value of k: (0.015119022689759731, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.9928])\n",
      "目前模型的Data狀態 torch.Size([53, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.9141],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [1.0041],\n",
      "        [0.9511],\n",
      "        [0.9389],\n",
      "        [0.9431],\n",
      "        [0.9753],\n",
      "        [0.8743],\n",
      "        [0.9816],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8951],\n",
      "        [0.8743],\n",
      "        [0.8978],\n",
      "        [1.0036],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [1.0279],\n",
      "        [0.9381],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743],\n",
      "        [0.8743]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0703],\n",
      "        [0.0607],\n",
      "        [0.0322],\n",
      "        [0.0296],\n",
      "        [0.0252],\n",
      "        [0.0209],\n",
      "        [0.0195],\n",
      "        [0.0289],\n",
      "        [0.0814],\n",
      "        [0.0678],\n",
      "        [0.0428],\n",
      "        [0.0707],\n",
      "        [0.0251],\n",
      "        [0.0448],\n",
      "        [0.0247],\n",
      "        [0.0700],\n",
      "        [0.0649],\n",
      "        [0.0543],\n",
      "        [0.0598],\n",
      "        [0.0220],\n",
      "        [0.0221],\n",
      "        [0.0022],\n",
      "        [0.0251],\n",
      "        [0.0006],\n",
      "        [0.0446],\n",
      "        [0.0049],\n",
      "        [0.0310],\n",
      "        [0.0280],\n",
      "        [0.0219],\n",
      "        [0.0208],\n",
      "        [0.0069],\n",
      "        [0.0377],\n",
      "        [0.0036],\n",
      "        [0.0092],\n",
      "        [0.0149],\n",
      "        [0.0183],\n",
      "        [0.0235],\n",
      "        [0.0275],\n",
      "        [0.0316],\n",
      "        [0.0353],\n",
      "        [0.0398],\n",
      "        [0.0457],\n",
      "        [0.0594],\n",
      "        [0.0613],\n",
      "        [0.0695],\n",
      "        [0.0778],\n",
      "        [0.0926],\n",
      "        [0.0930],\n",
      "        [0.1012],\n",
      "        [0.1047],\n",
      "        [0.1057],\n",
      "        [0.1091],\n",
      "        [0.1185]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0731],\n",
      "        [0.0635],\n",
      "        [0.0349],\n",
      "        [0.0324],\n",
      "        [0.0246],\n",
      "        [0.0237],\n",
      "        [0.0223],\n",
      "        [0.0316],\n",
      "        [0.0842],\n",
      "        [0.0705],\n",
      "        [0.0455],\n",
      "        [0.0734],\n",
      "        [0.0279],\n",
      "        [0.0475],\n",
      "        [0.0275],\n",
      "        [0.0728],\n",
      "        [0.0676],\n",
      "        [0.0571],\n",
      "        [0.0626],\n",
      "        [0.0212],\n",
      "        [0.0215],\n",
      "        [0.0016],\n",
      "        [0.0268],\n",
      "        [0.0013],\n",
      "        [0.0474],\n",
      "        [0.0061],\n",
      "        [0.0338],\n",
      "        [0.0308],\n",
      "        [0.0246],\n",
      "        [0.0209],\n",
      "        [0.0096],\n",
      "        [0.0376],\n",
      "        [0.0013],\n",
      "        [0.0064],\n",
      "        [0.0122],\n",
      "        [0.0155],\n",
      "        [0.0208],\n",
      "        [0.0248],\n",
      "        [0.0289],\n",
      "        [0.0325],\n",
      "        [0.0371],\n",
      "        [0.0429],\n",
      "        [0.0566],\n",
      "        [0.0596],\n",
      "        [0.0667],\n",
      "        [0.0751],\n",
      "        [0.0899],\n",
      "        [0.0903],\n",
      "        [0.0985],\n",
      "        [0.1020],\n",
      "        [0.1030],\n",
      "        [0.1063],\n",
      "        [0.1157]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 8.902260065078735\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 54\n",
      "剩餘X 資料 torch.Size([106, 18])\n",
      "剩餘Y 資料 torch.Size([106, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.014450069516897202, 0)\n",
      "The second_loss value of k: (0.017409423366189003, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.9973])\n",
      "目前模型的Data狀態 torch.Size([54, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.9135],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [1.0033],\n",
      "        [0.9505],\n",
      "        [0.9383],\n",
      "        [0.9414],\n",
      "        [0.9760],\n",
      "        [0.8770],\n",
      "        [0.9828],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8951],\n",
      "        [0.8770],\n",
      "        [0.8978],\n",
      "        [1.0013],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [1.0251],\n",
      "        [0.9398],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770],\n",
      "        [0.8770]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0731],\n",
      "        [0.0635],\n",
      "        [0.0349],\n",
      "        [0.0324],\n",
      "        [0.0246],\n",
      "        [0.0237],\n",
      "        [0.0223],\n",
      "        [0.0316],\n",
      "        [0.0842],\n",
      "        [0.0705],\n",
      "        [0.0455],\n",
      "        [0.0734],\n",
      "        [0.0279],\n",
      "        [0.0475],\n",
      "        [0.0275],\n",
      "        [0.0728],\n",
      "        [0.0676],\n",
      "        [0.0571],\n",
      "        [0.0626],\n",
      "        [0.0212],\n",
      "        [0.0215],\n",
      "        [0.0016],\n",
      "        [0.0268],\n",
      "        [0.0013],\n",
      "        [0.0474],\n",
      "        [0.0061],\n",
      "        [0.0338],\n",
      "        [0.0308],\n",
      "        [0.0246],\n",
      "        [0.0209],\n",
      "        [0.0096],\n",
      "        [0.0376],\n",
      "        [0.0013],\n",
      "        [0.0064],\n",
      "        [0.0122],\n",
      "        [0.0155],\n",
      "        [0.0208],\n",
      "        [0.0248],\n",
      "        [0.0289],\n",
      "        [0.0325],\n",
      "        [0.0371],\n",
      "        [0.0429],\n",
      "        [0.0566],\n",
      "        [0.0596],\n",
      "        [0.0667],\n",
      "        [0.0751],\n",
      "        [0.0899],\n",
      "        [0.0903],\n",
      "        [0.0985],\n",
      "        [0.1020],\n",
      "        [0.1030],\n",
      "        [0.1063],\n",
      "        [0.1157],\n",
      "        [0.1202]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0760],\n",
      "        [0.0664],\n",
      "        [0.0378],\n",
      "        [0.0352],\n",
      "        [0.0245],\n",
      "        [0.0266],\n",
      "        [0.0252],\n",
      "        [0.0345],\n",
      "        [0.0870],\n",
      "        [0.0734],\n",
      "        [0.0484],\n",
      "        [0.0763],\n",
      "        [0.0307],\n",
      "        [0.0504],\n",
      "        [0.0303],\n",
      "        [0.0756],\n",
      "        [0.0705],\n",
      "        [0.0599],\n",
      "        [0.0655],\n",
      "        [0.0205],\n",
      "        [0.0209],\n",
      "        [0.0016],\n",
      "        [0.0273],\n",
      "        [0.0019],\n",
      "        [0.0503],\n",
      "        [0.0073],\n",
      "        [0.0366],\n",
      "        [0.0337],\n",
      "        [0.0275],\n",
      "        [0.0204],\n",
      "        [0.0125],\n",
      "        [0.0369],\n",
      "        [0.0001],\n",
      "        [0.0036],\n",
      "        [0.0093],\n",
      "        [0.0127],\n",
      "        [0.0179],\n",
      "        [0.0219],\n",
      "        [0.0260],\n",
      "        [0.0297],\n",
      "        [0.0342],\n",
      "        [0.0401],\n",
      "        [0.0549],\n",
      "        [0.0581],\n",
      "        [0.0639],\n",
      "        [0.0722],\n",
      "        [0.0870],\n",
      "        [0.0874],\n",
      "        [0.0956],\n",
      "        [0.0991],\n",
      "        [0.1001],\n",
      "        [0.1034],\n",
      "        [0.1128],\n",
      "        [0.1173]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 9.155510187149048\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 55\n",
      "剩餘X 資料 torch.Size([105, 18])\n",
      "剩餘Y 資料 torch.Size([105, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.016846265643835068, 3)\n",
      "The second_loss value of k: (0.029583197087049484, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.9530])\n",
      "目前模型的Data狀態 torch.Size([55, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.9134],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [1.0026],\n",
      "        [0.9498],\n",
      "        [0.9383],\n",
      "        [0.9408],\n",
      "        [0.9767],\n",
      "        [0.8799],\n",
      "        [0.9840],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8955],\n",
      "        [0.8799],\n",
      "        [0.8986],\n",
      "        [1.0001],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [1.0234],\n",
      "        [0.9413],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [0.8799],\n",
      "        [1.0828]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0760],\n",
      "        [    0.0664],\n",
      "        [    0.0378],\n",
      "        [    0.0352],\n",
      "        [    0.0245],\n",
      "        [    0.0266],\n",
      "        [    0.0252],\n",
      "        [    0.0345],\n",
      "        [    0.0870],\n",
      "        [    0.0734],\n",
      "        [    0.0484],\n",
      "        [    0.0763],\n",
      "        [    0.0307],\n",
      "        [    0.0504],\n",
      "        [    0.0303],\n",
      "        [    0.0756],\n",
      "        [    0.0705],\n",
      "        [    0.0599],\n",
      "        [    0.0655],\n",
      "        [    0.0205],\n",
      "        [    0.0209],\n",
      "        [    0.0016],\n",
      "        [    0.0273],\n",
      "        [    0.0019],\n",
      "        [    0.0503],\n",
      "        [    0.0073],\n",
      "        [    0.0366],\n",
      "        [    0.0337],\n",
      "        [    0.0275],\n",
      "        [    0.0204],\n",
      "        [    0.0125],\n",
      "        [    0.0369],\n",
      "        [    0.0001],\n",
      "        [    0.0036],\n",
      "        [    0.0093],\n",
      "        [    0.0127],\n",
      "        [    0.0179],\n",
      "        [    0.0219],\n",
      "        [    0.0260],\n",
      "        [    0.0297],\n",
      "        [    0.0342],\n",
      "        [    0.0401],\n",
      "        [    0.0549],\n",
      "        [    0.0581],\n",
      "        [    0.0639],\n",
      "        [    0.0722],\n",
      "        [    0.0870],\n",
      "        [    0.0874],\n",
      "        [    0.0956],\n",
      "        [    0.0991],\n",
      "        [    0.1001],\n",
      "        [    0.1034],\n",
      "        [    0.1128],\n",
      "        [    0.1173],\n",
      "        [    0.1298]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0757],\n",
      "        [0.0661],\n",
      "        [0.0375],\n",
      "        [0.0349],\n",
      "        [0.0191],\n",
      "        [0.0263],\n",
      "        [0.0249],\n",
      "        [0.0342],\n",
      "        [0.0867],\n",
      "        [0.0731],\n",
      "        [0.0481],\n",
      "        [0.0760],\n",
      "        [0.0304],\n",
      "        [0.0501],\n",
      "        [0.0300],\n",
      "        [0.0753],\n",
      "        [0.0702],\n",
      "        [0.0596],\n",
      "        [0.0652],\n",
      "        [0.0169],\n",
      "        [0.0190],\n",
      "        [0.0061],\n",
      "        [0.0455],\n",
      "        [0.0010],\n",
      "        [0.0500],\n",
      "        [0.0037],\n",
      "        [0.0363],\n",
      "        [0.0334],\n",
      "        [0.0272],\n",
      "        [0.0265],\n",
      "        [0.0122],\n",
      "        [0.0457],\n",
      "        [0.0196],\n",
      "        [0.0039],\n",
      "        [0.0096],\n",
      "        [0.0130],\n",
      "        [0.0182],\n",
      "        [0.0222],\n",
      "        [0.0263],\n",
      "        [0.0300],\n",
      "        [0.0345],\n",
      "        [0.0404],\n",
      "        [0.0350],\n",
      "        [0.0597],\n",
      "        [0.0642],\n",
      "        [0.0725],\n",
      "        [0.0873],\n",
      "        [0.0877],\n",
      "        [0.0959],\n",
      "        [0.0994],\n",
      "        [0.1004],\n",
      "        [0.1037],\n",
      "        [0.1131],\n",
      "        [0.1176],\n",
      "        [0.1063]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 9.406365633010864\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 56\n",
      "剩餘X 資料 torch.Size([104, 18])\n",
      "剩餘Y 資料 torch.Size([104, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.026492200791835785, 0)\n",
      "The second_loss value of k: (0.029483305290341377, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.8400])\n",
      "目前模型的Data狀態 torch.Size([56, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.9080],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.9990],\n",
      "        [0.9480],\n",
      "        [0.9306],\n",
      "        [0.9227],\n",
      "        [0.9737],\n",
      "        [0.8796],\n",
      "        [0.9804],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8894],\n",
      "        [0.8796],\n",
      "        [0.8898],\n",
      "        [0.9804],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [1.0034],\n",
      "        [0.9397],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [0.8796],\n",
      "        [1.0593],\n",
      "        [1.0028]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0757],\n",
      "        [0.0661],\n",
      "        [0.0375],\n",
      "        [0.0349],\n",
      "        [0.0191],\n",
      "        [0.0263],\n",
      "        [0.0249],\n",
      "        [0.0342],\n",
      "        [0.0867],\n",
      "        [0.0731],\n",
      "        [0.0481],\n",
      "        [0.0760],\n",
      "        [0.0304],\n",
      "        [0.0501],\n",
      "        [0.0300],\n",
      "        [0.0753],\n",
      "        [0.0702],\n",
      "        [0.0596],\n",
      "        [0.0652],\n",
      "        [0.0169],\n",
      "        [0.0190],\n",
      "        [0.0061],\n",
      "        [0.0455],\n",
      "        [0.0010],\n",
      "        [0.0500],\n",
      "        [0.0037],\n",
      "        [0.0363],\n",
      "        [0.0334],\n",
      "        [0.0272],\n",
      "        [0.0265],\n",
      "        [0.0122],\n",
      "        [0.0457],\n",
      "        [0.0196],\n",
      "        [0.0039],\n",
      "        [0.0096],\n",
      "        [0.0130],\n",
      "        [0.0182],\n",
      "        [0.0222],\n",
      "        [0.0263],\n",
      "        [0.0300],\n",
      "        [0.0345],\n",
      "        [0.0404],\n",
      "        [0.0350],\n",
      "        [0.0597],\n",
      "        [0.0642],\n",
      "        [0.0725],\n",
      "        [0.0873],\n",
      "        [0.0877],\n",
      "        [0.0959],\n",
      "        [0.0994],\n",
      "        [0.1004],\n",
      "        [0.1037],\n",
      "        [0.1131],\n",
      "        [0.1176],\n",
      "        [0.1063],\n",
      "        [0.1628]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0774],\n",
      "        [0.0678],\n",
      "        [0.0392],\n",
      "        [0.0366],\n",
      "        [0.0022],\n",
      "        [0.0280],\n",
      "        [0.0266],\n",
      "        [0.0359],\n",
      "        [0.0885],\n",
      "        [0.0748],\n",
      "        [0.0498],\n",
      "        [0.0777],\n",
      "        [0.0321],\n",
      "        [0.0518],\n",
      "        [0.0317],\n",
      "        [0.0771],\n",
      "        [0.0719],\n",
      "        [0.0613],\n",
      "        [0.0669],\n",
      "        [0.0056],\n",
      "        [0.0110],\n",
      "        [0.0280],\n",
      "        [0.0745],\n",
      "        [0.0084],\n",
      "        [0.0517],\n",
      "        [0.0043],\n",
      "        [0.0380],\n",
      "        [0.0351],\n",
      "        [0.0289],\n",
      "        [0.0346],\n",
      "        [0.0139],\n",
      "        [0.0541],\n",
      "        [0.0509],\n",
      "        [0.0021],\n",
      "        [0.0079],\n",
      "        [0.0113],\n",
      "        [0.0165],\n",
      "        [0.0205],\n",
      "        [0.0246],\n",
      "        [0.0283],\n",
      "        [0.0328],\n",
      "        [0.0387],\n",
      "        [0.0036],\n",
      "        [0.0662],\n",
      "        [0.0625],\n",
      "        [0.0708],\n",
      "        [0.0856],\n",
      "        [0.0860],\n",
      "        [0.0942],\n",
      "        [0.0977],\n",
      "        [0.0987],\n",
      "        [0.1020],\n",
      "        [0.1114],\n",
      "        [0.1159],\n",
      "        [0.0707],\n",
      "        [0.1279]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 9.655922174453735\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 57\n",
      "剩餘X 資料 torch.Size([103, 18])\n",
      "剩餘Y 資料 torch.Size([103, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.01986551471054554, 3)\n",
      "The second_loss value of k: (0.030071957036852837, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.9594])\n",
      "目前模型的Data狀態 torch.Size([57, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8868],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.9877],\n",
      "        [0.9399],\n",
      "        [0.9087],\n",
      "        [0.8937],\n",
      "        [0.9664],\n",
      "        [0.8813],\n",
      "        [0.9724],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.9491],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.9721],\n",
      "        [0.9332],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [0.8813],\n",
      "        [1.0237],\n",
      "        [0.9680],\n",
      "        [1.1003]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0774],\n",
      "        [0.0678],\n",
      "        [0.0392],\n",
      "        [0.0366],\n",
      "        [0.0022],\n",
      "        [0.0280],\n",
      "        [0.0266],\n",
      "        [0.0359],\n",
      "        [0.0885],\n",
      "        [0.0748],\n",
      "        [0.0498],\n",
      "        [0.0777],\n",
      "        [0.0321],\n",
      "        [0.0518],\n",
      "        [0.0317],\n",
      "        [0.0771],\n",
      "        [0.0719],\n",
      "        [0.0613],\n",
      "        [0.0669],\n",
      "        [0.0056],\n",
      "        [0.0110],\n",
      "        [0.0280],\n",
      "        [0.0745],\n",
      "        [0.0084],\n",
      "        [0.0517],\n",
      "        [0.0043],\n",
      "        [0.0380],\n",
      "        [0.0351],\n",
      "        [0.0289],\n",
      "        [0.0346],\n",
      "        [0.0139],\n",
      "        [0.0541],\n",
      "        [0.0509],\n",
      "        [0.0021],\n",
      "        [0.0079],\n",
      "        [0.0113],\n",
      "        [0.0165],\n",
      "        [0.0205],\n",
      "        [0.0246],\n",
      "        [0.0283],\n",
      "        [0.0328],\n",
      "        [0.0387],\n",
      "        [0.0036],\n",
      "        [0.0662],\n",
      "        [0.0625],\n",
      "        [0.0708],\n",
      "        [0.0856],\n",
      "        [0.0860],\n",
      "        [0.0942],\n",
      "        [0.0977],\n",
      "        [0.0987],\n",
      "        [0.1020],\n",
      "        [0.1114],\n",
      "        [0.1159],\n",
      "        [0.0707],\n",
      "        [0.1279],\n",
      "        [0.1409]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0799],\n",
      "        [0.0703],\n",
      "        [0.0417],\n",
      "        [0.0391],\n",
      "        [0.0051],\n",
      "        [0.0305],\n",
      "        [0.0291],\n",
      "        [0.0384],\n",
      "        [0.0909],\n",
      "        [0.0773],\n",
      "        [0.0523],\n",
      "        [0.0802],\n",
      "        [0.0346],\n",
      "        [0.0543],\n",
      "        [0.0342],\n",
      "        [0.0795],\n",
      "        [0.0744],\n",
      "        [0.0638],\n",
      "        [0.0694],\n",
      "        [0.0013],\n",
      "        [0.0112],\n",
      "        [0.0476],\n",
      "        [0.0844],\n",
      "        [0.0051],\n",
      "        [0.0542],\n",
      "        [0.0013],\n",
      "        [0.0405],\n",
      "        [0.0376],\n",
      "        [0.0314],\n",
      "        [0.0321],\n",
      "        [0.0164],\n",
      "        [0.0516],\n",
      "        [0.0810],\n",
      "        [0.0003],\n",
      "        [0.0054],\n",
      "        [0.0088],\n",
      "        [0.0140],\n",
      "        [0.0180],\n",
      "        [0.0221],\n",
      "        [0.0258],\n",
      "        [0.0303],\n",
      "        [0.0362],\n",
      "        [0.0274],\n",
      "        [0.0604],\n",
      "        [0.0600],\n",
      "        [0.0683],\n",
      "        [0.0831],\n",
      "        [0.0835],\n",
      "        [0.0917],\n",
      "        [0.0952],\n",
      "        [0.0962],\n",
      "        [0.0995],\n",
      "        [0.1089],\n",
      "        [0.1134],\n",
      "        [0.0330],\n",
      "        [0.0896],\n",
      "        [0.0912]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 9.906113624572754\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 58\n",
      "剩餘X 資料 torch.Size([102, 18])\n",
      "剩餘Y 資料 torch.Size([102, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.025472071021795273, 0)\n",
      "The second_loss value of k: (0.03094206191599369, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.8425])\n",
      "目前模型的Data狀態 torch.Size([58, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.9808],\n",
      "        [0.9402],\n",
      "        [0.8890],\n",
      "        [0.8838],\n",
      "        [0.9697],\n",
      "        [0.8838],\n",
      "        [0.9754],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.9190],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.9410],\n",
      "        [0.9390],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.8838],\n",
      "        [0.9860],\n",
      "        [0.9297],\n",
      "        [1.0505],\n",
      "        [1.0021]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0799],\n",
      "        [0.0703],\n",
      "        [0.0417],\n",
      "        [0.0391],\n",
      "        [0.0051],\n",
      "        [0.0305],\n",
      "        [0.0291],\n",
      "        [0.0384],\n",
      "        [0.0909],\n",
      "        [0.0773],\n",
      "        [0.0523],\n",
      "        [0.0802],\n",
      "        [0.0346],\n",
      "        [0.0543],\n",
      "        [0.0342],\n",
      "        [0.0795],\n",
      "        [0.0744],\n",
      "        [0.0638],\n",
      "        [0.0694],\n",
      "        [0.0013],\n",
      "        [0.0112],\n",
      "        [0.0476],\n",
      "        [0.0844],\n",
      "        [0.0051],\n",
      "        [0.0542],\n",
      "        [0.0013],\n",
      "        [0.0405],\n",
      "        [0.0376],\n",
      "        [0.0314],\n",
      "        [0.0321],\n",
      "        [0.0164],\n",
      "        [0.0516],\n",
      "        [0.0810],\n",
      "        [0.0003],\n",
      "        [0.0054],\n",
      "        [0.0088],\n",
      "        [0.0140],\n",
      "        [0.0180],\n",
      "        [0.0221],\n",
      "        [0.0258],\n",
      "        [0.0303],\n",
      "        [0.0362],\n",
      "        [0.0274],\n",
      "        [0.0604],\n",
      "        [0.0600],\n",
      "        [0.0683],\n",
      "        [0.0831],\n",
      "        [0.0835],\n",
      "        [0.0917],\n",
      "        [0.0952],\n",
      "        [0.0962],\n",
      "        [0.0995],\n",
      "        [0.1089],\n",
      "        [0.1134],\n",
      "        [0.0330],\n",
      "        [0.0896],\n",
      "        [0.0912],\n",
      "        [0.1596]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0814],\n",
      "        [0.0718],\n",
      "        [0.0432],\n",
      "        [0.0407],\n",
      "        [0.0036],\n",
      "        [0.0320],\n",
      "        [0.0306],\n",
      "        [0.0399],\n",
      "        [0.0925],\n",
      "        [0.0788],\n",
      "        [0.0538],\n",
      "        [0.0817],\n",
      "        [0.0362],\n",
      "        [0.0558],\n",
      "        [0.0358],\n",
      "        [0.0811],\n",
      "        [0.0759],\n",
      "        [0.0654],\n",
      "        [0.0709],\n",
      "        [0.0097],\n",
      "        [0.0065],\n",
      "        [0.0513],\n",
      "        [0.0828],\n",
      "        [0.0085],\n",
      "        [0.0557],\n",
      "        [0.0060],\n",
      "        [0.0421],\n",
      "        [0.0391],\n",
      "        [0.0330],\n",
      "        [0.0305],\n",
      "        [0.0180],\n",
      "        [0.0501],\n",
      "        [0.1081],\n",
      "        [0.0019],\n",
      "        [0.0039],\n",
      "        [0.0072],\n",
      "        [0.0125],\n",
      "        [0.0165],\n",
      "        [0.0206],\n",
      "        [0.0242],\n",
      "        [0.0287],\n",
      "        [0.0346],\n",
      "        [0.0550],\n",
      "        [0.0630],\n",
      "        [0.0584],\n",
      "        [0.0668],\n",
      "        [0.0816],\n",
      "        [0.0820],\n",
      "        [0.0902],\n",
      "        [0.0937],\n",
      "        [0.0947],\n",
      "        [0.0980],\n",
      "        [0.1074],\n",
      "        [0.1119],\n",
      "        [0.0016],\n",
      "        [0.0583],\n",
      "        [0.0570],\n",
      "        [0.1264]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 10.1629638671875\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 59\n",
      "剩餘X 資料 torch.Size([101, 18])\n",
      "剩餘Y 資料 torch.Size([101, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.02640858292579651, 2)\n",
      "The second_loss value of k: (0.03148839622735977, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.9552])\n",
      "目前模型的Data狀態 torch.Size([59, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.9724],\n",
      "        [0.9355],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.9662],\n",
      "        [0.8854],\n",
      "        [0.9707],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8919],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.9135],\n",
      "        [0.9364],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.8854],\n",
      "        [0.9546],\n",
      "        [0.8983],\n",
      "        [1.0164],\n",
      "        [0.9689],\n",
      "        [1.1178]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0814],\n",
      "        [0.0718],\n",
      "        [0.0432],\n",
      "        [0.0407],\n",
      "        [0.0036],\n",
      "        [0.0320],\n",
      "        [0.0306],\n",
      "        [0.0399],\n",
      "        [0.0925],\n",
      "        [0.0788],\n",
      "        [0.0538],\n",
      "        [0.0817],\n",
      "        [0.0362],\n",
      "        [0.0558],\n",
      "        [0.0358],\n",
      "        [0.0811],\n",
      "        [0.0759],\n",
      "        [0.0654],\n",
      "        [0.0709],\n",
      "        [0.0097],\n",
      "        [0.0065],\n",
      "        [0.0513],\n",
      "        [0.0828],\n",
      "        [0.0085],\n",
      "        [0.0557],\n",
      "        [0.0060],\n",
      "        [0.0421],\n",
      "        [0.0391],\n",
      "        [0.0330],\n",
      "        [0.0305],\n",
      "        [0.0180],\n",
      "        [0.0501],\n",
      "        [0.1081],\n",
      "        [0.0019],\n",
      "        [0.0039],\n",
      "        [0.0072],\n",
      "        [0.0125],\n",
      "        [0.0165],\n",
      "        [0.0206],\n",
      "        [0.0242],\n",
      "        [0.0287],\n",
      "        [0.0346],\n",
      "        [0.0550],\n",
      "        [0.0630],\n",
      "        [0.0584],\n",
      "        [0.0668],\n",
      "        [0.0816],\n",
      "        [0.0820],\n",
      "        [0.0902],\n",
      "        [0.0937],\n",
      "        [0.0947],\n",
      "        [0.0980],\n",
      "        [0.1074],\n",
      "        [0.1119],\n",
      "        [0.0016],\n",
      "        [0.0583],\n",
      "        [0.0570],\n",
      "        [0.1264],\n",
      "        [0.1625]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0853],\n",
      "        [    0.0757],\n",
      "        [    0.0471],\n",
      "        [    0.0445],\n",
      "        [    0.0003],\n",
      "        [    0.0359],\n",
      "        [    0.0345],\n",
      "        [    0.0438],\n",
      "        [    0.0963],\n",
      "        [    0.0827],\n",
      "        [    0.0577],\n",
      "        [    0.0856],\n",
      "        [    0.0400],\n",
      "        [    0.0597],\n",
      "        [    0.0396],\n",
      "        [    0.0849],\n",
      "        [    0.0798],\n",
      "        [    0.0692],\n",
      "        [    0.0748],\n",
      "        [    0.0222],\n",
      "        [    0.0005],\n",
      "        [    0.0474],\n",
      "        [    0.0790],\n",
      "        [    0.0138],\n",
      "        [    0.0596],\n",
      "        [    0.0142],\n",
      "        [    0.0459],\n",
      "        [    0.0430],\n",
      "        [    0.0368],\n",
      "        [    0.0267],\n",
      "        [    0.0218],\n",
      "        [    0.0462],\n",
      "        [    0.1108],\n",
      "        [    0.0057],\n",
      "        [    0.0000],\n",
      "        [    0.0034],\n",
      "        [    0.0086],\n",
      "        [    0.0126],\n",
      "        [    0.0167],\n",
      "        [    0.0204],\n",
      "        [    0.0249],\n",
      "        [    0.0308],\n",
      "        [    0.0793],\n",
      "        [    0.0665],\n",
      "        [    0.0546],\n",
      "        [    0.0629],\n",
      "        [    0.0777],\n",
      "        [    0.0781],\n",
      "        [    0.0863],\n",
      "        [    0.0898],\n",
      "        [    0.0908],\n",
      "        [    0.0941],\n",
      "        [    0.1035],\n",
      "        [    0.1080],\n",
      "        [    0.0450],\n",
      "        [    0.0492],\n",
      "        [    0.0022],\n",
      "        [    0.0775],\n",
      "        [    0.0998]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 10.413843631744385\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 60\n",
      "剩餘X 資料 torch.Size([100, 18])\n",
      "剩餘Y 資料 torch.Size([100, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.01665356755256653, 4)\n",
      "The second_loss value of k: (0.023229744285345078, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引4，y= tensor([0.8617])\n",
      "目前模型的Data狀態 torch.Size([60, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.9599],\n",
      "        [0.9284],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.9609],\n",
      "        [0.8892],\n",
      "        [0.9625],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.9329],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.8892],\n",
      "        [0.9080],\n",
      "        [0.8892],\n",
      "        [0.9616],\n",
      "        [0.9200],\n",
      "        [1.0551],\n",
      "        [0.9907]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0853],\n",
      "        [    0.0757],\n",
      "        [    0.0471],\n",
      "        [    0.0445],\n",
      "        [    0.0003],\n",
      "        [    0.0359],\n",
      "        [    0.0345],\n",
      "        [    0.0438],\n",
      "        [    0.0963],\n",
      "        [    0.0827],\n",
      "        [    0.0577],\n",
      "        [    0.0856],\n",
      "        [    0.0400],\n",
      "        [    0.0597],\n",
      "        [    0.0396],\n",
      "        [    0.0849],\n",
      "        [    0.0798],\n",
      "        [    0.0692],\n",
      "        [    0.0748],\n",
      "        [    0.0222],\n",
      "        [    0.0005],\n",
      "        [    0.0474],\n",
      "        [    0.0790],\n",
      "        [    0.0138],\n",
      "        [    0.0596],\n",
      "        [    0.0142],\n",
      "        [    0.0459],\n",
      "        [    0.0430],\n",
      "        [    0.0368],\n",
      "        [    0.0267],\n",
      "        [    0.0218],\n",
      "        [    0.0462],\n",
      "        [    0.1108],\n",
      "        [    0.0057],\n",
      "        [    0.0000],\n",
      "        [    0.0034],\n",
      "        [    0.0086],\n",
      "        [    0.0126],\n",
      "        [    0.0167],\n",
      "        [    0.0204],\n",
      "        [    0.0249],\n",
      "        [    0.0308],\n",
      "        [    0.0793],\n",
      "        [    0.0665],\n",
      "        [    0.0546],\n",
      "        [    0.0629],\n",
      "        [    0.0777],\n",
      "        [    0.0781],\n",
      "        [    0.0863],\n",
      "        [    0.0898],\n",
      "        [    0.0908],\n",
      "        [    0.0941],\n",
      "        [    0.1035],\n",
      "        [    0.1080],\n",
      "        [    0.0450],\n",
      "        [    0.0492],\n",
      "        [    0.0022],\n",
      "        [    0.0775],\n",
      "        [    0.0998],\n",
      "        [    0.1290]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0854],\n",
      "        [0.0758],\n",
      "        [0.0473],\n",
      "        [0.0469],\n",
      "        [0.0004],\n",
      "        [0.0361],\n",
      "        [0.0346],\n",
      "        [0.0440],\n",
      "        [0.0965],\n",
      "        [0.0829],\n",
      "        [0.0579],\n",
      "        [0.0858],\n",
      "        [0.0402],\n",
      "        [0.0599],\n",
      "        [0.0398],\n",
      "        [0.0851],\n",
      "        [0.0800],\n",
      "        [0.0694],\n",
      "        [0.0749],\n",
      "        [0.0268],\n",
      "        [0.0024],\n",
      "        [0.0473],\n",
      "        [0.0788],\n",
      "        [0.0137],\n",
      "        [0.0597],\n",
      "        [0.0160],\n",
      "        [0.0461],\n",
      "        [0.0431],\n",
      "        [0.0370],\n",
      "        [0.0265],\n",
      "        [0.0220],\n",
      "        [0.0460],\n",
      "        [0.1106],\n",
      "        [0.0059],\n",
      "        [0.0002],\n",
      "        [0.0032],\n",
      "        [0.0084],\n",
      "        [0.0124],\n",
      "        [0.0165],\n",
      "        [0.0202],\n",
      "        [0.0247],\n",
      "        [0.0306],\n",
      "        [0.0791],\n",
      "        [0.0658],\n",
      "        [0.0544],\n",
      "        [0.0627],\n",
      "        [0.0775],\n",
      "        [0.0779],\n",
      "        [0.0861],\n",
      "        [0.0896],\n",
      "        [0.0906],\n",
      "        [0.0940],\n",
      "        [0.1034],\n",
      "        [0.1079],\n",
      "        [0.0636],\n",
      "        [0.0494],\n",
      "        [0.0317],\n",
      "        [0.0470],\n",
      "        [0.0629],\n",
      "        [0.0881]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 10.665643692016602\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 61\n",
      "剩餘X 資料 torch.Size([99, 18])\n",
      "剩餘Y 資料 torch.Size([99, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.013923557475209236, 0)\n",
      "The second_loss value of k: (0.015413438901305199, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.8635])\n",
      "目前模型的Data狀態 torch.Size([61, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8916],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.9554],\n",
      "        [0.9266],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.9611],\n",
      "        [0.8894],\n",
      "        [0.9607],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.9336],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.8894],\n",
      "        [0.9277],\n",
      "        [0.8895],\n",
      "        [1.0181],\n",
      "        [0.9497],\n",
      "        [0.9815]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0854],\n",
      "        [0.0758],\n",
      "        [0.0473],\n",
      "        [0.0469],\n",
      "        [0.0004],\n",
      "        [0.0361],\n",
      "        [0.0346],\n",
      "        [0.0440],\n",
      "        [0.0965],\n",
      "        [0.0829],\n",
      "        [0.0579],\n",
      "        [0.0858],\n",
      "        [0.0402],\n",
      "        [0.0599],\n",
      "        [0.0398],\n",
      "        [0.0851],\n",
      "        [0.0800],\n",
      "        [0.0694],\n",
      "        [0.0749],\n",
      "        [0.0268],\n",
      "        [0.0024],\n",
      "        [0.0473],\n",
      "        [0.0788],\n",
      "        [0.0137],\n",
      "        [0.0597],\n",
      "        [0.0160],\n",
      "        [0.0461],\n",
      "        [0.0431],\n",
      "        [0.0370],\n",
      "        [0.0265],\n",
      "        [0.0220],\n",
      "        [0.0460],\n",
      "        [0.1106],\n",
      "        [0.0059],\n",
      "        [0.0002],\n",
      "        [0.0032],\n",
      "        [0.0084],\n",
      "        [0.0124],\n",
      "        [0.0165],\n",
      "        [0.0202],\n",
      "        [0.0247],\n",
      "        [0.0306],\n",
      "        [0.0791],\n",
      "        [0.0658],\n",
      "        [0.0544],\n",
      "        [0.0627],\n",
      "        [0.0775],\n",
      "        [0.0779],\n",
      "        [0.0861],\n",
      "        [0.0896],\n",
      "        [0.0906],\n",
      "        [0.0940],\n",
      "        [0.1034],\n",
      "        [0.1079],\n",
      "        [0.0636],\n",
      "        [0.0494],\n",
      "        [0.0317],\n",
      "        [0.0470],\n",
      "        [0.0629],\n",
      "        [0.0881],\n",
      "        [0.1180]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0844],\n",
      "        [0.0748],\n",
      "        [0.0462],\n",
      "        [0.0437],\n",
      "        [0.0006],\n",
      "        [0.0350],\n",
      "        [0.0336],\n",
      "        [0.0429],\n",
      "        [0.0955],\n",
      "        [0.0818],\n",
      "        [0.0568],\n",
      "        [0.0847],\n",
      "        [0.0392],\n",
      "        [0.0588],\n",
      "        [0.0388],\n",
      "        [0.0841],\n",
      "        [0.0789],\n",
      "        [0.0684],\n",
      "        [0.0739],\n",
      "        [0.0352],\n",
      "        [0.0091],\n",
      "        [0.0483],\n",
      "        [0.0798],\n",
      "        [0.0197],\n",
      "        [0.0587],\n",
      "        [0.0240],\n",
      "        [0.0451],\n",
      "        [0.0421],\n",
      "        [0.0359],\n",
      "        [0.0276],\n",
      "        [0.0209],\n",
      "        [0.0471],\n",
      "        [0.1117],\n",
      "        [0.0049],\n",
      "        [0.0009],\n",
      "        [0.0042],\n",
      "        [0.0095],\n",
      "        [0.0135],\n",
      "        [0.0176],\n",
      "        [0.0212],\n",
      "        [0.0258],\n",
      "        [0.0316],\n",
      "        [0.0801],\n",
      "        [0.0717],\n",
      "        [0.0554],\n",
      "        [0.0638],\n",
      "        [0.0786],\n",
      "        [0.0790],\n",
      "        [0.0872],\n",
      "        [0.0907],\n",
      "        [0.0917],\n",
      "        [0.0950],\n",
      "        [0.1044],\n",
      "        [0.1089],\n",
      "        [0.0646],\n",
      "        [0.0483],\n",
      "        [0.0638],\n",
      "        [0.0458],\n",
      "        [0.0286],\n",
      "        [0.0504],\n",
      "        [0.0804]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 10.91700029373169\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 62\n",
      "剩餘X 資料 torch.Size([98, 18])\n",
      "剩餘Y 資料 torch.Size([98, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.007179319392889738, 3)\n",
      "The second_loss value of k: (0.007323526777327061, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.8743])\n",
      "目前模型的Data狀態 torch.Size([62, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.9469],\n",
      "        [0.9199],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.9551],\n",
      "        [0.8883],\n",
      "        [0.9527],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.9277],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8883],\n",
      "        [0.8956],\n",
      "        [0.8883],\n",
      "        [0.9839],\n",
      "        [0.9121],\n",
      "        [0.9439],\n",
      "        [0.9591]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0844],\n",
      "        [0.0748],\n",
      "        [0.0462],\n",
      "        [0.0437],\n",
      "        [0.0006],\n",
      "        [0.0350],\n",
      "        [0.0336],\n",
      "        [0.0429],\n",
      "        [0.0955],\n",
      "        [0.0818],\n",
      "        [0.0568],\n",
      "        [0.0847],\n",
      "        [0.0392],\n",
      "        [0.0588],\n",
      "        [0.0388],\n",
      "        [0.0841],\n",
      "        [0.0789],\n",
      "        [0.0684],\n",
      "        [0.0739],\n",
      "        [0.0352],\n",
      "        [0.0091],\n",
      "        [0.0483],\n",
      "        [0.0798],\n",
      "        [0.0197],\n",
      "        [0.0587],\n",
      "        [0.0240],\n",
      "        [0.0451],\n",
      "        [0.0421],\n",
      "        [0.0359],\n",
      "        [0.0276],\n",
      "        [0.0209],\n",
      "        [0.0471],\n",
      "        [0.1117],\n",
      "        [0.0049],\n",
      "        [0.0009],\n",
      "        [0.0042],\n",
      "        [0.0095],\n",
      "        [0.0135],\n",
      "        [0.0176],\n",
      "        [0.0212],\n",
      "        [0.0258],\n",
      "        [0.0316],\n",
      "        [0.0801],\n",
      "        [0.0717],\n",
      "        [0.0554],\n",
      "        [0.0638],\n",
      "        [0.0786],\n",
      "        [0.0790],\n",
      "        [0.0872],\n",
      "        [0.0907],\n",
      "        [0.0917],\n",
      "        [0.0950],\n",
      "        [0.1044],\n",
      "        [0.1089],\n",
      "        [0.0646],\n",
      "        [0.0483],\n",
      "        [0.0638],\n",
      "        [0.0458],\n",
      "        [0.0286],\n",
      "        [0.0504],\n",
      "        [0.0804],\n",
      "        [0.0847]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 80\n",
      "Number of shrink: 20\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0877],\n",
      "        [0.0781],\n",
      "        [0.0495],\n",
      "        [0.0500],\n",
      "        [0.0027],\n",
      "        [0.0383],\n",
      "        [0.0369],\n",
      "        [0.0462],\n",
      "        [0.0988],\n",
      "        [0.0851],\n",
      "        [0.0601],\n",
      "        [0.0880],\n",
      "        [0.0424],\n",
      "        [0.0621],\n",
      "        [0.0420],\n",
      "        [0.0874],\n",
      "        [0.0822],\n",
      "        [0.0717],\n",
      "        [0.0772],\n",
      "        [0.0329],\n",
      "        [0.0055],\n",
      "        [0.0450],\n",
      "        [0.0765],\n",
      "        [0.0142],\n",
      "        [0.0620],\n",
      "        [0.0201],\n",
      "        [0.0484],\n",
      "        [0.0454],\n",
      "        [0.0392],\n",
      "        [0.0243],\n",
      "        [0.0242],\n",
      "        [0.0438],\n",
      "        [0.1084],\n",
      "        [0.0082],\n",
      "        [0.0024],\n",
      "        [0.0009],\n",
      "        [0.0062],\n",
      "        [0.0102],\n",
      "        [0.0143],\n",
      "        [0.0180],\n",
      "        [0.0225],\n",
      "        [0.0283],\n",
      "        [0.0768],\n",
      "        [0.0665],\n",
      "        [0.0522],\n",
      "        [0.0605],\n",
      "        [0.0753],\n",
      "        [0.0757],\n",
      "        [0.0839],\n",
      "        [0.0874],\n",
      "        [0.0884],\n",
      "        [0.0917],\n",
      "        [0.1011],\n",
      "        [0.1056],\n",
      "        [0.0614],\n",
      "        [0.0516],\n",
      "        [0.0677],\n",
      "        [0.0491],\n",
      "        [0.0073],\n",
      "        [0.0300],\n",
      "        [0.0572],\n",
      "        [0.0576]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 11.169861555099487\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 63\n",
      "剩餘X 資料 torch.Size([97, 18])\n",
      "剩餘Y 資料 torch.Size([97, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.003588229650631547, 2)\n",
      "The second_loss value of k: (0.004102801904082298, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.8833])\n",
      "目前模型的Data狀態 torch.Size([63, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8947],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.9492],\n",
      "        [0.9234],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.9605],\n",
      "        [0.8916],\n",
      "        [0.9566],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.9329],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.8916],\n",
      "        [0.9626],\n",
      "        [0.8916],\n",
      "        [0.9207],\n",
      "        [0.9319],\n",
      "        [0.9432]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0877],\n",
      "        [0.0781],\n",
      "        [0.0495],\n",
      "        [0.0500],\n",
      "        [0.0027],\n",
      "        [0.0383],\n",
      "        [0.0369],\n",
      "        [0.0462],\n",
      "        [0.0988],\n",
      "        [0.0851],\n",
      "        [0.0601],\n",
      "        [0.0880],\n",
      "        [0.0424],\n",
      "        [0.0621],\n",
      "        [0.0420],\n",
      "        [0.0874],\n",
      "        [0.0822],\n",
      "        [0.0717],\n",
      "        [0.0772],\n",
      "        [0.0329],\n",
      "        [0.0055],\n",
      "        [0.0450],\n",
      "        [0.0765],\n",
      "        [0.0142],\n",
      "        [0.0620],\n",
      "        [0.0201],\n",
      "        [0.0484],\n",
      "        [0.0454],\n",
      "        [0.0392],\n",
      "        [0.0243],\n",
      "        [0.0242],\n",
      "        [0.0438],\n",
      "        [0.1084],\n",
      "        [0.0082],\n",
      "        [0.0024],\n",
      "        [0.0009],\n",
      "        [0.0062],\n",
      "        [0.0102],\n",
      "        [0.0143],\n",
      "        [0.0180],\n",
      "        [0.0225],\n",
      "        [0.0283],\n",
      "        [0.0768],\n",
      "        [0.0665],\n",
      "        [0.0522],\n",
      "        [0.0605],\n",
      "        [0.0753],\n",
      "        [0.0757],\n",
      "        [0.0839],\n",
      "        [0.0874],\n",
      "        [0.0884],\n",
      "        [0.0917],\n",
      "        [0.1011],\n",
      "        [0.1056],\n",
      "        [0.0614],\n",
      "        [0.0516],\n",
      "        [0.0677],\n",
      "        [0.0491],\n",
      "        [0.0073],\n",
      "        [0.0300],\n",
      "        [0.0572],\n",
      "        [0.0576],\n",
      "        [0.0599]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0869],\n",
      "        [0.0773],\n",
      "        [0.0488],\n",
      "        [0.0522],\n",
      "        [0.0019],\n",
      "        [0.0375],\n",
      "        [0.0361],\n",
      "        [0.0455],\n",
      "        [0.0980],\n",
      "        [0.0844],\n",
      "        [0.0594],\n",
      "        [0.0873],\n",
      "        [0.0417],\n",
      "        [0.0614],\n",
      "        [0.0413],\n",
      "        [0.0866],\n",
      "        [0.0815],\n",
      "        [0.0709],\n",
      "        [0.0764],\n",
      "        [0.0328],\n",
      "        [0.0051],\n",
      "        [0.0458],\n",
      "        [0.0773],\n",
      "        [0.0114],\n",
      "        [0.0612],\n",
      "        [0.0184],\n",
      "        [0.0476],\n",
      "        [0.0446],\n",
      "        [0.0385],\n",
      "        [0.0250],\n",
      "        [0.0235],\n",
      "        [0.0445],\n",
      "        [0.1091],\n",
      "        [0.0074],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0069],\n",
      "        [0.0109],\n",
      "        [0.0150],\n",
      "        [0.0187],\n",
      "        [0.0232],\n",
      "        [0.0291],\n",
      "        [0.0776],\n",
      "        [0.0642],\n",
      "        [0.0529],\n",
      "        [0.0612],\n",
      "        [0.0760],\n",
      "        [0.0764],\n",
      "        [0.0846],\n",
      "        [0.0881],\n",
      "        [0.0891],\n",
      "        [0.0925],\n",
      "        [0.1019],\n",
      "        [0.1064],\n",
      "        [0.0621],\n",
      "        [0.0509],\n",
      "        [0.0685],\n",
      "        [0.0484],\n",
      "        [0.0108],\n",
      "        [0.0292],\n",
      "        [0.0375],\n",
      "        [0.0344],\n",
      "        [0.0381]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 11.421976804733276\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 64\n",
      "剩餘X 資料 torch.Size([96, 18])\n",
      "剩餘Y 資料 torch.Size([96, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0018934693653136492, 1)\n",
      "The second_loss value of k: (0.0334840789437294, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.9203])\n",
      "目前模型的Data狀態 torch.Size([64, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8969],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.9493],\n",
      "        [0.9239],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.9633],\n",
      "        [0.8909],\n",
      "        [0.9583],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.9352],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.8909],\n",
      "        [0.9445],\n",
      "        [0.8909],\n",
      "        [0.9010],\n",
      "        [0.9087],\n",
      "        [0.9214],\n",
      "        [0.9638]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0869],\n",
      "        [0.0773],\n",
      "        [0.0488],\n",
      "        [0.0522],\n",
      "        [0.0019],\n",
      "        [0.0375],\n",
      "        [0.0361],\n",
      "        [0.0455],\n",
      "        [0.0980],\n",
      "        [0.0844],\n",
      "        [0.0594],\n",
      "        [0.0873],\n",
      "        [0.0417],\n",
      "        [0.0614],\n",
      "        [0.0413],\n",
      "        [0.0866],\n",
      "        [0.0815],\n",
      "        [0.0709],\n",
      "        [0.0764],\n",
      "        [0.0328],\n",
      "        [0.0051],\n",
      "        [0.0458],\n",
      "        [0.0773],\n",
      "        [0.0114],\n",
      "        [0.0612],\n",
      "        [0.0184],\n",
      "        [0.0476],\n",
      "        [0.0446],\n",
      "        [0.0385],\n",
      "        [0.0250],\n",
      "        [0.0235],\n",
      "        [0.0445],\n",
      "        [0.1091],\n",
      "        [0.0074],\n",
      "        [0.0017],\n",
      "        [0.0017],\n",
      "        [0.0069],\n",
      "        [0.0109],\n",
      "        [0.0150],\n",
      "        [0.0187],\n",
      "        [0.0232],\n",
      "        [0.0291],\n",
      "        [0.0776],\n",
      "        [0.0642],\n",
      "        [0.0529],\n",
      "        [0.0612],\n",
      "        [0.0760],\n",
      "        [0.0764],\n",
      "        [0.0846],\n",
      "        [0.0881],\n",
      "        [0.0891],\n",
      "        [0.0925],\n",
      "        [0.1019],\n",
      "        [0.1064],\n",
      "        [0.0621],\n",
      "        [0.0509],\n",
      "        [0.0685],\n",
      "        [0.0484],\n",
      "        [0.0108],\n",
      "        [0.0292],\n",
      "        [0.0375],\n",
      "        [0.0344],\n",
      "        [0.0381],\n",
      "        [0.0435]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0864],\n",
      "        [0.0768],\n",
      "        [0.0482],\n",
      "        [0.0580],\n",
      "        [0.0014],\n",
      "        [0.0370],\n",
      "        [0.0356],\n",
      "        [0.0449],\n",
      "        [0.0975],\n",
      "        [0.0838],\n",
      "        [0.0588],\n",
      "        [0.0867],\n",
      "        [0.0412],\n",
      "        [0.0608],\n",
      "        [0.0408],\n",
      "        [0.0861],\n",
      "        [0.0809],\n",
      "        [0.0704],\n",
      "        [0.0759],\n",
      "        [0.0279],\n",
      "        [0.0004],\n",
      "        [0.0463],\n",
      "        [0.0778],\n",
      "        [0.0042],\n",
      "        [0.0607],\n",
      "        [0.0119],\n",
      "        [0.0471],\n",
      "        [0.0441],\n",
      "        [0.0379],\n",
      "        [0.0255],\n",
      "        [0.0229],\n",
      "        [0.0451],\n",
      "        [0.1096],\n",
      "        [0.0069],\n",
      "        [0.0011],\n",
      "        [0.0022],\n",
      "        [0.0075],\n",
      "        [0.0115],\n",
      "        [0.0156],\n",
      "        [0.0192],\n",
      "        [0.0237],\n",
      "        [0.0296],\n",
      "        [0.0781],\n",
      "        [0.0574],\n",
      "        [0.0534],\n",
      "        [0.0618],\n",
      "        [0.0766],\n",
      "        [0.0770],\n",
      "        [0.0852],\n",
      "        [0.0887],\n",
      "        [0.0897],\n",
      "        [0.0930],\n",
      "        [0.1024],\n",
      "        [0.1069],\n",
      "        [0.0626],\n",
      "        [0.0503],\n",
      "        [0.0690],\n",
      "        [0.0478],\n",
      "        [0.0210],\n",
      "        [0.0287],\n",
      "        [0.0268],\n",
      "        [0.0197],\n",
      "        [0.0248],\n",
      "        [0.0314]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 11.67347526550293\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 65\n",
      "剩餘X 資料 torch.Size([95, 18])\n",
      "剩餘Y 資料 torch.Size([95, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03328486904501915, 0)\n",
      "The second_loss value of k: (0.03985252603888512, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.7079])\n",
      "目前模型的Data狀態 torch.Size([65, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.9027],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.9542],\n",
      "        [0.9286],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.9705],\n",
      "        [0.8904],\n",
      "        [0.9648],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.9420],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.9343],\n",
      "        [0.8904],\n",
      "        [0.8904],\n",
      "        [0.8941],\n",
      "        [0.9081],\n",
      "        [0.9517],\n",
      "        [0.8904]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0864],\n",
      "        [0.0768],\n",
      "        [0.0482],\n",
      "        [0.0580],\n",
      "        [0.0014],\n",
      "        [0.0370],\n",
      "        [0.0356],\n",
      "        [0.0449],\n",
      "        [0.0975],\n",
      "        [0.0838],\n",
      "        [0.0588],\n",
      "        [0.0867],\n",
      "        [0.0412],\n",
      "        [0.0608],\n",
      "        [0.0408],\n",
      "        [0.0861],\n",
      "        [0.0809],\n",
      "        [0.0704],\n",
      "        [0.0759],\n",
      "        [0.0279],\n",
      "        [0.0004],\n",
      "        [0.0463],\n",
      "        [0.0778],\n",
      "        [0.0042],\n",
      "        [0.0607],\n",
      "        [0.0119],\n",
      "        [0.0471],\n",
      "        [0.0441],\n",
      "        [0.0379],\n",
      "        [0.0255],\n",
      "        [0.0229],\n",
      "        [0.0451],\n",
      "        [0.1096],\n",
      "        [0.0069],\n",
      "        [0.0011],\n",
      "        [0.0022],\n",
      "        [0.0075],\n",
      "        [0.0115],\n",
      "        [0.0156],\n",
      "        [0.0192],\n",
      "        [0.0237],\n",
      "        [0.0296],\n",
      "        [0.0781],\n",
      "        [0.0574],\n",
      "        [0.0534],\n",
      "        [0.0618],\n",
      "        [0.0766],\n",
      "        [0.0770],\n",
      "        [0.0852],\n",
      "        [0.0887],\n",
      "        [0.0897],\n",
      "        [0.0930],\n",
      "        [0.1024],\n",
      "        [0.1069],\n",
      "        [0.0626],\n",
      "        [0.0503],\n",
      "        [0.0690],\n",
      "        [0.0478],\n",
      "        [0.0210],\n",
      "        [0.0287],\n",
      "        [0.0268],\n",
      "        [0.0197],\n",
      "        [0.0248],\n",
      "        [0.0314],\n",
      "        [0.1824]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0826],\n",
      "        [0.0730],\n",
      "        [0.0444],\n",
      "        [0.0598],\n",
      "        [0.0024],\n",
      "        [0.0332],\n",
      "        [0.0318],\n",
      "        [0.0411],\n",
      "        [0.0937],\n",
      "        [0.0800],\n",
      "        [0.0551],\n",
      "        [0.0829],\n",
      "        [0.0374],\n",
      "        [0.0570],\n",
      "        [0.0370],\n",
      "        [0.0823],\n",
      "        [0.0771],\n",
      "        [0.0666],\n",
      "        [0.0721],\n",
      "        [0.0256],\n",
      "        [0.0013],\n",
      "        [0.0501],\n",
      "        [0.0816],\n",
      "        [0.0008],\n",
      "        [0.0569],\n",
      "        [0.0085],\n",
      "        [0.0433],\n",
      "        [0.0403],\n",
      "        [0.0342],\n",
      "        [0.0293],\n",
      "        [0.0192],\n",
      "        [0.0489],\n",
      "        [0.1134],\n",
      "        [0.0031],\n",
      "        [0.0026],\n",
      "        [0.0060],\n",
      "        [0.0113],\n",
      "        [0.0153],\n",
      "        [0.0193],\n",
      "        [0.0230],\n",
      "        [0.0275],\n",
      "        [0.0334],\n",
      "        [0.0819],\n",
      "        [0.0541],\n",
      "        [0.0572],\n",
      "        [0.0656],\n",
      "        [0.0804],\n",
      "        [0.0807],\n",
      "        [0.0889],\n",
      "        [0.0925],\n",
      "        [0.0934],\n",
      "        [0.0968],\n",
      "        [0.1062],\n",
      "        [0.1107],\n",
      "        [0.0664],\n",
      "        [0.0466],\n",
      "        [0.0728],\n",
      "        [0.0440],\n",
      "        [0.0251],\n",
      "        [0.0249],\n",
      "        [0.0230],\n",
      "        [0.0135],\n",
      "        [0.0193],\n",
      "        [0.0267],\n",
      "        [0.1787]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 11.91999864578247\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 66\n",
      "剩餘X 資料 torch.Size([94, 18])\n",
      "剩餘Y 資料 torch.Size([94, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03833318501710892, 0)\n",
      "The second_loss value of k: (0.0565086230635643, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.8356])\n",
      "目前模型的Data狀態 torch.Size([66, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.9045],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.9565],\n",
      "        [0.9303],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.9740],\n",
      "        [0.8866],\n",
      "        [0.9682],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.9453],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.9302],\n",
      "        [0.8866],\n",
      "        [0.8866],\n",
      "        [0.8878],\n",
      "        [0.9026],\n",
      "        [0.9470],\n",
      "        [0.8866],\n",
      "        [1.0314]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0826],\n",
      "        [0.0730],\n",
      "        [0.0444],\n",
      "        [0.0598],\n",
      "        [0.0024],\n",
      "        [0.0332],\n",
      "        [0.0318],\n",
      "        [0.0411],\n",
      "        [0.0937],\n",
      "        [0.0800],\n",
      "        [0.0551],\n",
      "        [0.0829],\n",
      "        [0.0374],\n",
      "        [0.0570],\n",
      "        [0.0370],\n",
      "        [0.0823],\n",
      "        [0.0771],\n",
      "        [0.0666],\n",
      "        [0.0721],\n",
      "        [0.0256],\n",
      "        [0.0013],\n",
      "        [0.0501],\n",
      "        [0.0816],\n",
      "        [0.0008],\n",
      "        [0.0569],\n",
      "        [0.0085],\n",
      "        [0.0433],\n",
      "        [0.0403],\n",
      "        [0.0342],\n",
      "        [0.0293],\n",
      "        [0.0192],\n",
      "        [0.0489],\n",
      "        [0.1134],\n",
      "        [0.0031],\n",
      "        [0.0026],\n",
      "        [0.0060],\n",
      "        [0.0113],\n",
      "        [0.0153],\n",
      "        [0.0193],\n",
      "        [0.0230],\n",
      "        [0.0275],\n",
      "        [0.0334],\n",
      "        [0.0819],\n",
      "        [0.0541],\n",
      "        [0.0572],\n",
      "        [0.0656],\n",
      "        [0.0804],\n",
      "        [0.0807],\n",
      "        [0.0889],\n",
      "        [0.0925],\n",
      "        [0.0934],\n",
      "        [0.0968],\n",
      "        [0.1062],\n",
      "        [0.1107],\n",
      "        [0.0664],\n",
      "        [0.0466],\n",
      "        [0.0728],\n",
      "        [0.0440],\n",
      "        [0.0251],\n",
      "        [0.0249],\n",
      "        [0.0230],\n",
      "        [0.0135],\n",
      "        [0.0193],\n",
      "        [0.0267],\n",
      "        [0.1787],\n",
      "        [0.1958]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0812],\n",
      "        [0.0716],\n",
      "        [0.0430],\n",
      "        [0.0484],\n",
      "        [0.0038],\n",
      "        [0.0318],\n",
      "        [0.0304],\n",
      "        [0.0397],\n",
      "        [0.0923],\n",
      "        [0.0786],\n",
      "        [0.0536],\n",
      "        [0.0815],\n",
      "        [0.0359],\n",
      "        [0.0556],\n",
      "        [0.0355],\n",
      "        [0.0809],\n",
      "        [0.0757],\n",
      "        [0.0652],\n",
      "        [0.0707],\n",
      "        [0.0426],\n",
      "        [0.0137],\n",
      "        [0.0515],\n",
      "        [0.0830],\n",
      "        [0.0133],\n",
      "        [0.0555],\n",
      "        [0.0218],\n",
      "        [0.0418],\n",
      "        [0.0389],\n",
      "        [0.0327],\n",
      "        [0.0308],\n",
      "        [0.0177],\n",
      "        [0.0503],\n",
      "        [0.1149],\n",
      "        [0.0017],\n",
      "        [0.0041],\n",
      "        [0.0074],\n",
      "        [0.0127],\n",
      "        [0.0167],\n",
      "        [0.0208],\n",
      "        [0.0245],\n",
      "        [0.0290],\n",
      "        [0.0348],\n",
      "        [0.0833],\n",
      "        [0.0658],\n",
      "        [0.0587],\n",
      "        [0.0670],\n",
      "        [0.0818],\n",
      "        [0.0822],\n",
      "        [0.0904],\n",
      "        [0.0939],\n",
      "        [0.0949],\n",
      "        [0.0982],\n",
      "        [0.1076],\n",
      "        [0.1121],\n",
      "        [0.0679],\n",
      "        [0.0451],\n",
      "        [0.0743],\n",
      "        [0.0426],\n",
      "        [0.0606],\n",
      "        [0.0235],\n",
      "        [0.0216],\n",
      "        [0.0108],\n",
      "        [0.0018],\n",
      "        [0.0123],\n",
      "        [0.1772],\n",
      "        [0.1503]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 12.166792392730713\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 67\n",
      "剩餘X 資料 torch.Size([93, 18])\n",
      "剩餘Y 資料 torch.Size([93, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03905106335878372, 0)\n",
      "The second_loss value of k: (0.20945525169372559, 67)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.8355])\n",
      "目前模型的Data狀態 torch.Size([67, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8931],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.9396],\n",
      "        [0.9153],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.9614],\n",
      "        [0.8851],\n",
      "        [0.9549],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.9336],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8946],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.8851],\n",
      "        [0.9080],\n",
      "        [0.8851],\n",
      "        [0.9860],\n",
      "        [1.0331]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0812],\n",
      "        [0.0716],\n",
      "        [0.0430],\n",
      "        [0.0484],\n",
      "        [0.0038],\n",
      "        [0.0318],\n",
      "        [0.0304],\n",
      "        [0.0397],\n",
      "        [0.0923],\n",
      "        [0.0786],\n",
      "        [0.0536],\n",
      "        [0.0815],\n",
      "        [0.0359],\n",
      "        [0.0556],\n",
      "        [0.0355],\n",
      "        [0.0809],\n",
      "        [0.0757],\n",
      "        [0.0652],\n",
      "        [0.0707],\n",
      "        [0.0426],\n",
      "        [0.0137],\n",
      "        [0.0515],\n",
      "        [0.0830],\n",
      "        [0.0133],\n",
      "        [0.0555],\n",
      "        [0.0218],\n",
      "        [0.0418],\n",
      "        [0.0389],\n",
      "        [0.0327],\n",
      "        [0.0308],\n",
      "        [0.0177],\n",
      "        [0.0503],\n",
      "        [0.1149],\n",
      "        [0.0017],\n",
      "        [0.0041],\n",
      "        [0.0074],\n",
      "        [0.0127],\n",
      "        [0.0167],\n",
      "        [0.0208],\n",
      "        [0.0245],\n",
      "        [0.0290],\n",
      "        [0.0348],\n",
      "        [0.0833],\n",
      "        [0.0658],\n",
      "        [0.0587],\n",
      "        [0.0670],\n",
      "        [0.0818],\n",
      "        [0.0822],\n",
      "        [0.0904],\n",
      "        [0.0939],\n",
      "        [0.0949],\n",
      "        [0.0982],\n",
      "        [0.1076],\n",
      "        [0.1121],\n",
      "        [0.0679],\n",
      "        [0.0451],\n",
      "        [0.0743],\n",
      "        [0.0426],\n",
      "        [0.0606],\n",
      "        [0.0235],\n",
      "        [0.0216],\n",
      "        [0.0108],\n",
      "        [0.0018],\n",
      "        [0.0123],\n",
      "        [0.1772],\n",
      "        [0.1503],\n",
      "        [0.1976]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0851],\n",
      "        [    0.0755],\n",
      "        [    0.0470],\n",
      "        [    0.0473],\n",
      "        [    0.0001],\n",
      "        [    0.0358],\n",
      "        [    0.0343],\n",
      "        [    0.0437],\n",
      "        [    0.0962],\n",
      "        [    0.0826],\n",
      "        [    0.0576],\n",
      "        [    0.0855],\n",
      "        [    0.0399],\n",
      "        [    0.0596],\n",
      "        [    0.0395],\n",
      "        [    0.0848],\n",
      "        [    0.0797],\n",
      "        [    0.0691],\n",
      "        [    0.0746],\n",
      "        [    0.0585],\n",
      "        [    0.0242],\n",
      "        [    0.0476],\n",
      "        [    0.0791],\n",
      "        [    0.0197],\n",
      "        [    0.0594],\n",
      "        [    0.0301],\n",
      "        [    0.0458],\n",
      "        [    0.0428],\n",
      "        [    0.0367],\n",
      "        [    0.0268],\n",
      "        [    0.0217],\n",
      "        [    0.0463],\n",
      "        [    0.1109],\n",
      "        [    0.0056],\n",
      "        [    0.0001],\n",
      "        [    0.0035],\n",
      "        [    0.0087],\n",
      "        [    0.0127],\n",
      "        [    0.0168],\n",
      "        [    0.0205],\n",
      "        [    0.0250],\n",
      "        [    0.0309],\n",
      "        [    0.0794],\n",
      "        [    0.0712],\n",
      "        [    0.0547],\n",
      "        [    0.0630],\n",
      "        [    0.0778],\n",
      "        [    0.0782],\n",
      "        [    0.0864],\n",
      "        [    0.0899],\n",
      "        [    0.0909],\n",
      "        [    0.0943],\n",
      "        [    0.1037],\n",
      "        [    0.1082],\n",
      "        [    0.0639],\n",
      "        [    0.0491],\n",
      "        [    0.0703],\n",
      "        [    0.0466],\n",
      "        [    0.0661],\n",
      "        [    0.0274],\n",
      "        [    0.0256],\n",
      "        [    0.0148],\n",
      "        [    0.0058],\n",
      "        [    0.0312],\n",
      "        [    0.1812],\n",
      "        [    0.0742],\n",
      "        [    0.1318]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 12.412169456481934\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 68\n",
      "剩餘X 資料 torch.Size([92, 18])\n",
      "剩餘Y 資料 torch.Size([92, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14842498302459717, 0)\n",
      "The second_loss value of k: (0.18289847671985626, 66)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.7936])\n",
      "目前模型的Data狀態 torch.Size([68, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8920],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.9236],\n",
      "        [0.9048],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.9550],\n",
      "        [0.8891],\n",
      "        [0.9466],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.9282],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.8891],\n",
      "        [0.9099],\n",
      "        [0.9673],\n",
      "        [1.1789]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0851],\n",
      "        [    0.0755],\n",
      "        [    0.0470],\n",
      "        [    0.0473],\n",
      "        [    0.0001],\n",
      "        [    0.0358],\n",
      "        [    0.0343],\n",
      "        [    0.0437],\n",
      "        [    0.0962],\n",
      "        [    0.0826],\n",
      "        [    0.0576],\n",
      "        [    0.0855],\n",
      "        [    0.0399],\n",
      "        [    0.0596],\n",
      "        [    0.0395],\n",
      "        [    0.0848],\n",
      "        [    0.0797],\n",
      "        [    0.0691],\n",
      "        [    0.0746],\n",
      "        [    0.0585],\n",
      "        [    0.0242],\n",
      "        [    0.0476],\n",
      "        [    0.0791],\n",
      "        [    0.0197],\n",
      "        [    0.0594],\n",
      "        [    0.0301],\n",
      "        [    0.0458],\n",
      "        [    0.0428],\n",
      "        [    0.0367],\n",
      "        [    0.0268],\n",
      "        [    0.0217],\n",
      "        [    0.0463],\n",
      "        [    0.1109],\n",
      "        [    0.0056],\n",
      "        [    0.0001],\n",
      "        [    0.0035],\n",
      "        [    0.0087],\n",
      "        [    0.0127],\n",
      "        [    0.0168],\n",
      "        [    0.0205],\n",
      "        [    0.0250],\n",
      "        [    0.0309],\n",
      "        [    0.0794],\n",
      "        [    0.0712],\n",
      "        [    0.0547],\n",
      "        [    0.0630],\n",
      "        [    0.0778],\n",
      "        [    0.0782],\n",
      "        [    0.0864],\n",
      "        [    0.0899],\n",
      "        [    0.0909],\n",
      "        [    0.0943],\n",
      "        [    0.1037],\n",
      "        [    0.1082],\n",
      "        [    0.0639],\n",
      "        [    0.0491],\n",
      "        [    0.0703],\n",
      "        [    0.0466],\n",
      "        [    0.0661],\n",
      "        [    0.0274],\n",
      "        [    0.0256],\n",
      "        [    0.0148],\n",
      "        [    0.0058],\n",
      "        [    0.0312],\n",
      "        [    0.1812],\n",
      "        [    0.0742],\n",
      "        [    0.1318],\n",
      "        [    0.3853]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0866],\n",
      "        [0.0770],\n",
      "        [0.0485],\n",
      "        [0.0459],\n",
      "        [0.0016],\n",
      "        [0.0373],\n",
      "        [0.0358],\n",
      "        [0.0452],\n",
      "        [0.0977],\n",
      "        [0.0841],\n",
      "        [0.0591],\n",
      "        [0.0870],\n",
      "        [0.0414],\n",
      "        [0.0611],\n",
      "        [0.0410],\n",
      "        [0.0863],\n",
      "        [0.0812],\n",
      "        [0.0706],\n",
      "        [0.0761],\n",
      "        [0.0915],\n",
      "        [0.0384],\n",
      "        [0.0461],\n",
      "        [0.0776],\n",
      "        [0.0841],\n",
      "        [0.0609],\n",
      "        [0.0861],\n",
      "        [0.0473],\n",
      "        [0.0443],\n",
      "        [0.0382],\n",
      "        [0.0253],\n",
      "        [0.0232],\n",
      "        [0.0448],\n",
      "        [0.1094],\n",
      "        [0.0071],\n",
      "        [0.0014],\n",
      "        [0.0020],\n",
      "        [0.0072],\n",
      "        [0.0112],\n",
      "        [0.0153],\n",
      "        [0.0190],\n",
      "        [0.0235],\n",
      "        [0.0294],\n",
      "        [0.0779],\n",
      "        [0.1088],\n",
      "        [0.0532],\n",
      "        [0.0615],\n",
      "        [0.0763],\n",
      "        [0.0767],\n",
      "        [0.0849],\n",
      "        [0.0884],\n",
      "        [0.0894],\n",
      "        [0.0928],\n",
      "        [0.1022],\n",
      "        [0.1067],\n",
      "        [0.0624],\n",
      "        [0.0506],\n",
      "        [0.0688],\n",
      "        [0.0481],\n",
      "        [0.0646],\n",
      "        [0.0289],\n",
      "        [0.0271],\n",
      "        [0.0163],\n",
      "        [0.0073],\n",
      "        [0.0297],\n",
      "        [0.1827],\n",
      "        [0.0550],\n",
      "        [0.0551],\n",
      "        [0.0970]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 12.660897731781006\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 69\n",
      "剩餘X 資料 torch.Size([91, 18])\n",
      "剩餘Y 資料 torch.Size([91, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03734742850065231, 0)\n",
      "The second_loss value of k: (0.04573953151702881, 64)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.8166])\n",
      "目前模型的Data狀態 torch.Size([69, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [0.8906],\n",
      "        [1.0099]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0866],\n",
      "        [0.0770],\n",
      "        [0.0485],\n",
      "        [0.0459],\n",
      "        [0.0016],\n",
      "        [0.0373],\n",
      "        [0.0358],\n",
      "        [0.0452],\n",
      "        [0.0977],\n",
      "        [0.0841],\n",
      "        [0.0591],\n",
      "        [0.0870],\n",
      "        [0.0414],\n",
      "        [0.0611],\n",
      "        [0.0410],\n",
      "        [0.0863],\n",
      "        [0.0812],\n",
      "        [0.0706],\n",
      "        [0.0761],\n",
      "        [0.0915],\n",
      "        [0.0384],\n",
      "        [0.0461],\n",
      "        [0.0776],\n",
      "        [0.0841],\n",
      "        [0.0609],\n",
      "        [0.0861],\n",
      "        [0.0473],\n",
      "        [0.0443],\n",
      "        [0.0382],\n",
      "        [0.0253],\n",
      "        [0.0232],\n",
      "        [0.0448],\n",
      "        [0.1094],\n",
      "        [0.0071],\n",
      "        [0.0014],\n",
      "        [0.0020],\n",
      "        [0.0072],\n",
      "        [0.0112],\n",
      "        [0.0153],\n",
      "        [0.0190],\n",
      "        [0.0235],\n",
      "        [0.0294],\n",
      "        [0.0779],\n",
      "        [0.1088],\n",
      "        [0.0532],\n",
      "        [0.0615],\n",
      "        [0.0763],\n",
      "        [0.0767],\n",
      "        [0.0849],\n",
      "        [0.0884],\n",
      "        [0.0894],\n",
      "        [0.0928],\n",
      "        [0.1022],\n",
      "        [0.1067],\n",
      "        [0.0624],\n",
      "        [0.0506],\n",
      "        [0.0688],\n",
      "        [0.0481],\n",
      "        [0.0646],\n",
      "        [0.0289],\n",
      "        [0.0271],\n",
      "        [0.0163],\n",
      "        [0.0073],\n",
      "        [0.0297],\n",
      "        [0.1827],\n",
      "        [0.0550],\n",
      "        [0.0551],\n",
      "        [0.0970],\n",
      "        [0.1933]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0856],\n",
      "        [0.0760],\n",
      "        [0.0474],\n",
      "        [0.0449],\n",
      "        [0.0006],\n",
      "        [0.0362],\n",
      "        [0.0348],\n",
      "        [0.0441],\n",
      "        [0.0967],\n",
      "        [0.0830],\n",
      "        [0.0580],\n",
      "        [0.0859],\n",
      "        [0.0404],\n",
      "        [0.0600],\n",
      "        [0.0400],\n",
      "        [0.0853],\n",
      "        [0.0801],\n",
      "        [0.0696],\n",
      "        [0.0751],\n",
      "        [0.0926],\n",
      "        [0.0394],\n",
      "        [0.0471],\n",
      "        [0.0786],\n",
      "        [0.0852],\n",
      "        [0.0599],\n",
      "        [0.0871],\n",
      "        [0.0463],\n",
      "        [0.0433],\n",
      "        [0.0371],\n",
      "        [0.0264],\n",
      "        [0.0221],\n",
      "        [0.0459],\n",
      "        [0.1105],\n",
      "        [0.0061],\n",
      "        [0.0003],\n",
      "        [0.0030],\n",
      "        [0.0083],\n",
      "        [0.0123],\n",
      "        [0.0164],\n",
      "        [0.0200],\n",
      "        [0.0246],\n",
      "        [0.0304],\n",
      "        [0.0789],\n",
      "        [0.1099],\n",
      "        [0.0542],\n",
      "        [0.0626],\n",
      "        [0.0774],\n",
      "        [0.0778],\n",
      "        [0.0860],\n",
      "        [0.0895],\n",
      "        [0.0905],\n",
      "        [0.0938],\n",
      "        [0.1032],\n",
      "        [0.1077],\n",
      "        [0.0634],\n",
      "        [0.0495],\n",
      "        [0.0698],\n",
      "        [0.0470],\n",
      "        [0.0657],\n",
      "        [0.0279],\n",
      "        [0.0260],\n",
      "        [0.0152],\n",
      "        [0.0062],\n",
      "        [0.0307],\n",
      "        [0.1816],\n",
      "        [0.0539],\n",
      "        [0.0541],\n",
      "        [0.0959],\n",
      "        [0.0729]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 12.908148050308228\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 70\n",
      "剩餘X 資料 torch.Size([90, 18])\n",
      "剩餘Y 資料 torch.Size([90, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03530397266149521, 0)\n",
      "The second_loss value of k: (0.045289915055036545, 63)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.7038])\n",
      "目前模型的Data狀態 torch.Size([70, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8895],\n",
      "        [0.8917]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0856],\n",
      "        [0.0760],\n",
      "        [0.0474],\n",
      "        [0.0449],\n",
      "        [0.0006],\n",
      "        [0.0362],\n",
      "        [0.0348],\n",
      "        [0.0441],\n",
      "        [0.0967],\n",
      "        [0.0830],\n",
      "        [0.0580],\n",
      "        [0.0859],\n",
      "        [0.0404],\n",
      "        [0.0600],\n",
      "        [0.0400],\n",
      "        [0.0853],\n",
      "        [0.0801],\n",
      "        [0.0696],\n",
      "        [0.0751],\n",
      "        [0.0926],\n",
      "        [0.0394],\n",
      "        [0.0471],\n",
      "        [0.0786],\n",
      "        [0.0852],\n",
      "        [0.0599],\n",
      "        [0.0871],\n",
      "        [0.0463],\n",
      "        [0.0433],\n",
      "        [0.0371],\n",
      "        [0.0264],\n",
      "        [0.0221],\n",
      "        [0.0459],\n",
      "        [0.1105],\n",
      "        [0.0061],\n",
      "        [0.0003],\n",
      "        [0.0030],\n",
      "        [0.0083],\n",
      "        [0.0123],\n",
      "        [0.0164],\n",
      "        [0.0200],\n",
      "        [0.0246],\n",
      "        [0.0304],\n",
      "        [0.0789],\n",
      "        [0.1099],\n",
      "        [0.0542],\n",
      "        [0.0626],\n",
      "        [0.0774],\n",
      "        [0.0778],\n",
      "        [0.0860],\n",
      "        [0.0895],\n",
      "        [0.0905],\n",
      "        [0.0938],\n",
      "        [0.1032],\n",
      "        [0.1077],\n",
      "        [0.0634],\n",
      "        [0.0495],\n",
      "        [0.0698],\n",
      "        [0.0470],\n",
      "        [0.0657],\n",
      "        [0.0279],\n",
      "        [0.0260],\n",
      "        [0.0152],\n",
      "        [0.0062],\n",
      "        [0.0307],\n",
      "        [0.1816],\n",
      "        [0.0539],\n",
      "        [0.0541],\n",
      "        [0.0959],\n",
      "        [0.0729],\n",
      "        [0.1879]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0829],\n",
      "        [0.0733],\n",
      "        [0.0447],\n",
      "        [0.0422],\n",
      "        [0.0021],\n",
      "        [0.0335],\n",
      "        [0.0321],\n",
      "        [0.0415],\n",
      "        [0.0940],\n",
      "        [0.0804],\n",
      "        [0.0554],\n",
      "        [0.0833],\n",
      "        [0.0377],\n",
      "        [0.0574],\n",
      "        [0.0373],\n",
      "        [0.0826],\n",
      "        [0.0775],\n",
      "        [0.0669],\n",
      "        [0.0724],\n",
      "        [0.0952],\n",
      "        [0.0421],\n",
      "        [0.0498],\n",
      "        [0.0813],\n",
      "        [0.0878],\n",
      "        [0.0572],\n",
      "        [0.0898],\n",
      "        [0.0436],\n",
      "        [0.0406],\n",
      "        [0.0345],\n",
      "        [0.0290],\n",
      "        [0.0195],\n",
      "        [0.0486],\n",
      "        [0.1131],\n",
      "        [0.0034],\n",
      "        [0.0023],\n",
      "        [0.0057],\n",
      "        [0.0110],\n",
      "        [0.0149],\n",
      "        [0.0190],\n",
      "        [0.0227],\n",
      "        [0.0272],\n",
      "        [0.0331],\n",
      "        [0.0816],\n",
      "        [0.1125],\n",
      "        [0.0569],\n",
      "        [0.0652],\n",
      "        [0.0800],\n",
      "        [0.0804],\n",
      "        [0.0886],\n",
      "        [0.0921],\n",
      "        [0.0931],\n",
      "        [0.0965],\n",
      "        [0.1059],\n",
      "        [0.1104],\n",
      "        [0.0661],\n",
      "        [0.0469],\n",
      "        [0.0725],\n",
      "        [0.0444],\n",
      "        [0.0684],\n",
      "        [0.0252],\n",
      "        [0.0234],\n",
      "        [0.0126],\n",
      "        [0.0036],\n",
      "        [0.0334],\n",
      "        [0.1790],\n",
      "        [0.0513],\n",
      "        [0.0514],\n",
      "        [0.0933],\n",
      "        [0.0702],\n",
      "        [0.1831]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 13.154199838638306\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 71\n",
      "剩餘X 資料 torch.Size([89, 18])\n",
      "剩餘Y 資料 torch.Size([89, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.04416808485984802, 62)\n",
      "The second_loss value of k: (0.046300601214170456, 63)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引62，y= tensor([0.6767])\n",
      "目前模型的Data狀態 torch.Size([71, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869],\n",
      "        [0.8869]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0829],\n",
      "        [0.0733],\n",
      "        [0.0447],\n",
      "        [0.0422],\n",
      "        [0.0021],\n",
      "        [0.0335],\n",
      "        [0.0321],\n",
      "        [0.0415],\n",
      "        [0.0940],\n",
      "        [0.0804],\n",
      "        [0.0554],\n",
      "        [0.0833],\n",
      "        [0.0377],\n",
      "        [0.0574],\n",
      "        [0.0373],\n",
      "        [0.0826],\n",
      "        [0.0775],\n",
      "        [0.0669],\n",
      "        [0.0724],\n",
      "        [0.0952],\n",
      "        [0.0421],\n",
      "        [0.0498],\n",
      "        [0.0813],\n",
      "        [0.0878],\n",
      "        [0.0572],\n",
      "        [0.0898],\n",
      "        [0.0436],\n",
      "        [0.0406],\n",
      "        [0.0345],\n",
      "        [0.0290],\n",
      "        [0.0195],\n",
      "        [0.0486],\n",
      "        [0.1131],\n",
      "        [0.0034],\n",
      "        [0.0023],\n",
      "        [0.0057],\n",
      "        [0.0110],\n",
      "        [0.0149],\n",
      "        [0.0190],\n",
      "        [0.0227],\n",
      "        [0.0272],\n",
      "        [0.0331],\n",
      "        [0.0816],\n",
      "        [0.1125],\n",
      "        [0.0569],\n",
      "        [0.0652],\n",
      "        [0.0800],\n",
      "        [0.0804],\n",
      "        [0.0886],\n",
      "        [0.0921],\n",
      "        [0.0931],\n",
      "        [0.0965],\n",
      "        [0.1059],\n",
      "        [0.1104],\n",
      "        [0.0661],\n",
      "        [0.0469],\n",
      "        [0.0725],\n",
      "        [0.0444],\n",
      "        [0.0684],\n",
      "        [0.0252],\n",
      "        [0.0234],\n",
      "        [0.0126],\n",
      "        [0.0036],\n",
      "        [0.0334],\n",
      "        [0.1790],\n",
      "        [0.0513],\n",
      "        [0.0514],\n",
      "        [0.0933],\n",
      "        [0.0702],\n",
      "        [0.1831],\n",
      "        [0.2102]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0800],\n",
      "        [0.0704],\n",
      "        [0.0418],\n",
      "        [0.0392],\n",
      "        [0.0050],\n",
      "        [0.0306],\n",
      "        [0.0292],\n",
      "        [0.0385],\n",
      "        [0.0911],\n",
      "        [0.0774],\n",
      "        [0.0524],\n",
      "        [0.0803],\n",
      "        [0.0347],\n",
      "        [0.0544],\n",
      "        [0.0343],\n",
      "        [0.0797],\n",
      "        [0.0745],\n",
      "        [0.0640],\n",
      "        [0.0695],\n",
      "        [0.0982],\n",
      "        [0.0450],\n",
      "        [0.0527],\n",
      "        [0.0842],\n",
      "        [0.0908],\n",
      "        [0.0543],\n",
      "        [0.0928],\n",
      "        [0.0407],\n",
      "        [0.0377],\n",
      "        [0.0315],\n",
      "        [0.0320],\n",
      "        [0.0165],\n",
      "        [0.0515],\n",
      "        [0.1161],\n",
      "        [0.0005],\n",
      "        [0.0053],\n",
      "        [0.0086],\n",
      "        [0.0139],\n",
      "        [0.0179],\n",
      "        [0.0220],\n",
      "        [0.0257],\n",
      "        [0.0302],\n",
      "        [0.0360],\n",
      "        [0.0845],\n",
      "        [0.1155],\n",
      "        [0.0599],\n",
      "        [0.0682],\n",
      "        [0.0830],\n",
      "        [0.0834],\n",
      "        [0.0916],\n",
      "        [0.0951],\n",
      "        [0.0961],\n",
      "        [0.0994],\n",
      "        [0.1088],\n",
      "        [0.1133],\n",
      "        [0.0691],\n",
      "        [0.0439],\n",
      "        [0.0754],\n",
      "        [0.0414],\n",
      "        [0.0713],\n",
      "        [0.0223],\n",
      "        [0.0204],\n",
      "        [0.0096],\n",
      "        [0.0006],\n",
      "        [0.0364],\n",
      "        [0.1760],\n",
      "        [0.0483],\n",
      "        [0.0484],\n",
      "        [0.0903],\n",
      "        [0.0673],\n",
      "        [0.1801],\n",
      "        [0.2072]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 13.400261878967285\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 72\n",
      "剩餘X 資料 torch.Size([88, 18])\n",
      "剩餘Y 資料 torch.Size([88, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.04503553733229637, 62)\n",
      "The second_loss value of k: (0.05759376659989357, 69)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引62，y= tensor([0.6717])\n",
      "目前模型的Data狀態 torch.Size([72, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839],\n",
      "        [0.8839]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0800],\n",
      "        [0.0704],\n",
      "        [0.0418],\n",
      "        [0.0392],\n",
      "        [0.0050],\n",
      "        [0.0306],\n",
      "        [0.0292],\n",
      "        [0.0385],\n",
      "        [0.0911],\n",
      "        [0.0774],\n",
      "        [0.0524],\n",
      "        [0.0803],\n",
      "        [0.0347],\n",
      "        [0.0544],\n",
      "        [0.0343],\n",
      "        [0.0797],\n",
      "        [0.0745],\n",
      "        [0.0640],\n",
      "        [0.0695],\n",
      "        [0.0982],\n",
      "        [0.0450],\n",
      "        [0.0527],\n",
      "        [0.0842],\n",
      "        [0.0908],\n",
      "        [0.0543],\n",
      "        [0.0928],\n",
      "        [0.0407],\n",
      "        [0.0377],\n",
      "        [0.0315],\n",
      "        [0.0320],\n",
      "        [0.0165],\n",
      "        [0.0515],\n",
      "        [0.1161],\n",
      "        [0.0005],\n",
      "        [0.0053],\n",
      "        [0.0086],\n",
      "        [0.0139],\n",
      "        [0.0179],\n",
      "        [0.0220],\n",
      "        [0.0257],\n",
      "        [0.0302],\n",
      "        [0.0360],\n",
      "        [0.0845],\n",
      "        [0.1155],\n",
      "        [0.0599],\n",
      "        [0.0682],\n",
      "        [0.0830],\n",
      "        [0.0834],\n",
      "        [0.0916],\n",
      "        [0.0951],\n",
      "        [0.0961],\n",
      "        [0.0994],\n",
      "        [0.1088],\n",
      "        [0.1133],\n",
      "        [0.0691],\n",
      "        [0.0439],\n",
      "        [0.0754],\n",
      "        [0.0414],\n",
      "        [0.0713],\n",
      "        [0.0223],\n",
      "        [0.0204],\n",
      "        [0.0096],\n",
      "        [0.0006],\n",
      "        [0.0364],\n",
      "        [0.1760],\n",
      "        [0.0483],\n",
      "        [0.0484],\n",
      "        [0.0903],\n",
      "        [0.0673],\n",
      "        [0.1801],\n",
      "        [0.2072],\n",
      "        [0.2122]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0770],\n",
      "        [0.0674],\n",
      "        [0.0388],\n",
      "        [0.0363],\n",
      "        [0.0080],\n",
      "        [0.0276],\n",
      "        [0.0262],\n",
      "        [0.0355],\n",
      "        [0.0881],\n",
      "        [0.0744],\n",
      "        [0.0495],\n",
      "        [0.0773],\n",
      "        [0.0318],\n",
      "        [0.0514],\n",
      "        [0.0314],\n",
      "        [0.0767],\n",
      "        [0.0715],\n",
      "        [0.0610],\n",
      "        [0.0665],\n",
      "        [0.1011],\n",
      "        [0.0480],\n",
      "        [0.0557],\n",
      "        [0.0872],\n",
      "        [0.0938],\n",
      "        [0.0513],\n",
      "        [0.0957],\n",
      "        [0.0377],\n",
      "        [0.0347],\n",
      "        [0.0286],\n",
      "        [0.0349],\n",
      "        [0.0136],\n",
      "        [0.0545],\n",
      "        [0.1190],\n",
      "        [0.0025],\n",
      "        [0.0082],\n",
      "        [0.0116],\n",
      "        [0.0169],\n",
      "        [0.0209],\n",
      "        [0.0249],\n",
      "        [0.0286],\n",
      "        [0.0331],\n",
      "        [0.0390],\n",
      "        [0.0875],\n",
      "        [0.1184],\n",
      "        [0.0628],\n",
      "        [0.0712],\n",
      "        [0.0860],\n",
      "        [0.0864],\n",
      "        [0.0945],\n",
      "        [0.0981],\n",
      "        [0.0990],\n",
      "        [0.1024],\n",
      "        [0.1118],\n",
      "        [0.1163],\n",
      "        [0.0720],\n",
      "        [0.0410],\n",
      "        [0.0784],\n",
      "        [0.0384],\n",
      "        [0.0743],\n",
      "        [0.0193],\n",
      "        [0.0174],\n",
      "        [0.0066],\n",
      "        [0.0023],\n",
      "        [0.0393],\n",
      "        [0.1731],\n",
      "        [0.0453],\n",
      "        [0.0455],\n",
      "        [0.0874],\n",
      "        [0.0643],\n",
      "        [0.1771],\n",
      "        [0.2042],\n",
      "        [0.2093]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 13.648025274276733\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 73\n",
      "剩餘X 資料 torch.Size([87, 18])\n",
      "剩餘Y 資料 torch.Size([87, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.056184787303209305, 68)\n",
      "The second_loss value of k: (0.06090029701590538, 62)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引68，y= tensor([0.6439])\n",
      "目前模型的Data狀態 torch.Size([73, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810],\n",
      "        [0.8810]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0770],\n",
      "        [0.0674],\n",
      "        [0.0388],\n",
      "        [0.0363],\n",
      "        [0.0080],\n",
      "        [0.0276],\n",
      "        [0.0262],\n",
      "        [0.0355],\n",
      "        [0.0881],\n",
      "        [0.0744],\n",
      "        [0.0495],\n",
      "        [0.0773],\n",
      "        [0.0318],\n",
      "        [0.0514],\n",
      "        [0.0314],\n",
      "        [0.0767],\n",
      "        [0.0715],\n",
      "        [0.0610],\n",
      "        [0.0665],\n",
      "        [0.1011],\n",
      "        [0.0480],\n",
      "        [0.0557],\n",
      "        [0.0872],\n",
      "        [0.0938],\n",
      "        [0.0513],\n",
      "        [0.0957],\n",
      "        [0.0377],\n",
      "        [0.0347],\n",
      "        [0.0286],\n",
      "        [0.0349],\n",
      "        [0.0136],\n",
      "        [0.0545],\n",
      "        [0.1190],\n",
      "        [0.0025],\n",
      "        [0.0082],\n",
      "        [0.0116],\n",
      "        [0.0169],\n",
      "        [0.0209],\n",
      "        [0.0249],\n",
      "        [0.0286],\n",
      "        [0.0331],\n",
      "        [0.0390],\n",
      "        [0.0875],\n",
      "        [0.1184],\n",
      "        [0.0628],\n",
      "        [0.0712],\n",
      "        [0.0860],\n",
      "        [0.0864],\n",
      "        [0.0945],\n",
      "        [0.0981],\n",
      "        [0.0990],\n",
      "        [0.1024],\n",
      "        [0.1118],\n",
      "        [0.1163],\n",
      "        [0.0720],\n",
      "        [0.0410],\n",
      "        [0.0784],\n",
      "        [0.0384],\n",
      "        [0.0743],\n",
      "        [0.0193],\n",
      "        [0.0174],\n",
      "        [0.0066],\n",
      "        [0.0023],\n",
      "        [0.0393],\n",
      "        [0.1731],\n",
      "        [0.0453],\n",
      "        [0.0455],\n",
      "        [0.0874],\n",
      "        [0.0643],\n",
      "        [0.1771],\n",
      "        [0.2042],\n",
      "        [0.2093],\n",
      "        [0.2370]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0738],\n",
      "        [0.0642],\n",
      "        [0.0356],\n",
      "        [0.0330],\n",
      "        [0.0112],\n",
      "        [0.0244],\n",
      "        [0.0230],\n",
      "        [0.0323],\n",
      "        [0.0849],\n",
      "        [0.0712],\n",
      "        [0.0462],\n",
      "        [0.0741],\n",
      "        [0.0285],\n",
      "        [0.0482],\n",
      "        [0.0282],\n",
      "        [0.0735],\n",
      "        [0.0683],\n",
      "        [0.0578],\n",
      "        [0.0633],\n",
      "        [0.1044],\n",
      "        [0.0512],\n",
      "        [0.0589],\n",
      "        [0.0904],\n",
      "        [0.0970],\n",
      "        [0.0481],\n",
      "        [0.0990],\n",
      "        [0.0345],\n",
      "        [0.0315],\n",
      "        [0.0253],\n",
      "        [0.0382],\n",
      "        [0.0103],\n",
      "        [0.0577],\n",
      "        [0.1223],\n",
      "        [0.0057],\n",
      "        [0.0115],\n",
      "        [0.0148],\n",
      "        [0.0201],\n",
      "        [0.0241],\n",
      "        [0.0282],\n",
      "        [0.0319],\n",
      "        [0.0364],\n",
      "        [0.0422],\n",
      "        [0.0907],\n",
      "        [0.1217],\n",
      "        [0.0661],\n",
      "        [0.0744],\n",
      "        [0.0892],\n",
      "        [0.0896],\n",
      "        [0.0978],\n",
      "        [0.1013],\n",
      "        [0.1023],\n",
      "        [0.1056],\n",
      "        [0.1150],\n",
      "        [0.1195],\n",
      "        [0.0753],\n",
      "        [0.0377],\n",
      "        [0.0816],\n",
      "        [0.0352],\n",
      "        [0.0775],\n",
      "        [0.0161],\n",
      "        [0.0142],\n",
      "        [0.0034],\n",
      "        [0.0056],\n",
      "        [0.0425],\n",
      "        [0.1698],\n",
      "        [0.0421],\n",
      "        [0.0423],\n",
      "        [0.0841],\n",
      "        [0.0611],\n",
      "        [0.1739],\n",
      "        [0.2010],\n",
      "        [0.2060],\n",
      "        [0.2338]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 13.891911268234253\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 74\n",
      "剩餘X 資料 torch.Size([86, 18])\n",
      "剩餘Y 資料 torch.Size([86, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0593109130859375, 62)\n",
      "The second_loss value of k: (0.06110180541872978, 61)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引62，y= tensor([0.6430])\n",
      "目前模型的Data狀態 torch.Size([74, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8777],\n",
      "        [0.8865]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0738],\n",
      "        [0.0642],\n",
      "        [0.0356],\n",
      "        [0.0330],\n",
      "        [0.0112],\n",
      "        [0.0244],\n",
      "        [0.0230],\n",
      "        [0.0323],\n",
      "        [0.0849],\n",
      "        [0.0712],\n",
      "        [0.0462],\n",
      "        [0.0741],\n",
      "        [0.0285],\n",
      "        [0.0482],\n",
      "        [0.0282],\n",
      "        [0.0735],\n",
      "        [0.0683],\n",
      "        [0.0578],\n",
      "        [0.0633],\n",
      "        [0.1044],\n",
      "        [0.0512],\n",
      "        [0.0589],\n",
      "        [0.0904],\n",
      "        [0.0970],\n",
      "        [0.0481],\n",
      "        [0.0990],\n",
      "        [0.0345],\n",
      "        [0.0315],\n",
      "        [0.0253],\n",
      "        [0.0382],\n",
      "        [0.0103],\n",
      "        [0.0577],\n",
      "        [0.1223],\n",
      "        [0.0057],\n",
      "        [0.0115],\n",
      "        [0.0148],\n",
      "        [0.0201],\n",
      "        [0.0241],\n",
      "        [0.0282],\n",
      "        [0.0319],\n",
      "        [0.0364],\n",
      "        [0.0422],\n",
      "        [0.0907],\n",
      "        [0.1217],\n",
      "        [0.0661],\n",
      "        [0.0744],\n",
      "        [0.0892],\n",
      "        [0.0896],\n",
      "        [0.0978],\n",
      "        [0.1013],\n",
      "        [0.1023],\n",
      "        [0.1056],\n",
      "        [0.1150],\n",
      "        [0.1195],\n",
      "        [0.0753],\n",
      "        [0.0377],\n",
      "        [0.0816],\n",
      "        [0.0352],\n",
      "        [0.0775],\n",
      "        [0.0161],\n",
      "        [0.0142],\n",
      "        [0.0034],\n",
      "        [0.0056],\n",
      "        [0.0425],\n",
      "        [0.1698],\n",
      "        [0.0421],\n",
      "        [0.0423],\n",
      "        [0.0841],\n",
      "        [0.0611],\n",
      "        [0.1739],\n",
      "        [0.2010],\n",
      "        [0.2060],\n",
      "        [0.2338],\n",
      "        [0.2435]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0706],\n",
      "        [    0.0610],\n",
      "        [    0.0324],\n",
      "        [    0.0299],\n",
      "        [    0.0144],\n",
      "        [    0.0212],\n",
      "        [    0.0198],\n",
      "        [    0.0291],\n",
      "        [    0.0817],\n",
      "        [    0.0680],\n",
      "        [    0.0430],\n",
      "        [    0.0709],\n",
      "        [    0.0254],\n",
      "        [    0.0450],\n",
      "        [    0.0250],\n",
      "        [    0.0703],\n",
      "        [    0.0651],\n",
      "        [    0.0546],\n",
      "        [    0.0601],\n",
      "        [    0.1075],\n",
      "        [    0.0544],\n",
      "        [    0.0621],\n",
      "        [    0.0936],\n",
      "        [    0.1002],\n",
      "        [    0.0449],\n",
      "        [    0.1021],\n",
      "        [    0.0313],\n",
      "        [    0.0283],\n",
      "        [    0.0222],\n",
      "        [    0.0413],\n",
      "        [    0.0072],\n",
      "        [    0.0609],\n",
      "        [    0.1254],\n",
      "        [    0.0089],\n",
      "        [    0.0147],\n",
      "        [    0.0180],\n",
      "        [    0.0233],\n",
      "        [    0.0273],\n",
      "        [    0.0313],\n",
      "        [    0.0350],\n",
      "        [    0.0395],\n",
      "        [    0.0454],\n",
      "        [    0.0939],\n",
      "        [    0.1248],\n",
      "        [    0.0692],\n",
      "        [    0.0776],\n",
      "        [    0.0924],\n",
      "        [    0.0928],\n",
      "        [    0.1010],\n",
      "        [    0.1045],\n",
      "        [    0.1055],\n",
      "        [    0.1088],\n",
      "        [    0.1182],\n",
      "        [    0.1227],\n",
      "        [    0.0784],\n",
      "        [    0.0345],\n",
      "        [    0.0848],\n",
      "        [    0.0320],\n",
      "        [    0.0807],\n",
      "        [    0.0129],\n",
      "        [    0.0110],\n",
      "        [    0.0002],\n",
      "        [    0.0087],\n",
      "        [    0.0457],\n",
      "        [    0.1667],\n",
      "        [    0.0389],\n",
      "        [    0.0391],\n",
      "        [    0.0810],\n",
      "        [    0.0579],\n",
      "        [    0.1707],\n",
      "        [    0.1978],\n",
      "        [    0.2028],\n",
      "        [    0.2306],\n",
      "        [    0.2316]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 14.14669394493103\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 75\n",
      "剩餘X 資料 torch.Size([85, 18])\n",
      "剩餘Y 資料 torch.Size([85, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05954374745488167, 61)\n",
      "The second_loss value of k: (0.06048412248492241, 64)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引61，y= tensor([0.6306])\n",
      "目前模型的Data狀態 torch.Size([75, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746],\n",
      "        [0.8746]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0706],\n",
      "        [    0.0610],\n",
      "        [    0.0324],\n",
      "        [    0.0299],\n",
      "        [    0.0144],\n",
      "        [    0.0212],\n",
      "        [    0.0198],\n",
      "        [    0.0291],\n",
      "        [    0.0817],\n",
      "        [    0.0680],\n",
      "        [    0.0430],\n",
      "        [    0.0709],\n",
      "        [    0.0254],\n",
      "        [    0.0450],\n",
      "        [    0.0250],\n",
      "        [    0.0703],\n",
      "        [    0.0651],\n",
      "        [    0.0546],\n",
      "        [    0.0601],\n",
      "        [    0.1075],\n",
      "        [    0.0544],\n",
      "        [    0.0621],\n",
      "        [    0.0936],\n",
      "        [    0.1002],\n",
      "        [    0.0449],\n",
      "        [    0.1021],\n",
      "        [    0.0313],\n",
      "        [    0.0283],\n",
      "        [    0.0222],\n",
      "        [    0.0413],\n",
      "        [    0.0072],\n",
      "        [    0.0609],\n",
      "        [    0.1254],\n",
      "        [    0.0089],\n",
      "        [    0.0147],\n",
      "        [    0.0180],\n",
      "        [    0.0233],\n",
      "        [    0.0273],\n",
      "        [    0.0313],\n",
      "        [    0.0350],\n",
      "        [    0.0395],\n",
      "        [    0.0454],\n",
      "        [    0.0939],\n",
      "        [    0.1248],\n",
      "        [    0.0692],\n",
      "        [    0.0776],\n",
      "        [    0.0924],\n",
      "        [    0.0928],\n",
      "        [    0.1010],\n",
      "        [    0.1045],\n",
      "        [    0.1055],\n",
      "        [    0.1088],\n",
      "        [    0.1182],\n",
      "        [    0.1227],\n",
      "        [    0.0784],\n",
      "        [    0.0345],\n",
      "        [    0.0848],\n",
      "        [    0.0320],\n",
      "        [    0.0807],\n",
      "        [    0.0129],\n",
      "        [    0.0110],\n",
      "        [    0.0002],\n",
      "        [    0.0087],\n",
      "        [    0.0457],\n",
      "        [    0.1667],\n",
      "        [    0.0389],\n",
      "        [    0.0391],\n",
      "        [    0.0810],\n",
      "        [    0.0579],\n",
      "        [    0.1707],\n",
      "        [    0.1978],\n",
      "        [    0.2028],\n",
      "        [    0.2306],\n",
      "        [    0.2316],\n",
      "        [    0.2440]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0674],\n",
      "        [0.0578],\n",
      "        [0.0292],\n",
      "        [0.0266],\n",
      "        [0.0176],\n",
      "        [0.0180],\n",
      "        [0.0166],\n",
      "        [0.0259],\n",
      "        [0.0784],\n",
      "        [0.0648],\n",
      "        [0.0398],\n",
      "        [0.0677],\n",
      "        [0.0221],\n",
      "        [0.0418],\n",
      "        [0.0217],\n",
      "        [0.0670],\n",
      "        [0.0619],\n",
      "        [0.0513],\n",
      "        [0.0569],\n",
      "        [0.1108],\n",
      "        [0.0577],\n",
      "        [0.0654],\n",
      "        [0.0969],\n",
      "        [0.1034],\n",
      "        [0.0417],\n",
      "        [0.1054],\n",
      "        [0.0280],\n",
      "        [0.0251],\n",
      "        [0.0189],\n",
      "        [0.0446],\n",
      "        [0.0039],\n",
      "        [0.0641],\n",
      "        [0.1287],\n",
      "        [0.0122],\n",
      "        [0.0179],\n",
      "        [0.0213],\n",
      "        [0.0265],\n",
      "        [0.0305],\n",
      "        [0.0346],\n",
      "        [0.0383],\n",
      "        [0.0428],\n",
      "        [0.0487],\n",
      "        [0.0972],\n",
      "        [0.1281],\n",
      "        [0.0725],\n",
      "        [0.0808],\n",
      "        [0.0956],\n",
      "        [0.0960],\n",
      "        [0.1042],\n",
      "        [0.1077],\n",
      "        [0.1087],\n",
      "        [0.1120],\n",
      "        [0.1214],\n",
      "        [0.1259],\n",
      "        [0.0817],\n",
      "        [0.0313],\n",
      "        [0.0881],\n",
      "        [0.0288],\n",
      "        [0.0839],\n",
      "        [0.0097],\n",
      "        [0.0078],\n",
      "        [0.0030],\n",
      "        [0.0120],\n",
      "        [0.0490],\n",
      "        [0.1634],\n",
      "        [0.0357],\n",
      "        [0.0358],\n",
      "        [0.0777],\n",
      "        [0.0547],\n",
      "        [0.1675],\n",
      "        [0.1946],\n",
      "        [0.1996],\n",
      "        [0.2274],\n",
      "        [0.2283],\n",
      "        [0.2408]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 14.398259162902832\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 76\n",
      "剩餘X 資料 torch.Size([84, 18])\n",
      "剩餘Y 資料 torch.Size([84, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.058892518281936646, 63)\n",
      "The second_loss value of k: (0.05931149423122406, 66)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引63，y= tensor([0.6286])\n",
      "目前模型的Data狀態 torch.Size([76, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713],\n",
      "        [0.8713]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0674],\n",
      "        [0.0578],\n",
      "        [0.0292],\n",
      "        [0.0266],\n",
      "        [0.0176],\n",
      "        [0.0180],\n",
      "        [0.0166],\n",
      "        [0.0259],\n",
      "        [0.0784],\n",
      "        [0.0648],\n",
      "        [0.0398],\n",
      "        [0.0677],\n",
      "        [0.0221],\n",
      "        [0.0418],\n",
      "        [0.0217],\n",
      "        [0.0670],\n",
      "        [0.0619],\n",
      "        [0.0513],\n",
      "        [0.0569],\n",
      "        [0.1108],\n",
      "        [0.0577],\n",
      "        [0.0654],\n",
      "        [0.0969],\n",
      "        [0.1034],\n",
      "        [0.0417],\n",
      "        [0.1054],\n",
      "        [0.0280],\n",
      "        [0.0251],\n",
      "        [0.0189],\n",
      "        [0.0446],\n",
      "        [0.0039],\n",
      "        [0.0641],\n",
      "        [0.1287],\n",
      "        [0.0122],\n",
      "        [0.0179],\n",
      "        [0.0213],\n",
      "        [0.0265],\n",
      "        [0.0305],\n",
      "        [0.0346],\n",
      "        [0.0383],\n",
      "        [0.0428],\n",
      "        [0.0487],\n",
      "        [0.0972],\n",
      "        [0.1281],\n",
      "        [0.0725],\n",
      "        [0.0808],\n",
      "        [0.0956],\n",
      "        [0.0960],\n",
      "        [0.1042],\n",
      "        [0.1077],\n",
      "        [0.1087],\n",
      "        [0.1120],\n",
      "        [0.1214],\n",
      "        [0.1259],\n",
      "        [0.0817],\n",
      "        [0.0313],\n",
      "        [0.0881],\n",
      "        [0.0288],\n",
      "        [0.0839],\n",
      "        [0.0097],\n",
      "        [0.0078],\n",
      "        [0.0030],\n",
      "        [0.0120],\n",
      "        [0.0490],\n",
      "        [0.1634],\n",
      "        [0.0357],\n",
      "        [0.0358],\n",
      "        [0.0777],\n",
      "        [0.0547],\n",
      "        [0.1675],\n",
      "        [0.1946],\n",
      "        [0.1996],\n",
      "        [0.2274],\n",
      "        [0.2283],\n",
      "        [0.2408],\n",
      "        [0.2427]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0641],\n",
      "        [0.0546],\n",
      "        [0.0260],\n",
      "        [0.0234],\n",
      "        [0.0208],\n",
      "        [0.0148],\n",
      "        [0.0133],\n",
      "        [0.0227],\n",
      "        [0.0752],\n",
      "        [0.0616],\n",
      "        [0.0366],\n",
      "        [0.0645],\n",
      "        [0.0189],\n",
      "        [0.0386],\n",
      "        [0.0185],\n",
      "        [0.0638],\n",
      "        [0.0587],\n",
      "        [0.0481],\n",
      "        [0.0537],\n",
      "        [0.1140],\n",
      "        [0.0609],\n",
      "        [0.0686],\n",
      "        [0.1001],\n",
      "        [0.1066],\n",
      "        [0.0385],\n",
      "        [0.1086],\n",
      "        [0.0248],\n",
      "        [0.0218],\n",
      "        [0.0157],\n",
      "        [0.0478],\n",
      "        [0.0007],\n",
      "        [0.0673],\n",
      "        [0.1319],\n",
      "        [0.0154],\n",
      "        [0.0211],\n",
      "        [0.0245],\n",
      "        [0.0297],\n",
      "        [0.0337],\n",
      "        [0.0378],\n",
      "        [0.0415],\n",
      "        [0.0460],\n",
      "        [0.0519],\n",
      "        [0.1004],\n",
      "        [0.1313],\n",
      "        [0.0757],\n",
      "        [0.0840],\n",
      "        [0.0988],\n",
      "        [0.0992],\n",
      "        [0.1074],\n",
      "        [0.1109],\n",
      "        [0.1119],\n",
      "        [0.1152],\n",
      "        [0.1246],\n",
      "        [0.1292],\n",
      "        [0.0849],\n",
      "        [0.0281],\n",
      "        [0.0913],\n",
      "        [0.0256],\n",
      "        [0.0871],\n",
      "        [0.0065],\n",
      "        [0.0046],\n",
      "        [0.0062],\n",
      "        [0.0152],\n",
      "        [0.0522],\n",
      "        [0.1602],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0745],\n",
      "        [0.0515],\n",
      "        [0.1643],\n",
      "        [0.1914],\n",
      "        [0.1964],\n",
      "        [0.2242],\n",
      "        [0.2251],\n",
      "        [0.2376],\n",
      "        [0.2395]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 14.64995265007019\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 77\n",
      "剩餘X 資料 torch.Size([83, 18])\n",
      "剩餘Y 資料 torch.Size([83, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05776004493236542, 65)\n",
      "The second_loss value of k: (0.0632702186703682, 64)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引65，y= tensor([0.6278])\n",
      "目前模型的Data狀態 torch.Size([77, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681],\n",
      "        [0.8681]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0641],\n",
      "        [0.0546],\n",
      "        [0.0260],\n",
      "        [0.0234],\n",
      "        [0.0208],\n",
      "        [0.0148],\n",
      "        [0.0133],\n",
      "        [0.0227],\n",
      "        [0.0752],\n",
      "        [0.0616],\n",
      "        [0.0366],\n",
      "        [0.0645],\n",
      "        [0.0189],\n",
      "        [0.0386],\n",
      "        [0.0185],\n",
      "        [0.0638],\n",
      "        [0.0587],\n",
      "        [0.0481],\n",
      "        [0.0537],\n",
      "        [0.1140],\n",
      "        [0.0609],\n",
      "        [0.0686],\n",
      "        [0.1001],\n",
      "        [0.1066],\n",
      "        [0.0385],\n",
      "        [0.1086],\n",
      "        [0.0248],\n",
      "        [0.0218],\n",
      "        [0.0157],\n",
      "        [0.0478],\n",
      "        [0.0007],\n",
      "        [0.0673],\n",
      "        [0.1319],\n",
      "        [0.0154],\n",
      "        [0.0211],\n",
      "        [0.0245],\n",
      "        [0.0297],\n",
      "        [0.0337],\n",
      "        [0.0378],\n",
      "        [0.0415],\n",
      "        [0.0460],\n",
      "        [0.0519],\n",
      "        [0.1004],\n",
      "        [0.1313],\n",
      "        [0.0757],\n",
      "        [0.0840],\n",
      "        [0.0988],\n",
      "        [0.0992],\n",
      "        [0.1074],\n",
      "        [0.1109],\n",
      "        [0.1119],\n",
      "        [0.1152],\n",
      "        [0.1246],\n",
      "        [0.1292],\n",
      "        [0.0849],\n",
      "        [0.0281],\n",
      "        [0.0913],\n",
      "        [0.0256],\n",
      "        [0.0871],\n",
      "        [0.0065],\n",
      "        [0.0046],\n",
      "        [0.0062],\n",
      "        [0.0152],\n",
      "        [0.0522],\n",
      "        [0.1602],\n",
      "        [0.0325],\n",
      "        [0.0326],\n",
      "        [0.0745],\n",
      "        [0.0515],\n",
      "        [0.1643],\n",
      "        [0.1914],\n",
      "        [0.1964],\n",
      "        [0.2242],\n",
      "        [0.2251],\n",
      "        [0.2376],\n",
      "        [0.2395],\n",
      "        [0.2403]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0610],\n",
      "        [0.0514],\n",
      "        [0.0229],\n",
      "        [0.0203],\n",
      "        [0.0240],\n",
      "        [0.0117],\n",
      "        [0.0102],\n",
      "        [0.0196],\n",
      "        [0.0721],\n",
      "        [0.0585],\n",
      "        [0.0335],\n",
      "        [0.0614],\n",
      "        [0.0158],\n",
      "        [0.0355],\n",
      "        [0.0154],\n",
      "        [0.0607],\n",
      "        [0.0556],\n",
      "        [0.0450],\n",
      "        [0.0505],\n",
      "        [0.1171],\n",
      "        [0.0640],\n",
      "        [0.0717],\n",
      "        [0.1032],\n",
      "        [0.1097],\n",
      "        [0.0353],\n",
      "        [0.1117],\n",
      "        [0.0217],\n",
      "        [0.0187],\n",
      "        [0.0126],\n",
      "        [0.0509],\n",
      "        [0.0024],\n",
      "        [0.0704],\n",
      "        [0.1350],\n",
      "        [0.0185],\n",
      "        [0.0242],\n",
      "        [0.0276],\n",
      "        [0.0328],\n",
      "        [0.0368],\n",
      "        [0.0409],\n",
      "        [0.0446],\n",
      "        [0.0491],\n",
      "        [0.0550],\n",
      "        [0.1035],\n",
      "        [0.1344],\n",
      "        [0.0788],\n",
      "        [0.0871],\n",
      "        [0.1019],\n",
      "        [0.1023],\n",
      "        [0.1105],\n",
      "        [0.1140],\n",
      "        [0.1150],\n",
      "        [0.1184],\n",
      "        [0.1278],\n",
      "        [0.1323],\n",
      "        [0.0880],\n",
      "        [0.0250],\n",
      "        [0.0944],\n",
      "        [0.0225],\n",
      "        [0.0902],\n",
      "        [0.0033],\n",
      "        [0.0015],\n",
      "        [0.0093],\n",
      "        [0.0183],\n",
      "        [0.0553],\n",
      "        [0.1571],\n",
      "        [0.0294],\n",
      "        [0.0295],\n",
      "        [0.0714],\n",
      "        [0.0484],\n",
      "        [0.1612],\n",
      "        [0.1883],\n",
      "        [0.1933],\n",
      "        [0.2211],\n",
      "        [0.2220],\n",
      "        [0.2344],\n",
      "        [0.2364],\n",
      "        [0.2372]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 14.896797895431519\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 78\n",
      "剩餘X 資料 torch.Size([82, 18])\n",
      "剩餘Y 資料 torch.Size([82, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06171806529164314, 64)\n",
      "The second_loss value of k: (0.06409484148025513, 62)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引64，y= tensor([0.6166])\n",
      "目前模型的Data狀態 torch.Size([78, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650],\n",
      "        [0.8650]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0610],\n",
      "        [0.0514],\n",
      "        [0.0229],\n",
      "        [0.0203],\n",
      "        [0.0240],\n",
      "        [0.0117],\n",
      "        [0.0102],\n",
      "        [0.0196],\n",
      "        [0.0721],\n",
      "        [0.0585],\n",
      "        [0.0335],\n",
      "        [0.0614],\n",
      "        [0.0158],\n",
      "        [0.0355],\n",
      "        [0.0154],\n",
      "        [0.0607],\n",
      "        [0.0556],\n",
      "        [0.0450],\n",
      "        [0.0505],\n",
      "        [0.1171],\n",
      "        [0.0640],\n",
      "        [0.0717],\n",
      "        [0.1032],\n",
      "        [0.1097],\n",
      "        [0.0353],\n",
      "        [0.1117],\n",
      "        [0.0217],\n",
      "        [0.0187],\n",
      "        [0.0126],\n",
      "        [0.0509],\n",
      "        [0.0024],\n",
      "        [0.0704],\n",
      "        [0.1350],\n",
      "        [0.0185],\n",
      "        [0.0242],\n",
      "        [0.0276],\n",
      "        [0.0328],\n",
      "        [0.0368],\n",
      "        [0.0409],\n",
      "        [0.0446],\n",
      "        [0.0491],\n",
      "        [0.0550],\n",
      "        [0.1035],\n",
      "        [0.1344],\n",
      "        [0.0788],\n",
      "        [0.0871],\n",
      "        [0.1019],\n",
      "        [0.1023],\n",
      "        [0.1105],\n",
      "        [0.1140],\n",
      "        [0.1150],\n",
      "        [0.1184],\n",
      "        [0.1278],\n",
      "        [0.1323],\n",
      "        [0.0880],\n",
      "        [0.0250],\n",
      "        [0.0944],\n",
      "        [0.0225],\n",
      "        [0.0902],\n",
      "        [0.0033],\n",
      "        [0.0015],\n",
      "        [0.0093],\n",
      "        [0.0183],\n",
      "        [0.0553],\n",
      "        [0.1571],\n",
      "        [0.0294],\n",
      "        [0.0295],\n",
      "        [0.0714],\n",
      "        [0.0484],\n",
      "        [0.1612],\n",
      "        [0.1883],\n",
      "        [0.1933],\n",
      "        [0.2211],\n",
      "        [0.2220],\n",
      "        [0.2344],\n",
      "        [0.2364],\n",
      "        [0.2372],\n",
      "        [0.2484]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 81\n",
      "Number of shrink: 19\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0579],\n",
      "        [    0.0483],\n",
      "        [    0.0197],\n",
      "        [    0.0171],\n",
      "        [    0.0271],\n",
      "        [    0.0085],\n",
      "        [    0.0071],\n",
      "        [    0.0164],\n",
      "        [    0.0689],\n",
      "        [    0.0553],\n",
      "        [    0.0303],\n",
      "        [    0.0582],\n",
      "        [    0.0126],\n",
      "        [    0.0323],\n",
      "        [    0.0122],\n",
      "        [    0.0575],\n",
      "        [    0.0524],\n",
      "        [    0.0418],\n",
      "        [    0.0474],\n",
      "        [    0.1203],\n",
      "        [    0.0672],\n",
      "        [    0.0748],\n",
      "        [    0.1064],\n",
      "        [    0.1129],\n",
      "        [    0.0322],\n",
      "        [    0.1149],\n",
      "        [    0.0185],\n",
      "        [    0.0156],\n",
      "        [    0.0094],\n",
      "        [    0.0541],\n",
      "        [    0.0056],\n",
      "        [    0.0736],\n",
      "        [    0.1382],\n",
      "        [    0.0217],\n",
      "        [    0.0274],\n",
      "        [    0.0308],\n",
      "        [    0.0360],\n",
      "        [    0.0400],\n",
      "        [    0.0441],\n",
      "        [    0.0478],\n",
      "        [    0.0523],\n",
      "        [    0.0582],\n",
      "        [    0.1067],\n",
      "        [    0.1376],\n",
      "        [    0.0820],\n",
      "        [    0.0903],\n",
      "        [    0.1051],\n",
      "        [    0.1055],\n",
      "        [    0.1137],\n",
      "        [    0.1172],\n",
      "        [    0.1182],\n",
      "        [    0.1215],\n",
      "        [    0.1309],\n",
      "        [    0.1354],\n",
      "        [    0.0912],\n",
      "        [    0.0218],\n",
      "        [    0.0976],\n",
      "        [    0.0193],\n",
      "        [    0.0934],\n",
      "        [    0.0002],\n",
      "        [    0.0017],\n",
      "        [    0.0125],\n",
      "        [    0.0215],\n",
      "        [    0.0585],\n",
      "        [    0.1539],\n",
      "        [    0.0262],\n",
      "        [    0.0263],\n",
      "        [    0.0682],\n",
      "        [    0.0452],\n",
      "        [    0.1580],\n",
      "        [    0.1851],\n",
      "        [    0.1901],\n",
      "        [    0.2179],\n",
      "        [    0.2188],\n",
      "        [    0.2313],\n",
      "        [    0.2332],\n",
      "        [    0.2340],\n",
      "        [    0.2452]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 15.143584251403809\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 79\n",
      "剩餘X 資料 torch.Size([81, 18])\n",
      "剩餘Y 資料 torch.Size([81, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06249228119850159, 62)\n",
      "The second_loss value of k: (0.0649448037147522, 63)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引62，y= tensor([0.6333])\n",
      "目前模型的Data狀態 torch.Size([79, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8618],\n",
      "        [0.8833]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0579],\n",
      "        [    0.0483],\n",
      "        [    0.0197],\n",
      "        [    0.0171],\n",
      "        [    0.0271],\n",
      "        [    0.0085],\n",
      "        [    0.0071],\n",
      "        [    0.0164],\n",
      "        [    0.0689],\n",
      "        [    0.0553],\n",
      "        [    0.0303],\n",
      "        [    0.0582],\n",
      "        [    0.0126],\n",
      "        [    0.0323],\n",
      "        [    0.0122],\n",
      "        [    0.0575],\n",
      "        [    0.0524],\n",
      "        [    0.0418],\n",
      "        [    0.0474],\n",
      "        [    0.1203],\n",
      "        [    0.0672],\n",
      "        [    0.0748],\n",
      "        [    0.1064],\n",
      "        [    0.1129],\n",
      "        [    0.0322],\n",
      "        [    0.1149],\n",
      "        [    0.0185],\n",
      "        [    0.0156],\n",
      "        [    0.0094],\n",
      "        [    0.0541],\n",
      "        [    0.0056],\n",
      "        [    0.0736],\n",
      "        [    0.1382],\n",
      "        [    0.0217],\n",
      "        [    0.0274],\n",
      "        [    0.0308],\n",
      "        [    0.0360],\n",
      "        [    0.0400],\n",
      "        [    0.0441],\n",
      "        [    0.0478],\n",
      "        [    0.0523],\n",
      "        [    0.0582],\n",
      "        [    0.1067],\n",
      "        [    0.1376],\n",
      "        [    0.0820],\n",
      "        [    0.0903],\n",
      "        [    0.1051],\n",
      "        [    0.1055],\n",
      "        [    0.1137],\n",
      "        [    0.1172],\n",
      "        [    0.1182],\n",
      "        [    0.1215],\n",
      "        [    0.1309],\n",
      "        [    0.1354],\n",
      "        [    0.0912],\n",
      "        [    0.0218],\n",
      "        [    0.0976],\n",
      "        [    0.0193],\n",
      "        [    0.0934],\n",
      "        [    0.0002],\n",
      "        [    0.0017],\n",
      "        [    0.0125],\n",
      "        [    0.0215],\n",
      "        [    0.0585],\n",
      "        [    0.1539],\n",
      "        [    0.0262],\n",
      "        [    0.0263],\n",
      "        [    0.0682],\n",
      "        [    0.0452],\n",
      "        [    0.1580],\n",
      "        [    0.1851],\n",
      "        [    0.1901],\n",
      "        [    0.2179],\n",
      "        [    0.2188],\n",
      "        [    0.2313],\n",
      "        [    0.2332],\n",
      "        [    0.2340],\n",
      "        [    0.2452],\n",
      "        [    0.2500]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0550],\n",
      "        [0.0454],\n",
      "        [0.0168],\n",
      "        [0.0142],\n",
      "        [0.0300],\n",
      "        [0.0056],\n",
      "        [0.0042],\n",
      "        [0.0135],\n",
      "        [0.0660],\n",
      "        [0.0524],\n",
      "        [0.0274],\n",
      "        [0.0553],\n",
      "        [0.0097],\n",
      "        [0.0294],\n",
      "        [0.0093],\n",
      "        [0.0546],\n",
      "        [0.0495],\n",
      "        [0.0389],\n",
      "        [0.0445],\n",
      "        [0.1232],\n",
      "        [0.0701],\n",
      "        [0.0777],\n",
      "        [0.1093],\n",
      "        [0.1158],\n",
      "        [0.0293],\n",
      "        [0.1178],\n",
      "        [0.0156],\n",
      "        [0.0127],\n",
      "        [0.0065],\n",
      "        [0.0570],\n",
      "        [0.0085],\n",
      "        [0.0765],\n",
      "        [0.1411],\n",
      "        [0.0246],\n",
      "        [0.0303],\n",
      "        [0.0337],\n",
      "        [0.0389],\n",
      "        [0.0429],\n",
      "        [0.0470],\n",
      "        [0.0507],\n",
      "        [0.0552],\n",
      "        [0.0611],\n",
      "        [0.1096],\n",
      "        [0.1405],\n",
      "        [0.0849],\n",
      "        [0.0932],\n",
      "        [0.1080],\n",
      "        [0.1084],\n",
      "        [0.1166],\n",
      "        [0.1201],\n",
      "        [0.1211],\n",
      "        [0.1244],\n",
      "        [0.1338],\n",
      "        [0.1383],\n",
      "        [0.0941],\n",
      "        [0.0189],\n",
      "        [0.1005],\n",
      "        [0.0164],\n",
      "        [0.0963],\n",
      "        [0.0027],\n",
      "        [0.0046],\n",
      "        [0.0154],\n",
      "        [0.0244],\n",
      "        [0.0614],\n",
      "        [0.1510],\n",
      "        [0.0233],\n",
      "        [0.0234],\n",
      "        [0.0653],\n",
      "        [0.0423],\n",
      "        [0.1551],\n",
      "        [0.1822],\n",
      "        [0.1872],\n",
      "        [0.2150],\n",
      "        [0.2159],\n",
      "        [0.2284],\n",
      "        [0.2303],\n",
      "        [0.2311],\n",
      "        [0.2423],\n",
      "        [0.2256]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 15.393832921981812\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 80\n",
      "剩餘X 資料 torch.Size([80, 18])\n",
      "剩餘Y 資料 torch.Size([80, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06347589939832687, 62)\n",
      "The second_loss value of k: (0.0727158710360527, 61)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引62，y= tensor([0.6070])\n",
      "目前模型的Data狀態 torch.Size([80, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589],\n",
      "        [0.8589]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0550],\n",
      "        [0.0454],\n",
      "        [0.0168],\n",
      "        [0.0142],\n",
      "        [0.0300],\n",
      "        [0.0056],\n",
      "        [0.0042],\n",
      "        [0.0135],\n",
      "        [0.0660],\n",
      "        [0.0524],\n",
      "        [0.0274],\n",
      "        [0.0553],\n",
      "        [0.0097],\n",
      "        [0.0294],\n",
      "        [0.0093],\n",
      "        [0.0546],\n",
      "        [0.0495],\n",
      "        [0.0389],\n",
      "        [0.0445],\n",
      "        [0.1232],\n",
      "        [0.0701],\n",
      "        [0.0777],\n",
      "        [0.1093],\n",
      "        [0.1158],\n",
      "        [0.0293],\n",
      "        [0.1178],\n",
      "        [0.0156],\n",
      "        [0.0127],\n",
      "        [0.0065],\n",
      "        [0.0570],\n",
      "        [0.0085],\n",
      "        [0.0765],\n",
      "        [0.1411],\n",
      "        [0.0246],\n",
      "        [0.0303],\n",
      "        [0.0337],\n",
      "        [0.0389],\n",
      "        [0.0429],\n",
      "        [0.0470],\n",
      "        [0.0507],\n",
      "        [0.0552],\n",
      "        [0.0611],\n",
      "        [0.1096],\n",
      "        [0.1405],\n",
      "        [0.0849],\n",
      "        [0.0932],\n",
      "        [0.1080],\n",
      "        [0.1084],\n",
      "        [0.1166],\n",
      "        [0.1201],\n",
      "        [0.1211],\n",
      "        [0.1244],\n",
      "        [0.1338],\n",
      "        [0.1383],\n",
      "        [0.0941],\n",
      "        [0.0189],\n",
      "        [0.1005],\n",
      "        [0.0164],\n",
      "        [0.0963],\n",
      "        [0.0027],\n",
      "        [0.0046],\n",
      "        [0.0154],\n",
      "        [0.0244],\n",
      "        [0.0614],\n",
      "        [0.1510],\n",
      "        [0.0233],\n",
      "        [0.0234],\n",
      "        [0.0653],\n",
      "        [0.0423],\n",
      "        [0.1551],\n",
      "        [0.1822],\n",
      "        [0.1872],\n",
      "        [0.2150],\n",
      "        [0.2159],\n",
      "        [0.2284],\n",
      "        [0.2303],\n",
      "        [0.2311],\n",
      "        [0.2423],\n",
      "        [0.2256],\n",
      "        [0.2519]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0518],\n",
      "        [0.0422],\n",
      "        [0.0136],\n",
      "        [0.0111],\n",
      "        [0.0332],\n",
      "        [0.0024],\n",
      "        [0.0010],\n",
      "        [0.0103],\n",
      "        [0.0629],\n",
      "        [0.0492],\n",
      "        [0.0242],\n",
      "        [0.0521],\n",
      "        [0.0066],\n",
      "        [0.0262],\n",
      "        [0.0062],\n",
      "        [0.0515],\n",
      "        [0.0463],\n",
      "        [0.0358],\n",
      "        [0.0413],\n",
      "        [0.1263],\n",
      "        [0.0732],\n",
      "        [0.0809],\n",
      "        [0.1124],\n",
      "        [0.1190],\n",
      "        [0.0261],\n",
      "        [0.1209],\n",
      "        [0.0125],\n",
      "        [0.0095],\n",
      "        [0.0034],\n",
      "        [0.0601],\n",
      "        [0.0116],\n",
      "        [0.0797],\n",
      "        [0.1442],\n",
      "        [0.0277],\n",
      "        [0.0335],\n",
      "        [0.0368],\n",
      "        [0.0421],\n",
      "        [0.0461],\n",
      "        [0.0501],\n",
      "        [0.0538],\n",
      "        [0.0583],\n",
      "        [0.0642],\n",
      "        [0.1127],\n",
      "        [0.1436],\n",
      "        [0.0880],\n",
      "        [0.0964],\n",
      "        [0.1112],\n",
      "        [0.1116],\n",
      "        [0.1197],\n",
      "        [0.1233],\n",
      "        [0.1243],\n",
      "        [0.1276],\n",
      "        [0.1370],\n",
      "        [0.1415],\n",
      "        [0.0972],\n",
      "        [0.0157],\n",
      "        [0.1036],\n",
      "        [0.0132],\n",
      "        [0.0995],\n",
      "        [0.0059],\n",
      "        [0.0078],\n",
      "        [0.0186],\n",
      "        [0.0275],\n",
      "        [0.0645],\n",
      "        [0.1479],\n",
      "        [0.0201],\n",
      "        [0.0203],\n",
      "        [0.0622],\n",
      "        [0.0391],\n",
      "        [0.1519],\n",
      "        [0.1790],\n",
      "        [0.1841],\n",
      "        [0.2118],\n",
      "        [0.2128],\n",
      "        [0.2252],\n",
      "        [0.2271],\n",
      "        [0.2280],\n",
      "        [0.2392],\n",
      "        [0.2224],\n",
      "        [0.2488]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 15.640946626663208\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 81\n",
      "剩餘X 資料 torch.Size([79, 18])\n",
      "剩餘Y 資料 torch.Size([79, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.07103066891431808, 61)\n",
      "The second_loss value of k: (0.07810782641172409, 63)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引61，y= tensor([0.6216])\n",
      "目前模型的Data狀態 torch.Size([81, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8558],\n",
      "        [0.8881]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0518],\n",
      "        [0.0422],\n",
      "        [0.0136],\n",
      "        [0.0111],\n",
      "        [0.0332],\n",
      "        [0.0024],\n",
      "        [0.0010],\n",
      "        [0.0103],\n",
      "        [0.0629],\n",
      "        [0.0492],\n",
      "        [0.0242],\n",
      "        [0.0521],\n",
      "        [0.0066],\n",
      "        [0.0262],\n",
      "        [0.0062],\n",
      "        [0.0515],\n",
      "        [0.0463],\n",
      "        [0.0358],\n",
      "        [0.0413],\n",
      "        [0.1263],\n",
      "        [0.0732],\n",
      "        [0.0809],\n",
      "        [0.1124],\n",
      "        [0.1190],\n",
      "        [0.0261],\n",
      "        [0.1209],\n",
      "        [0.0125],\n",
      "        [0.0095],\n",
      "        [0.0034],\n",
      "        [0.0601],\n",
      "        [0.0116],\n",
      "        [0.0797],\n",
      "        [0.1442],\n",
      "        [0.0277],\n",
      "        [0.0335],\n",
      "        [0.0368],\n",
      "        [0.0421],\n",
      "        [0.0461],\n",
      "        [0.0501],\n",
      "        [0.0538],\n",
      "        [0.0583],\n",
      "        [0.0642],\n",
      "        [0.1127],\n",
      "        [0.1436],\n",
      "        [0.0880],\n",
      "        [0.0964],\n",
      "        [0.1112],\n",
      "        [0.1116],\n",
      "        [0.1197],\n",
      "        [0.1233],\n",
      "        [0.1243],\n",
      "        [0.1276],\n",
      "        [0.1370],\n",
      "        [0.1415],\n",
      "        [0.0972],\n",
      "        [0.0157],\n",
      "        [0.1036],\n",
      "        [0.0132],\n",
      "        [0.0995],\n",
      "        [0.0059],\n",
      "        [0.0078],\n",
      "        [0.0186],\n",
      "        [0.0275],\n",
      "        [0.0645],\n",
      "        [0.1479],\n",
      "        [0.0201],\n",
      "        [0.0203],\n",
      "        [0.0622],\n",
      "        [0.0391],\n",
      "        [0.1519],\n",
      "        [0.1790],\n",
      "        [0.1841],\n",
      "        [0.2118],\n",
      "        [0.2128],\n",
      "        [0.2252],\n",
      "        [0.2271],\n",
      "        [0.2280],\n",
      "        [0.2392],\n",
      "        [0.2224],\n",
      "        [0.2488],\n",
      "        [0.2665]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0489],\n",
      "        [0.0393],\n",
      "        [0.0107],\n",
      "        [0.0082],\n",
      "        [0.0361],\n",
      "        [0.0005],\n",
      "        [0.0019],\n",
      "        [0.0074],\n",
      "        [0.0600],\n",
      "        [0.0463],\n",
      "        [0.0213],\n",
      "        [0.0492],\n",
      "        [0.0037],\n",
      "        [0.0233],\n",
      "        [0.0033],\n",
      "        [0.0486],\n",
      "        [0.0434],\n",
      "        [0.0329],\n",
      "        [0.0384],\n",
      "        [0.1292],\n",
      "        [0.0761],\n",
      "        [0.0838],\n",
      "        [0.1153],\n",
      "        [0.1219],\n",
      "        [0.0232],\n",
      "        [0.1238],\n",
      "        [0.0096],\n",
      "        [0.0066],\n",
      "        [0.0005],\n",
      "        [0.0630],\n",
      "        [0.0145],\n",
      "        [0.0826],\n",
      "        [0.1471],\n",
      "        [0.0306],\n",
      "        [0.0364],\n",
      "        [0.0397],\n",
      "        [0.0450],\n",
      "        [0.0490],\n",
      "        [0.0530],\n",
      "        [0.0567],\n",
      "        [0.0612],\n",
      "        [0.0671],\n",
      "        [0.1156],\n",
      "        [0.1465],\n",
      "        [0.0909],\n",
      "        [0.0993],\n",
      "        [0.1141],\n",
      "        [0.1145],\n",
      "        [0.1227],\n",
      "        [0.1262],\n",
      "        [0.1272],\n",
      "        [0.1305],\n",
      "        [0.1399],\n",
      "        [0.1444],\n",
      "        [0.1001],\n",
      "        [0.0128],\n",
      "        [0.1065],\n",
      "        [0.0103],\n",
      "        [0.1024],\n",
      "        [0.0088],\n",
      "        [0.0107],\n",
      "        [0.0215],\n",
      "        [0.0304],\n",
      "        [0.0674],\n",
      "        [0.1450],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0592],\n",
      "        [0.0362],\n",
      "        [0.1490],\n",
      "        [0.1761],\n",
      "        [0.1811],\n",
      "        [0.2089],\n",
      "        [0.2099],\n",
      "        [0.2223],\n",
      "        [0.2242],\n",
      "        [0.2251],\n",
      "        [0.2363],\n",
      "        [0.2195],\n",
      "        [0.2459],\n",
      "        [0.2313]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 15.915714979171753\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 82\n",
      "剩餘X 資料 torch.Size([78, 18])\n",
      "剩餘Y 資料 torch.Size([78, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05886373668909073, 0)\n",
      "The second_loss value of k: (0.06599636375904083, 61)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.6492])\n",
      "目前模型的Data狀態 torch.Size([82, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8529],\n",
      "        [0.8919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0489],\n",
      "        [0.0393],\n",
      "        [0.0107],\n",
      "        [0.0082],\n",
      "        [0.0361],\n",
      "        [0.0005],\n",
      "        [0.0019],\n",
      "        [0.0074],\n",
      "        [0.0600],\n",
      "        [0.0463],\n",
      "        [0.0213],\n",
      "        [0.0492],\n",
      "        [0.0037],\n",
      "        [0.0233],\n",
      "        [0.0033],\n",
      "        [0.0486],\n",
      "        [0.0434],\n",
      "        [0.0329],\n",
      "        [0.0384],\n",
      "        [0.1292],\n",
      "        [0.0761],\n",
      "        [0.0838],\n",
      "        [0.1153],\n",
      "        [0.1219],\n",
      "        [0.0232],\n",
      "        [0.1238],\n",
      "        [0.0096],\n",
      "        [0.0066],\n",
      "        [0.0005],\n",
      "        [0.0630],\n",
      "        [0.0145],\n",
      "        [0.0826],\n",
      "        [0.1471],\n",
      "        [0.0306],\n",
      "        [0.0364],\n",
      "        [0.0397],\n",
      "        [0.0450],\n",
      "        [0.0490],\n",
      "        [0.0530],\n",
      "        [0.0567],\n",
      "        [0.0612],\n",
      "        [0.0671],\n",
      "        [0.1156],\n",
      "        [0.1465],\n",
      "        [0.0909],\n",
      "        [0.0993],\n",
      "        [0.1141],\n",
      "        [0.1145],\n",
      "        [0.1227],\n",
      "        [0.1262],\n",
      "        [0.1272],\n",
      "        [0.1305],\n",
      "        [0.1399],\n",
      "        [0.1444],\n",
      "        [0.1001],\n",
      "        [0.0128],\n",
      "        [0.1065],\n",
      "        [0.0103],\n",
      "        [0.1024],\n",
      "        [0.0088],\n",
      "        [0.0107],\n",
      "        [0.0215],\n",
      "        [0.0304],\n",
      "        [0.0674],\n",
      "        [0.1450],\n",
      "        [0.0172],\n",
      "        [0.0174],\n",
      "        [0.0592],\n",
      "        [0.0362],\n",
      "        [0.1490],\n",
      "        [0.1761],\n",
      "        [0.1811],\n",
      "        [0.2089],\n",
      "        [0.2099],\n",
      "        [0.2223],\n",
      "        [0.2242],\n",
      "        [0.2251],\n",
      "        [0.2363],\n",
      "        [0.2195],\n",
      "        [0.2459],\n",
      "        [0.2313],\n",
      "        [0.2426]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 80\n",
      "Number of shrink: 20\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0464],\n",
      "        [0.0368],\n",
      "        [0.0083],\n",
      "        [0.0057],\n",
      "        [0.0386],\n",
      "        [0.0030],\n",
      "        [0.0044],\n",
      "        [0.0050],\n",
      "        [0.0575],\n",
      "        [0.0439],\n",
      "        [0.0189],\n",
      "        [0.0468],\n",
      "        [0.0012],\n",
      "        [0.0209],\n",
      "        [0.0008],\n",
      "        [0.0461],\n",
      "        [0.0410],\n",
      "        [0.0304],\n",
      "        [0.0359],\n",
      "        [0.1317],\n",
      "        [0.0786],\n",
      "        [0.0863],\n",
      "        [0.1178],\n",
      "        [0.1243],\n",
      "        [0.0207],\n",
      "        [0.1263],\n",
      "        [0.0071],\n",
      "        [0.0041],\n",
      "        [0.0020],\n",
      "        [0.0655],\n",
      "        [0.0170],\n",
      "        [0.0850],\n",
      "        [0.1496],\n",
      "        [0.0331],\n",
      "        [0.0388],\n",
      "        [0.0422],\n",
      "        [0.0474],\n",
      "        [0.0514],\n",
      "        [0.0555],\n",
      "        [0.0592],\n",
      "        [0.0637],\n",
      "        [0.0696],\n",
      "        [0.1181],\n",
      "        [0.1490],\n",
      "        [0.0934],\n",
      "        [0.1017],\n",
      "        [0.1165],\n",
      "        [0.1169],\n",
      "        [0.1251],\n",
      "        [0.1286],\n",
      "        [0.1296],\n",
      "        [0.1330],\n",
      "        [0.1424],\n",
      "        [0.1469],\n",
      "        [0.1026],\n",
      "        [0.0104],\n",
      "        [0.1090],\n",
      "        [0.0079],\n",
      "        [0.1049],\n",
      "        [0.0113],\n",
      "        [0.0131],\n",
      "        [0.0239],\n",
      "        [0.0329],\n",
      "        [0.0699],\n",
      "        [0.1425],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0568],\n",
      "        [0.0338],\n",
      "        [0.1466],\n",
      "        [0.1737],\n",
      "        [0.1787],\n",
      "        [0.2065],\n",
      "        [0.2074],\n",
      "        [0.2198],\n",
      "        [0.2218],\n",
      "        [0.2226],\n",
      "        [0.2338],\n",
      "        [0.2171],\n",
      "        [0.2434],\n",
      "        [0.2288],\n",
      "        [0.2012]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 16.21140193939209\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 83\n",
      "剩餘X 資料 torch.Size([77, 18])\n",
      "剩餘Y 資料 torch.Size([77, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0591229647397995, 60)\n",
      "The second_loss value of k: (0.06745495647192001, 61)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引60，y= tensor([0.6072])\n",
      "目前模型的Data狀態 torch.Size([83, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504],\n",
      "        [0.8504]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0464],\n",
      "        [0.0368],\n",
      "        [0.0083],\n",
      "        [0.0057],\n",
      "        [0.0386],\n",
      "        [0.0030],\n",
      "        [0.0044],\n",
      "        [0.0050],\n",
      "        [0.0575],\n",
      "        [0.0439],\n",
      "        [0.0189],\n",
      "        [0.0468],\n",
      "        [0.0012],\n",
      "        [0.0209],\n",
      "        [0.0008],\n",
      "        [0.0461],\n",
      "        [0.0410],\n",
      "        [0.0304],\n",
      "        [0.0359],\n",
      "        [0.1317],\n",
      "        [0.0786],\n",
      "        [0.0863],\n",
      "        [0.1178],\n",
      "        [0.1243],\n",
      "        [0.0207],\n",
      "        [0.1263],\n",
      "        [0.0071],\n",
      "        [0.0041],\n",
      "        [0.0020],\n",
      "        [0.0655],\n",
      "        [0.0170],\n",
      "        [0.0850],\n",
      "        [0.1496],\n",
      "        [0.0331],\n",
      "        [0.0388],\n",
      "        [0.0422],\n",
      "        [0.0474],\n",
      "        [0.0514],\n",
      "        [0.0555],\n",
      "        [0.0592],\n",
      "        [0.0637],\n",
      "        [0.0696],\n",
      "        [0.1181],\n",
      "        [0.1490],\n",
      "        [0.0934],\n",
      "        [0.1017],\n",
      "        [0.1165],\n",
      "        [0.1169],\n",
      "        [0.1251],\n",
      "        [0.1286],\n",
      "        [0.1296],\n",
      "        [0.1330],\n",
      "        [0.1424],\n",
      "        [0.1469],\n",
      "        [0.1026],\n",
      "        [0.0104],\n",
      "        [0.1090],\n",
      "        [0.0079],\n",
      "        [0.1049],\n",
      "        [0.0113],\n",
      "        [0.0131],\n",
      "        [0.0239],\n",
      "        [0.0329],\n",
      "        [0.0699],\n",
      "        [0.1425],\n",
      "        [0.0148],\n",
      "        [0.0149],\n",
      "        [0.0568],\n",
      "        [0.0338],\n",
      "        [0.1466],\n",
      "        [0.1737],\n",
      "        [0.1787],\n",
      "        [0.2065],\n",
      "        [0.2074],\n",
      "        [0.2198],\n",
      "        [0.2218],\n",
      "        [0.2226],\n",
      "        [0.2338],\n",
      "        [0.2171],\n",
      "        [0.2434],\n",
      "        [0.2288],\n",
      "        [0.2012],\n",
      "        [0.2432]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0435],\n",
      "        [0.0339],\n",
      "        [0.0053],\n",
      "        [0.0028],\n",
      "        [0.0415],\n",
      "        [0.0059],\n",
      "        [0.0073],\n",
      "        [0.0020],\n",
      "        [0.0546],\n",
      "        [0.0409],\n",
      "        [0.0159],\n",
      "        [0.0438],\n",
      "        [0.0017],\n",
      "        [0.0179],\n",
      "        [0.0021],\n",
      "        [0.0432],\n",
      "        [0.0380],\n",
      "        [0.0275],\n",
      "        [0.0330],\n",
      "        [0.1346],\n",
      "        [0.0815],\n",
      "        [0.0892],\n",
      "        [0.1207],\n",
      "        [0.1273],\n",
      "        [0.0178],\n",
      "        [0.1292],\n",
      "        [0.0042],\n",
      "        [0.0012],\n",
      "        [0.0049],\n",
      "        [0.0684],\n",
      "        [0.0199],\n",
      "        [0.0880],\n",
      "        [0.1525],\n",
      "        [0.0360],\n",
      "        [0.0418],\n",
      "        [0.0451],\n",
      "        [0.0504],\n",
      "        [0.0544],\n",
      "        [0.0584],\n",
      "        [0.0621],\n",
      "        [0.0666],\n",
      "        [0.0725],\n",
      "        [0.1210],\n",
      "        [0.1519],\n",
      "        [0.0963],\n",
      "        [0.1047],\n",
      "        [0.1195],\n",
      "        [0.1199],\n",
      "        [0.1280],\n",
      "        [0.1316],\n",
      "        [0.1326],\n",
      "        [0.1359],\n",
      "        [0.1453],\n",
      "        [0.1498],\n",
      "        [0.1055],\n",
      "        [0.0074],\n",
      "        [0.1119],\n",
      "        [0.0049],\n",
      "        [0.1078],\n",
      "        [0.0142],\n",
      "        [0.0161],\n",
      "        [0.0269],\n",
      "        [0.0358],\n",
      "        [0.0728],\n",
      "        [0.1396],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0539],\n",
      "        [0.0308],\n",
      "        [0.1436],\n",
      "        [0.1707],\n",
      "        [0.1757],\n",
      "        [0.2035],\n",
      "        [0.2045],\n",
      "        [0.2169],\n",
      "        [0.2188],\n",
      "        [0.2197],\n",
      "        [0.2309],\n",
      "        [0.2141],\n",
      "        [0.2405],\n",
      "        [0.2259],\n",
      "        [0.1982],\n",
      "        [0.2402]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 16.489747285842896\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 84\n",
      "剩餘X 資料 torch.Size([76, 18])\n",
      "剩餘Y 資料 torch.Size([76, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06594186276197433, 60)\n",
      "The second_loss value of k: (0.06791405379772186, 43)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引60，y= tensor([0.5907])\n",
      "目前模型的Data狀態 torch.Size([84, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475],\n",
      "        [0.8475]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0435],\n",
      "        [0.0339],\n",
      "        [0.0053],\n",
      "        [0.0028],\n",
      "        [0.0415],\n",
      "        [0.0059],\n",
      "        [0.0073],\n",
      "        [0.0020],\n",
      "        [0.0546],\n",
      "        [0.0409],\n",
      "        [0.0159],\n",
      "        [0.0438],\n",
      "        [0.0017],\n",
      "        [0.0179],\n",
      "        [0.0021],\n",
      "        [0.0432],\n",
      "        [0.0380],\n",
      "        [0.0275],\n",
      "        [0.0330],\n",
      "        [0.1346],\n",
      "        [0.0815],\n",
      "        [0.0892],\n",
      "        [0.1207],\n",
      "        [0.1273],\n",
      "        [0.0178],\n",
      "        [0.1292],\n",
      "        [0.0042],\n",
      "        [0.0012],\n",
      "        [0.0049],\n",
      "        [0.0684],\n",
      "        [0.0199],\n",
      "        [0.0880],\n",
      "        [0.1525],\n",
      "        [0.0360],\n",
      "        [0.0418],\n",
      "        [0.0451],\n",
      "        [0.0504],\n",
      "        [0.0544],\n",
      "        [0.0584],\n",
      "        [0.0621],\n",
      "        [0.0666],\n",
      "        [0.0725],\n",
      "        [0.1210],\n",
      "        [0.1519],\n",
      "        [0.0963],\n",
      "        [0.1047],\n",
      "        [0.1195],\n",
      "        [0.1199],\n",
      "        [0.1280],\n",
      "        [0.1316],\n",
      "        [0.1326],\n",
      "        [0.1359],\n",
      "        [0.1453],\n",
      "        [0.1498],\n",
      "        [0.1055],\n",
      "        [0.0074],\n",
      "        [0.1119],\n",
      "        [0.0049],\n",
      "        [0.1078],\n",
      "        [0.0142],\n",
      "        [0.0161],\n",
      "        [0.0269],\n",
      "        [0.0358],\n",
      "        [0.0728],\n",
      "        [0.1396],\n",
      "        [0.0118],\n",
      "        [0.0120],\n",
      "        [0.0539],\n",
      "        [0.0308],\n",
      "        [0.1436],\n",
      "        [0.1707],\n",
      "        [0.1757],\n",
      "        [0.2035],\n",
      "        [0.2045],\n",
      "        [0.2169],\n",
      "        [0.2188],\n",
      "        [0.2197],\n",
      "        [0.2309],\n",
      "        [0.2141],\n",
      "        [0.2405],\n",
      "        [0.2259],\n",
      "        [0.1982],\n",
      "        [0.2402],\n",
      "        [0.2568]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0405],\n",
      "        [0.0309],\n",
      "        [0.0023],\n",
      "        [0.0003],\n",
      "        [0.0445],\n",
      "        [0.0089],\n",
      "        [0.0103],\n",
      "        [0.0010],\n",
      "        [0.0515],\n",
      "        [0.0379],\n",
      "        [0.0129],\n",
      "        [0.0408],\n",
      "        [0.0048],\n",
      "        [0.0149],\n",
      "        [0.0052],\n",
      "        [0.0401],\n",
      "        [0.0350],\n",
      "        [0.0244],\n",
      "        [0.0300],\n",
      "        [0.1377],\n",
      "        [0.0846],\n",
      "        [0.0922],\n",
      "        [0.1238],\n",
      "        [0.1303],\n",
      "        [0.0148],\n",
      "        [0.1323],\n",
      "        [0.0011],\n",
      "        [0.0018],\n",
      "        [0.0080],\n",
      "        [0.0715],\n",
      "        [0.0230],\n",
      "        [0.0910],\n",
      "        [0.1556],\n",
      "        [0.0391],\n",
      "        [0.0448],\n",
      "        [0.0482],\n",
      "        [0.0534],\n",
      "        [0.0574],\n",
      "        [0.0615],\n",
      "        [0.0652],\n",
      "        [0.0697],\n",
      "        [0.0756],\n",
      "        [0.1241],\n",
      "        [0.1550],\n",
      "        [0.0994],\n",
      "        [0.1077],\n",
      "        [0.1225],\n",
      "        [0.1229],\n",
      "        [0.1311],\n",
      "        [0.1346],\n",
      "        [0.1356],\n",
      "        [0.1389],\n",
      "        [0.1483],\n",
      "        [0.1528],\n",
      "        [0.1086],\n",
      "        [0.0044],\n",
      "        [0.1150],\n",
      "        [0.0019],\n",
      "        [0.1108],\n",
      "        [0.0172],\n",
      "        [0.0191],\n",
      "        [0.0299],\n",
      "        [0.0389],\n",
      "        [0.0759],\n",
      "        [0.1365],\n",
      "        [0.0088],\n",
      "        [0.0089],\n",
      "        [0.0508],\n",
      "        [0.0278],\n",
      "        [0.1406],\n",
      "        [0.1677],\n",
      "        [0.1727],\n",
      "        [0.2005],\n",
      "        [0.2014],\n",
      "        [0.2139],\n",
      "        [0.2158],\n",
      "        [0.2166],\n",
      "        [0.2278],\n",
      "        [0.2111],\n",
      "        [0.2374],\n",
      "        [0.2228],\n",
      "        [0.1952],\n",
      "        [0.2372],\n",
      "        [0.2537]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 16.775548696517944\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 85\n",
      "剩餘X 資料 torch.Size([75, 18])\n",
      "剩餘Y 資料 torch.Size([75, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06633304804563522, 43)\n",
      "The second_loss value of k: (0.07011232525110245, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引43，y= tensor([0.5888])\n",
      "目前模型的Data狀態 torch.Size([85, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8444],\n",
      "        [0.8464]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0405],\n",
      "        [0.0309],\n",
      "        [0.0023],\n",
      "        [0.0003],\n",
      "        [0.0445],\n",
      "        [0.0089],\n",
      "        [0.0103],\n",
      "        [0.0010],\n",
      "        [0.0515],\n",
      "        [0.0379],\n",
      "        [0.0129],\n",
      "        [0.0408],\n",
      "        [0.0048],\n",
      "        [0.0149],\n",
      "        [0.0052],\n",
      "        [0.0401],\n",
      "        [0.0350],\n",
      "        [0.0244],\n",
      "        [0.0300],\n",
      "        [0.1377],\n",
      "        [0.0846],\n",
      "        [0.0922],\n",
      "        [0.1238],\n",
      "        [0.1303],\n",
      "        [0.0148],\n",
      "        [0.1323],\n",
      "        [0.0011],\n",
      "        [0.0018],\n",
      "        [0.0080],\n",
      "        [0.0715],\n",
      "        [0.0230],\n",
      "        [0.0910],\n",
      "        [0.1556],\n",
      "        [0.0391],\n",
      "        [0.0448],\n",
      "        [0.0482],\n",
      "        [0.0534],\n",
      "        [0.0574],\n",
      "        [0.0615],\n",
      "        [0.0652],\n",
      "        [0.0697],\n",
      "        [0.0756],\n",
      "        [0.1241],\n",
      "        [0.1550],\n",
      "        [0.0994],\n",
      "        [0.1077],\n",
      "        [0.1225],\n",
      "        [0.1229],\n",
      "        [0.1311],\n",
      "        [0.1346],\n",
      "        [0.1356],\n",
      "        [0.1389],\n",
      "        [0.1483],\n",
      "        [0.1528],\n",
      "        [0.1086],\n",
      "        [0.0044],\n",
      "        [0.1150],\n",
      "        [0.0019],\n",
      "        [0.1108],\n",
      "        [0.0172],\n",
      "        [0.0191],\n",
      "        [0.0299],\n",
      "        [0.0389],\n",
      "        [0.0759],\n",
      "        [0.1365],\n",
      "        [0.0088],\n",
      "        [0.0089],\n",
      "        [0.0508],\n",
      "        [0.0278],\n",
      "        [0.1406],\n",
      "        [0.1677],\n",
      "        [0.1727],\n",
      "        [0.2005],\n",
      "        [0.2014],\n",
      "        [0.2139],\n",
      "        [0.2158],\n",
      "        [0.2166],\n",
      "        [0.2278],\n",
      "        [0.2111],\n",
      "        [0.2374],\n",
      "        [0.2228],\n",
      "        [0.1952],\n",
      "        [0.2372],\n",
      "        [0.2537],\n",
      "        [0.2576]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0374],\n",
      "        [0.0279],\n",
      "        [0.0007],\n",
      "        [0.0033],\n",
      "        [0.0475],\n",
      "        [0.0119],\n",
      "        [0.0134],\n",
      "        [0.0040],\n",
      "        [0.0485],\n",
      "        [0.0349],\n",
      "        [0.0099],\n",
      "        [0.0378],\n",
      "        [0.0078],\n",
      "        [0.0119],\n",
      "        [0.0082],\n",
      "        [0.0371],\n",
      "        [0.0320],\n",
      "        [0.0214],\n",
      "        [0.0270],\n",
      "        [0.1407],\n",
      "        [0.0876],\n",
      "        [0.0953],\n",
      "        [0.1268],\n",
      "        [0.1333],\n",
      "        [0.0118],\n",
      "        [0.1353],\n",
      "        [0.0019],\n",
      "        [0.0049],\n",
      "        [0.0110],\n",
      "        [0.0745],\n",
      "        [0.0260],\n",
      "        [0.0940],\n",
      "        [0.1586],\n",
      "        [0.0421],\n",
      "        [0.0478],\n",
      "        [0.0512],\n",
      "        [0.0564],\n",
      "        [0.0604],\n",
      "        [0.0645],\n",
      "        [0.0682],\n",
      "        [0.0727],\n",
      "        [0.0786],\n",
      "        [0.1271],\n",
      "        [0.1580],\n",
      "        [0.1024],\n",
      "        [0.1107],\n",
      "        [0.1255],\n",
      "        [0.1259],\n",
      "        [0.1341],\n",
      "        [0.1376],\n",
      "        [0.1386],\n",
      "        [0.1419],\n",
      "        [0.1513],\n",
      "        [0.1559],\n",
      "        [0.1116],\n",
      "        [0.0014],\n",
      "        [0.1180],\n",
      "        [0.0011],\n",
      "        [0.1138],\n",
      "        [0.0202],\n",
      "        [0.0221],\n",
      "        [0.0329],\n",
      "        [0.0419],\n",
      "        [0.0789],\n",
      "        [0.1335],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0478],\n",
      "        [0.0248],\n",
      "        [0.1376],\n",
      "        [0.1647],\n",
      "        [0.1697],\n",
      "        [0.1975],\n",
      "        [0.1984],\n",
      "        [0.2109],\n",
      "        [0.2128],\n",
      "        [0.2136],\n",
      "        [0.2248],\n",
      "        [0.2081],\n",
      "        [0.2344],\n",
      "        [0.2198],\n",
      "        [0.1922],\n",
      "        [0.2342],\n",
      "        [0.2507],\n",
      "        [0.2526]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 17.020885705947876\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 86\n",
      "剩餘X 資料 torch.Size([74, 18])\n",
      "剩餘Y 資料 torch.Size([74, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06725893169641495, 1)\n",
      "The second_loss value of k: (0.0753522515296936, 43)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.7461])\n",
      "目前模型的Data狀態 torch.Size([86, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [0.8414],\n",
      "        [1.0055]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0374],\n",
      "        [0.0279],\n",
      "        [0.0007],\n",
      "        [0.0033],\n",
      "        [0.0475],\n",
      "        [0.0119],\n",
      "        [0.0134],\n",
      "        [0.0040],\n",
      "        [0.0485],\n",
      "        [0.0349],\n",
      "        [0.0099],\n",
      "        [0.0378],\n",
      "        [0.0078],\n",
      "        [0.0119],\n",
      "        [0.0082],\n",
      "        [0.0371],\n",
      "        [0.0320],\n",
      "        [0.0214],\n",
      "        [0.0270],\n",
      "        [0.1407],\n",
      "        [0.0876],\n",
      "        [0.0953],\n",
      "        [0.1268],\n",
      "        [0.1333],\n",
      "        [0.0118],\n",
      "        [0.1353],\n",
      "        [0.0019],\n",
      "        [0.0049],\n",
      "        [0.0110],\n",
      "        [0.0745],\n",
      "        [0.0260],\n",
      "        [0.0940],\n",
      "        [0.1586],\n",
      "        [0.0421],\n",
      "        [0.0478],\n",
      "        [0.0512],\n",
      "        [0.0564],\n",
      "        [0.0604],\n",
      "        [0.0645],\n",
      "        [0.0682],\n",
      "        [0.0727],\n",
      "        [0.0786],\n",
      "        [0.1271],\n",
      "        [0.1580],\n",
      "        [0.1024],\n",
      "        [0.1107],\n",
      "        [0.1255],\n",
      "        [0.1259],\n",
      "        [0.1341],\n",
      "        [0.1376],\n",
      "        [0.1386],\n",
      "        [0.1419],\n",
      "        [0.1513],\n",
      "        [0.1559],\n",
      "        [0.1116],\n",
      "        [0.0014],\n",
      "        [0.1180],\n",
      "        [0.0011],\n",
      "        [0.1138],\n",
      "        [0.0202],\n",
      "        [0.0221],\n",
      "        [0.0329],\n",
      "        [0.0419],\n",
      "        [0.0789],\n",
      "        [0.1335],\n",
      "        [0.0058],\n",
      "        [0.0059],\n",
      "        [0.0478],\n",
      "        [0.0248],\n",
      "        [0.1376],\n",
      "        [0.1647],\n",
      "        [0.1697],\n",
      "        [0.1975],\n",
      "        [0.1984],\n",
      "        [0.2109],\n",
      "        [0.2128],\n",
      "        [0.2136],\n",
      "        [0.2248],\n",
      "        [0.2081],\n",
      "        [0.2344],\n",
      "        [0.2198],\n",
      "        [0.1922],\n",
      "        [0.2342],\n",
      "        [0.2507],\n",
      "        [0.2526],\n",
      "        [0.2593]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0363],\n",
      "        [    0.0267],\n",
      "        [    0.0019],\n",
      "        [    0.0044],\n",
      "        [    0.0487],\n",
      "        [    0.0131],\n",
      "        [    0.0145],\n",
      "        [    0.0052],\n",
      "        [    0.0474],\n",
      "        [    0.0337],\n",
      "        [    0.0088],\n",
      "        [    0.0366],\n",
      "        [    0.0089],\n",
      "        [    0.0107],\n",
      "        [    0.0093],\n",
      "        [    0.0360],\n",
      "        [    0.0308],\n",
      "        [    0.0203],\n",
      "        [    0.0258],\n",
      "        [    0.1418],\n",
      "        [    0.0887],\n",
      "        [    0.0964],\n",
      "        [    0.1279],\n",
      "        [    0.1345],\n",
      "        [    0.0106],\n",
      "        [    0.1364],\n",
      "        [    0.0030],\n",
      "        [    0.0060],\n",
      "        [    0.0121],\n",
      "        [    0.0756],\n",
      "        [    0.0271],\n",
      "        [    0.0952],\n",
      "        [    0.1597],\n",
      "        [    0.0432],\n",
      "        [    0.0489],\n",
      "        [    0.0523],\n",
      "        [    0.0576],\n",
      "        [    0.0616],\n",
      "        [    0.0656],\n",
      "        [    0.0693],\n",
      "        [    0.0738],\n",
      "        [    0.0797],\n",
      "        [    0.1282],\n",
      "        [    0.1591],\n",
      "        [    0.1035],\n",
      "        [    0.1119],\n",
      "        [    0.1267],\n",
      "        [    0.1271],\n",
      "        [    0.1352],\n",
      "        [    0.1388],\n",
      "        [    0.1397],\n",
      "        [    0.1431],\n",
      "        [    0.1525],\n",
      "        [    0.1570],\n",
      "        [    0.1127],\n",
      "        [    0.0003],\n",
      "        [    0.1191],\n",
      "        [    0.0023],\n",
      "        [    0.1150],\n",
      "        [    0.0214],\n",
      "        [    0.0233],\n",
      "        [    0.0341],\n",
      "        [    0.0430],\n",
      "        [    0.0800],\n",
      "        [    0.1324],\n",
      "        [    0.0046],\n",
      "        [    0.0048],\n",
      "        [    0.0467],\n",
      "        [    0.0236],\n",
      "        [    0.1364],\n",
      "        [    0.1635],\n",
      "        [    0.1686],\n",
      "        [    0.1963],\n",
      "        [    0.1973],\n",
      "        [    0.2097],\n",
      "        [    0.2116],\n",
      "        [    0.2125],\n",
      "        [    0.2237],\n",
      "        [    0.2069],\n",
      "        [    0.2333],\n",
      "        [    0.2187],\n",
      "        [    0.1910],\n",
      "        [    0.2330],\n",
      "        [    0.2496],\n",
      "        [    0.2514],\n",
      "        [    0.0941]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 17.268604040145874\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 87\n",
      "剩餘X 資料 torch.Size([73, 18])\n",
      "剩餘Y 資料 torch.Size([73, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.02874431200325489, 33)\n",
      "The second_loss value of k: (0.03281025588512421, 31)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引33，y= tensor([0.6794])\n",
      "目前模型的Data狀態 torch.Size([87, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8403],\n",
      "        [0.8490]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0363],\n",
      "        [    0.0267],\n",
      "        [    0.0019],\n",
      "        [    0.0044],\n",
      "        [    0.0487],\n",
      "        [    0.0131],\n",
      "        [    0.0145],\n",
      "        [    0.0052],\n",
      "        [    0.0474],\n",
      "        [    0.0337],\n",
      "        [    0.0088],\n",
      "        [    0.0366],\n",
      "        [    0.0089],\n",
      "        [    0.0107],\n",
      "        [    0.0093],\n",
      "        [    0.0360],\n",
      "        [    0.0308],\n",
      "        [    0.0203],\n",
      "        [    0.0258],\n",
      "        [    0.1418],\n",
      "        [    0.0887],\n",
      "        [    0.0964],\n",
      "        [    0.1279],\n",
      "        [    0.1345],\n",
      "        [    0.0106],\n",
      "        [    0.1364],\n",
      "        [    0.0030],\n",
      "        [    0.0060],\n",
      "        [    0.0121],\n",
      "        [    0.0756],\n",
      "        [    0.0271],\n",
      "        [    0.0952],\n",
      "        [    0.1597],\n",
      "        [    0.0432],\n",
      "        [    0.0489],\n",
      "        [    0.0523],\n",
      "        [    0.0576],\n",
      "        [    0.0616],\n",
      "        [    0.0656],\n",
      "        [    0.0693],\n",
      "        [    0.0738],\n",
      "        [    0.0797],\n",
      "        [    0.1282],\n",
      "        [    0.1591],\n",
      "        [    0.1035],\n",
      "        [    0.1119],\n",
      "        [    0.1267],\n",
      "        [    0.1271],\n",
      "        [    0.1352],\n",
      "        [    0.1388],\n",
      "        [    0.1397],\n",
      "        [    0.1431],\n",
      "        [    0.1525],\n",
      "        [    0.1570],\n",
      "        [    0.1127],\n",
      "        [    0.0003],\n",
      "        [    0.1191],\n",
      "        [    0.0023],\n",
      "        [    0.1150],\n",
      "        [    0.0214],\n",
      "        [    0.0233],\n",
      "        [    0.0341],\n",
      "        [    0.0430],\n",
      "        [    0.0800],\n",
      "        [    0.1324],\n",
      "        [    0.0046],\n",
      "        [    0.0048],\n",
      "        [    0.0467],\n",
      "        [    0.0236],\n",
      "        [    0.1364],\n",
      "        [    0.1635],\n",
      "        [    0.1686],\n",
      "        [    0.1963],\n",
      "        [    0.1973],\n",
      "        [    0.2097],\n",
      "        [    0.2116],\n",
      "        [    0.2125],\n",
      "        [    0.2237],\n",
      "        [    0.2069],\n",
      "        [    0.2333],\n",
      "        [    0.2187],\n",
      "        [    0.1910],\n",
      "        [    0.2330],\n",
      "        [    0.2496],\n",
      "        [    0.2514],\n",
      "        [    0.0941],\n",
      "        [    0.1695]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0345],\n",
      "        [0.0249],\n",
      "        [0.0037],\n",
      "        [0.0062],\n",
      "        [0.0505],\n",
      "        [0.0149],\n",
      "        [0.0163],\n",
      "        [0.0070],\n",
      "        [0.0456],\n",
      "        [0.0319],\n",
      "        [0.0069],\n",
      "        [0.0348],\n",
      "        [0.0107],\n",
      "        [0.0089],\n",
      "        [0.0111],\n",
      "        [0.0342],\n",
      "        [0.0290],\n",
      "        [0.0185],\n",
      "        [0.0240],\n",
      "        [0.1437],\n",
      "        [0.0905],\n",
      "        [0.0982],\n",
      "        [0.1297],\n",
      "        [0.1363],\n",
      "        [0.0088],\n",
      "        [0.1382],\n",
      "        [0.0048],\n",
      "        [0.0078],\n",
      "        [0.0140],\n",
      "        [0.0775],\n",
      "        [0.0290],\n",
      "        [0.0970],\n",
      "        [0.1616],\n",
      "        [0.0450],\n",
      "        [0.0508],\n",
      "        [0.0541],\n",
      "        [0.0594],\n",
      "        [0.0634],\n",
      "        [0.0675],\n",
      "        [0.0711],\n",
      "        [0.0757],\n",
      "        [0.0815],\n",
      "        [0.1300],\n",
      "        [0.1610],\n",
      "        [0.1053],\n",
      "        [0.1137],\n",
      "        [0.1285],\n",
      "        [0.1289],\n",
      "        [0.1371],\n",
      "        [0.1406],\n",
      "        [0.1416],\n",
      "        [0.1449],\n",
      "        [0.1543],\n",
      "        [0.1588],\n",
      "        [0.1145],\n",
      "        [0.0016],\n",
      "        [0.1209],\n",
      "        [0.0041],\n",
      "        [0.1168],\n",
      "        [0.0232],\n",
      "        [0.0251],\n",
      "        [0.0359],\n",
      "        [0.0449],\n",
      "        [0.0818],\n",
      "        [0.1305],\n",
      "        [0.0028],\n",
      "        [0.0030],\n",
      "        [0.0448],\n",
      "        [0.0218],\n",
      "        [0.1346],\n",
      "        [0.1617],\n",
      "        [0.1667],\n",
      "        [0.1945],\n",
      "        [0.1955],\n",
      "        [0.2079],\n",
      "        [0.2098],\n",
      "        [0.2107],\n",
      "        [0.2219],\n",
      "        [0.2051],\n",
      "        [0.2315],\n",
      "        [0.2169],\n",
      "        [0.1892],\n",
      "        [0.2312],\n",
      "        [0.2478],\n",
      "        [0.2496],\n",
      "        [0.0923],\n",
      "        [0.1590]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 17.513103008270264\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 88\n",
      "剩餘X 資料 torch.Size([72, 18])\n",
      "剩餘Y 資料 torch.Size([72, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03182274475693703, 1)\n",
      "The second_loss value of k: (0.03214949741959572, 31)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.7455])\n",
      "目前模型的Data狀態 torch.Size([88, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.8384],\n",
      "        [0.9239]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0345],\n",
      "        [0.0249],\n",
      "        [0.0037],\n",
      "        [0.0062],\n",
      "        [0.0505],\n",
      "        [0.0149],\n",
      "        [0.0163],\n",
      "        [0.0070],\n",
      "        [0.0456],\n",
      "        [0.0319],\n",
      "        [0.0069],\n",
      "        [0.0348],\n",
      "        [0.0107],\n",
      "        [0.0089],\n",
      "        [0.0111],\n",
      "        [0.0342],\n",
      "        [0.0290],\n",
      "        [0.0185],\n",
      "        [0.0240],\n",
      "        [0.1437],\n",
      "        [0.0905],\n",
      "        [0.0982],\n",
      "        [0.1297],\n",
      "        [0.1363],\n",
      "        [0.0088],\n",
      "        [0.1382],\n",
      "        [0.0048],\n",
      "        [0.0078],\n",
      "        [0.0140],\n",
      "        [0.0775],\n",
      "        [0.0290],\n",
      "        [0.0970],\n",
      "        [0.1616],\n",
      "        [0.0450],\n",
      "        [0.0508],\n",
      "        [0.0541],\n",
      "        [0.0594],\n",
      "        [0.0634],\n",
      "        [0.0675],\n",
      "        [0.0711],\n",
      "        [0.0757],\n",
      "        [0.0815],\n",
      "        [0.1300],\n",
      "        [0.1610],\n",
      "        [0.1053],\n",
      "        [0.1137],\n",
      "        [0.1285],\n",
      "        [0.1289],\n",
      "        [0.1371],\n",
      "        [0.1406],\n",
      "        [0.1416],\n",
      "        [0.1449],\n",
      "        [0.1543],\n",
      "        [0.1588],\n",
      "        [0.1145],\n",
      "        [0.0016],\n",
      "        [0.1209],\n",
      "        [0.0041],\n",
      "        [0.1168],\n",
      "        [0.0232],\n",
      "        [0.0251],\n",
      "        [0.0359],\n",
      "        [0.0449],\n",
      "        [0.0818],\n",
      "        [0.1305],\n",
      "        [0.0028],\n",
      "        [0.0030],\n",
      "        [0.0448],\n",
      "        [0.0218],\n",
      "        [0.1346],\n",
      "        [0.1617],\n",
      "        [0.1667],\n",
      "        [0.1945],\n",
      "        [0.1955],\n",
      "        [0.2079],\n",
      "        [0.2098],\n",
      "        [0.2107],\n",
      "        [0.2219],\n",
      "        [0.2051],\n",
      "        [0.2315],\n",
      "        [0.2169],\n",
      "        [0.1892],\n",
      "        [0.2312],\n",
      "        [0.2478],\n",
      "        [0.2496],\n",
      "        [0.0923],\n",
      "        [0.1590],\n",
      "        [0.1784]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 65\n",
      "Number of shrink: 35\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0334],\n",
      "        [0.0238],\n",
      "        [0.0048],\n",
      "        [0.0073],\n",
      "        [0.0516],\n",
      "        [0.0160],\n",
      "        [0.0174],\n",
      "        [0.0081],\n",
      "        [0.0445],\n",
      "        [0.0308],\n",
      "        [0.0058],\n",
      "        [0.0337],\n",
      "        [0.0118],\n",
      "        [0.0078],\n",
      "        [0.0122],\n",
      "        [0.0331],\n",
      "        [0.0279],\n",
      "        [0.0174],\n",
      "        [0.0229],\n",
      "        [0.1447],\n",
      "        [0.0916],\n",
      "        [0.0993],\n",
      "        [0.1308],\n",
      "        [0.1374],\n",
      "        [0.0077],\n",
      "        [0.1393],\n",
      "        [0.0059],\n",
      "        [0.0089],\n",
      "        [0.0150],\n",
      "        [0.0785],\n",
      "        [0.0300],\n",
      "        [0.0981],\n",
      "        [0.1626],\n",
      "        [0.0461],\n",
      "        [0.0519],\n",
      "        [0.0552],\n",
      "        [0.0605],\n",
      "        [0.0645],\n",
      "        [0.0686],\n",
      "        [0.0722],\n",
      "        [0.0767],\n",
      "        [0.0826],\n",
      "        [0.1311],\n",
      "        [0.1620],\n",
      "        [0.1064],\n",
      "        [0.1148],\n",
      "        [0.1296],\n",
      "        [0.1300],\n",
      "        [0.1382],\n",
      "        [0.1417],\n",
      "        [0.1427],\n",
      "        [0.1460],\n",
      "        [0.1554],\n",
      "        [0.1599],\n",
      "        [0.1156],\n",
      "        [0.0027],\n",
      "        [0.1220],\n",
      "        [0.0052],\n",
      "        [0.1179],\n",
      "        [0.0243],\n",
      "        [0.0262],\n",
      "        [0.0370],\n",
      "        [0.0459],\n",
      "        [0.0829],\n",
      "        [0.1295],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0437],\n",
      "        [0.0207],\n",
      "        [0.1335],\n",
      "        [0.1606],\n",
      "        [0.1656],\n",
      "        [0.1934],\n",
      "        [0.1944],\n",
      "        [0.2068],\n",
      "        [0.2087],\n",
      "        [0.2096],\n",
      "        [0.2208],\n",
      "        [0.2040],\n",
      "        [0.2304],\n",
      "        [0.2158],\n",
      "        [0.1881],\n",
      "        [0.2301],\n",
      "        [0.2467],\n",
      "        [0.2485],\n",
      "        [0.0912],\n",
      "        [0.1579],\n",
      "        [0.0918]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 17.764708995819092\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 89\n",
      "剩餘X 資料 torch.Size([71, 18])\n",
      "剩餘Y 資料 torch.Size([71, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.010395307093858719, 1)\n",
      "The second_loss value of k: (0.01981668546795845, 32)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.7606])\n",
      "目前模型的Data狀態 torch.Size([89, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8374],\n",
      "        [0.8626]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0334],\n",
      "        [0.0238],\n",
      "        [0.0048],\n",
      "        [0.0073],\n",
      "        [0.0516],\n",
      "        [0.0160],\n",
      "        [0.0174],\n",
      "        [0.0081],\n",
      "        [0.0445],\n",
      "        [0.0308],\n",
      "        [0.0058],\n",
      "        [0.0337],\n",
      "        [0.0118],\n",
      "        [0.0078],\n",
      "        [0.0122],\n",
      "        [0.0331],\n",
      "        [0.0279],\n",
      "        [0.0174],\n",
      "        [0.0229],\n",
      "        [0.1447],\n",
      "        [0.0916],\n",
      "        [0.0993],\n",
      "        [0.1308],\n",
      "        [0.1374],\n",
      "        [0.0077],\n",
      "        [0.1393],\n",
      "        [0.0059],\n",
      "        [0.0089],\n",
      "        [0.0150],\n",
      "        [0.0785],\n",
      "        [0.0300],\n",
      "        [0.0981],\n",
      "        [0.1626],\n",
      "        [0.0461],\n",
      "        [0.0519],\n",
      "        [0.0552],\n",
      "        [0.0605],\n",
      "        [0.0645],\n",
      "        [0.0686],\n",
      "        [0.0722],\n",
      "        [0.0767],\n",
      "        [0.0826],\n",
      "        [0.1311],\n",
      "        [0.1620],\n",
      "        [0.1064],\n",
      "        [0.1148],\n",
      "        [0.1296],\n",
      "        [0.1300],\n",
      "        [0.1382],\n",
      "        [0.1417],\n",
      "        [0.1427],\n",
      "        [0.1460],\n",
      "        [0.1554],\n",
      "        [0.1599],\n",
      "        [0.1156],\n",
      "        [0.0027],\n",
      "        [0.1220],\n",
      "        [0.0052],\n",
      "        [0.1179],\n",
      "        [0.0243],\n",
      "        [0.0262],\n",
      "        [0.0370],\n",
      "        [0.0459],\n",
      "        [0.0829],\n",
      "        [0.1295],\n",
      "        [0.0017],\n",
      "        [0.0019],\n",
      "        [0.0437],\n",
      "        [0.0207],\n",
      "        [0.1335],\n",
      "        [0.1606],\n",
      "        [0.1656],\n",
      "        [0.1934],\n",
      "        [0.1944],\n",
      "        [0.2068],\n",
      "        [0.2087],\n",
      "        [0.2096],\n",
      "        [0.2208],\n",
      "        [0.2040],\n",
      "        [0.2304],\n",
      "        [0.2158],\n",
      "        [0.1881],\n",
      "        [0.2301],\n",
      "        [0.2467],\n",
      "        [0.2485],\n",
      "        [0.0912],\n",
      "        [0.1579],\n",
      "        [0.0918],\n",
      "        [0.1020]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0326],\n",
      "        [0.0230],\n",
      "        [0.0056],\n",
      "        [0.0082],\n",
      "        [0.0524],\n",
      "        [0.0168],\n",
      "        [0.0182],\n",
      "        [0.0089],\n",
      "        [0.0437],\n",
      "        [0.0300],\n",
      "        [0.0050],\n",
      "        [0.0329],\n",
      "        [0.0127],\n",
      "        [0.0070],\n",
      "        [0.0131],\n",
      "        [0.0323],\n",
      "        [0.0271],\n",
      "        [0.0165],\n",
      "        [0.0221],\n",
      "        [0.1456],\n",
      "        [0.0924],\n",
      "        [0.1001],\n",
      "        [0.1317],\n",
      "        [0.1382],\n",
      "        [0.0069],\n",
      "        [0.1402],\n",
      "        [0.0068],\n",
      "        [0.0097],\n",
      "        [0.0159],\n",
      "        [0.0794],\n",
      "        [0.0309],\n",
      "        [0.0989],\n",
      "        [0.1635],\n",
      "        [0.0469],\n",
      "        [0.0527],\n",
      "        [0.0561],\n",
      "        [0.0613],\n",
      "        [0.0653],\n",
      "        [0.0694],\n",
      "        [0.0731],\n",
      "        [0.0776],\n",
      "        [0.0835],\n",
      "        [0.1319],\n",
      "        [0.1629],\n",
      "        [0.1073],\n",
      "        [0.1156],\n",
      "        [0.1304],\n",
      "        [0.1308],\n",
      "        [0.1390],\n",
      "        [0.1425],\n",
      "        [0.1435],\n",
      "        [0.1468],\n",
      "        [0.1562],\n",
      "        [0.1607],\n",
      "        [0.1165],\n",
      "        [0.0035],\n",
      "        [0.1229],\n",
      "        [0.0060],\n",
      "        [0.1187],\n",
      "        [0.0251],\n",
      "        [0.0270],\n",
      "        [0.0378],\n",
      "        [0.0468],\n",
      "        [0.0838],\n",
      "        [0.1286],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0429],\n",
      "        [0.0199],\n",
      "        [0.1327],\n",
      "        [0.1598],\n",
      "        [0.1648],\n",
      "        [0.1926],\n",
      "        [0.1936],\n",
      "        [0.2060],\n",
      "        [0.2079],\n",
      "        [0.2088],\n",
      "        [0.2200],\n",
      "        [0.2032],\n",
      "        [0.2296],\n",
      "        [0.2149],\n",
      "        [0.1873],\n",
      "        [0.2293],\n",
      "        [0.2458],\n",
      "        [0.2477],\n",
      "        [0.0904],\n",
      "        [0.1571],\n",
      "        [0.0910],\n",
      "        [0.0759]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 18.014528512954712\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 90\n",
      "剩餘X 資料 torch.Size([70, 18])\n",
      "剩餘Y 資料 torch.Size([70, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.01958097517490387, 31)\n",
      "The second_loss value of k: (0.02125982753932476, 32)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引31，y= tensor([0.6966])\n",
      "目前模型的Data狀態 torch.Size([90, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365],\n",
      "        [0.8365]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0326],\n",
      "        [0.0230],\n",
      "        [0.0056],\n",
      "        [0.0082],\n",
      "        [0.0524],\n",
      "        [0.0168],\n",
      "        [0.0182],\n",
      "        [0.0089],\n",
      "        [0.0437],\n",
      "        [0.0300],\n",
      "        [0.0050],\n",
      "        [0.0329],\n",
      "        [0.0127],\n",
      "        [0.0070],\n",
      "        [0.0131],\n",
      "        [0.0323],\n",
      "        [0.0271],\n",
      "        [0.0165],\n",
      "        [0.0221],\n",
      "        [0.1456],\n",
      "        [0.0924],\n",
      "        [0.1001],\n",
      "        [0.1317],\n",
      "        [0.1382],\n",
      "        [0.0069],\n",
      "        [0.1402],\n",
      "        [0.0068],\n",
      "        [0.0097],\n",
      "        [0.0159],\n",
      "        [0.0794],\n",
      "        [0.0309],\n",
      "        [0.0989],\n",
      "        [0.1635],\n",
      "        [0.0469],\n",
      "        [0.0527],\n",
      "        [0.0561],\n",
      "        [0.0613],\n",
      "        [0.0653],\n",
      "        [0.0694],\n",
      "        [0.0731],\n",
      "        [0.0776],\n",
      "        [0.0835],\n",
      "        [0.1319],\n",
      "        [0.1629],\n",
      "        [0.1073],\n",
      "        [0.1156],\n",
      "        [0.1304],\n",
      "        [0.1308],\n",
      "        [0.1390],\n",
      "        [0.1425],\n",
      "        [0.1435],\n",
      "        [0.1468],\n",
      "        [0.1562],\n",
      "        [0.1607],\n",
      "        [0.1165],\n",
      "        [0.0035],\n",
      "        [0.1229],\n",
      "        [0.0060],\n",
      "        [0.1187],\n",
      "        [0.0251],\n",
      "        [0.0270],\n",
      "        [0.0378],\n",
      "        [0.0468],\n",
      "        [0.0838],\n",
      "        [0.1286],\n",
      "        [0.0009],\n",
      "        [0.0010],\n",
      "        [0.0429],\n",
      "        [0.0199],\n",
      "        [0.1327],\n",
      "        [0.1598],\n",
      "        [0.1648],\n",
      "        [0.1926],\n",
      "        [0.1936],\n",
      "        [0.2060],\n",
      "        [0.2079],\n",
      "        [0.2088],\n",
      "        [0.2200],\n",
      "        [0.2032],\n",
      "        [0.2296],\n",
      "        [0.2149],\n",
      "        [0.1873],\n",
      "        [0.2293],\n",
      "        [0.2458],\n",
      "        [0.2477],\n",
      "        [0.0904],\n",
      "        [0.1571],\n",
      "        [0.0910],\n",
      "        [0.0759],\n",
      "        [0.1399]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0310],\n",
      "        [0.0214],\n",
      "        [0.0072],\n",
      "        [0.0097],\n",
      "        [0.0540],\n",
      "        [0.0184],\n",
      "        [0.0198],\n",
      "        [0.0105],\n",
      "        [0.0421],\n",
      "        [0.0284],\n",
      "        [0.0034],\n",
      "        [0.0313],\n",
      "        [0.0142],\n",
      "        [0.0054],\n",
      "        [0.0146],\n",
      "        [0.0307],\n",
      "        [0.0255],\n",
      "        [0.0150],\n",
      "        [0.0205],\n",
      "        [0.1471],\n",
      "        [0.0940],\n",
      "        [0.1017],\n",
      "        [0.1332],\n",
      "        [0.1398],\n",
      "        [0.0053],\n",
      "        [0.1417],\n",
      "        [0.0083],\n",
      "        [0.0113],\n",
      "        [0.0174],\n",
      "        [0.0809],\n",
      "        [0.0324],\n",
      "        [0.1005],\n",
      "        [0.1650],\n",
      "        [0.0485],\n",
      "        [0.0543],\n",
      "        [0.0576],\n",
      "        [0.0629],\n",
      "        [0.0669],\n",
      "        [0.0709],\n",
      "        [0.0746],\n",
      "        [0.0791],\n",
      "        [0.0850],\n",
      "        [0.1335],\n",
      "        [0.1644],\n",
      "        [0.1088],\n",
      "        [0.1172],\n",
      "        [0.1320],\n",
      "        [0.1324],\n",
      "        [0.1405],\n",
      "        [0.1441],\n",
      "        [0.1450],\n",
      "        [0.1484],\n",
      "        [0.1578],\n",
      "        [0.1623],\n",
      "        [0.1180],\n",
      "        [0.0051],\n",
      "        [0.1244],\n",
      "        [0.0076],\n",
      "        [0.1203],\n",
      "        [0.0267],\n",
      "        [0.0286],\n",
      "        [0.0394],\n",
      "        [0.0483],\n",
      "        [0.0853],\n",
      "        [0.1271],\n",
      "        [0.0007],\n",
      "        [0.0005],\n",
      "        [0.0414],\n",
      "        [0.0183],\n",
      "        [0.1311],\n",
      "        [0.1582],\n",
      "        [0.1633],\n",
      "        [0.1910],\n",
      "        [0.1920],\n",
      "        [0.2044],\n",
      "        [0.2063],\n",
      "        [0.2072],\n",
      "        [0.2184],\n",
      "        [0.2016],\n",
      "        [0.2280],\n",
      "        [0.2134],\n",
      "        [0.1857],\n",
      "        [0.2277],\n",
      "        [0.2443],\n",
      "        [0.2461],\n",
      "        [0.0888],\n",
      "        [0.1555],\n",
      "        [0.0895],\n",
      "        [0.0744],\n",
      "        [0.1384]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 18.259845972061157\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 91\n",
      "剩餘X 資料 torch.Size([69, 18])\n",
      "剩餘Y 資料 torch.Size([69, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.020810376852750778, 31)\n",
      "The second_loss value of k: (0.02123061940073967, 32)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引31，y= tensor([0.6907])\n",
      "目前模型的Data狀態 torch.Size([91, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350],\n",
      "        [0.8350]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0310],\n",
      "        [0.0214],\n",
      "        [0.0072],\n",
      "        [0.0097],\n",
      "        [0.0540],\n",
      "        [0.0184],\n",
      "        [0.0198],\n",
      "        [0.0105],\n",
      "        [0.0421],\n",
      "        [0.0284],\n",
      "        [0.0034],\n",
      "        [0.0313],\n",
      "        [0.0142],\n",
      "        [0.0054],\n",
      "        [0.0146],\n",
      "        [0.0307],\n",
      "        [0.0255],\n",
      "        [0.0150],\n",
      "        [0.0205],\n",
      "        [0.1471],\n",
      "        [0.0940],\n",
      "        [0.1017],\n",
      "        [0.1332],\n",
      "        [0.1398],\n",
      "        [0.0053],\n",
      "        [0.1417],\n",
      "        [0.0083],\n",
      "        [0.0113],\n",
      "        [0.0174],\n",
      "        [0.0809],\n",
      "        [0.0324],\n",
      "        [0.1005],\n",
      "        [0.1650],\n",
      "        [0.0485],\n",
      "        [0.0543],\n",
      "        [0.0576],\n",
      "        [0.0629],\n",
      "        [0.0669],\n",
      "        [0.0709],\n",
      "        [0.0746],\n",
      "        [0.0791],\n",
      "        [0.0850],\n",
      "        [0.1335],\n",
      "        [0.1644],\n",
      "        [0.1088],\n",
      "        [0.1172],\n",
      "        [0.1320],\n",
      "        [0.1324],\n",
      "        [0.1405],\n",
      "        [0.1441],\n",
      "        [0.1450],\n",
      "        [0.1484],\n",
      "        [0.1578],\n",
      "        [0.1623],\n",
      "        [0.1180],\n",
      "        [0.0051],\n",
      "        [0.1244],\n",
      "        [0.0076],\n",
      "        [0.1203],\n",
      "        [0.0267],\n",
      "        [0.0286],\n",
      "        [0.0394],\n",
      "        [0.0483],\n",
      "        [0.0853],\n",
      "        [0.1271],\n",
      "        [0.0007],\n",
      "        [0.0005],\n",
      "        [0.0414],\n",
      "        [0.0183],\n",
      "        [0.1311],\n",
      "        [0.1582],\n",
      "        [0.1633],\n",
      "        [0.1910],\n",
      "        [0.1920],\n",
      "        [0.2044],\n",
      "        [0.2063],\n",
      "        [0.2072],\n",
      "        [0.2184],\n",
      "        [0.2016],\n",
      "        [0.2280],\n",
      "        [0.2134],\n",
      "        [0.1857],\n",
      "        [0.2277],\n",
      "        [0.2443],\n",
      "        [0.2461],\n",
      "        [0.0888],\n",
      "        [0.1555],\n",
      "        [0.0895],\n",
      "        [0.0744],\n",
      "        [0.1384],\n",
      "        [0.1443]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0294],\n",
      "        [0.0198],\n",
      "        [0.0088],\n",
      "        [0.0113],\n",
      "        [0.0556],\n",
      "        [0.0200],\n",
      "        [0.0214],\n",
      "        [0.0121],\n",
      "        [0.0405],\n",
      "        [0.0268],\n",
      "        [0.0019],\n",
      "        [0.0297],\n",
      "        [0.0158],\n",
      "        [0.0038],\n",
      "        [0.0162],\n",
      "        [0.0291],\n",
      "        [0.0239],\n",
      "        [0.0134],\n",
      "        [0.0189],\n",
      "        [0.1487],\n",
      "        [0.0956],\n",
      "        [0.1033],\n",
      "        [0.1348],\n",
      "        [0.1414],\n",
      "        [0.0037],\n",
      "        [0.1433],\n",
      "        [0.0099],\n",
      "        [0.0129],\n",
      "        [0.0190],\n",
      "        [0.0825],\n",
      "        [0.0340],\n",
      "        [0.1021],\n",
      "        [0.1666],\n",
      "        [0.0501],\n",
      "        [0.0558],\n",
      "        [0.0592],\n",
      "        [0.0645],\n",
      "        [0.0685],\n",
      "        [0.0725],\n",
      "        [0.0762],\n",
      "        [0.0807],\n",
      "        [0.0866],\n",
      "        [0.1351],\n",
      "        [0.1660],\n",
      "        [0.1104],\n",
      "        [0.1188],\n",
      "        [0.1336],\n",
      "        [0.1339],\n",
      "        [0.1421],\n",
      "        [0.1457],\n",
      "        [0.1466],\n",
      "        [0.1500],\n",
      "        [0.1594],\n",
      "        [0.1639],\n",
      "        [0.1196],\n",
      "        [0.0066],\n",
      "        [0.1260],\n",
      "        [0.0092],\n",
      "        [0.1219],\n",
      "        [0.0283],\n",
      "        [0.0301],\n",
      "        [0.0410],\n",
      "        [0.0499],\n",
      "        [0.0869],\n",
      "        [0.1255],\n",
      "        [0.0023],\n",
      "        [0.0021],\n",
      "        [0.0398],\n",
      "        [0.0167],\n",
      "        [0.1295],\n",
      "        [0.1566],\n",
      "        [0.1617],\n",
      "        [0.1894],\n",
      "        [0.1904],\n",
      "        [0.2028],\n",
      "        [0.2048],\n",
      "        [0.2056],\n",
      "        [0.2168],\n",
      "        [0.2000],\n",
      "        [0.2264],\n",
      "        [0.2118],\n",
      "        [0.1841],\n",
      "        [0.2261],\n",
      "        [0.2427],\n",
      "        [0.2445],\n",
      "        [0.0872],\n",
      "        [0.1539],\n",
      "        [0.0879],\n",
      "        [0.0728],\n",
      "        [0.1368],\n",
      "        [0.1427]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 18.50699210166931\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 92\n",
      "剩餘X 資料 torch.Size([68, 18])\n",
      "剩餘Y 資料 torch.Size([68, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.020769331604242325, 31)\n",
      "The second_loss value of k: (0.026033923029899597, 32)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引31，y= tensor([0.6893])\n",
      "目前模型的Data狀態 torch.Size([92, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334],\n",
      "        [0.8334]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0294],\n",
      "        [0.0198],\n",
      "        [0.0088],\n",
      "        [0.0113],\n",
      "        [0.0556],\n",
      "        [0.0200],\n",
      "        [0.0214],\n",
      "        [0.0121],\n",
      "        [0.0405],\n",
      "        [0.0268],\n",
      "        [0.0019],\n",
      "        [0.0297],\n",
      "        [0.0158],\n",
      "        [0.0038],\n",
      "        [0.0162],\n",
      "        [0.0291],\n",
      "        [0.0239],\n",
      "        [0.0134],\n",
      "        [0.0189],\n",
      "        [0.1487],\n",
      "        [0.0956],\n",
      "        [0.1033],\n",
      "        [0.1348],\n",
      "        [0.1414],\n",
      "        [0.0037],\n",
      "        [0.1433],\n",
      "        [0.0099],\n",
      "        [0.0129],\n",
      "        [0.0190],\n",
      "        [0.0825],\n",
      "        [0.0340],\n",
      "        [0.1021],\n",
      "        [0.1666],\n",
      "        [0.0501],\n",
      "        [0.0558],\n",
      "        [0.0592],\n",
      "        [0.0645],\n",
      "        [0.0685],\n",
      "        [0.0725],\n",
      "        [0.0762],\n",
      "        [0.0807],\n",
      "        [0.0866],\n",
      "        [0.1351],\n",
      "        [0.1660],\n",
      "        [0.1104],\n",
      "        [0.1188],\n",
      "        [0.1336],\n",
      "        [0.1339],\n",
      "        [0.1421],\n",
      "        [0.1457],\n",
      "        [0.1466],\n",
      "        [0.1500],\n",
      "        [0.1594],\n",
      "        [0.1639],\n",
      "        [0.1196],\n",
      "        [0.0066],\n",
      "        [0.1260],\n",
      "        [0.0092],\n",
      "        [0.1219],\n",
      "        [0.0283],\n",
      "        [0.0301],\n",
      "        [0.0410],\n",
      "        [0.0499],\n",
      "        [0.0869],\n",
      "        [0.1255],\n",
      "        [0.0023],\n",
      "        [0.0021],\n",
      "        [0.0398],\n",
      "        [0.0167],\n",
      "        [0.1295],\n",
      "        [0.1566],\n",
      "        [0.1617],\n",
      "        [0.1894],\n",
      "        [0.1904],\n",
      "        [0.2028],\n",
      "        [0.2048],\n",
      "        [0.2056],\n",
      "        [0.2168],\n",
      "        [0.2000],\n",
      "        [0.2264],\n",
      "        [0.2118],\n",
      "        [0.1841],\n",
      "        [0.2261],\n",
      "        [0.2427],\n",
      "        [0.2445],\n",
      "        [0.0872],\n",
      "        [0.1539],\n",
      "        [0.0879],\n",
      "        [0.0728],\n",
      "        [0.1368],\n",
      "        [0.1427],\n",
      "        [0.1441]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 80\n",
      "Number of shrink: 20\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0279],\n",
      "        [0.0183],\n",
      "        [0.0103],\n",
      "        [0.0129],\n",
      "        [0.0571],\n",
      "        [0.0215],\n",
      "        [0.0229],\n",
      "        [0.0136],\n",
      "        [0.0389],\n",
      "        [0.0253],\n",
      "        [0.0003],\n",
      "        [0.0282],\n",
      "        [0.0174],\n",
      "        [0.0023],\n",
      "        [0.0178],\n",
      "        [0.0275],\n",
      "        [0.0224],\n",
      "        [0.0118],\n",
      "        [0.0174],\n",
      "        [0.1503],\n",
      "        [0.0972],\n",
      "        [0.1048],\n",
      "        [0.1364],\n",
      "        [0.1429],\n",
      "        [0.0022],\n",
      "        [0.1449],\n",
      "        [0.0115],\n",
      "        [0.0144],\n",
      "        [0.0206],\n",
      "        [0.0841],\n",
      "        [0.0356],\n",
      "        [0.1036],\n",
      "        [0.1682],\n",
      "        [0.0517],\n",
      "        [0.0574],\n",
      "        [0.0608],\n",
      "        [0.0660],\n",
      "        [0.0700],\n",
      "        [0.0741],\n",
      "        [0.0778],\n",
      "        [0.0823],\n",
      "        [0.0882],\n",
      "        [0.1367],\n",
      "        [0.1676],\n",
      "        [0.1120],\n",
      "        [0.1203],\n",
      "        [0.1351],\n",
      "        [0.1355],\n",
      "        [0.1437],\n",
      "        [0.1472],\n",
      "        [0.1482],\n",
      "        [0.1515],\n",
      "        [0.1609],\n",
      "        [0.1654],\n",
      "        [0.1212],\n",
      "        [0.0082],\n",
      "        [0.1276],\n",
      "        [0.0107],\n",
      "        [0.1234],\n",
      "        [0.0298],\n",
      "        [0.0317],\n",
      "        [0.0425],\n",
      "        [0.0515],\n",
      "        [0.0885],\n",
      "        [0.1239],\n",
      "        [0.0038],\n",
      "        [0.0037],\n",
      "        [0.0382],\n",
      "        [0.0152],\n",
      "        [0.1280],\n",
      "        [0.1551],\n",
      "        [0.1601],\n",
      "        [0.1879],\n",
      "        [0.1888],\n",
      "        [0.2013],\n",
      "        [0.2032],\n",
      "        [0.2040],\n",
      "        [0.2152],\n",
      "        [0.1985],\n",
      "        [0.2248],\n",
      "        [0.2102],\n",
      "        [0.1826],\n",
      "        [0.2246],\n",
      "        [0.2411],\n",
      "        [0.2430],\n",
      "        [0.0857],\n",
      "        [0.1524],\n",
      "        [0.0863],\n",
      "        [0.0712],\n",
      "        [0.1352],\n",
      "        [0.1411],\n",
      "        [0.1426]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 18.750141859054565\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 93\n",
      "剩餘X 資料 torch.Size([67, 18])\n",
      "剩餘Y 資料 torch.Size([67, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.025531180202960968, 31)\n",
      "The second_loss value of k: (0.029815467074513435, 29)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引31，y= tensor([0.6720])\n",
      "目前模型的Data狀態 torch.Size([93, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318],\n",
      "        [0.8318]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0279],\n",
      "        [0.0183],\n",
      "        [0.0103],\n",
      "        [0.0129],\n",
      "        [0.0571],\n",
      "        [0.0215],\n",
      "        [0.0229],\n",
      "        [0.0136],\n",
      "        [0.0389],\n",
      "        [0.0253],\n",
      "        [0.0003],\n",
      "        [0.0282],\n",
      "        [0.0174],\n",
      "        [0.0023],\n",
      "        [0.0178],\n",
      "        [0.0275],\n",
      "        [0.0224],\n",
      "        [0.0118],\n",
      "        [0.0174],\n",
      "        [0.1503],\n",
      "        [0.0972],\n",
      "        [0.1048],\n",
      "        [0.1364],\n",
      "        [0.1429],\n",
      "        [0.0022],\n",
      "        [0.1449],\n",
      "        [0.0115],\n",
      "        [0.0144],\n",
      "        [0.0206],\n",
      "        [0.0841],\n",
      "        [0.0356],\n",
      "        [0.1036],\n",
      "        [0.1682],\n",
      "        [0.0517],\n",
      "        [0.0574],\n",
      "        [0.0608],\n",
      "        [0.0660],\n",
      "        [0.0700],\n",
      "        [0.0741],\n",
      "        [0.0778],\n",
      "        [0.0823],\n",
      "        [0.0882],\n",
      "        [0.1367],\n",
      "        [0.1676],\n",
      "        [0.1120],\n",
      "        [0.1203],\n",
      "        [0.1351],\n",
      "        [0.1355],\n",
      "        [0.1437],\n",
      "        [0.1472],\n",
      "        [0.1482],\n",
      "        [0.1515],\n",
      "        [0.1609],\n",
      "        [0.1654],\n",
      "        [0.1212],\n",
      "        [0.0082],\n",
      "        [0.1276],\n",
      "        [0.0107],\n",
      "        [0.1234],\n",
      "        [0.0298],\n",
      "        [0.0317],\n",
      "        [0.0425],\n",
      "        [0.0515],\n",
      "        [0.0885],\n",
      "        [0.1239],\n",
      "        [0.0038],\n",
      "        [0.0037],\n",
      "        [0.0382],\n",
      "        [0.0152],\n",
      "        [0.1280],\n",
      "        [0.1551],\n",
      "        [0.1601],\n",
      "        [0.1879],\n",
      "        [0.1888],\n",
      "        [0.2013],\n",
      "        [0.2032],\n",
      "        [0.2040],\n",
      "        [0.2152],\n",
      "        [0.1985],\n",
      "        [0.2248],\n",
      "        [0.2102],\n",
      "        [0.1826],\n",
      "        [0.2246],\n",
      "        [0.2411],\n",
      "        [0.2430],\n",
      "        [0.0857],\n",
      "        [0.1524],\n",
      "        [0.0863],\n",
      "        [0.0712],\n",
      "        [0.1352],\n",
      "        [0.1411],\n",
      "        [0.1426],\n",
      "        [0.1598]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0261],\n",
      "        [0.0166],\n",
      "        [0.0120],\n",
      "        [0.0146],\n",
      "        [0.0588],\n",
      "        [0.0232],\n",
      "        [0.0247],\n",
      "        [0.0153],\n",
      "        [0.0372],\n",
      "        [0.0236],\n",
      "        [0.0014],\n",
      "        [0.0265],\n",
      "        [0.0191],\n",
      "        [0.0006],\n",
      "        [0.0195],\n",
      "        [0.0258],\n",
      "        [0.0207],\n",
      "        [0.0101],\n",
      "        [0.0157],\n",
      "        [0.1520],\n",
      "        [0.0989],\n",
      "        [0.1066],\n",
      "        [0.1381],\n",
      "        [0.1446],\n",
      "        [0.0005],\n",
      "        [0.1466],\n",
      "        [0.0132],\n",
      "        [0.0162],\n",
      "        [0.0223],\n",
      "        [0.0858],\n",
      "        [0.0373],\n",
      "        [0.1053],\n",
      "        [0.1699],\n",
      "        [0.0534],\n",
      "        [0.0591],\n",
      "        [0.0625],\n",
      "        [0.0677],\n",
      "        [0.0717],\n",
      "        [0.0758],\n",
      "        [0.0795],\n",
      "        [0.0840],\n",
      "        [0.0899],\n",
      "        [0.1384],\n",
      "        [0.1693],\n",
      "        [0.1137],\n",
      "        [0.1220],\n",
      "        [0.1368],\n",
      "        [0.1372],\n",
      "        [0.1454],\n",
      "        [0.1489],\n",
      "        [0.1499],\n",
      "        [0.1532],\n",
      "        [0.1626],\n",
      "        [0.1672],\n",
      "        [0.1229],\n",
      "        [0.0099],\n",
      "        [0.1293],\n",
      "        [0.0124],\n",
      "        [0.1251],\n",
      "        [0.0315],\n",
      "        [0.0334],\n",
      "        [0.0442],\n",
      "        [0.0532],\n",
      "        [0.0902],\n",
      "        [0.1222],\n",
      "        [0.0055],\n",
      "        [0.0054],\n",
      "        [0.0365],\n",
      "        [0.0135],\n",
      "        [0.1263],\n",
      "        [0.1534],\n",
      "        [0.1584],\n",
      "        [0.1862],\n",
      "        [0.1871],\n",
      "        [0.1996],\n",
      "        [0.2015],\n",
      "        [0.2023],\n",
      "        [0.2135],\n",
      "        [0.1968],\n",
      "        [0.2231],\n",
      "        [0.2085],\n",
      "        [0.1809],\n",
      "        [0.2229],\n",
      "        [0.2394],\n",
      "        [0.2413],\n",
      "        [0.0840],\n",
      "        [0.1507],\n",
      "        [0.0846],\n",
      "        [0.0695],\n",
      "        [0.1335],\n",
      "        [0.1394],\n",
      "        [0.1408],\n",
      "        [0.1581]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 18.99588632583618\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 94\n",
      "剩餘X 資料 torch.Size([66, 18])\n",
      "剩餘Y 資料 torch.Size([66, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.029226815328001976, 29)\n",
      "The second_loss value of k: (0.029280418530106544, 30)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引29，y= tensor([0.6591])\n",
      "目前模型的Data狀態 torch.Size([94, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301],\n",
      "        [0.8301]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0261],\n",
      "        [0.0166],\n",
      "        [0.0120],\n",
      "        [0.0146],\n",
      "        [0.0588],\n",
      "        [0.0232],\n",
      "        [0.0247],\n",
      "        [0.0153],\n",
      "        [0.0372],\n",
      "        [0.0236],\n",
      "        [0.0014],\n",
      "        [0.0265],\n",
      "        [0.0191],\n",
      "        [0.0006],\n",
      "        [0.0195],\n",
      "        [0.0258],\n",
      "        [0.0207],\n",
      "        [0.0101],\n",
      "        [0.0157],\n",
      "        [0.1520],\n",
      "        [0.0989],\n",
      "        [0.1066],\n",
      "        [0.1381],\n",
      "        [0.1446],\n",
      "        [0.0005],\n",
      "        [0.1466],\n",
      "        [0.0132],\n",
      "        [0.0162],\n",
      "        [0.0223],\n",
      "        [0.0858],\n",
      "        [0.0373],\n",
      "        [0.1053],\n",
      "        [0.1699],\n",
      "        [0.0534],\n",
      "        [0.0591],\n",
      "        [0.0625],\n",
      "        [0.0677],\n",
      "        [0.0717],\n",
      "        [0.0758],\n",
      "        [0.0795],\n",
      "        [0.0840],\n",
      "        [0.0899],\n",
      "        [0.1384],\n",
      "        [0.1693],\n",
      "        [0.1137],\n",
      "        [0.1220],\n",
      "        [0.1368],\n",
      "        [0.1372],\n",
      "        [0.1454],\n",
      "        [0.1489],\n",
      "        [0.1499],\n",
      "        [0.1532],\n",
      "        [0.1626],\n",
      "        [0.1672],\n",
      "        [0.1229],\n",
      "        [0.0099],\n",
      "        [0.1293],\n",
      "        [0.0124],\n",
      "        [0.1251],\n",
      "        [0.0315],\n",
      "        [0.0334],\n",
      "        [0.0442],\n",
      "        [0.0532],\n",
      "        [0.0902],\n",
      "        [0.1222],\n",
      "        [0.0055],\n",
      "        [0.0054],\n",
      "        [0.0365],\n",
      "        [0.0135],\n",
      "        [0.1263],\n",
      "        [0.1534],\n",
      "        [0.1584],\n",
      "        [0.1862],\n",
      "        [0.1871],\n",
      "        [0.1996],\n",
      "        [0.2015],\n",
      "        [0.2023],\n",
      "        [0.2135],\n",
      "        [0.1968],\n",
      "        [0.2231],\n",
      "        [0.2085],\n",
      "        [0.1809],\n",
      "        [0.2229],\n",
      "        [0.2394],\n",
      "        [0.2413],\n",
      "        [0.0840],\n",
      "        [0.1507],\n",
      "        [0.0846],\n",
      "        [0.0695],\n",
      "        [0.1335],\n",
      "        [0.1394],\n",
      "        [0.1408],\n",
      "        [0.1581],\n",
      "        [0.1710]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0243],\n",
      "        [0.0147],\n",
      "        [0.0139],\n",
      "        [0.0164],\n",
      "        [0.0607],\n",
      "        [0.0251],\n",
      "        [0.0265],\n",
      "        [0.0172],\n",
      "        [0.0354],\n",
      "        [0.0217],\n",
      "        [0.0033],\n",
      "        [0.0246],\n",
      "        [0.0209],\n",
      "        [0.0013],\n",
      "        [0.0213],\n",
      "        [0.0240],\n",
      "        [0.0188],\n",
      "        [0.0083],\n",
      "        [0.0138],\n",
      "        [0.1538],\n",
      "        [0.1007],\n",
      "        [0.1084],\n",
      "        [0.1399],\n",
      "        [0.1465],\n",
      "        [0.0014],\n",
      "        [0.1484],\n",
      "        [0.0150],\n",
      "        [0.0180],\n",
      "        [0.0241],\n",
      "        [0.0876],\n",
      "        [0.0391],\n",
      "        [0.1072],\n",
      "        [0.1717],\n",
      "        [0.0552],\n",
      "        [0.0609],\n",
      "        [0.0643],\n",
      "        [0.0696],\n",
      "        [0.0736],\n",
      "        [0.0776],\n",
      "        [0.0813],\n",
      "        [0.0858],\n",
      "        [0.0917],\n",
      "        [0.1402],\n",
      "        [0.1711],\n",
      "        [0.1155],\n",
      "        [0.1239],\n",
      "        [0.1387],\n",
      "        [0.1391],\n",
      "        [0.1472],\n",
      "        [0.1508],\n",
      "        [0.1517],\n",
      "        [0.1551],\n",
      "        [0.1645],\n",
      "        [0.1690],\n",
      "        [0.1247],\n",
      "        [0.0118],\n",
      "        [0.1311],\n",
      "        [0.0143],\n",
      "        [0.1270],\n",
      "        [0.0334],\n",
      "        [0.0353],\n",
      "        [0.0461],\n",
      "        [0.0550],\n",
      "        [0.0920],\n",
      "        [0.1204],\n",
      "        [0.0074],\n",
      "        [0.0072],\n",
      "        [0.0347],\n",
      "        [0.0116],\n",
      "        [0.1244],\n",
      "        [0.1515],\n",
      "        [0.1566],\n",
      "        [0.1843],\n",
      "        [0.1853],\n",
      "        [0.1977],\n",
      "        [0.1996],\n",
      "        [0.2005],\n",
      "        [0.2117],\n",
      "        [0.1949],\n",
      "        [0.2213],\n",
      "        [0.2067],\n",
      "        [0.1790],\n",
      "        [0.2210],\n",
      "        [0.2376],\n",
      "        [0.2394],\n",
      "        [0.0821],\n",
      "        [0.1488],\n",
      "        [0.0828],\n",
      "        [0.0677],\n",
      "        [0.1317],\n",
      "        [0.1376],\n",
      "        [0.1390],\n",
      "        [0.1562],\n",
      "        [0.1691]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 19.240164518356323\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 95\n",
      "剩餘X 資料 torch.Size([65, 18])\n",
      "剩餘Y 資料 torch.Size([65, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.028659731149673462, 29)\n",
      "The second_loss value of k: (0.02883239835500717, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引29，y= tensor([0.6590])\n",
      "目前模型的Data狀態 torch.Size([95, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283],\n",
      "        [0.8283]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0243],\n",
      "        [0.0147],\n",
      "        [0.0139],\n",
      "        [0.0164],\n",
      "        [0.0607],\n",
      "        [0.0251],\n",
      "        [0.0265],\n",
      "        [0.0172],\n",
      "        [0.0354],\n",
      "        [0.0217],\n",
      "        [0.0033],\n",
      "        [0.0246],\n",
      "        [0.0209],\n",
      "        [0.0013],\n",
      "        [0.0213],\n",
      "        [0.0240],\n",
      "        [0.0188],\n",
      "        [0.0083],\n",
      "        [0.0138],\n",
      "        [0.1538],\n",
      "        [0.1007],\n",
      "        [0.1084],\n",
      "        [0.1399],\n",
      "        [0.1465],\n",
      "        [0.0014],\n",
      "        [0.1484],\n",
      "        [0.0150],\n",
      "        [0.0180],\n",
      "        [0.0241],\n",
      "        [0.0876],\n",
      "        [0.0391],\n",
      "        [0.1072],\n",
      "        [0.1717],\n",
      "        [0.0552],\n",
      "        [0.0609],\n",
      "        [0.0643],\n",
      "        [0.0696],\n",
      "        [0.0736],\n",
      "        [0.0776],\n",
      "        [0.0813],\n",
      "        [0.0858],\n",
      "        [0.0917],\n",
      "        [0.1402],\n",
      "        [0.1711],\n",
      "        [0.1155],\n",
      "        [0.1239],\n",
      "        [0.1387],\n",
      "        [0.1391],\n",
      "        [0.1472],\n",
      "        [0.1508],\n",
      "        [0.1517],\n",
      "        [0.1551],\n",
      "        [0.1645],\n",
      "        [0.1690],\n",
      "        [0.1247],\n",
      "        [0.0118],\n",
      "        [0.1311],\n",
      "        [0.0143],\n",
      "        [0.1270],\n",
      "        [0.0334],\n",
      "        [0.0353],\n",
      "        [0.0461],\n",
      "        [0.0550],\n",
      "        [0.0920],\n",
      "        [0.1204],\n",
      "        [0.0074],\n",
      "        [0.0072],\n",
      "        [0.0347],\n",
      "        [0.0116],\n",
      "        [0.1244],\n",
      "        [0.1515],\n",
      "        [0.1566],\n",
      "        [0.1843],\n",
      "        [0.1853],\n",
      "        [0.1977],\n",
      "        [0.1996],\n",
      "        [0.2005],\n",
      "        [0.2117],\n",
      "        [0.1949],\n",
      "        [0.2213],\n",
      "        [0.2067],\n",
      "        [0.1790],\n",
      "        [0.2210],\n",
      "        [0.2376],\n",
      "        [0.2394],\n",
      "        [0.0821],\n",
      "        [0.1488],\n",
      "        [0.0828],\n",
      "        [0.0677],\n",
      "        [0.1317],\n",
      "        [0.1376],\n",
      "        [0.1390],\n",
      "        [0.1562],\n",
      "        [0.1691],\n",
      "        [0.1693]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0225],\n",
      "        [0.0129],\n",
      "        [0.0156],\n",
      "        [0.0182],\n",
      "        [0.0625],\n",
      "        [0.0269],\n",
      "        [0.0283],\n",
      "        [0.0189],\n",
      "        [0.0336],\n",
      "        [0.0200],\n",
      "        [0.0050],\n",
      "        [0.0229],\n",
      "        [0.0227],\n",
      "        [0.0030],\n",
      "        [0.0231],\n",
      "        [0.0222],\n",
      "        [0.0171],\n",
      "        [0.0065],\n",
      "        [0.0120],\n",
      "        [0.1556],\n",
      "        [0.1025],\n",
      "        [0.1102],\n",
      "        [0.1417],\n",
      "        [0.1482],\n",
      "        [0.0032],\n",
      "        [0.1502],\n",
      "        [0.0168],\n",
      "        [0.0198],\n",
      "        [0.0259],\n",
      "        [0.0894],\n",
      "        [0.0409],\n",
      "        [0.1090],\n",
      "        [0.1735],\n",
      "        [0.0570],\n",
      "        [0.0627],\n",
      "        [0.0661],\n",
      "        [0.0713],\n",
      "        [0.0753],\n",
      "        [0.0794],\n",
      "        [0.0831],\n",
      "        [0.0876],\n",
      "        [0.0935],\n",
      "        [0.1420],\n",
      "        [0.1729],\n",
      "        [0.1173],\n",
      "        [0.1256],\n",
      "        [0.1404],\n",
      "        [0.1408],\n",
      "        [0.1490],\n",
      "        [0.1525],\n",
      "        [0.1535],\n",
      "        [0.1569],\n",
      "        [0.1663],\n",
      "        [0.1708],\n",
      "        [0.1265],\n",
      "        [0.0135],\n",
      "        [0.1329],\n",
      "        [0.0160],\n",
      "        [0.1288],\n",
      "        [0.0352],\n",
      "        [0.0370],\n",
      "        [0.0478],\n",
      "        [0.0568],\n",
      "        [0.0938],\n",
      "        [0.1186],\n",
      "        [0.0091],\n",
      "        [0.0090],\n",
      "        [0.0329],\n",
      "        [0.0099],\n",
      "        [0.1227],\n",
      "        [0.1498],\n",
      "        [0.1548],\n",
      "        [0.1825],\n",
      "        [0.1835],\n",
      "        [0.1959],\n",
      "        [0.1979],\n",
      "        [0.1987],\n",
      "        [0.2099],\n",
      "        [0.1932],\n",
      "        [0.2195],\n",
      "        [0.2049],\n",
      "        [0.1773],\n",
      "        [0.2193],\n",
      "        [0.2358],\n",
      "        [0.2377],\n",
      "        [0.0804],\n",
      "        [0.1471],\n",
      "        [0.0810],\n",
      "        [0.0659],\n",
      "        [0.1299],\n",
      "        [0.1358],\n",
      "        [0.1372],\n",
      "        [0.1545],\n",
      "        [0.1674],\n",
      "        [0.1675]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 19.486778020858765\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 96\n",
      "剩餘X 資料 torch.Size([64, 18])\n",
      "剩餘Y 資料 torch.Size([64, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.02822992205619812, 0)\n",
      "The second_loss value of k: (0.032063715159893036, 27)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.6585])\n",
      "目前模型的Data狀態 torch.Size([96, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265],\n",
      "        [0.8265]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0225],\n",
      "        [0.0129],\n",
      "        [0.0156],\n",
      "        [0.0182],\n",
      "        [0.0625],\n",
      "        [0.0269],\n",
      "        [0.0283],\n",
      "        [0.0189],\n",
      "        [0.0336],\n",
      "        [0.0200],\n",
      "        [0.0050],\n",
      "        [0.0229],\n",
      "        [0.0227],\n",
      "        [0.0030],\n",
      "        [0.0231],\n",
      "        [0.0222],\n",
      "        [0.0171],\n",
      "        [0.0065],\n",
      "        [0.0120],\n",
      "        [0.1556],\n",
      "        [0.1025],\n",
      "        [0.1102],\n",
      "        [0.1417],\n",
      "        [0.1482],\n",
      "        [0.0032],\n",
      "        [0.1502],\n",
      "        [0.0168],\n",
      "        [0.0198],\n",
      "        [0.0259],\n",
      "        [0.0894],\n",
      "        [0.0409],\n",
      "        [0.1090],\n",
      "        [0.1735],\n",
      "        [0.0570],\n",
      "        [0.0627],\n",
      "        [0.0661],\n",
      "        [0.0713],\n",
      "        [0.0753],\n",
      "        [0.0794],\n",
      "        [0.0831],\n",
      "        [0.0876],\n",
      "        [0.0935],\n",
      "        [0.1420],\n",
      "        [0.1729],\n",
      "        [0.1173],\n",
      "        [0.1256],\n",
      "        [0.1404],\n",
      "        [0.1408],\n",
      "        [0.1490],\n",
      "        [0.1525],\n",
      "        [0.1535],\n",
      "        [0.1569],\n",
      "        [0.1663],\n",
      "        [0.1708],\n",
      "        [0.1265],\n",
      "        [0.0135],\n",
      "        [0.1329],\n",
      "        [0.0160],\n",
      "        [0.1288],\n",
      "        [0.0352],\n",
      "        [0.0370],\n",
      "        [0.0478],\n",
      "        [0.0568],\n",
      "        [0.0938],\n",
      "        [0.1186],\n",
      "        [0.0091],\n",
      "        [0.0090],\n",
      "        [0.0329],\n",
      "        [0.0099],\n",
      "        [0.1227],\n",
      "        [0.1498],\n",
      "        [0.1548],\n",
      "        [0.1825],\n",
      "        [0.1835],\n",
      "        [0.1959],\n",
      "        [0.1979],\n",
      "        [0.1987],\n",
      "        [0.2099],\n",
      "        [0.1932],\n",
      "        [0.2195],\n",
      "        [0.2049],\n",
      "        [0.1773],\n",
      "        [0.2193],\n",
      "        [0.2358],\n",
      "        [0.2377],\n",
      "        [0.0804],\n",
      "        [0.1471],\n",
      "        [0.0810],\n",
      "        [0.0659],\n",
      "        [0.1299],\n",
      "        [0.1358],\n",
      "        [0.1372],\n",
      "        [0.1545],\n",
      "        [0.1674],\n",
      "        [0.1675],\n",
      "        [0.1680]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 81\n",
      "Number of shrink: 19\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0208],\n",
      "        [0.0112],\n",
      "        [0.0174],\n",
      "        [0.0199],\n",
      "        [0.0642],\n",
      "        [0.0286],\n",
      "        [0.0300],\n",
      "        [0.0207],\n",
      "        [0.0319],\n",
      "        [0.0182],\n",
      "        [0.0068],\n",
      "        [0.0211],\n",
      "        [0.0244],\n",
      "        [0.0048],\n",
      "        [0.0248],\n",
      "        [0.0205],\n",
      "        [0.0153],\n",
      "        [0.0048],\n",
      "        [0.0103],\n",
      "        [0.1574],\n",
      "        [0.1042],\n",
      "        [0.1119],\n",
      "        [0.1434],\n",
      "        [0.1500],\n",
      "        [0.0049],\n",
      "        [0.1519],\n",
      "        [0.0185],\n",
      "        [0.0215],\n",
      "        [0.0277],\n",
      "        [0.0911],\n",
      "        [0.0427],\n",
      "        [0.1107],\n",
      "        [0.1752],\n",
      "        [0.0587],\n",
      "        [0.0645],\n",
      "        [0.0678],\n",
      "        [0.0731],\n",
      "        [0.0771],\n",
      "        [0.0812],\n",
      "        [0.0848],\n",
      "        [0.0893],\n",
      "        [0.0952],\n",
      "        [0.1437],\n",
      "        [0.1747],\n",
      "        [0.1190],\n",
      "        [0.1274],\n",
      "        [0.1422],\n",
      "        [0.1426],\n",
      "        [0.1508],\n",
      "        [0.1543],\n",
      "        [0.1553],\n",
      "        [0.1586],\n",
      "        [0.1680],\n",
      "        [0.1725],\n",
      "        [0.1282],\n",
      "        [0.0153],\n",
      "        [0.1346],\n",
      "        [0.0178],\n",
      "        [0.1305],\n",
      "        [0.0369],\n",
      "        [0.0388],\n",
      "        [0.0496],\n",
      "        [0.0586],\n",
      "        [0.0955],\n",
      "        [0.1168],\n",
      "        [0.0109],\n",
      "        [0.0107],\n",
      "        [0.0311],\n",
      "        [0.0081],\n",
      "        [0.1209],\n",
      "        [0.1480],\n",
      "        [0.1530],\n",
      "        [0.1808],\n",
      "        [0.1818],\n",
      "        [0.1942],\n",
      "        [0.1961],\n",
      "        [0.1970],\n",
      "        [0.2082],\n",
      "        [0.1914],\n",
      "        [0.2178],\n",
      "        [0.2032],\n",
      "        [0.1755],\n",
      "        [0.2175],\n",
      "        [0.2341],\n",
      "        [0.2359],\n",
      "        [0.0786],\n",
      "        [0.1453],\n",
      "        [0.0792],\n",
      "        [0.0642],\n",
      "        [0.1282],\n",
      "        [0.1340],\n",
      "        [0.1355],\n",
      "        [0.1527],\n",
      "        [0.1656],\n",
      "        [0.1658],\n",
      "        [0.1663]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 19.728906393051147\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 97\n",
      "剩餘X 資料 torch.Size([63, 18])\n",
      "剩餘Y 資料 torch.Size([63, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.031442150473594666, 26)\n",
      "The second_loss value of k: (0.0321732722222805, 29)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引26，y= tensor([0.6474])\n",
      "目前模型的Data狀態 torch.Size([97, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248],\n",
      "        [0.8248]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0208],\n",
      "        [0.0112],\n",
      "        [0.0174],\n",
      "        [0.0199],\n",
      "        [0.0642],\n",
      "        [0.0286],\n",
      "        [0.0300],\n",
      "        [0.0207],\n",
      "        [0.0319],\n",
      "        [0.0182],\n",
      "        [0.0068],\n",
      "        [0.0211],\n",
      "        [0.0244],\n",
      "        [0.0048],\n",
      "        [0.0248],\n",
      "        [0.0205],\n",
      "        [0.0153],\n",
      "        [0.0048],\n",
      "        [0.0103],\n",
      "        [0.1574],\n",
      "        [0.1042],\n",
      "        [0.1119],\n",
      "        [0.1434],\n",
      "        [0.1500],\n",
      "        [0.0049],\n",
      "        [0.1519],\n",
      "        [0.0185],\n",
      "        [0.0215],\n",
      "        [0.0277],\n",
      "        [0.0911],\n",
      "        [0.0427],\n",
      "        [0.1107],\n",
      "        [0.1752],\n",
      "        [0.0587],\n",
      "        [0.0645],\n",
      "        [0.0678],\n",
      "        [0.0731],\n",
      "        [0.0771],\n",
      "        [0.0812],\n",
      "        [0.0848],\n",
      "        [0.0893],\n",
      "        [0.0952],\n",
      "        [0.1437],\n",
      "        [0.1747],\n",
      "        [0.1190],\n",
      "        [0.1274],\n",
      "        [0.1422],\n",
      "        [0.1426],\n",
      "        [0.1508],\n",
      "        [0.1543],\n",
      "        [0.1553],\n",
      "        [0.1586],\n",
      "        [0.1680],\n",
      "        [0.1725],\n",
      "        [0.1282],\n",
      "        [0.0153],\n",
      "        [0.1346],\n",
      "        [0.0178],\n",
      "        [0.1305],\n",
      "        [0.0369],\n",
      "        [0.0388],\n",
      "        [0.0496],\n",
      "        [0.0586],\n",
      "        [0.0955],\n",
      "        [0.1168],\n",
      "        [0.0109],\n",
      "        [0.0107],\n",
      "        [0.0311],\n",
      "        [0.0081],\n",
      "        [0.1209],\n",
      "        [0.1480],\n",
      "        [0.1530],\n",
      "        [0.1808],\n",
      "        [0.1818],\n",
      "        [0.1942],\n",
      "        [0.1961],\n",
      "        [0.1970],\n",
      "        [0.2082],\n",
      "        [0.1914],\n",
      "        [0.2178],\n",
      "        [0.2032],\n",
      "        [0.1755],\n",
      "        [0.2175],\n",
      "        [0.2341],\n",
      "        [0.2359],\n",
      "        [0.0786],\n",
      "        [0.1453],\n",
      "        [0.0792],\n",
      "        [0.0642],\n",
      "        [0.1282],\n",
      "        [0.1340],\n",
      "        [0.1355],\n",
      "        [0.1527],\n",
      "        [0.1656],\n",
      "        [0.1658],\n",
      "        [0.1663],\n",
      "        [0.1773]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0190],\n",
      "        [0.0094],\n",
      "        [0.0192],\n",
      "        [0.0218],\n",
      "        [0.0660],\n",
      "        [0.0304],\n",
      "        [0.0318],\n",
      "        [0.0225],\n",
      "        [0.0301],\n",
      "        [0.0164],\n",
      "        [0.0086],\n",
      "        [0.0193],\n",
      "        [0.0263],\n",
      "        [0.0066],\n",
      "        [0.0267],\n",
      "        [0.0187],\n",
      "        [0.0135],\n",
      "        [0.0029],\n",
      "        [0.0085],\n",
      "        [0.1592],\n",
      "        [0.1060],\n",
      "        [0.1137],\n",
      "        [0.1452],\n",
      "        [0.1518],\n",
      "        [0.0067],\n",
      "        [0.1538],\n",
      "        [0.0204],\n",
      "        [0.0233],\n",
      "        [0.0295],\n",
      "        [0.0930],\n",
      "        [0.0445],\n",
      "        [0.1125],\n",
      "        [0.1771],\n",
      "        [0.0605],\n",
      "        [0.0663],\n",
      "        [0.0697],\n",
      "        [0.0749],\n",
      "        [0.0789],\n",
      "        [0.0830],\n",
      "        [0.0867],\n",
      "        [0.0912],\n",
      "        [0.0971],\n",
      "        [0.1455],\n",
      "        [0.1765],\n",
      "        [0.1209],\n",
      "        [0.1292],\n",
      "        [0.1440],\n",
      "        [0.1444],\n",
      "        [0.1526],\n",
      "        [0.1561],\n",
      "        [0.1571],\n",
      "        [0.1604],\n",
      "        [0.1698],\n",
      "        [0.1743],\n",
      "        [0.1301],\n",
      "        [0.0171],\n",
      "        [0.1365],\n",
      "        [0.0196],\n",
      "        [0.1323],\n",
      "        [0.0387],\n",
      "        [0.0406],\n",
      "        [0.0514],\n",
      "        [0.0604],\n",
      "        [0.0974],\n",
      "        [0.1150],\n",
      "        [0.0127],\n",
      "        [0.0126],\n",
      "        [0.0293],\n",
      "        [0.0063],\n",
      "        [0.1191],\n",
      "        [0.1462],\n",
      "        [0.1512],\n",
      "        [0.1790],\n",
      "        [0.1800],\n",
      "        [0.1924],\n",
      "        [0.1943],\n",
      "        [0.1952],\n",
      "        [0.2064],\n",
      "        [0.1896],\n",
      "        [0.2160],\n",
      "        [0.2013],\n",
      "        [0.1737],\n",
      "        [0.2157],\n",
      "        [0.2322],\n",
      "        [0.2341],\n",
      "        [0.0768],\n",
      "        [0.1435],\n",
      "        [0.0774],\n",
      "        [0.0623],\n",
      "        [0.1263],\n",
      "        [0.1322],\n",
      "        [0.1337],\n",
      "        [0.1509],\n",
      "        [0.1638],\n",
      "        [0.1639],\n",
      "        [0.1644],\n",
      "        [0.1755]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 19.971635103225708\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 98\n",
      "剩餘X 資料 torch.Size([62, 18])\n",
      "剩餘Y 資料 torch.Size([62, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03152083232998848, 28)\n",
      "The second_loss value of k: (0.032896339893341064, 27)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引28，y= tensor([0.6454])\n",
      "目前模型的Data狀態 torch.Size([98, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229],\n",
      "        [0.8229]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0190],\n",
      "        [0.0094],\n",
      "        [0.0192],\n",
      "        [0.0218],\n",
      "        [0.0660],\n",
      "        [0.0304],\n",
      "        [0.0318],\n",
      "        [0.0225],\n",
      "        [0.0301],\n",
      "        [0.0164],\n",
      "        [0.0086],\n",
      "        [0.0193],\n",
      "        [0.0263],\n",
      "        [0.0066],\n",
      "        [0.0267],\n",
      "        [0.0187],\n",
      "        [0.0135],\n",
      "        [0.0029],\n",
      "        [0.0085],\n",
      "        [0.1592],\n",
      "        [0.1060],\n",
      "        [0.1137],\n",
      "        [0.1452],\n",
      "        [0.1518],\n",
      "        [0.0067],\n",
      "        [0.1538],\n",
      "        [0.0204],\n",
      "        [0.0233],\n",
      "        [0.0295],\n",
      "        [0.0930],\n",
      "        [0.0445],\n",
      "        [0.1125],\n",
      "        [0.1771],\n",
      "        [0.0605],\n",
      "        [0.0663],\n",
      "        [0.0697],\n",
      "        [0.0749],\n",
      "        [0.0789],\n",
      "        [0.0830],\n",
      "        [0.0867],\n",
      "        [0.0912],\n",
      "        [0.0971],\n",
      "        [0.1455],\n",
      "        [0.1765],\n",
      "        [0.1209],\n",
      "        [0.1292],\n",
      "        [0.1440],\n",
      "        [0.1444],\n",
      "        [0.1526],\n",
      "        [0.1561],\n",
      "        [0.1571],\n",
      "        [0.1604],\n",
      "        [0.1698],\n",
      "        [0.1743],\n",
      "        [0.1301],\n",
      "        [0.0171],\n",
      "        [0.1365],\n",
      "        [0.0196],\n",
      "        [0.1323],\n",
      "        [0.0387],\n",
      "        [0.0406],\n",
      "        [0.0514],\n",
      "        [0.0604],\n",
      "        [0.0974],\n",
      "        [0.1150],\n",
      "        [0.0127],\n",
      "        [0.0126],\n",
      "        [0.0293],\n",
      "        [0.0063],\n",
      "        [0.1191],\n",
      "        [0.1462],\n",
      "        [0.1512],\n",
      "        [0.1790],\n",
      "        [0.1800],\n",
      "        [0.1924],\n",
      "        [0.1943],\n",
      "        [0.1952],\n",
      "        [0.2064],\n",
      "        [0.1896],\n",
      "        [0.2160],\n",
      "        [0.2013],\n",
      "        [0.1737],\n",
      "        [0.2157],\n",
      "        [0.2322],\n",
      "        [0.2341],\n",
      "        [0.0768],\n",
      "        [0.1435],\n",
      "        [0.0774],\n",
      "        [0.0623],\n",
      "        [0.1263],\n",
      "        [0.1322],\n",
      "        [0.1337],\n",
      "        [0.1509],\n",
      "        [0.1638],\n",
      "        [0.1639],\n",
      "        [0.1644],\n",
      "        [0.1755],\n",
      "        [0.1775]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 73\n",
      "Number of shrink: 27\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0172],\n",
      "        [0.0076],\n",
      "        [0.0210],\n",
      "        [0.0236],\n",
      "        [0.0678],\n",
      "        [0.0322],\n",
      "        [0.0337],\n",
      "        [0.0243],\n",
      "        [0.0282],\n",
      "        [0.0146],\n",
      "        [0.0104],\n",
      "        [0.0175],\n",
      "        [0.0281],\n",
      "        [0.0084],\n",
      "        [0.0285],\n",
      "        [0.0168],\n",
      "        [0.0117],\n",
      "        [0.0011],\n",
      "        [0.0067],\n",
      "        [0.1610],\n",
      "        [0.1079],\n",
      "        [0.1156],\n",
      "        [0.1471],\n",
      "        [0.1536],\n",
      "        [0.0085],\n",
      "        [0.1556],\n",
      "        [0.0222],\n",
      "        [0.0252],\n",
      "        [0.0313],\n",
      "        [0.0948],\n",
      "        [0.0463],\n",
      "        [0.1143],\n",
      "        [0.1789],\n",
      "        [0.0624],\n",
      "        [0.0681],\n",
      "        [0.0715],\n",
      "        [0.0767],\n",
      "        [0.0807],\n",
      "        [0.0848],\n",
      "        [0.0885],\n",
      "        [0.0930],\n",
      "        [0.0989],\n",
      "        [0.1474],\n",
      "        [0.1783],\n",
      "        [0.1227],\n",
      "        [0.1310],\n",
      "        [0.1458],\n",
      "        [0.1462],\n",
      "        [0.1544],\n",
      "        [0.1579],\n",
      "        [0.1589],\n",
      "        [0.1622],\n",
      "        [0.1716],\n",
      "        [0.1762],\n",
      "        [0.1319],\n",
      "        [0.0189],\n",
      "        [0.1383],\n",
      "        [0.0214],\n",
      "        [0.1341],\n",
      "        [0.0405],\n",
      "        [0.0424],\n",
      "        [0.0532],\n",
      "        [0.0622],\n",
      "        [0.0992],\n",
      "        [0.1132],\n",
      "        [0.0145],\n",
      "        [0.0144],\n",
      "        [0.0275],\n",
      "        [0.0045],\n",
      "        [0.1173],\n",
      "        [0.1444],\n",
      "        [0.1494],\n",
      "        [0.1772],\n",
      "        [0.1781],\n",
      "        [0.1906],\n",
      "        [0.1925],\n",
      "        [0.1933],\n",
      "        [0.2045],\n",
      "        [0.1878],\n",
      "        [0.2141],\n",
      "        [0.1995],\n",
      "        [0.1719],\n",
      "        [0.2139],\n",
      "        [0.2304],\n",
      "        [0.2323],\n",
      "        [0.0750],\n",
      "        [0.1417],\n",
      "        [0.0756],\n",
      "        [0.0605],\n",
      "        [0.1245],\n",
      "        [0.1304],\n",
      "        [0.1318],\n",
      "        [0.1491],\n",
      "        [0.1620],\n",
      "        [0.1621],\n",
      "        [0.1626],\n",
      "        [0.1737],\n",
      "        [0.1757]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 20.216411352157593\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 99\n",
      "剩餘X 資料 torch.Size([61, 18])\n",
      "剩餘Y 資料 torch.Size([61, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03224010393023491, 27)\n",
      "The second_loss value of k: (0.032378505915403366, 26)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引27，y= tensor([0.6416])\n",
      "目前模型的Data狀態 torch.Size([99, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211],\n",
      "        [0.8211]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0172],\n",
      "        [0.0076],\n",
      "        [0.0210],\n",
      "        [0.0236],\n",
      "        [0.0678],\n",
      "        [0.0322],\n",
      "        [0.0337],\n",
      "        [0.0243],\n",
      "        [0.0282],\n",
      "        [0.0146],\n",
      "        [0.0104],\n",
      "        [0.0175],\n",
      "        [0.0281],\n",
      "        [0.0084],\n",
      "        [0.0285],\n",
      "        [0.0168],\n",
      "        [0.0117],\n",
      "        [0.0011],\n",
      "        [0.0067],\n",
      "        [0.1610],\n",
      "        [0.1079],\n",
      "        [0.1156],\n",
      "        [0.1471],\n",
      "        [0.1536],\n",
      "        [0.0085],\n",
      "        [0.1556],\n",
      "        [0.0222],\n",
      "        [0.0252],\n",
      "        [0.0313],\n",
      "        [0.0948],\n",
      "        [0.0463],\n",
      "        [0.1143],\n",
      "        [0.1789],\n",
      "        [0.0624],\n",
      "        [0.0681],\n",
      "        [0.0715],\n",
      "        [0.0767],\n",
      "        [0.0807],\n",
      "        [0.0848],\n",
      "        [0.0885],\n",
      "        [0.0930],\n",
      "        [0.0989],\n",
      "        [0.1474],\n",
      "        [0.1783],\n",
      "        [0.1227],\n",
      "        [0.1310],\n",
      "        [0.1458],\n",
      "        [0.1462],\n",
      "        [0.1544],\n",
      "        [0.1579],\n",
      "        [0.1589],\n",
      "        [0.1622],\n",
      "        [0.1716],\n",
      "        [0.1762],\n",
      "        [0.1319],\n",
      "        [0.0189],\n",
      "        [0.1383],\n",
      "        [0.0214],\n",
      "        [0.1341],\n",
      "        [0.0405],\n",
      "        [0.0424],\n",
      "        [0.0532],\n",
      "        [0.0622],\n",
      "        [0.0992],\n",
      "        [0.1132],\n",
      "        [0.0145],\n",
      "        [0.0144],\n",
      "        [0.0275],\n",
      "        [0.0045],\n",
      "        [0.1173],\n",
      "        [0.1444],\n",
      "        [0.1494],\n",
      "        [0.1772],\n",
      "        [0.1781],\n",
      "        [0.1906],\n",
      "        [0.1925],\n",
      "        [0.1933],\n",
      "        [0.2045],\n",
      "        [0.1878],\n",
      "        [0.2141],\n",
      "        [0.1995],\n",
      "        [0.1719],\n",
      "        [0.2139],\n",
      "        [0.2304],\n",
      "        [0.2323],\n",
      "        [0.0750],\n",
      "        [0.1417],\n",
      "        [0.0756],\n",
      "        [0.0605],\n",
      "        [0.1245],\n",
      "        [0.1304],\n",
      "        [0.1318],\n",
      "        [0.1491],\n",
      "        [0.1620],\n",
      "        [0.1621],\n",
      "        [0.1626],\n",
      "        [0.1737],\n",
      "        [0.1757],\n",
      "        [0.1796]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0154],\n",
      "        [0.0058],\n",
      "        [0.0228],\n",
      "        [0.0254],\n",
      "        [0.0696],\n",
      "        [0.0340],\n",
      "        [0.0355],\n",
      "        [0.0261],\n",
      "        [0.0264],\n",
      "        [0.0128],\n",
      "        [0.0122],\n",
      "        [0.0157],\n",
      "        [0.0299],\n",
      "        [0.0102],\n",
      "        [0.0303],\n",
      "        [0.0150],\n",
      "        [0.0099],\n",
      "        [0.0007],\n",
      "        [0.0049],\n",
      "        [0.1628],\n",
      "        [0.1097],\n",
      "        [0.1174],\n",
      "        [0.1489],\n",
      "        [0.1554],\n",
      "        [0.0103],\n",
      "        [0.1574],\n",
      "        [0.0240],\n",
      "        [0.0270],\n",
      "        [0.0331],\n",
      "        [0.0966],\n",
      "        [0.0481],\n",
      "        [0.1161],\n",
      "        [0.1807],\n",
      "        [0.0642],\n",
      "        [0.0699],\n",
      "        [0.0733],\n",
      "        [0.0785],\n",
      "        [0.0825],\n",
      "        [0.0866],\n",
      "        [0.0903],\n",
      "        [0.0948],\n",
      "        [0.1007],\n",
      "        [0.1492],\n",
      "        [0.1801],\n",
      "        [0.1245],\n",
      "        [0.1328],\n",
      "        [0.1476],\n",
      "        [0.1480],\n",
      "        [0.1562],\n",
      "        [0.1597],\n",
      "        [0.1607],\n",
      "        [0.1640],\n",
      "        [0.1734],\n",
      "        [0.1780],\n",
      "        [0.1337],\n",
      "        [0.0207],\n",
      "        [0.1401],\n",
      "        [0.0232],\n",
      "        [0.1359],\n",
      "        [0.0423],\n",
      "        [0.0442],\n",
      "        [0.0550],\n",
      "        [0.0640],\n",
      "        [0.1010],\n",
      "        [0.1114],\n",
      "        [0.0163],\n",
      "        [0.0162],\n",
      "        [0.0257],\n",
      "        [0.0027],\n",
      "        [0.1155],\n",
      "        [0.1426],\n",
      "        [0.1476],\n",
      "        [0.1754],\n",
      "        [0.1763],\n",
      "        [0.1888],\n",
      "        [0.1907],\n",
      "        [0.1915],\n",
      "        [0.2027],\n",
      "        [0.1860],\n",
      "        [0.2123],\n",
      "        [0.1977],\n",
      "        [0.1701],\n",
      "        [0.2121],\n",
      "        [0.2286],\n",
      "        [0.2305],\n",
      "        [0.0732],\n",
      "        [0.1399],\n",
      "        [0.0738],\n",
      "        [0.0587],\n",
      "        [0.1227],\n",
      "        [0.1286],\n",
      "        [0.1300],\n",
      "        [0.1473],\n",
      "        [0.1602],\n",
      "        [0.1603],\n",
      "        [0.1608],\n",
      "        [0.1719],\n",
      "        [0.1739],\n",
      "        [0.1778]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 20.4650981426239\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 100\n",
      "剩餘X 資料 torch.Size([60, 18])\n",
      "剩餘Y 資料 torch.Size([60, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.031733471900224686, 26)\n",
      "The second_loss value of k: (0.03345886990427971, 27)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引26，y= tensor([0.6412])\n",
      "目前模型的Data狀態 torch.Size([100, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193],\n",
      "        [0.8193]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0154],\n",
      "        [0.0058],\n",
      "        [0.0228],\n",
      "        [0.0254],\n",
      "        [0.0696],\n",
      "        [0.0340],\n",
      "        [0.0355],\n",
      "        [0.0261],\n",
      "        [0.0264],\n",
      "        [0.0128],\n",
      "        [0.0122],\n",
      "        [0.0157],\n",
      "        [0.0299],\n",
      "        [0.0102],\n",
      "        [0.0303],\n",
      "        [0.0150],\n",
      "        [0.0099],\n",
      "        [0.0007],\n",
      "        [0.0049],\n",
      "        [0.1628],\n",
      "        [0.1097],\n",
      "        [0.1174],\n",
      "        [0.1489],\n",
      "        [0.1554],\n",
      "        [0.0103],\n",
      "        [0.1574],\n",
      "        [0.0240],\n",
      "        [0.0270],\n",
      "        [0.0331],\n",
      "        [0.0966],\n",
      "        [0.0481],\n",
      "        [0.1161],\n",
      "        [0.1807],\n",
      "        [0.0642],\n",
      "        [0.0699],\n",
      "        [0.0733],\n",
      "        [0.0785],\n",
      "        [0.0825],\n",
      "        [0.0866],\n",
      "        [0.0903],\n",
      "        [0.0948],\n",
      "        [0.1007],\n",
      "        [0.1492],\n",
      "        [0.1801],\n",
      "        [0.1245],\n",
      "        [0.1328],\n",
      "        [0.1476],\n",
      "        [0.1480],\n",
      "        [0.1562],\n",
      "        [0.1597],\n",
      "        [0.1607],\n",
      "        [0.1640],\n",
      "        [0.1734],\n",
      "        [0.1780],\n",
      "        [0.1337],\n",
      "        [0.0207],\n",
      "        [0.1401],\n",
      "        [0.0232],\n",
      "        [0.1359],\n",
      "        [0.0423],\n",
      "        [0.0442],\n",
      "        [0.0550],\n",
      "        [0.0640],\n",
      "        [0.1010],\n",
      "        [0.1114],\n",
      "        [0.0163],\n",
      "        [0.0162],\n",
      "        [0.0257],\n",
      "        [0.0027],\n",
      "        [0.1155],\n",
      "        [0.1426],\n",
      "        [0.1476],\n",
      "        [0.1754],\n",
      "        [0.1763],\n",
      "        [0.1888],\n",
      "        [0.1907],\n",
      "        [0.1915],\n",
      "        [0.2027],\n",
      "        [0.1860],\n",
      "        [0.2123],\n",
      "        [0.1977],\n",
      "        [0.1701],\n",
      "        [0.2121],\n",
      "        [0.2286],\n",
      "        [0.2305],\n",
      "        [0.0732],\n",
      "        [0.1399],\n",
      "        [0.0738],\n",
      "        [0.0587],\n",
      "        [0.1227],\n",
      "        [0.1286],\n",
      "        [0.1300],\n",
      "        [0.1473],\n",
      "        [0.1602],\n",
      "        [0.1603],\n",
      "        [0.1608],\n",
      "        [0.1719],\n",
      "        [0.1739],\n",
      "        [0.1778],\n",
      "        [0.1781]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 71\n",
      "Number of shrink: 29\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0136],\n",
      "        [0.0040],\n",
      "        [0.0246],\n",
      "        [0.0272],\n",
      "        [0.0714],\n",
      "        [0.0358],\n",
      "        [0.0372],\n",
      "        [0.0279],\n",
      "        [0.0246],\n",
      "        [0.0110],\n",
      "        [0.0140],\n",
      "        [0.0139],\n",
      "        [0.0317],\n",
      "        [0.0120],\n",
      "        [0.0321],\n",
      "        [0.0132],\n",
      "        [0.0081],\n",
      "        [0.0025],\n",
      "        [0.0031],\n",
      "        [0.1646],\n",
      "        [0.1115],\n",
      "        [0.1191],\n",
      "        [0.1507],\n",
      "        [0.1572],\n",
      "        [0.0121],\n",
      "        [0.1592],\n",
      "        [0.0258],\n",
      "        [0.0287],\n",
      "        [0.0349],\n",
      "        [0.0984],\n",
      "        [0.0499],\n",
      "        [0.1179],\n",
      "        [0.1825],\n",
      "        [0.0660],\n",
      "        [0.0717],\n",
      "        [0.0751],\n",
      "        [0.0803],\n",
      "        [0.0843],\n",
      "        [0.0884],\n",
      "        [0.0921],\n",
      "        [0.0966],\n",
      "        [0.1025],\n",
      "        [0.1510],\n",
      "        [0.1819],\n",
      "        [0.1263],\n",
      "        [0.1346],\n",
      "        [0.1494],\n",
      "        [0.1498],\n",
      "        [0.1580],\n",
      "        [0.1615],\n",
      "        [0.1625],\n",
      "        [0.1658],\n",
      "        [0.1752],\n",
      "        [0.1797],\n",
      "        [0.1355],\n",
      "        [0.0225],\n",
      "        [0.1419],\n",
      "        [0.0250],\n",
      "        [0.1377],\n",
      "        [0.0441],\n",
      "        [0.0460],\n",
      "        [0.0568],\n",
      "        [0.0658],\n",
      "        [0.1028],\n",
      "        [0.1096],\n",
      "        [0.0181],\n",
      "        [0.0180],\n",
      "        [0.0239],\n",
      "        [0.0009],\n",
      "        [0.1137],\n",
      "        [0.1408],\n",
      "        [0.1458],\n",
      "        [0.1736],\n",
      "        [0.1745],\n",
      "        [0.1870],\n",
      "        [0.1889],\n",
      "        [0.1897],\n",
      "        [0.2009],\n",
      "        [0.1842],\n",
      "        [0.2105],\n",
      "        [0.1959],\n",
      "        [0.1683],\n",
      "        [0.2103],\n",
      "        [0.2268],\n",
      "        [0.2287],\n",
      "        [0.0714],\n",
      "        [0.1381],\n",
      "        [0.0720],\n",
      "        [0.0569],\n",
      "        [0.1209],\n",
      "        [0.1268],\n",
      "        [0.1283],\n",
      "        [0.1455],\n",
      "        [0.1584],\n",
      "        [0.1585],\n",
      "        [0.1590],\n",
      "        [0.1701],\n",
      "        [0.1721],\n",
      "        [0.1760],\n",
      "        [0.1764]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 20.709432125091553\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 101\n",
      "剩餘X 資料 torch.Size([59, 18])\n",
      "剩餘Y 資料 torch.Size([59, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03280792385339737, 26)\n",
      "The second_loss value of k: (0.03294995799660683, 27)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引26，y= tensor([0.6364])\n",
      "目前模型的Data狀態 torch.Size([101, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175],\n",
      "        [0.8175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0136],\n",
      "        [0.0040],\n",
      "        [0.0246],\n",
      "        [0.0272],\n",
      "        [0.0714],\n",
      "        [0.0358],\n",
      "        [0.0372],\n",
      "        [0.0279],\n",
      "        [0.0246],\n",
      "        [0.0110],\n",
      "        [0.0140],\n",
      "        [0.0139],\n",
      "        [0.0317],\n",
      "        [0.0120],\n",
      "        [0.0321],\n",
      "        [0.0132],\n",
      "        [0.0081],\n",
      "        [0.0025],\n",
      "        [0.0031],\n",
      "        [0.1646],\n",
      "        [0.1115],\n",
      "        [0.1191],\n",
      "        [0.1507],\n",
      "        [0.1572],\n",
      "        [0.0121],\n",
      "        [0.1592],\n",
      "        [0.0258],\n",
      "        [0.0287],\n",
      "        [0.0349],\n",
      "        [0.0984],\n",
      "        [0.0499],\n",
      "        [0.1179],\n",
      "        [0.1825],\n",
      "        [0.0660],\n",
      "        [0.0717],\n",
      "        [0.0751],\n",
      "        [0.0803],\n",
      "        [0.0843],\n",
      "        [0.0884],\n",
      "        [0.0921],\n",
      "        [0.0966],\n",
      "        [0.1025],\n",
      "        [0.1510],\n",
      "        [0.1819],\n",
      "        [0.1263],\n",
      "        [0.1346],\n",
      "        [0.1494],\n",
      "        [0.1498],\n",
      "        [0.1580],\n",
      "        [0.1615],\n",
      "        [0.1625],\n",
      "        [0.1658],\n",
      "        [0.1752],\n",
      "        [0.1797],\n",
      "        [0.1355],\n",
      "        [0.0225],\n",
      "        [0.1419],\n",
      "        [0.0250],\n",
      "        [0.1377],\n",
      "        [0.0441],\n",
      "        [0.0460],\n",
      "        [0.0568],\n",
      "        [0.0658],\n",
      "        [0.1028],\n",
      "        [0.1096],\n",
      "        [0.0181],\n",
      "        [0.0180],\n",
      "        [0.0239],\n",
      "        [0.0009],\n",
      "        [0.1137],\n",
      "        [0.1408],\n",
      "        [0.1458],\n",
      "        [0.1736],\n",
      "        [0.1745],\n",
      "        [0.1870],\n",
      "        [0.1889],\n",
      "        [0.1897],\n",
      "        [0.2009],\n",
      "        [0.1842],\n",
      "        [0.2105],\n",
      "        [0.1959],\n",
      "        [0.1683],\n",
      "        [0.2103],\n",
      "        [0.2268],\n",
      "        [0.2287],\n",
      "        [0.0714],\n",
      "        [0.1381],\n",
      "        [0.0720],\n",
      "        [0.0569],\n",
      "        [0.1209],\n",
      "        [0.1268],\n",
      "        [0.1283],\n",
      "        [0.1455],\n",
      "        [0.1584],\n",
      "        [0.1585],\n",
      "        [0.1590],\n",
      "        [0.1701],\n",
      "        [0.1721],\n",
      "        [0.1760],\n",
      "        [0.1764],\n",
      "        [0.1811]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0118],\n",
      "        [0.0022],\n",
      "        [0.0264],\n",
      "        [0.0290],\n",
      "        [0.0732],\n",
      "        [0.0376],\n",
      "        [0.0390],\n",
      "        [0.0297],\n",
      "        [0.0229],\n",
      "        [0.0092],\n",
      "        [0.0158],\n",
      "        [0.0121],\n",
      "        [0.0335],\n",
      "        [0.0138],\n",
      "        [0.0339],\n",
      "        [0.0115],\n",
      "        [0.0063],\n",
      "        [0.0043],\n",
      "        [0.0013],\n",
      "        [0.1664],\n",
      "        [0.1132],\n",
      "        [0.1209],\n",
      "        [0.1525],\n",
      "        [0.1590],\n",
      "        [0.0139],\n",
      "        [0.1610],\n",
      "        [0.0276],\n",
      "        [0.0305],\n",
      "        [0.0367],\n",
      "        [0.1002],\n",
      "        [0.0517],\n",
      "        [0.1197],\n",
      "        [0.1843],\n",
      "        [0.0677],\n",
      "        [0.0735],\n",
      "        [0.0769],\n",
      "        [0.0821],\n",
      "        [0.0861],\n",
      "        [0.0902],\n",
      "        [0.0939],\n",
      "        [0.0984],\n",
      "        [0.1043],\n",
      "        [0.1527],\n",
      "        [0.1837],\n",
      "        [0.1281],\n",
      "        [0.1364],\n",
      "        [0.1512],\n",
      "        [0.1516],\n",
      "        [0.1598],\n",
      "        [0.1633],\n",
      "        [0.1643],\n",
      "        [0.1676],\n",
      "        [0.1770],\n",
      "        [0.1815],\n",
      "        [0.1373],\n",
      "        [0.0243],\n",
      "        [0.1437],\n",
      "        [0.0268],\n",
      "        [0.1395],\n",
      "        [0.0459],\n",
      "        [0.0478],\n",
      "        [0.0586],\n",
      "        [0.0676],\n",
      "        [0.1046],\n",
      "        [0.1078],\n",
      "        [0.0199],\n",
      "        [0.0198],\n",
      "        [0.0221],\n",
      "        [0.0009],\n",
      "        [0.1119],\n",
      "        [0.1390],\n",
      "        [0.1440],\n",
      "        [0.1718],\n",
      "        [0.1728],\n",
      "        [0.1852],\n",
      "        [0.1871],\n",
      "        [0.1880],\n",
      "        [0.1992],\n",
      "        [0.1824],\n",
      "        [0.2088],\n",
      "        [0.1941],\n",
      "        [0.1665],\n",
      "        [0.2085],\n",
      "        [0.2250],\n",
      "        [0.2269],\n",
      "        [0.0696],\n",
      "        [0.1363],\n",
      "        [0.0702],\n",
      "        [0.0551],\n",
      "        [0.1191],\n",
      "        [0.1250],\n",
      "        [0.1265],\n",
      "        [0.1437],\n",
      "        [0.1566],\n",
      "        [0.1567],\n",
      "        [0.1572],\n",
      "        [0.1683],\n",
      "        [0.1703],\n",
      "        [0.1742],\n",
      "        [0.1746],\n",
      "        [0.1793]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 20.950703859329224\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 102\n",
      "剩餘X 資料 torch.Size([58, 18])\n",
      "剩餘Y 資料 torch.Size([58, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03230182081460953, 26)\n",
      "The second_loss value of k: (0.036813974380493164, 37)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引26，y= tensor([0.6360])\n",
      "目前模型的Data狀態 torch.Size([102, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157],\n",
      "        [0.8157]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0118],\n",
      "        [0.0022],\n",
      "        [0.0264],\n",
      "        [0.0290],\n",
      "        [0.0732],\n",
      "        [0.0376],\n",
      "        [0.0390],\n",
      "        [0.0297],\n",
      "        [0.0229],\n",
      "        [0.0092],\n",
      "        [0.0158],\n",
      "        [0.0121],\n",
      "        [0.0335],\n",
      "        [0.0138],\n",
      "        [0.0339],\n",
      "        [0.0115],\n",
      "        [0.0063],\n",
      "        [0.0043],\n",
      "        [0.0013],\n",
      "        [0.1664],\n",
      "        [0.1132],\n",
      "        [0.1209],\n",
      "        [0.1525],\n",
      "        [0.1590],\n",
      "        [0.0139],\n",
      "        [0.1610],\n",
      "        [0.0276],\n",
      "        [0.0305],\n",
      "        [0.0367],\n",
      "        [0.1002],\n",
      "        [0.0517],\n",
      "        [0.1197],\n",
      "        [0.1843],\n",
      "        [0.0677],\n",
      "        [0.0735],\n",
      "        [0.0769],\n",
      "        [0.0821],\n",
      "        [0.0861],\n",
      "        [0.0902],\n",
      "        [0.0939],\n",
      "        [0.0984],\n",
      "        [0.1043],\n",
      "        [0.1527],\n",
      "        [0.1837],\n",
      "        [0.1281],\n",
      "        [0.1364],\n",
      "        [0.1512],\n",
      "        [0.1516],\n",
      "        [0.1598],\n",
      "        [0.1633],\n",
      "        [0.1643],\n",
      "        [0.1676],\n",
      "        [0.1770],\n",
      "        [0.1815],\n",
      "        [0.1373],\n",
      "        [0.0243],\n",
      "        [0.1437],\n",
      "        [0.0268],\n",
      "        [0.1395],\n",
      "        [0.0459],\n",
      "        [0.0478],\n",
      "        [0.0586],\n",
      "        [0.0676],\n",
      "        [0.1046],\n",
      "        [0.1078],\n",
      "        [0.0199],\n",
      "        [0.0198],\n",
      "        [0.0221],\n",
      "        [0.0009],\n",
      "        [0.1119],\n",
      "        [0.1390],\n",
      "        [0.1440],\n",
      "        [0.1718],\n",
      "        [0.1728],\n",
      "        [0.1852],\n",
      "        [0.1871],\n",
      "        [0.1880],\n",
      "        [0.1992],\n",
      "        [0.1824],\n",
      "        [0.2088],\n",
      "        [0.1941],\n",
      "        [0.1665],\n",
      "        [0.2085],\n",
      "        [0.2250],\n",
      "        [0.2269],\n",
      "        [0.0696],\n",
      "        [0.1363],\n",
      "        [0.0702],\n",
      "        [0.0551],\n",
      "        [0.1191],\n",
      "        [0.1250],\n",
      "        [0.1265],\n",
      "        [0.1437],\n",
      "        [0.1566],\n",
      "        [0.1567],\n",
      "        [0.1572],\n",
      "        [0.1683],\n",
      "        [0.1703],\n",
      "        [0.1742],\n",
      "        [0.1746],\n",
      "        [0.1793],\n",
      "        [0.1797]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0100],\n",
      "        [0.0004],\n",
      "        [0.0282],\n",
      "        [0.0307],\n",
      "        [0.0750],\n",
      "        [0.0394],\n",
      "        [0.0408],\n",
      "        [0.0315],\n",
      "        [0.0211],\n",
      "        [0.0074],\n",
      "        [0.0176],\n",
      "        [0.0103],\n",
      "        [0.0352],\n",
      "        [0.0156],\n",
      "        [0.0356],\n",
      "        [0.0097],\n",
      "        [0.0045],\n",
      "        [0.0060],\n",
      "        [0.0005],\n",
      "        [0.1682],\n",
      "        [0.1150],\n",
      "        [0.1227],\n",
      "        [0.1542],\n",
      "        [0.1608],\n",
      "        [0.0157],\n",
      "        [0.1627],\n",
      "        [0.0293],\n",
      "        [0.0323],\n",
      "        [0.0385],\n",
      "        [0.1019],\n",
      "        [0.0535],\n",
      "        [0.1215],\n",
      "        [0.1860],\n",
      "        [0.0695],\n",
      "        [0.0753],\n",
      "        [0.0786],\n",
      "        [0.0839],\n",
      "        [0.0879],\n",
      "        [0.0920],\n",
      "        [0.0956],\n",
      "        [0.1001],\n",
      "        [0.1060],\n",
      "        [0.1545],\n",
      "        [0.1855],\n",
      "        [0.1298],\n",
      "        [0.1382],\n",
      "        [0.1530],\n",
      "        [0.1534],\n",
      "        [0.1616],\n",
      "        [0.1651],\n",
      "        [0.1661],\n",
      "        [0.1694],\n",
      "        [0.1788],\n",
      "        [0.1833],\n",
      "        [0.1390],\n",
      "        [0.0261],\n",
      "        [0.1454],\n",
      "        [0.0286],\n",
      "        [0.1413],\n",
      "        [0.0477],\n",
      "        [0.0496],\n",
      "        [0.0604],\n",
      "        [0.0694],\n",
      "        [0.1063],\n",
      "        [0.1060],\n",
      "        [0.0217],\n",
      "        [0.0215],\n",
      "        [0.0203],\n",
      "        [0.0027],\n",
      "        [0.1101],\n",
      "        [0.1372],\n",
      "        [0.1422],\n",
      "        [0.1700],\n",
      "        [0.1710],\n",
      "        [0.1834],\n",
      "        [0.1853],\n",
      "        [0.1862],\n",
      "        [0.1974],\n",
      "        [0.1806],\n",
      "        [0.2070],\n",
      "        [0.1924],\n",
      "        [0.1647],\n",
      "        [0.2067],\n",
      "        [0.2233],\n",
      "        [0.2251],\n",
      "        [0.0678],\n",
      "        [0.1345],\n",
      "        [0.0684],\n",
      "        [0.0534],\n",
      "        [0.1174],\n",
      "        [0.1232],\n",
      "        [0.1247],\n",
      "        [0.1419],\n",
      "        [0.1548],\n",
      "        [0.1550],\n",
      "        [0.1555],\n",
      "        [0.1665],\n",
      "        [0.1686],\n",
      "        [0.1724],\n",
      "        [0.1728],\n",
      "        [0.1776],\n",
      "        [0.1780]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 21.195611715316772\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 103\n",
      "剩餘X 資料 torch.Size([57, 18])\n",
      "剩餘Y 資料 torch.Size([57, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03613908216357231, 36)\n",
      "The second_loss value of k: (0.03753526136279106, 24)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引36，y= tensor([0.6239])\n",
      "目前模型的Data狀態 torch.Size([103, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140],\n",
      "        [0.8140]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0100],\n",
      "        [0.0004],\n",
      "        [0.0282],\n",
      "        [0.0307],\n",
      "        [0.0750],\n",
      "        [0.0394],\n",
      "        [0.0408],\n",
      "        [0.0315],\n",
      "        [0.0211],\n",
      "        [0.0074],\n",
      "        [0.0176],\n",
      "        [0.0103],\n",
      "        [0.0352],\n",
      "        [0.0156],\n",
      "        [0.0356],\n",
      "        [0.0097],\n",
      "        [0.0045],\n",
      "        [0.0060],\n",
      "        [0.0005],\n",
      "        [0.1682],\n",
      "        [0.1150],\n",
      "        [0.1227],\n",
      "        [0.1542],\n",
      "        [0.1608],\n",
      "        [0.0157],\n",
      "        [0.1627],\n",
      "        [0.0293],\n",
      "        [0.0323],\n",
      "        [0.0385],\n",
      "        [0.1019],\n",
      "        [0.0535],\n",
      "        [0.1215],\n",
      "        [0.1860],\n",
      "        [0.0695],\n",
      "        [0.0753],\n",
      "        [0.0786],\n",
      "        [0.0839],\n",
      "        [0.0879],\n",
      "        [0.0920],\n",
      "        [0.0956],\n",
      "        [0.1001],\n",
      "        [0.1060],\n",
      "        [0.1545],\n",
      "        [0.1855],\n",
      "        [0.1298],\n",
      "        [0.1382],\n",
      "        [0.1530],\n",
      "        [0.1534],\n",
      "        [0.1616],\n",
      "        [0.1651],\n",
      "        [0.1661],\n",
      "        [0.1694],\n",
      "        [0.1788],\n",
      "        [0.1833],\n",
      "        [0.1390],\n",
      "        [0.0261],\n",
      "        [0.1454],\n",
      "        [0.0286],\n",
      "        [0.1413],\n",
      "        [0.0477],\n",
      "        [0.0496],\n",
      "        [0.0604],\n",
      "        [0.0694],\n",
      "        [0.1063],\n",
      "        [0.1060],\n",
      "        [0.0217],\n",
      "        [0.0215],\n",
      "        [0.0203],\n",
      "        [0.0027],\n",
      "        [0.1101],\n",
      "        [0.1372],\n",
      "        [0.1422],\n",
      "        [0.1700],\n",
      "        [0.1710],\n",
      "        [0.1834],\n",
      "        [0.1853],\n",
      "        [0.1862],\n",
      "        [0.1974],\n",
      "        [0.1806],\n",
      "        [0.2070],\n",
      "        [0.1924],\n",
      "        [0.1647],\n",
      "        [0.2067],\n",
      "        [0.2233],\n",
      "        [0.2251],\n",
      "        [0.0678],\n",
      "        [0.1345],\n",
      "        [0.0684],\n",
      "        [0.0534],\n",
      "        [0.1174],\n",
      "        [0.1232],\n",
      "        [0.1247],\n",
      "        [0.1419],\n",
      "        [0.1548],\n",
      "        [0.1550],\n",
      "        [0.1555],\n",
      "        [0.1665],\n",
      "        [0.1686],\n",
      "        [0.1724],\n",
      "        [0.1728],\n",
      "        [0.1776],\n",
      "        [0.1780],\n",
      "        [0.1901]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0082],\n",
      "        [0.0014],\n",
      "        [0.0300],\n",
      "        [0.0326],\n",
      "        [0.0768],\n",
      "        [0.0412],\n",
      "        [0.0426],\n",
      "        [0.0333],\n",
      "        [0.0192],\n",
      "        [0.0056],\n",
      "        [0.0194],\n",
      "        [0.0085],\n",
      "        [0.0371],\n",
      "        [0.0174],\n",
      "        [0.0375],\n",
      "        [0.0078],\n",
      "        [0.0027],\n",
      "        [0.0079],\n",
      "        [0.0023],\n",
      "        [0.1700],\n",
      "        [0.1169],\n",
      "        [0.1245],\n",
      "        [0.1561],\n",
      "        [0.1626],\n",
      "        [0.0175],\n",
      "        [0.1646],\n",
      "        [0.0312],\n",
      "        [0.0341],\n",
      "        [0.0403],\n",
      "        [0.1038],\n",
      "        [0.0553],\n",
      "        [0.1233],\n",
      "        [0.1879],\n",
      "        [0.0714],\n",
      "        [0.0771],\n",
      "        [0.0805],\n",
      "        [0.0857],\n",
      "        [0.0897],\n",
      "        [0.0938],\n",
      "        [0.0975],\n",
      "        [0.1020],\n",
      "        [0.1079],\n",
      "        [0.1564],\n",
      "        [0.1873],\n",
      "        [0.1317],\n",
      "        [0.1400],\n",
      "        [0.1548],\n",
      "        [0.1552],\n",
      "        [0.1634],\n",
      "        [0.1669],\n",
      "        [0.1679],\n",
      "        [0.1712],\n",
      "        [0.1806],\n",
      "        [0.1851],\n",
      "        [0.1409],\n",
      "        [0.0279],\n",
      "        [0.1473],\n",
      "        [0.0304],\n",
      "        [0.1431],\n",
      "        [0.0495],\n",
      "        [0.0514],\n",
      "        [0.0622],\n",
      "        [0.0712],\n",
      "        [0.1082],\n",
      "        [0.1042],\n",
      "        [0.0235],\n",
      "        [0.0234],\n",
      "        [0.0185],\n",
      "        [0.0045],\n",
      "        [0.1083],\n",
      "        [0.1354],\n",
      "        [0.1404],\n",
      "        [0.1682],\n",
      "        [0.1691],\n",
      "        [0.1816],\n",
      "        [0.1835],\n",
      "        [0.1843],\n",
      "        [0.1955],\n",
      "        [0.1788],\n",
      "        [0.2051],\n",
      "        [0.1905],\n",
      "        [0.1629],\n",
      "        [0.2049],\n",
      "        [0.2214],\n",
      "        [0.2233],\n",
      "        [0.0660],\n",
      "        [0.1327],\n",
      "        [0.0666],\n",
      "        [0.0515],\n",
      "        [0.1155],\n",
      "        [0.1214],\n",
      "        [0.1228],\n",
      "        [0.1401],\n",
      "        [0.1530],\n",
      "        [0.1531],\n",
      "        [0.1536],\n",
      "        [0.1647],\n",
      "        [0.1667],\n",
      "        [0.1706],\n",
      "        [0.1709],\n",
      "        [0.1757],\n",
      "        [0.1761],\n",
      "        [0.1883]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 21.44062113761902\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 104\n",
      "剩餘X 資料 torch.Size([56, 18])\n",
      "剩餘Y 資料 torch.Size([56, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.036825068295001984, 24)\n",
      "The second_loss value of k: (0.037216804921627045, 25)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引24，y= tensor([0.6597])\n",
      "目前模型的Data狀態 torch.Size([104, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8121],\n",
      "        [0.8516]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0082],\n",
      "        [0.0014],\n",
      "        [0.0300],\n",
      "        [0.0326],\n",
      "        [0.0768],\n",
      "        [0.0412],\n",
      "        [0.0426],\n",
      "        [0.0333],\n",
      "        [0.0192],\n",
      "        [0.0056],\n",
      "        [0.0194],\n",
      "        [0.0085],\n",
      "        [0.0371],\n",
      "        [0.0174],\n",
      "        [0.0375],\n",
      "        [0.0078],\n",
      "        [0.0027],\n",
      "        [0.0079],\n",
      "        [0.0023],\n",
      "        [0.1700],\n",
      "        [0.1169],\n",
      "        [0.1245],\n",
      "        [0.1561],\n",
      "        [0.1626],\n",
      "        [0.0175],\n",
      "        [0.1646],\n",
      "        [0.0312],\n",
      "        [0.0341],\n",
      "        [0.0403],\n",
      "        [0.1038],\n",
      "        [0.0553],\n",
      "        [0.1233],\n",
      "        [0.1879],\n",
      "        [0.0714],\n",
      "        [0.0771],\n",
      "        [0.0805],\n",
      "        [0.0857],\n",
      "        [0.0897],\n",
      "        [0.0938],\n",
      "        [0.0975],\n",
      "        [0.1020],\n",
      "        [0.1079],\n",
      "        [0.1564],\n",
      "        [0.1873],\n",
      "        [0.1317],\n",
      "        [0.1400],\n",
      "        [0.1548],\n",
      "        [0.1552],\n",
      "        [0.1634],\n",
      "        [0.1669],\n",
      "        [0.1679],\n",
      "        [0.1712],\n",
      "        [0.1806],\n",
      "        [0.1851],\n",
      "        [0.1409],\n",
      "        [0.0279],\n",
      "        [0.1473],\n",
      "        [0.0304],\n",
      "        [0.1431],\n",
      "        [0.0495],\n",
      "        [0.0514],\n",
      "        [0.0622],\n",
      "        [0.0712],\n",
      "        [0.1082],\n",
      "        [0.1042],\n",
      "        [0.0235],\n",
      "        [0.0234],\n",
      "        [0.0185],\n",
      "        [0.0045],\n",
      "        [0.1083],\n",
      "        [0.1354],\n",
      "        [0.1404],\n",
      "        [0.1682],\n",
      "        [0.1691],\n",
      "        [0.1816],\n",
      "        [0.1835],\n",
      "        [0.1843],\n",
      "        [0.1955],\n",
      "        [0.1788],\n",
      "        [0.2051],\n",
      "        [0.1905],\n",
      "        [0.1629],\n",
      "        [0.2049],\n",
      "        [0.2214],\n",
      "        [0.2233],\n",
      "        [0.0660],\n",
      "        [0.1327],\n",
      "        [0.0666],\n",
      "        [0.0515],\n",
      "        [0.1155],\n",
      "        [0.1214],\n",
      "        [0.1228],\n",
      "        [0.1401],\n",
      "        [0.1530],\n",
      "        [0.1531],\n",
      "        [0.1536],\n",
      "        [0.1647],\n",
      "        [0.1667],\n",
      "        [0.1706],\n",
      "        [0.1709],\n",
      "        [0.1757],\n",
      "        [0.1761],\n",
      "        [0.1883],\n",
      "        [0.1919]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0067],\n",
      "        [0.0029],\n",
      "        [0.0315],\n",
      "        [0.0340],\n",
      "        [0.0783],\n",
      "        [0.0427],\n",
      "        [0.0441],\n",
      "        [0.0348],\n",
      "        [0.0178],\n",
      "        [0.0041],\n",
      "        [0.0209],\n",
      "        [0.0070],\n",
      "        [0.0386],\n",
      "        [0.0189],\n",
      "        [0.0389],\n",
      "        [0.0064],\n",
      "        [0.0012],\n",
      "        [0.0093],\n",
      "        [0.0038],\n",
      "        [0.1715],\n",
      "        [0.1183],\n",
      "        [0.1260],\n",
      "        [0.1575],\n",
      "        [0.1641],\n",
      "        [0.0190],\n",
      "        [0.1661],\n",
      "        [0.0326],\n",
      "        [0.0356],\n",
      "        [0.0418],\n",
      "        [0.1053],\n",
      "        [0.0568],\n",
      "        [0.1248],\n",
      "        [0.1894],\n",
      "        [0.0728],\n",
      "        [0.0786],\n",
      "        [0.0819],\n",
      "        [0.0872],\n",
      "        [0.0912],\n",
      "        [0.0953],\n",
      "        [0.0990],\n",
      "        [0.1035],\n",
      "        [0.1093],\n",
      "        [0.1578],\n",
      "        [0.1888],\n",
      "        [0.1331],\n",
      "        [0.1415],\n",
      "        [0.1563],\n",
      "        [0.1567],\n",
      "        [0.1649],\n",
      "        [0.1684],\n",
      "        [0.1694],\n",
      "        [0.1727],\n",
      "        [0.1821],\n",
      "        [0.1866],\n",
      "        [0.1424],\n",
      "        [0.0294],\n",
      "        [0.1487],\n",
      "        [0.0319],\n",
      "        [0.1446],\n",
      "        [0.0510],\n",
      "        [0.0529],\n",
      "        [0.0637],\n",
      "        [0.0727],\n",
      "        [0.1096],\n",
      "        [0.1027],\n",
      "        [0.0250],\n",
      "        [0.0248],\n",
      "        [0.0170],\n",
      "        [0.0060],\n",
      "        [0.1068],\n",
      "        [0.1339],\n",
      "        [0.1389],\n",
      "        [0.1667],\n",
      "        [0.1677],\n",
      "        [0.1801],\n",
      "        [0.1820],\n",
      "        [0.1829],\n",
      "        [0.1941],\n",
      "        [0.1773],\n",
      "        [0.2037],\n",
      "        [0.1891],\n",
      "        [0.1614],\n",
      "        [0.2034],\n",
      "        [0.2200],\n",
      "        [0.2218],\n",
      "        [0.0645],\n",
      "        [0.1312],\n",
      "        [0.0651],\n",
      "        [0.0500],\n",
      "        [0.1141],\n",
      "        [0.1199],\n",
      "        [0.1214],\n",
      "        [0.1386],\n",
      "        [0.1515],\n",
      "        [0.1517],\n",
      "        [0.1522],\n",
      "        [0.1632],\n",
      "        [0.1653],\n",
      "        [0.1691],\n",
      "        [0.1695],\n",
      "        [0.1743],\n",
      "        [0.1746],\n",
      "        [0.1868],\n",
      "        [0.1509]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 21.684929132461548\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 105\n",
      "剩餘X 資料 torch.Size([55, 18])\n",
      "剩餘Y 資料 torch.Size([55, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.023933669552206993, 0)\n",
      "The second_loss value of k: (0.030049387365579605, 23)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.7463])\n",
      "目前模型的Data狀態 torch.Size([105, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.8106],\n",
      "        [0.9010]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0067],\n",
      "        [0.0029],\n",
      "        [0.0315],\n",
      "        [0.0340],\n",
      "        [0.0783],\n",
      "        [0.0427],\n",
      "        [0.0441],\n",
      "        [0.0348],\n",
      "        [0.0178],\n",
      "        [0.0041],\n",
      "        [0.0209],\n",
      "        [0.0070],\n",
      "        [0.0386],\n",
      "        [0.0189],\n",
      "        [0.0389],\n",
      "        [0.0064],\n",
      "        [0.0012],\n",
      "        [0.0093],\n",
      "        [0.0038],\n",
      "        [0.1715],\n",
      "        [0.1183],\n",
      "        [0.1260],\n",
      "        [0.1575],\n",
      "        [0.1641],\n",
      "        [0.0190],\n",
      "        [0.1661],\n",
      "        [0.0326],\n",
      "        [0.0356],\n",
      "        [0.0418],\n",
      "        [0.1053],\n",
      "        [0.0568],\n",
      "        [0.1248],\n",
      "        [0.1894],\n",
      "        [0.0728],\n",
      "        [0.0786],\n",
      "        [0.0819],\n",
      "        [0.0872],\n",
      "        [0.0912],\n",
      "        [0.0953],\n",
      "        [0.0990],\n",
      "        [0.1035],\n",
      "        [0.1093],\n",
      "        [0.1578],\n",
      "        [0.1888],\n",
      "        [0.1331],\n",
      "        [0.1415],\n",
      "        [0.1563],\n",
      "        [0.1567],\n",
      "        [0.1649],\n",
      "        [0.1684],\n",
      "        [0.1694],\n",
      "        [0.1727],\n",
      "        [0.1821],\n",
      "        [0.1866],\n",
      "        [0.1424],\n",
      "        [0.0294],\n",
      "        [0.1487],\n",
      "        [0.0319],\n",
      "        [0.1446],\n",
      "        [0.0510],\n",
      "        [0.0529],\n",
      "        [0.0637],\n",
      "        [0.0727],\n",
      "        [0.1096],\n",
      "        [0.1027],\n",
      "        [0.0250],\n",
      "        [0.0248],\n",
      "        [0.0170],\n",
      "        [0.0060],\n",
      "        [0.1068],\n",
      "        [0.1339],\n",
      "        [0.1389],\n",
      "        [0.1667],\n",
      "        [0.1677],\n",
      "        [0.1801],\n",
      "        [0.1820],\n",
      "        [0.1829],\n",
      "        [0.1941],\n",
      "        [0.1773],\n",
      "        [0.2037],\n",
      "        [0.1891],\n",
      "        [0.1614],\n",
      "        [0.2034],\n",
      "        [0.2200],\n",
      "        [0.2218],\n",
      "        [0.0645],\n",
      "        [0.1312],\n",
      "        [0.0651],\n",
      "        [0.0500],\n",
      "        [0.1141],\n",
      "        [0.1199],\n",
      "        [0.1214],\n",
      "        [0.1386],\n",
      "        [0.1515],\n",
      "        [0.1517],\n",
      "        [0.1522],\n",
      "        [0.1632],\n",
      "        [0.1653],\n",
      "        [0.1691],\n",
      "        [0.1695],\n",
      "        [0.1743],\n",
      "        [0.1746],\n",
      "        [0.1868],\n",
      "        [0.1509],\n",
      "        [0.1547]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0061],\n",
      "        [0.0035],\n",
      "        [0.0321],\n",
      "        [0.0347],\n",
      "        [0.0789],\n",
      "        [0.0433],\n",
      "        [0.0447],\n",
      "        [0.0354],\n",
      "        [0.0172],\n",
      "        [0.0035],\n",
      "        [0.0215],\n",
      "        [0.0064],\n",
      "        [0.0392],\n",
      "        [0.0195],\n",
      "        [0.0395],\n",
      "        [0.0058],\n",
      "        [0.0006],\n",
      "        [0.0099],\n",
      "        [0.0044],\n",
      "        [0.1721],\n",
      "        [0.1189],\n",
      "        [0.1266],\n",
      "        [0.1581],\n",
      "        [0.1647],\n",
      "        [0.0196],\n",
      "        [0.1667],\n",
      "        [0.0332],\n",
      "        [0.0362],\n",
      "        [0.0424],\n",
      "        [0.1059],\n",
      "        [0.0574],\n",
      "        [0.1254],\n",
      "        [0.1900],\n",
      "        [0.0734],\n",
      "        [0.0792],\n",
      "        [0.0825],\n",
      "        [0.0878],\n",
      "        [0.0918],\n",
      "        [0.0959],\n",
      "        [0.0996],\n",
      "        [0.1041],\n",
      "        [0.1099],\n",
      "        [0.1584],\n",
      "        [0.1894],\n",
      "        [0.1338],\n",
      "        [0.1421],\n",
      "        [0.1569],\n",
      "        [0.1573],\n",
      "        [0.1655],\n",
      "        [0.1690],\n",
      "        [0.1700],\n",
      "        [0.1733],\n",
      "        [0.1827],\n",
      "        [0.1872],\n",
      "        [0.1430],\n",
      "        [0.0300],\n",
      "        [0.1493],\n",
      "        [0.0325],\n",
      "        [0.1452],\n",
      "        [0.0516],\n",
      "        [0.0535],\n",
      "        [0.0643],\n",
      "        [0.0733],\n",
      "        [0.1103],\n",
      "        [0.1021],\n",
      "        [0.0256],\n",
      "        [0.0254],\n",
      "        [0.0164],\n",
      "        [0.0066],\n",
      "        [0.1062],\n",
      "        [0.1333],\n",
      "        [0.1383],\n",
      "        [0.1661],\n",
      "        [0.1671],\n",
      "        [0.1795],\n",
      "        [0.1814],\n",
      "        [0.1823],\n",
      "        [0.1935],\n",
      "        [0.1767],\n",
      "        [0.2031],\n",
      "        [0.1885],\n",
      "        [0.1608],\n",
      "        [0.2028],\n",
      "        [0.2194],\n",
      "        [0.2212],\n",
      "        [0.0639],\n",
      "        [0.1306],\n",
      "        [0.0645],\n",
      "        [0.0494],\n",
      "        [0.1134],\n",
      "        [0.1193],\n",
      "        [0.1208],\n",
      "        [0.1380],\n",
      "        [0.1509],\n",
      "        [0.1511],\n",
      "        [0.1516],\n",
      "        [0.1626],\n",
      "        [0.1647],\n",
      "        [0.1685],\n",
      "        [0.1689],\n",
      "        [0.1737],\n",
      "        [0.1740],\n",
      "        [0.1862],\n",
      "        [0.1503],\n",
      "        [0.0637]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 21.927109479904175\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 106\n",
      "剩餘X 資料 torch.Size([54, 18])\n",
      "剩餘Y 資料 torch.Size([54, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.01322829257696867, 20)\n",
      "The second_loss value of k: (0.015760475769639015, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引20，y= tensor([0.6950])\n",
      "目前模型的Data狀態 torch.Size([106, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100],\n",
      "        [0.8100]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0061],\n",
      "        [0.0035],\n",
      "        [0.0321],\n",
      "        [0.0347],\n",
      "        [0.0789],\n",
      "        [0.0433],\n",
      "        [0.0447],\n",
      "        [0.0354],\n",
      "        [0.0172],\n",
      "        [0.0035],\n",
      "        [0.0215],\n",
      "        [0.0064],\n",
      "        [0.0392],\n",
      "        [0.0195],\n",
      "        [0.0395],\n",
      "        [0.0058],\n",
      "        [0.0006],\n",
      "        [0.0099],\n",
      "        [0.0044],\n",
      "        [0.1721],\n",
      "        [0.1189],\n",
      "        [0.1266],\n",
      "        [0.1581],\n",
      "        [0.1647],\n",
      "        [0.0196],\n",
      "        [0.1667],\n",
      "        [0.0332],\n",
      "        [0.0362],\n",
      "        [0.0424],\n",
      "        [0.1059],\n",
      "        [0.0574],\n",
      "        [0.1254],\n",
      "        [0.1900],\n",
      "        [0.0734],\n",
      "        [0.0792],\n",
      "        [0.0825],\n",
      "        [0.0878],\n",
      "        [0.0918],\n",
      "        [0.0959],\n",
      "        [0.0996],\n",
      "        [0.1041],\n",
      "        [0.1099],\n",
      "        [0.1584],\n",
      "        [0.1894],\n",
      "        [0.1338],\n",
      "        [0.1421],\n",
      "        [0.1569],\n",
      "        [0.1573],\n",
      "        [0.1655],\n",
      "        [0.1690],\n",
      "        [0.1700],\n",
      "        [0.1733],\n",
      "        [0.1827],\n",
      "        [0.1872],\n",
      "        [0.1430],\n",
      "        [0.0300],\n",
      "        [0.1493],\n",
      "        [0.0325],\n",
      "        [0.1452],\n",
      "        [0.0516],\n",
      "        [0.0535],\n",
      "        [0.0643],\n",
      "        [0.0733],\n",
      "        [0.1103],\n",
      "        [0.1021],\n",
      "        [0.0256],\n",
      "        [0.0254],\n",
      "        [0.0164],\n",
      "        [0.0066],\n",
      "        [0.1062],\n",
      "        [0.1333],\n",
      "        [0.1383],\n",
      "        [0.1661],\n",
      "        [0.1671],\n",
      "        [0.1795],\n",
      "        [0.1814],\n",
      "        [0.1823],\n",
      "        [0.1935],\n",
      "        [0.1767],\n",
      "        [0.2031],\n",
      "        [0.1885],\n",
      "        [0.1608],\n",
      "        [0.2028],\n",
      "        [0.2194],\n",
      "        [0.2212],\n",
      "        [0.0639],\n",
      "        [0.1306],\n",
      "        [0.0645],\n",
      "        [0.0494],\n",
      "        [0.1134],\n",
      "        [0.1193],\n",
      "        [0.1208],\n",
      "        [0.1380],\n",
      "        [0.1509],\n",
      "        [0.1511],\n",
      "        [0.1516],\n",
      "        [0.1626],\n",
      "        [0.1647],\n",
      "        [0.1685],\n",
      "        [0.1689],\n",
      "        [0.1737],\n",
      "        [0.1740],\n",
      "        [0.1862],\n",
      "        [0.1503],\n",
      "        [0.0637],\n",
      "        [0.1150]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0050],\n",
      "        [0.0046],\n",
      "        [0.0332],\n",
      "        [0.0357],\n",
      "        [0.0800],\n",
      "        [0.0444],\n",
      "        [0.0458],\n",
      "        [0.0365],\n",
      "        [0.0161],\n",
      "        [0.0024],\n",
      "        [0.0226],\n",
      "        [0.0053],\n",
      "        [0.0402],\n",
      "        [0.0206],\n",
      "        [0.0406],\n",
      "        [0.0047],\n",
      "        [0.0005],\n",
      "        [0.0110],\n",
      "        [0.0055],\n",
      "        [0.1732],\n",
      "        [0.1200],\n",
      "        [0.1277],\n",
      "        [0.1592],\n",
      "        [0.1658],\n",
      "        [0.0207],\n",
      "        [0.1677],\n",
      "        [0.0343],\n",
      "        [0.0373],\n",
      "        [0.0435],\n",
      "        [0.1069],\n",
      "        [0.0585],\n",
      "        [0.1265],\n",
      "        [0.1910],\n",
      "        [0.0745],\n",
      "        [0.0803],\n",
      "        [0.0836],\n",
      "        [0.0889],\n",
      "        [0.0929],\n",
      "        [0.0970],\n",
      "        [0.1006],\n",
      "        [0.1051],\n",
      "        [0.1110],\n",
      "        [0.1595],\n",
      "        [0.1905],\n",
      "        [0.1348],\n",
      "        [0.1432],\n",
      "        [0.1580],\n",
      "        [0.1584],\n",
      "        [0.1666],\n",
      "        [0.1701],\n",
      "        [0.1711],\n",
      "        [0.1744],\n",
      "        [0.1838],\n",
      "        [0.1883],\n",
      "        [0.1440],\n",
      "        [0.0311],\n",
      "        [0.1504],\n",
      "        [0.0336],\n",
      "        [0.1463],\n",
      "        [0.0527],\n",
      "        [0.0546],\n",
      "        [0.0654],\n",
      "        [0.0744],\n",
      "        [0.1113],\n",
      "        [0.1010],\n",
      "        [0.0267],\n",
      "        [0.0265],\n",
      "        [0.0153],\n",
      "        [0.0077],\n",
      "        [0.1051],\n",
      "        [0.1322],\n",
      "        [0.1372],\n",
      "        [0.1650],\n",
      "        [0.1660],\n",
      "        [0.1784],\n",
      "        [0.1803],\n",
      "        [0.1812],\n",
      "        [0.1924],\n",
      "        [0.1756],\n",
      "        [0.2020],\n",
      "        [0.1874],\n",
      "        [0.1597],\n",
      "        [0.2017],\n",
      "        [0.2183],\n",
      "        [0.2201],\n",
      "        [0.0628],\n",
      "        [0.1295],\n",
      "        [0.0634],\n",
      "        [0.0484],\n",
      "        [0.1124],\n",
      "        [0.1182],\n",
      "        [0.1197],\n",
      "        [0.1369],\n",
      "        [0.1498],\n",
      "        [0.1500],\n",
      "        [0.1505],\n",
      "        [0.1615],\n",
      "        [0.1636],\n",
      "        [0.1674],\n",
      "        [0.1678],\n",
      "        [0.1726],\n",
      "        [0.1730],\n",
      "        [0.1851],\n",
      "        [0.1492],\n",
      "        [0.0627],\n",
      "        [0.1139]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 22.170425176620483\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 107\n",
      "剩餘X 資料 torch.Size([53, 18])\n",
      "剩餘Y 資料 torch.Size([53, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.015489085577428341, 19)\n",
      "The second_loss value of k: (0.015726521611213684, 20)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引19，y= tensor([0.7050])\n",
      "目前模型的Data狀態 torch.Size([107, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8090],\n",
      "        [0.8295]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0050],\n",
      "        [0.0046],\n",
      "        [0.0332],\n",
      "        [0.0357],\n",
      "        [0.0800],\n",
      "        [0.0444],\n",
      "        [0.0458],\n",
      "        [0.0365],\n",
      "        [0.0161],\n",
      "        [0.0024],\n",
      "        [0.0226],\n",
      "        [0.0053],\n",
      "        [0.0402],\n",
      "        [0.0206],\n",
      "        [0.0406],\n",
      "        [0.0047],\n",
      "        [0.0005],\n",
      "        [0.0110],\n",
      "        [0.0055],\n",
      "        [0.1732],\n",
      "        [0.1200],\n",
      "        [0.1277],\n",
      "        [0.1592],\n",
      "        [0.1658],\n",
      "        [0.0207],\n",
      "        [0.1677],\n",
      "        [0.0343],\n",
      "        [0.0373],\n",
      "        [0.0435],\n",
      "        [0.1069],\n",
      "        [0.0585],\n",
      "        [0.1265],\n",
      "        [0.1910],\n",
      "        [0.0745],\n",
      "        [0.0803],\n",
      "        [0.0836],\n",
      "        [0.0889],\n",
      "        [0.0929],\n",
      "        [0.0970],\n",
      "        [0.1006],\n",
      "        [0.1051],\n",
      "        [0.1110],\n",
      "        [0.1595],\n",
      "        [0.1905],\n",
      "        [0.1348],\n",
      "        [0.1432],\n",
      "        [0.1580],\n",
      "        [0.1584],\n",
      "        [0.1666],\n",
      "        [0.1701],\n",
      "        [0.1711],\n",
      "        [0.1744],\n",
      "        [0.1838],\n",
      "        [0.1883],\n",
      "        [0.1440],\n",
      "        [0.0311],\n",
      "        [0.1504],\n",
      "        [0.0336],\n",
      "        [0.1463],\n",
      "        [0.0527],\n",
      "        [0.0546],\n",
      "        [0.0654],\n",
      "        [0.0744],\n",
      "        [0.1113],\n",
      "        [0.1010],\n",
      "        [0.0267],\n",
      "        [0.0265],\n",
      "        [0.0153],\n",
      "        [0.0077],\n",
      "        [0.1051],\n",
      "        [0.1322],\n",
      "        [0.1372],\n",
      "        [0.1650],\n",
      "        [0.1660],\n",
      "        [0.1784],\n",
      "        [0.1803],\n",
      "        [0.1812],\n",
      "        [0.1924],\n",
      "        [0.1756],\n",
      "        [0.2020],\n",
      "        [0.1874],\n",
      "        [0.1597],\n",
      "        [0.2017],\n",
      "        [0.2183],\n",
      "        [0.2201],\n",
      "        [0.0628],\n",
      "        [0.1295],\n",
      "        [0.0634],\n",
      "        [0.0484],\n",
      "        [0.1124],\n",
      "        [0.1182],\n",
      "        [0.1197],\n",
      "        [0.1369],\n",
      "        [0.1498],\n",
      "        [0.1500],\n",
      "        [0.1505],\n",
      "        [0.1615],\n",
      "        [0.1636],\n",
      "        [0.1674],\n",
      "        [0.1678],\n",
      "        [0.1726],\n",
      "        [0.1730],\n",
      "        [0.1851],\n",
      "        [0.1492],\n",
      "        [0.0627],\n",
      "        [0.1139],\n",
      "        [0.1245]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0040],\n",
      "        [0.0056],\n",
      "        [0.0342],\n",
      "        [0.0367],\n",
      "        [0.0810],\n",
      "        [0.0454],\n",
      "        [0.0468],\n",
      "        [0.0375],\n",
      "        [0.0151],\n",
      "        [0.0014],\n",
      "        [0.0236],\n",
      "        [0.0043],\n",
      "        [0.0412],\n",
      "        [0.0216],\n",
      "        [0.0416],\n",
      "        [0.0037],\n",
      "        [0.0015],\n",
      "        [0.0120],\n",
      "        [0.0065],\n",
      "        [0.1741],\n",
      "        [0.1210],\n",
      "        [0.1287],\n",
      "        [0.1602],\n",
      "        [0.1668],\n",
      "        [0.0217],\n",
      "        [0.1687],\n",
      "        [0.0353],\n",
      "        [0.0383],\n",
      "        [0.0444],\n",
      "        [0.1079],\n",
      "        [0.0594],\n",
      "        [0.1275],\n",
      "        [0.1920],\n",
      "        [0.0755],\n",
      "        [0.0813],\n",
      "        [0.0846],\n",
      "        [0.0899],\n",
      "        [0.0939],\n",
      "        [0.0979],\n",
      "        [0.1016],\n",
      "        [0.1061],\n",
      "        [0.1120],\n",
      "        [0.1605],\n",
      "        [0.1914],\n",
      "        [0.1358],\n",
      "        [0.1442],\n",
      "        [0.1590],\n",
      "        [0.1594],\n",
      "        [0.1675],\n",
      "        [0.1711],\n",
      "        [0.1720],\n",
      "        [0.1754],\n",
      "        [0.1848],\n",
      "        [0.1893],\n",
      "        [0.1450],\n",
      "        [0.0321],\n",
      "        [0.1514],\n",
      "        [0.0346],\n",
      "        [0.1473],\n",
      "        [0.0537],\n",
      "        [0.0556],\n",
      "        [0.0664],\n",
      "        [0.0753],\n",
      "        [0.1123],\n",
      "        [0.1001],\n",
      "        [0.0277],\n",
      "        [0.0275],\n",
      "        [0.0144],\n",
      "        [0.0087],\n",
      "        [0.1041],\n",
      "        [0.1312],\n",
      "        [0.1363],\n",
      "        [0.1640],\n",
      "        [0.1650],\n",
      "        [0.1774],\n",
      "        [0.1793],\n",
      "        [0.1802],\n",
      "        [0.1914],\n",
      "        [0.1746],\n",
      "        [0.2010],\n",
      "        [0.1864],\n",
      "        [0.1587],\n",
      "        [0.2007],\n",
      "        [0.2173],\n",
      "        [0.2191],\n",
      "        [0.0618],\n",
      "        [0.1285],\n",
      "        [0.0625],\n",
      "        [0.0474],\n",
      "        [0.1114],\n",
      "        [0.1173],\n",
      "        [0.1187],\n",
      "        [0.1359],\n",
      "        [0.1488],\n",
      "        [0.1490],\n",
      "        [0.1495],\n",
      "        [0.1605],\n",
      "        [0.1626],\n",
      "        [0.1664],\n",
      "        [0.1668],\n",
      "        [0.1716],\n",
      "        [0.1720],\n",
      "        [0.1841],\n",
      "        [0.1482],\n",
      "        [0.0617],\n",
      "        [0.1129],\n",
      "        [0.1030]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 22.413607835769653\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 108\n",
      "剩餘X 資料 torch.Size([52, 18])\n",
      "剩餘Y 資料 torch.Size([52, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.014861838892102242, 18)\n",
      "The second_loss value of k: (0.015481831505894661, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引18，y= tensor([0.7078])\n",
      "目前模型的Data狀態 torch.Size([108, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8080],\n",
      "        [0.8297]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0040],\n",
      "        [0.0056],\n",
      "        [0.0342],\n",
      "        [0.0367],\n",
      "        [0.0810],\n",
      "        [0.0454],\n",
      "        [0.0468],\n",
      "        [0.0375],\n",
      "        [0.0151],\n",
      "        [0.0014],\n",
      "        [0.0236],\n",
      "        [0.0043],\n",
      "        [0.0412],\n",
      "        [0.0216],\n",
      "        [0.0416],\n",
      "        [0.0037],\n",
      "        [0.0015],\n",
      "        [0.0120],\n",
      "        [0.0065],\n",
      "        [0.1741],\n",
      "        [0.1210],\n",
      "        [0.1287],\n",
      "        [0.1602],\n",
      "        [0.1668],\n",
      "        [0.0217],\n",
      "        [0.1687],\n",
      "        [0.0353],\n",
      "        [0.0383],\n",
      "        [0.0444],\n",
      "        [0.1079],\n",
      "        [0.0594],\n",
      "        [0.1275],\n",
      "        [0.1920],\n",
      "        [0.0755],\n",
      "        [0.0813],\n",
      "        [0.0846],\n",
      "        [0.0899],\n",
      "        [0.0939],\n",
      "        [0.0979],\n",
      "        [0.1016],\n",
      "        [0.1061],\n",
      "        [0.1120],\n",
      "        [0.1605],\n",
      "        [0.1914],\n",
      "        [0.1358],\n",
      "        [0.1442],\n",
      "        [0.1590],\n",
      "        [0.1594],\n",
      "        [0.1675],\n",
      "        [0.1711],\n",
      "        [0.1720],\n",
      "        [0.1754],\n",
      "        [0.1848],\n",
      "        [0.1893],\n",
      "        [0.1450],\n",
      "        [0.0321],\n",
      "        [0.1514],\n",
      "        [0.0346],\n",
      "        [0.1473],\n",
      "        [0.0537],\n",
      "        [0.0556],\n",
      "        [0.0664],\n",
      "        [0.0753],\n",
      "        [0.1123],\n",
      "        [0.1001],\n",
      "        [0.0277],\n",
      "        [0.0275],\n",
      "        [0.0144],\n",
      "        [0.0087],\n",
      "        [0.1041],\n",
      "        [0.1312],\n",
      "        [0.1363],\n",
      "        [0.1640],\n",
      "        [0.1650],\n",
      "        [0.1774],\n",
      "        [0.1793],\n",
      "        [0.1802],\n",
      "        [0.1914],\n",
      "        [0.1746],\n",
      "        [0.2010],\n",
      "        [0.1864],\n",
      "        [0.1587],\n",
      "        [0.2007],\n",
      "        [0.2173],\n",
      "        [0.2191],\n",
      "        [0.0618],\n",
      "        [0.1285],\n",
      "        [0.0625],\n",
      "        [0.0474],\n",
      "        [0.1114],\n",
      "        [0.1173],\n",
      "        [0.1187],\n",
      "        [0.1359],\n",
      "        [0.1488],\n",
      "        [0.1490],\n",
      "        [0.1495],\n",
      "        [0.1605],\n",
      "        [0.1626],\n",
      "        [0.1664],\n",
      "        [0.1668],\n",
      "        [0.1716],\n",
      "        [0.1720],\n",
      "        [0.1841],\n",
      "        [0.1482],\n",
      "        [0.0617],\n",
      "        [0.1129],\n",
      "        [0.1030],\n",
      "        [0.1219]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0031],\n",
      "        [0.0065],\n",
      "        [0.0351],\n",
      "        [0.0376],\n",
      "        [0.0819],\n",
      "        [0.0463],\n",
      "        [0.0477],\n",
      "        [0.0384],\n",
      "        [0.0142],\n",
      "        [0.0005],\n",
      "        [0.0245],\n",
      "        [0.0034],\n",
      "        [0.0421],\n",
      "        [0.0225],\n",
      "        [0.0425],\n",
      "        [0.0028],\n",
      "        [0.0024],\n",
      "        [0.0129],\n",
      "        [0.0074],\n",
      "        [0.1751],\n",
      "        [0.1219],\n",
      "        [0.1296],\n",
      "        [0.1611],\n",
      "        [0.1677],\n",
      "        [0.0226],\n",
      "        [0.1696],\n",
      "        [0.0362],\n",
      "        [0.0392],\n",
      "        [0.0454],\n",
      "        [0.1088],\n",
      "        [0.0604],\n",
      "        [0.1284],\n",
      "        [0.1929],\n",
      "        [0.0764],\n",
      "        [0.0822],\n",
      "        [0.0855],\n",
      "        [0.0908],\n",
      "        [0.0948],\n",
      "        [0.0989],\n",
      "        [0.1025],\n",
      "        [0.1070],\n",
      "        [0.1129],\n",
      "        [0.1614],\n",
      "        [0.1924],\n",
      "        [0.1367],\n",
      "        [0.1451],\n",
      "        [0.1599],\n",
      "        [0.1603],\n",
      "        [0.1685],\n",
      "        [0.1720],\n",
      "        [0.1730],\n",
      "        [0.1763],\n",
      "        [0.1857],\n",
      "        [0.1902],\n",
      "        [0.1459],\n",
      "        [0.0330],\n",
      "        [0.1523],\n",
      "        [0.0355],\n",
      "        [0.1482],\n",
      "        [0.0546],\n",
      "        [0.0565],\n",
      "        [0.0673],\n",
      "        [0.0763],\n",
      "        [0.1132],\n",
      "        [0.0991],\n",
      "        [0.0286],\n",
      "        [0.0284],\n",
      "        [0.0134],\n",
      "        [0.0096],\n",
      "        [0.1032],\n",
      "        [0.1303],\n",
      "        [0.1353],\n",
      "        [0.1631],\n",
      "        [0.1641],\n",
      "        [0.1765],\n",
      "        [0.1784],\n",
      "        [0.1793],\n",
      "        [0.1905],\n",
      "        [0.1737],\n",
      "        [0.2001],\n",
      "        [0.1855],\n",
      "        [0.1578],\n",
      "        [0.1998],\n",
      "        [0.2164],\n",
      "        [0.2182],\n",
      "        [0.0609],\n",
      "        [0.1276],\n",
      "        [0.0615],\n",
      "        [0.0465],\n",
      "        [0.1105],\n",
      "        [0.1163],\n",
      "        [0.1178],\n",
      "        [0.1350],\n",
      "        [0.1479],\n",
      "        [0.1481],\n",
      "        [0.1486],\n",
      "        [0.1596],\n",
      "        [0.1617],\n",
      "        [0.1655],\n",
      "        [0.1659],\n",
      "        [0.1707],\n",
      "        [0.1711],\n",
      "        [0.1832],\n",
      "        [0.1473],\n",
      "        [0.0608],\n",
      "        [0.1120],\n",
      "        [0.1020],\n",
      "        [0.0993]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 22.656307458877563\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 109\n",
      "剩餘X 資料 torch.Size([51, 18])\n",
      "剩餘Y 資料 torch.Size([51, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.015253734774887562, 18)\n",
      "The second_loss value of k: (0.01687157340347767, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引18，y= tensor([0.6835])\n",
      "目前模型的Data狀態 torch.Size([109, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071],\n",
      "        [0.8071]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0031],\n",
      "        [0.0065],\n",
      "        [0.0351],\n",
      "        [0.0376],\n",
      "        [0.0819],\n",
      "        [0.0463],\n",
      "        [0.0477],\n",
      "        [0.0384],\n",
      "        [0.0142],\n",
      "        [0.0005],\n",
      "        [0.0245],\n",
      "        [0.0034],\n",
      "        [0.0421],\n",
      "        [0.0225],\n",
      "        [0.0425],\n",
      "        [0.0028],\n",
      "        [0.0024],\n",
      "        [0.0129],\n",
      "        [0.0074],\n",
      "        [0.1751],\n",
      "        [0.1219],\n",
      "        [0.1296],\n",
      "        [0.1611],\n",
      "        [0.1677],\n",
      "        [0.0226],\n",
      "        [0.1696],\n",
      "        [0.0362],\n",
      "        [0.0392],\n",
      "        [0.0454],\n",
      "        [0.1088],\n",
      "        [0.0604],\n",
      "        [0.1284],\n",
      "        [0.1929],\n",
      "        [0.0764],\n",
      "        [0.0822],\n",
      "        [0.0855],\n",
      "        [0.0908],\n",
      "        [0.0948],\n",
      "        [0.0989],\n",
      "        [0.1025],\n",
      "        [0.1070],\n",
      "        [0.1129],\n",
      "        [0.1614],\n",
      "        [0.1924],\n",
      "        [0.1367],\n",
      "        [0.1451],\n",
      "        [0.1599],\n",
      "        [0.1603],\n",
      "        [0.1685],\n",
      "        [0.1720],\n",
      "        [0.1730],\n",
      "        [0.1763],\n",
      "        [0.1857],\n",
      "        [0.1902],\n",
      "        [0.1459],\n",
      "        [0.0330],\n",
      "        [0.1523],\n",
      "        [0.0355],\n",
      "        [0.1482],\n",
      "        [0.0546],\n",
      "        [0.0565],\n",
      "        [0.0673],\n",
      "        [0.0763],\n",
      "        [0.1132],\n",
      "        [0.0991],\n",
      "        [0.0286],\n",
      "        [0.0284],\n",
      "        [0.0134],\n",
      "        [0.0096],\n",
      "        [0.1032],\n",
      "        [0.1303],\n",
      "        [0.1353],\n",
      "        [0.1631],\n",
      "        [0.1641],\n",
      "        [0.1765],\n",
      "        [0.1784],\n",
      "        [0.1793],\n",
      "        [0.1905],\n",
      "        [0.1737],\n",
      "        [0.2001],\n",
      "        [0.1855],\n",
      "        [0.1578],\n",
      "        [0.1998],\n",
      "        [0.2164],\n",
      "        [0.2182],\n",
      "        [0.0609],\n",
      "        [0.1276],\n",
      "        [0.0615],\n",
      "        [0.0465],\n",
      "        [0.1105],\n",
      "        [0.1163],\n",
      "        [0.1178],\n",
      "        [0.1350],\n",
      "        [0.1479],\n",
      "        [0.1481],\n",
      "        [0.1486],\n",
      "        [0.1596],\n",
      "        [0.1617],\n",
      "        [0.1655],\n",
      "        [0.1659],\n",
      "        [0.1707],\n",
      "        [0.1711],\n",
      "        [0.1832],\n",
      "        [0.1473],\n",
      "        [0.0608],\n",
      "        [0.1120],\n",
      "        [0.1020],\n",
      "        [0.0993],\n",
      "        [0.1235]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0020],\n",
      "        [0.0076],\n",
      "        [0.0362],\n",
      "        [0.0388],\n",
      "        [0.0830],\n",
      "        [0.0474],\n",
      "        [0.0488],\n",
      "        [0.0395],\n",
      "        [0.0130],\n",
      "        [0.0006],\n",
      "        [0.0256],\n",
      "        [0.0023],\n",
      "        [0.0433],\n",
      "        [0.0236],\n",
      "        [0.0437],\n",
      "        [0.0016],\n",
      "        [0.0035],\n",
      "        [0.0141],\n",
      "        [0.0085],\n",
      "        [0.1762],\n",
      "        [0.1231],\n",
      "        [0.1308],\n",
      "        [0.1623],\n",
      "        [0.1688],\n",
      "        [0.0237],\n",
      "        [0.1708],\n",
      "        [0.0374],\n",
      "        [0.0403],\n",
      "        [0.0465],\n",
      "        [0.1100],\n",
      "        [0.0615],\n",
      "        [0.1295],\n",
      "        [0.1941],\n",
      "        [0.0776],\n",
      "        [0.0833],\n",
      "        [0.0867],\n",
      "        [0.0919],\n",
      "        [0.0959],\n",
      "        [0.1000],\n",
      "        [0.1037],\n",
      "        [0.1082],\n",
      "        [0.1141],\n",
      "        [0.1626],\n",
      "        [0.1935],\n",
      "        [0.1379],\n",
      "        [0.1462],\n",
      "        [0.1610],\n",
      "        [0.1614],\n",
      "        [0.1696],\n",
      "        [0.1731],\n",
      "        [0.1741],\n",
      "        [0.1774],\n",
      "        [0.1868],\n",
      "        [0.1913],\n",
      "        [0.1471],\n",
      "        [0.0341],\n",
      "        [0.1535],\n",
      "        [0.0366],\n",
      "        [0.1493],\n",
      "        [0.0557],\n",
      "        [0.0576],\n",
      "        [0.0684],\n",
      "        [0.0774],\n",
      "        [0.1144],\n",
      "        [0.0980],\n",
      "        [0.0297],\n",
      "        [0.0296],\n",
      "        [0.0123],\n",
      "        [0.0107],\n",
      "        [0.1021],\n",
      "        [0.1292],\n",
      "        [0.1342],\n",
      "        [0.1620],\n",
      "        [0.1629],\n",
      "        [0.1754],\n",
      "        [0.1773],\n",
      "        [0.1781],\n",
      "        [0.1893],\n",
      "        [0.1726],\n",
      "        [0.1989],\n",
      "        [0.1843],\n",
      "        [0.1567],\n",
      "        [0.1987],\n",
      "        [0.2152],\n",
      "        [0.2171],\n",
      "        [0.0598],\n",
      "        [0.1265],\n",
      "        [0.0604],\n",
      "        [0.0453],\n",
      "        [0.1093],\n",
      "        [0.1152],\n",
      "        [0.1166],\n",
      "        [0.1339],\n",
      "        [0.1468],\n",
      "        [0.1469],\n",
      "        [0.1474],\n",
      "        [0.1585],\n",
      "        [0.1605],\n",
      "        [0.1644],\n",
      "        [0.1647],\n",
      "        [0.1695],\n",
      "        [0.1699],\n",
      "        [0.1821],\n",
      "        [0.1462],\n",
      "        [0.0596],\n",
      "        [0.1109],\n",
      "        [0.1009],\n",
      "        [0.0981],\n",
      "        [0.1224]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 22.898616313934326\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 110\n",
      "剩餘X 資料 torch.Size([50, 18])\n",
      "剩餘Y 資料 torch.Size([50, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.016575664281845093, 18)\n",
      "The second_loss value of k: (0.0290974248200655, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引18，y= tensor([0.6772])\n",
      "目前模型的Data狀態 torch.Size([110, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059],\n",
      "        [0.8059]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0020],\n",
      "        [0.0076],\n",
      "        [0.0362],\n",
      "        [0.0388],\n",
      "        [0.0830],\n",
      "        [0.0474],\n",
      "        [0.0488],\n",
      "        [0.0395],\n",
      "        [0.0130],\n",
      "        [0.0006],\n",
      "        [0.0256],\n",
      "        [0.0023],\n",
      "        [0.0433],\n",
      "        [0.0236],\n",
      "        [0.0437],\n",
      "        [0.0016],\n",
      "        [0.0035],\n",
      "        [0.0141],\n",
      "        [0.0085],\n",
      "        [0.1762],\n",
      "        [0.1231],\n",
      "        [0.1308],\n",
      "        [0.1623],\n",
      "        [0.1688],\n",
      "        [0.0237],\n",
      "        [0.1708],\n",
      "        [0.0374],\n",
      "        [0.0403],\n",
      "        [0.0465],\n",
      "        [0.1100],\n",
      "        [0.0615],\n",
      "        [0.1295],\n",
      "        [0.1941],\n",
      "        [0.0776],\n",
      "        [0.0833],\n",
      "        [0.0867],\n",
      "        [0.0919],\n",
      "        [0.0959],\n",
      "        [0.1000],\n",
      "        [0.1037],\n",
      "        [0.1082],\n",
      "        [0.1141],\n",
      "        [0.1626],\n",
      "        [0.1935],\n",
      "        [0.1379],\n",
      "        [0.1462],\n",
      "        [0.1610],\n",
      "        [0.1614],\n",
      "        [0.1696],\n",
      "        [0.1731],\n",
      "        [0.1741],\n",
      "        [0.1774],\n",
      "        [0.1868],\n",
      "        [0.1913],\n",
      "        [0.1471],\n",
      "        [0.0341],\n",
      "        [0.1535],\n",
      "        [0.0366],\n",
      "        [0.1493],\n",
      "        [0.0557],\n",
      "        [0.0576],\n",
      "        [0.0684],\n",
      "        [0.0774],\n",
      "        [0.1144],\n",
      "        [0.0980],\n",
      "        [0.0297],\n",
      "        [0.0296],\n",
      "        [0.0123],\n",
      "        [0.0107],\n",
      "        [0.1021],\n",
      "        [0.1292],\n",
      "        [0.1342],\n",
      "        [0.1620],\n",
      "        [0.1629],\n",
      "        [0.1754],\n",
      "        [0.1773],\n",
      "        [0.1781],\n",
      "        [0.1893],\n",
      "        [0.1726],\n",
      "        [0.1989],\n",
      "        [0.1843],\n",
      "        [0.1567],\n",
      "        [0.1987],\n",
      "        [0.2152],\n",
      "        [0.2171],\n",
      "        [0.0598],\n",
      "        [0.1265],\n",
      "        [0.0604],\n",
      "        [0.0453],\n",
      "        [0.1093],\n",
      "        [0.1152],\n",
      "        [0.1166],\n",
      "        [0.1339],\n",
      "        [0.1468],\n",
      "        [0.1469],\n",
      "        [0.1474],\n",
      "        [0.1585],\n",
      "        [0.1605],\n",
      "        [0.1644],\n",
      "        [0.1647],\n",
      "        [0.1695],\n",
      "        [0.1699],\n",
      "        [0.1821],\n",
      "        [0.1462],\n",
      "        [0.0596],\n",
      "        [0.1109],\n",
      "        [0.1009],\n",
      "        [0.0981],\n",
      "        [0.1224],\n",
      "        [0.1287]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 65\n",
      "Number of shrink: 35\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0088],\n",
      "        [0.0374],\n",
      "        [0.0400],\n",
      "        [0.0842],\n",
      "        [0.0486],\n",
      "        [0.0500],\n",
      "        [0.0407],\n",
      "        [0.0119],\n",
      "        [0.0018],\n",
      "        [0.0268],\n",
      "        [0.0011],\n",
      "        [0.0445],\n",
      "        [0.0248],\n",
      "        [0.0449],\n",
      "        [0.0005],\n",
      "        [0.0047],\n",
      "        [0.0152],\n",
      "        [0.0097],\n",
      "        [0.1774],\n",
      "        [0.1242],\n",
      "        [0.1319],\n",
      "        [0.1634],\n",
      "        [0.1700],\n",
      "        [0.0249],\n",
      "        [0.1720],\n",
      "        [0.0386],\n",
      "        [0.0415],\n",
      "        [0.0477],\n",
      "        [0.1112],\n",
      "        [0.0627],\n",
      "        [0.1307],\n",
      "        [0.1953],\n",
      "        [0.0787],\n",
      "        [0.0845],\n",
      "        [0.0878],\n",
      "        [0.0931],\n",
      "        [0.0971],\n",
      "        [0.1012],\n",
      "        [0.1049],\n",
      "        [0.1094],\n",
      "        [0.1152],\n",
      "        [0.1637],\n",
      "        [0.1947],\n",
      "        [0.1391],\n",
      "        [0.1474],\n",
      "        [0.1622],\n",
      "        [0.1626],\n",
      "        [0.1708],\n",
      "        [0.1743],\n",
      "        [0.1753],\n",
      "        [0.1786],\n",
      "        [0.1880],\n",
      "        [0.1925],\n",
      "        [0.1483],\n",
      "        [0.0353],\n",
      "        [0.1546],\n",
      "        [0.0378],\n",
      "        [0.1505],\n",
      "        [0.0569],\n",
      "        [0.0588],\n",
      "        [0.0696],\n",
      "        [0.0786],\n",
      "        [0.1156],\n",
      "        [0.0968],\n",
      "        [0.0309],\n",
      "        [0.0308],\n",
      "        [0.0111],\n",
      "        [0.0119],\n",
      "        [0.1009],\n",
      "        [0.1280],\n",
      "        [0.1330],\n",
      "        [0.1608],\n",
      "        [0.1618],\n",
      "        [0.1742],\n",
      "        [0.1761],\n",
      "        [0.1770],\n",
      "        [0.1882],\n",
      "        [0.1714],\n",
      "        [0.1978],\n",
      "        [0.1831],\n",
      "        [0.1555],\n",
      "        [0.1975],\n",
      "        [0.2141],\n",
      "        [0.2159],\n",
      "        [0.0586],\n",
      "        [0.1253],\n",
      "        [0.0592],\n",
      "        [0.0441],\n",
      "        [0.1081],\n",
      "        [0.1140],\n",
      "        [0.1155],\n",
      "        [0.1327],\n",
      "        [0.1456],\n",
      "        [0.1457],\n",
      "        [0.1463],\n",
      "        [0.1573],\n",
      "        [0.1593],\n",
      "        [0.1632],\n",
      "        [0.1636],\n",
      "        [0.1683],\n",
      "        [0.1687],\n",
      "        [0.1809],\n",
      "        [0.1450],\n",
      "        [0.0584],\n",
      "        [0.1097],\n",
      "        [0.0997],\n",
      "        [0.0970],\n",
      "        [0.1212],\n",
      "        [0.1276]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 23.142107725143433\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 111\n",
      "剩餘X 資料 torch.Size([49, 18])\n",
      "剩餘Y 資料 torch.Size([49, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.028696512803435326, 18)\n",
      "The second_loss value of k: (0.030890246853232384, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引18，y= tensor([0.6353])\n",
      "目前模型的Data狀態 torch.Size([111, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047],\n",
      "        [0.8047]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0008],\n",
      "        [0.0088],\n",
      "        [0.0374],\n",
      "        [0.0400],\n",
      "        [0.0842],\n",
      "        [0.0486],\n",
      "        [0.0500],\n",
      "        [0.0407],\n",
      "        [0.0119],\n",
      "        [0.0018],\n",
      "        [0.0268],\n",
      "        [0.0011],\n",
      "        [0.0445],\n",
      "        [0.0248],\n",
      "        [0.0449],\n",
      "        [0.0005],\n",
      "        [0.0047],\n",
      "        [0.0152],\n",
      "        [0.0097],\n",
      "        [0.1774],\n",
      "        [0.1242],\n",
      "        [0.1319],\n",
      "        [0.1634],\n",
      "        [0.1700],\n",
      "        [0.0249],\n",
      "        [0.1720],\n",
      "        [0.0386],\n",
      "        [0.0415],\n",
      "        [0.0477],\n",
      "        [0.1112],\n",
      "        [0.0627],\n",
      "        [0.1307],\n",
      "        [0.1953],\n",
      "        [0.0787],\n",
      "        [0.0845],\n",
      "        [0.0878],\n",
      "        [0.0931],\n",
      "        [0.0971],\n",
      "        [0.1012],\n",
      "        [0.1049],\n",
      "        [0.1094],\n",
      "        [0.1152],\n",
      "        [0.1637],\n",
      "        [0.1947],\n",
      "        [0.1391],\n",
      "        [0.1474],\n",
      "        [0.1622],\n",
      "        [0.1626],\n",
      "        [0.1708],\n",
      "        [0.1743],\n",
      "        [0.1753],\n",
      "        [0.1786],\n",
      "        [0.1880],\n",
      "        [0.1925],\n",
      "        [0.1483],\n",
      "        [0.0353],\n",
      "        [0.1546],\n",
      "        [0.0378],\n",
      "        [0.1505],\n",
      "        [0.0569],\n",
      "        [0.0588],\n",
      "        [0.0696],\n",
      "        [0.0786],\n",
      "        [0.1156],\n",
      "        [0.0968],\n",
      "        [0.0309],\n",
      "        [0.0308],\n",
      "        [0.0111],\n",
      "        [0.0119],\n",
      "        [0.1009],\n",
      "        [0.1280],\n",
      "        [0.1330],\n",
      "        [0.1608],\n",
      "        [0.1618],\n",
      "        [0.1742],\n",
      "        [0.1761],\n",
      "        [0.1770],\n",
      "        [0.1882],\n",
      "        [0.1714],\n",
      "        [0.1978],\n",
      "        [0.1831],\n",
      "        [0.1555],\n",
      "        [0.1975],\n",
      "        [0.2141],\n",
      "        [0.2159],\n",
      "        [0.0586],\n",
      "        [0.1253],\n",
      "        [0.0592],\n",
      "        [0.0441],\n",
      "        [0.1081],\n",
      "        [0.1140],\n",
      "        [0.1155],\n",
      "        [0.1327],\n",
      "        [0.1456],\n",
      "        [0.1457],\n",
      "        [0.1463],\n",
      "        [0.1573],\n",
      "        [0.1593],\n",
      "        [0.1632],\n",
      "        [0.1636],\n",
      "        [0.1683],\n",
      "        [0.1687],\n",
      "        [0.1809],\n",
      "        [0.1450],\n",
      "        [0.0584],\n",
      "        [0.1097],\n",
      "        [0.0997],\n",
      "        [0.0970],\n",
      "        [0.1212],\n",
      "        [0.1276],\n",
      "        [0.1694]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0103],\n",
      "        [0.0389],\n",
      "        [0.0415],\n",
      "        [0.0857],\n",
      "        [0.0501],\n",
      "        [0.0515],\n",
      "        [0.0422],\n",
      "        [0.0104],\n",
      "        [0.0033],\n",
      "        [0.0283],\n",
      "        [0.0004],\n",
      "        [0.0460],\n",
      "        [0.0263],\n",
      "        [0.0464],\n",
      "        [0.0010],\n",
      "        [0.0062],\n",
      "        [0.0168],\n",
      "        [0.0112],\n",
      "        [0.1789],\n",
      "        [0.1257],\n",
      "        [0.1334],\n",
      "        [0.1650],\n",
      "        [0.1715],\n",
      "        [0.0264],\n",
      "        [0.1735],\n",
      "        [0.0401],\n",
      "        [0.0430],\n",
      "        [0.0492],\n",
      "        [0.1127],\n",
      "        [0.0642],\n",
      "        [0.1322],\n",
      "        [0.1968],\n",
      "        [0.0802],\n",
      "        [0.0860],\n",
      "        [0.0894],\n",
      "        [0.0946],\n",
      "        [0.0986],\n",
      "        [0.1027],\n",
      "        [0.1064],\n",
      "        [0.1109],\n",
      "        [0.1168],\n",
      "        [0.1652],\n",
      "        [0.1962],\n",
      "        [0.1406],\n",
      "        [0.1489],\n",
      "        [0.1637],\n",
      "        [0.1641],\n",
      "        [0.1723],\n",
      "        [0.1758],\n",
      "        [0.1768],\n",
      "        [0.1801],\n",
      "        [0.1895],\n",
      "        [0.1940],\n",
      "        [0.1498],\n",
      "        [0.0368],\n",
      "        [0.1562],\n",
      "        [0.0393],\n",
      "        [0.1520],\n",
      "        [0.0584],\n",
      "        [0.0603],\n",
      "        [0.0711],\n",
      "        [0.0801],\n",
      "        [0.1171],\n",
      "        [0.0953],\n",
      "        [0.0324],\n",
      "        [0.0323],\n",
      "        [0.0096],\n",
      "        [0.0134],\n",
      "        [0.0994],\n",
      "        [0.1265],\n",
      "        [0.1315],\n",
      "        [0.1593],\n",
      "        [0.1603],\n",
      "        [0.1727],\n",
      "        [0.1746],\n",
      "        [0.1755],\n",
      "        [0.1867],\n",
      "        [0.1699],\n",
      "        [0.1963],\n",
      "        [0.1816],\n",
      "        [0.1540],\n",
      "        [0.1960],\n",
      "        [0.2125],\n",
      "        [0.2144],\n",
      "        [0.0571],\n",
      "        [0.1238],\n",
      "        [0.0577],\n",
      "        [0.0426],\n",
      "        [0.1066],\n",
      "        [0.1125],\n",
      "        [0.1140],\n",
      "        [0.1312],\n",
      "        [0.1441],\n",
      "        [0.1442],\n",
      "        [0.1447],\n",
      "        [0.1558],\n",
      "        [0.1578],\n",
      "        [0.1617],\n",
      "        [0.1621],\n",
      "        [0.1668],\n",
      "        [0.1672],\n",
      "        [0.1794],\n",
      "        [0.1435],\n",
      "        [0.0569],\n",
      "        [0.1082],\n",
      "        [0.0982],\n",
      "        [0.0955],\n",
      "        [0.1197],\n",
      "        [0.1261],\n",
      "        [0.1679]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 23.383022785186768\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 112\n",
      "剩餘X 資料 torch.Size([48, 18])\n",
      "剩餘Y 資料 torch.Size([48, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.030362877994775772, 2)\n",
      "The second_loss value of k: (0.03404083475470543, 29)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.6630])\n",
      "目前模型的Data狀態 torch.Size([112, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8032],\n",
      "        [0.8373]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0007],\n",
      "        [0.0103],\n",
      "        [0.0389],\n",
      "        [0.0415],\n",
      "        [0.0857],\n",
      "        [0.0501],\n",
      "        [0.0515],\n",
      "        [0.0422],\n",
      "        [0.0104],\n",
      "        [0.0033],\n",
      "        [0.0283],\n",
      "        [0.0004],\n",
      "        [0.0460],\n",
      "        [0.0263],\n",
      "        [0.0464],\n",
      "        [0.0010],\n",
      "        [0.0062],\n",
      "        [0.0168],\n",
      "        [0.0112],\n",
      "        [0.1789],\n",
      "        [0.1257],\n",
      "        [0.1334],\n",
      "        [0.1650],\n",
      "        [0.1715],\n",
      "        [0.0264],\n",
      "        [0.1735],\n",
      "        [0.0401],\n",
      "        [0.0430],\n",
      "        [0.0492],\n",
      "        [0.1127],\n",
      "        [0.0642],\n",
      "        [0.1322],\n",
      "        [0.1968],\n",
      "        [0.0802],\n",
      "        [0.0860],\n",
      "        [0.0894],\n",
      "        [0.0946],\n",
      "        [0.0986],\n",
      "        [0.1027],\n",
      "        [0.1064],\n",
      "        [0.1109],\n",
      "        [0.1168],\n",
      "        [0.1652],\n",
      "        [0.1962],\n",
      "        [0.1406],\n",
      "        [0.1489],\n",
      "        [0.1637],\n",
      "        [0.1641],\n",
      "        [0.1723],\n",
      "        [0.1758],\n",
      "        [0.1768],\n",
      "        [0.1801],\n",
      "        [0.1895],\n",
      "        [0.1940],\n",
      "        [0.1498],\n",
      "        [0.0368],\n",
      "        [0.1562],\n",
      "        [0.0393],\n",
      "        [0.1520],\n",
      "        [0.0584],\n",
      "        [0.0603],\n",
      "        [0.0711],\n",
      "        [0.0801],\n",
      "        [0.1171],\n",
      "        [0.0953],\n",
      "        [0.0324],\n",
      "        [0.0323],\n",
      "        [0.0096],\n",
      "        [0.0134],\n",
      "        [0.0994],\n",
      "        [0.1265],\n",
      "        [0.1315],\n",
      "        [0.1593],\n",
      "        [0.1603],\n",
      "        [0.1727],\n",
      "        [0.1746],\n",
      "        [0.1755],\n",
      "        [0.1867],\n",
      "        [0.1699],\n",
      "        [0.1963],\n",
      "        [0.1816],\n",
      "        [0.1540],\n",
      "        [0.1960],\n",
      "        [0.2125],\n",
      "        [0.2144],\n",
      "        [0.0571],\n",
      "        [0.1238],\n",
      "        [0.0577],\n",
      "        [0.0426],\n",
      "        [0.1066],\n",
      "        [0.1125],\n",
      "        [0.1140],\n",
      "        [0.1312],\n",
      "        [0.1441],\n",
      "        [0.1442],\n",
      "        [0.1447],\n",
      "        [0.1558],\n",
      "        [0.1578],\n",
      "        [0.1617],\n",
      "        [0.1621],\n",
      "        [0.1668],\n",
      "        [0.1672],\n",
      "        [0.1794],\n",
      "        [0.1435],\n",
      "        [0.0569],\n",
      "        [0.1082],\n",
      "        [0.0982],\n",
      "        [0.0955],\n",
      "        [0.1197],\n",
      "        [0.1261],\n",
      "        [0.1679],\n",
      "        [0.1742]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0020],\n",
      "        [0.0116],\n",
      "        [0.0402],\n",
      "        [0.0427],\n",
      "        [0.0870],\n",
      "        [0.0514],\n",
      "        [0.0528],\n",
      "        [0.0435],\n",
      "        [0.0091],\n",
      "        [0.0046],\n",
      "        [0.0296],\n",
      "        [0.0017],\n",
      "        [0.0472],\n",
      "        [0.0276],\n",
      "        [0.0476],\n",
      "        [0.0023],\n",
      "        [0.0075],\n",
      "        [0.0180],\n",
      "        [0.0125],\n",
      "        [0.1801],\n",
      "        [0.1270],\n",
      "        [0.1347],\n",
      "        [0.1662],\n",
      "        [0.1728],\n",
      "        [0.0277],\n",
      "        [0.1747],\n",
      "        [0.0413],\n",
      "        [0.0443],\n",
      "        [0.0504],\n",
      "        [0.1139],\n",
      "        [0.0654],\n",
      "        [0.1335],\n",
      "        [0.1980],\n",
      "        [0.0815],\n",
      "        [0.0873],\n",
      "        [0.0906],\n",
      "        [0.0959],\n",
      "        [0.0999],\n",
      "        [0.1040],\n",
      "        [0.1076],\n",
      "        [0.1121],\n",
      "        [0.1180],\n",
      "        [0.1665],\n",
      "        [0.1974],\n",
      "        [0.1418],\n",
      "        [0.1502],\n",
      "        [0.1650],\n",
      "        [0.1654],\n",
      "        [0.1736],\n",
      "        [0.1771],\n",
      "        [0.1781],\n",
      "        [0.1814],\n",
      "        [0.1908],\n",
      "        [0.1953],\n",
      "        [0.1510],\n",
      "        [0.0381],\n",
      "        [0.1574],\n",
      "        [0.0406],\n",
      "        [0.1533],\n",
      "        [0.0597],\n",
      "        [0.0616],\n",
      "        [0.0724],\n",
      "        [0.0814],\n",
      "        [0.1183],\n",
      "        [0.0941],\n",
      "        [0.0337],\n",
      "        [0.0335],\n",
      "        [0.0083],\n",
      "        [0.0147],\n",
      "        [0.0981],\n",
      "        [0.1252],\n",
      "        [0.1302],\n",
      "        [0.1580],\n",
      "        [0.1590],\n",
      "        [0.1714],\n",
      "        [0.1733],\n",
      "        [0.1742],\n",
      "        [0.1854],\n",
      "        [0.1686],\n",
      "        [0.1950],\n",
      "        [0.1804],\n",
      "        [0.1527],\n",
      "        [0.1947],\n",
      "        [0.2113],\n",
      "        [0.2131],\n",
      "        [0.0558],\n",
      "        [0.1225],\n",
      "        [0.0564],\n",
      "        [0.0414],\n",
      "        [0.1054],\n",
      "        [0.1112],\n",
      "        [0.1127],\n",
      "        [0.1299],\n",
      "        [0.1428],\n",
      "        [0.1430],\n",
      "        [0.1435],\n",
      "        [0.1545],\n",
      "        [0.1566],\n",
      "        [0.1604],\n",
      "        [0.1608],\n",
      "        [0.1656],\n",
      "        [0.1660],\n",
      "        [0.1781],\n",
      "        [0.1422],\n",
      "        [0.0557],\n",
      "        [0.1069],\n",
      "        [0.0970],\n",
      "        [0.0942],\n",
      "        [0.1184],\n",
      "        [0.1248],\n",
      "        [0.1666],\n",
      "        [0.1389]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 23.625126361846924\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 113\n",
      "剩餘X 資料 torch.Size([47, 18])\n",
      "剩餘Y 資料 torch.Size([47, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.023072848096489906, 16)\n",
      "The second_loss value of k: (0.03357713669538498, 28)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.7161])\n",
      "目前模型的Data狀態 torch.Size([113, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8020],\n",
      "        [0.8680]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0020],\n",
      "        [0.0116],\n",
      "        [0.0402],\n",
      "        [0.0427],\n",
      "        [0.0870],\n",
      "        [0.0514],\n",
      "        [0.0528],\n",
      "        [0.0435],\n",
      "        [0.0091],\n",
      "        [0.0046],\n",
      "        [0.0296],\n",
      "        [0.0017],\n",
      "        [0.0472],\n",
      "        [0.0276],\n",
      "        [0.0476],\n",
      "        [0.0023],\n",
      "        [0.0075],\n",
      "        [0.0180],\n",
      "        [0.0125],\n",
      "        [0.1801],\n",
      "        [0.1270],\n",
      "        [0.1347],\n",
      "        [0.1662],\n",
      "        [0.1728],\n",
      "        [0.0277],\n",
      "        [0.1747],\n",
      "        [0.0413],\n",
      "        [0.0443],\n",
      "        [0.0504],\n",
      "        [0.1139],\n",
      "        [0.0654],\n",
      "        [0.1335],\n",
      "        [0.1980],\n",
      "        [0.0815],\n",
      "        [0.0873],\n",
      "        [0.0906],\n",
      "        [0.0959],\n",
      "        [0.0999],\n",
      "        [0.1040],\n",
      "        [0.1076],\n",
      "        [0.1121],\n",
      "        [0.1180],\n",
      "        [0.1665],\n",
      "        [0.1974],\n",
      "        [0.1418],\n",
      "        [0.1502],\n",
      "        [0.1650],\n",
      "        [0.1654],\n",
      "        [0.1736],\n",
      "        [0.1771],\n",
      "        [0.1781],\n",
      "        [0.1814],\n",
      "        [0.1908],\n",
      "        [0.1953],\n",
      "        [0.1510],\n",
      "        [0.0381],\n",
      "        [0.1574],\n",
      "        [0.0406],\n",
      "        [0.1533],\n",
      "        [0.0597],\n",
      "        [0.0616],\n",
      "        [0.0724],\n",
      "        [0.0814],\n",
      "        [0.1183],\n",
      "        [0.0941],\n",
      "        [0.0337],\n",
      "        [0.0335],\n",
      "        [0.0083],\n",
      "        [0.0147],\n",
      "        [0.0981],\n",
      "        [0.1252],\n",
      "        [0.1302],\n",
      "        [0.1580],\n",
      "        [0.1590],\n",
      "        [0.1714],\n",
      "        [0.1733],\n",
      "        [0.1742],\n",
      "        [0.1854],\n",
      "        [0.1686],\n",
      "        [0.1950],\n",
      "        [0.1804],\n",
      "        [0.1527],\n",
      "        [0.1947],\n",
      "        [0.2113],\n",
      "        [0.2131],\n",
      "        [0.0558],\n",
      "        [0.1225],\n",
      "        [0.0564],\n",
      "        [0.0414],\n",
      "        [0.1054],\n",
      "        [0.1112],\n",
      "        [0.1127],\n",
      "        [0.1299],\n",
      "        [0.1428],\n",
      "        [0.1430],\n",
      "        [0.1435],\n",
      "        [0.1545],\n",
      "        [0.1566],\n",
      "        [0.1604],\n",
      "        [0.1608],\n",
      "        [0.1656],\n",
      "        [0.1660],\n",
      "        [0.1781],\n",
      "        [0.1422],\n",
      "        [0.0557],\n",
      "        [0.1069],\n",
      "        [0.0970],\n",
      "        [0.0942],\n",
      "        [0.1184],\n",
      "        [0.1248],\n",
      "        [0.1666],\n",
      "        [0.1389],\n",
      "        [0.1519]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0027],\n",
      "        [0.0123],\n",
      "        [0.0409],\n",
      "        [0.0435],\n",
      "        [0.0877],\n",
      "        [0.0521],\n",
      "        [0.0535],\n",
      "        [0.0442],\n",
      "        [0.0083],\n",
      "        [0.0053],\n",
      "        [0.0303],\n",
      "        [0.0024],\n",
      "        [0.0480],\n",
      "        [0.0283],\n",
      "        [0.0484],\n",
      "        [0.0031],\n",
      "        [0.0082],\n",
      "        [0.0188],\n",
      "        [0.0132],\n",
      "        [0.1809],\n",
      "        [0.1278],\n",
      "        [0.1354],\n",
      "        [0.1670],\n",
      "        [0.1735],\n",
      "        [0.0284],\n",
      "        [0.1755],\n",
      "        [0.0421],\n",
      "        [0.0450],\n",
      "        [0.0512],\n",
      "        [0.1147],\n",
      "        [0.0662],\n",
      "        [0.1342],\n",
      "        [0.1988],\n",
      "        [0.0823],\n",
      "        [0.0880],\n",
      "        [0.0914],\n",
      "        [0.0966],\n",
      "        [0.1006],\n",
      "        [0.1047],\n",
      "        [0.1084],\n",
      "        [0.1129],\n",
      "        [0.1188],\n",
      "        [0.1673],\n",
      "        [0.1982],\n",
      "        [0.1426],\n",
      "        [0.1509],\n",
      "        [0.1657],\n",
      "        [0.1661],\n",
      "        [0.1743],\n",
      "        [0.1778],\n",
      "        [0.1788],\n",
      "        [0.1821],\n",
      "        [0.1915],\n",
      "        [0.1960],\n",
      "        [0.1518],\n",
      "        [0.0388],\n",
      "        [0.1582],\n",
      "        [0.0413],\n",
      "        [0.1540],\n",
      "        [0.0604],\n",
      "        [0.0623],\n",
      "        [0.0731],\n",
      "        [0.0821],\n",
      "        [0.1191],\n",
      "        [0.0933],\n",
      "        [0.0344],\n",
      "        [0.0343],\n",
      "        [0.0076],\n",
      "        [0.0154],\n",
      "        [0.0974],\n",
      "        [0.1245],\n",
      "        [0.1295],\n",
      "        [0.1573],\n",
      "        [0.1582],\n",
      "        [0.1707],\n",
      "        [0.1726],\n",
      "        [0.1734],\n",
      "        [0.1846],\n",
      "        [0.1679],\n",
      "        [0.1942],\n",
      "        [0.1796],\n",
      "        [0.1520],\n",
      "        [0.1940],\n",
      "        [0.2105],\n",
      "        [0.2124],\n",
      "        [0.0551],\n",
      "        [0.1218],\n",
      "        [0.0557],\n",
      "        [0.0406],\n",
      "        [0.1046],\n",
      "        [0.1105],\n",
      "        [0.1119],\n",
      "        [0.1292],\n",
      "        [0.1421],\n",
      "        [0.1422],\n",
      "        [0.1427],\n",
      "        [0.1538],\n",
      "        [0.1558],\n",
      "        [0.1597],\n",
      "        [0.1600],\n",
      "        [0.1648],\n",
      "        [0.1652],\n",
      "        [0.1774],\n",
      "        [0.1415],\n",
      "        [0.0549],\n",
      "        [0.1062],\n",
      "        [0.0962],\n",
      "        [0.0934],\n",
      "        [0.1177],\n",
      "        [0.1240],\n",
      "        [0.1659],\n",
      "        [0.1382],\n",
      "        [0.0851]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 23.88791537284851\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 114\n",
      "剩餘X 資料 torch.Size([46, 18])\n",
      "剩餘Y 資料 torch.Size([46, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03330276906490326, 27)\n",
      "The second_loss value of k: (0.03390592336654663, 26)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引27，y= tensor([0.6187])\n",
      "目前模型的Data狀態 torch.Size([114, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012],\n",
      "        [0.8012]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0027],\n",
      "        [0.0123],\n",
      "        [0.0409],\n",
      "        [0.0435],\n",
      "        [0.0877],\n",
      "        [0.0521],\n",
      "        [0.0535],\n",
      "        [0.0442],\n",
      "        [0.0083],\n",
      "        [0.0053],\n",
      "        [0.0303],\n",
      "        [0.0024],\n",
      "        [0.0480],\n",
      "        [0.0283],\n",
      "        [0.0484],\n",
      "        [0.0031],\n",
      "        [0.0082],\n",
      "        [0.0188],\n",
      "        [0.0132],\n",
      "        [0.1809],\n",
      "        [0.1278],\n",
      "        [0.1354],\n",
      "        [0.1670],\n",
      "        [0.1735],\n",
      "        [0.0284],\n",
      "        [0.1755],\n",
      "        [0.0421],\n",
      "        [0.0450],\n",
      "        [0.0512],\n",
      "        [0.1147],\n",
      "        [0.0662],\n",
      "        [0.1342],\n",
      "        [0.1988],\n",
      "        [0.0823],\n",
      "        [0.0880],\n",
      "        [0.0914],\n",
      "        [0.0966],\n",
      "        [0.1006],\n",
      "        [0.1047],\n",
      "        [0.1084],\n",
      "        [0.1129],\n",
      "        [0.1188],\n",
      "        [0.1673],\n",
      "        [0.1982],\n",
      "        [0.1426],\n",
      "        [0.1509],\n",
      "        [0.1657],\n",
      "        [0.1661],\n",
      "        [0.1743],\n",
      "        [0.1778],\n",
      "        [0.1788],\n",
      "        [0.1821],\n",
      "        [0.1915],\n",
      "        [0.1960],\n",
      "        [0.1518],\n",
      "        [0.0388],\n",
      "        [0.1582],\n",
      "        [0.0413],\n",
      "        [0.1540],\n",
      "        [0.0604],\n",
      "        [0.0623],\n",
      "        [0.0731],\n",
      "        [0.0821],\n",
      "        [0.1191],\n",
      "        [0.0933],\n",
      "        [0.0344],\n",
      "        [0.0343],\n",
      "        [0.0076],\n",
      "        [0.0154],\n",
      "        [0.0974],\n",
      "        [0.1245],\n",
      "        [0.1295],\n",
      "        [0.1573],\n",
      "        [0.1582],\n",
      "        [0.1707],\n",
      "        [0.1726],\n",
      "        [0.1734],\n",
      "        [0.1846],\n",
      "        [0.1679],\n",
      "        [0.1942],\n",
      "        [0.1796],\n",
      "        [0.1520],\n",
      "        [0.1940],\n",
      "        [0.2105],\n",
      "        [0.2124],\n",
      "        [0.0551],\n",
      "        [0.1218],\n",
      "        [0.0557],\n",
      "        [0.0406],\n",
      "        [0.1046],\n",
      "        [0.1105],\n",
      "        [0.1119],\n",
      "        [0.1292],\n",
      "        [0.1421],\n",
      "        [0.1422],\n",
      "        [0.1427],\n",
      "        [0.1538],\n",
      "        [0.1558],\n",
      "        [0.1597],\n",
      "        [0.1600],\n",
      "        [0.1648],\n",
      "        [0.1652],\n",
      "        [0.1774],\n",
      "        [0.1415],\n",
      "        [0.0549],\n",
      "        [0.1062],\n",
      "        [0.0962],\n",
      "        [0.0934],\n",
      "        [0.1177],\n",
      "        [0.1240],\n",
      "        [0.1659],\n",
      "        [0.1382],\n",
      "        [0.0851],\n",
      "        [0.1825]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0043],\n",
      "        [0.0139],\n",
      "        [0.0425],\n",
      "        [0.0451],\n",
      "        [0.0893],\n",
      "        [0.0537],\n",
      "        [0.0551],\n",
      "        [0.0458],\n",
      "        [0.0067],\n",
      "        [0.0069],\n",
      "        [0.0319],\n",
      "        [0.0040],\n",
      "        [0.0496],\n",
      "        [0.0299],\n",
      "        [0.0500],\n",
      "        [0.0047],\n",
      "        [0.0098],\n",
      "        [0.0204],\n",
      "        [0.0148],\n",
      "        [0.1825],\n",
      "        [0.1294],\n",
      "        [0.1370],\n",
      "        [0.1686],\n",
      "        [0.1751],\n",
      "        [0.0300],\n",
      "        [0.1771],\n",
      "        [0.0437],\n",
      "        [0.0466],\n",
      "        [0.0528],\n",
      "        [0.1163],\n",
      "        [0.0678],\n",
      "        [0.1358],\n",
      "        [0.2004],\n",
      "        [0.0839],\n",
      "        [0.0896],\n",
      "        [0.0930],\n",
      "        [0.0982],\n",
      "        [0.1022],\n",
      "        [0.1063],\n",
      "        [0.1100],\n",
      "        [0.1145],\n",
      "        [0.1204],\n",
      "        [0.1689],\n",
      "        [0.1998],\n",
      "        [0.1442],\n",
      "        [0.1525],\n",
      "        [0.1673],\n",
      "        [0.1677],\n",
      "        [0.1759],\n",
      "        [0.1794],\n",
      "        [0.1804],\n",
      "        [0.1837],\n",
      "        [0.1931],\n",
      "        [0.1976],\n",
      "        [0.1534],\n",
      "        [0.0404],\n",
      "        [0.1598],\n",
      "        [0.0429],\n",
      "        [0.1556],\n",
      "        [0.0620],\n",
      "        [0.0639],\n",
      "        [0.0747],\n",
      "        [0.0837],\n",
      "        [0.1207],\n",
      "        [0.0917],\n",
      "        [0.0360],\n",
      "        [0.0359],\n",
      "        [0.0060],\n",
      "        [0.0170],\n",
      "        [0.0958],\n",
      "        [0.1229],\n",
      "        [0.1279],\n",
      "        [0.1557],\n",
      "        [0.1566],\n",
      "        [0.1691],\n",
      "        [0.1710],\n",
      "        [0.1718],\n",
      "        [0.1830],\n",
      "        [0.1663],\n",
      "        [0.1926],\n",
      "        [0.1780],\n",
      "        [0.1504],\n",
      "        [0.1924],\n",
      "        [0.2089],\n",
      "        [0.2108],\n",
      "        [0.0535],\n",
      "        [0.1202],\n",
      "        [0.0541],\n",
      "        [0.0390],\n",
      "        [0.1030],\n",
      "        [0.1089],\n",
      "        [0.1103],\n",
      "        [0.1276],\n",
      "        [0.1405],\n",
      "        [0.1406],\n",
      "        [0.1411],\n",
      "        [0.1522],\n",
      "        [0.1542],\n",
      "        [0.1581],\n",
      "        [0.1584],\n",
      "        [0.1632],\n",
      "        [0.1636],\n",
      "        [0.1758],\n",
      "        [0.1399],\n",
      "        [0.0533],\n",
      "        [0.1046],\n",
      "        [0.0946],\n",
      "        [0.0918],\n",
      "        [0.1161],\n",
      "        [0.1224],\n",
      "        [0.1643],\n",
      "        [0.1366],\n",
      "        [0.0835],\n",
      "        [0.1809]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 24.163469791412354\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 115\n",
      "剩餘X 資料 torch.Size([45, 18])\n",
      "剩餘Y 資料 torch.Size([45, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.033318981528282166, 26)\n",
      "The second_loss value of k: (0.03372170403599739, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引26，y= tensor([0.6171])\n",
      "目前模型的Data狀態 torch.Size([115, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996],\n",
      "        [0.7996]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0043],\n",
      "        [0.0139],\n",
      "        [0.0425],\n",
      "        [0.0451],\n",
      "        [0.0893],\n",
      "        [0.0537],\n",
      "        [0.0551],\n",
      "        [0.0458],\n",
      "        [0.0067],\n",
      "        [0.0069],\n",
      "        [0.0319],\n",
      "        [0.0040],\n",
      "        [0.0496],\n",
      "        [0.0299],\n",
      "        [0.0500],\n",
      "        [0.0047],\n",
      "        [0.0098],\n",
      "        [0.0204],\n",
      "        [0.0148],\n",
      "        [0.1825],\n",
      "        [0.1294],\n",
      "        [0.1370],\n",
      "        [0.1686],\n",
      "        [0.1751],\n",
      "        [0.0300],\n",
      "        [0.1771],\n",
      "        [0.0437],\n",
      "        [0.0466],\n",
      "        [0.0528],\n",
      "        [0.1163],\n",
      "        [0.0678],\n",
      "        [0.1358],\n",
      "        [0.2004],\n",
      "        [0.0839],\n",
      "        [0.0896],\n",
      "        [0.0930],\n",
      "        [0.0982],\n",
      "        [0.1022],\n",
      "        [0.1063],\n",
      "        [0.1100],\n",
      "        [0.1145],\n",
      "        [0.1204],\n",
      "        [0.1689],\n",
      "        [0.1998],\n",
      "        [0.1442],\n",
      "        [0.1525],\n",
      "        [0.1673],\n",
      "        [0.1677],\n",
      "        [0.1759],\n",
      "        [0.1794],\n",
      "        [0.1804],\n",
      "        [0.1837],\n",
      "        [0.1931],\n",
      "        [0.1976],\n",
      "        [0.1534],\n",
      "        [0.0404],\n",
      "        [0.1598],\n",
      "        [0.0429],\n",
      "        [0.1556],\n",
      "        [0.0620],\n",
      "        [0.0639],\n",
      "        [0.0747],\n",
      "        [0.0837],\n",
      "        [0.1207],\n",
      "        [0.0917],\n",
      "        [0.0360],\n",
      "        [0.0359],\n",
      "        [0.0060],\n",
      "        [0.0170],\n",
      "        [0.0958],\n",
      "        [0.1229],\n",
      "        [0.1279],\n",
      "        [0.1557],\n",
      "        [0.1566],\n",
      "        [0.1691],\n",
      "        [0.1710],\n",
      "        [0.1718],\n",
      "        [0.1830],\n",
      "        [0.1663],\n",
      "        [0.1926],\n",
      "        [0.1780],\n",
      "        [0.1504],\n",
      "        [0.1924],\n",
      "        [0.2089],\n",
      "        [0.2108],\n",
      "        [0.0535],\n",
      "        [0.1202],\n",
      "        [0.0541],\n",
      "        [0.0390],\n",
      "        [0.1030],\n",
      "        [0.1089],\n",
      "        [0.1103],\n",
      "        [0.1276],\n",
      "        [0.1405],\n",
      "        [0.1406],\n",
      "        [0.1411],\n",
      "        [0.1522],\n",
      "        [0.1542],\n",
      "        [0.1581],\n",
      "        [0.1584],\n",
      "        [0.1632],\n",
      "        [0.1636],\n",
      "        [0.1758],\n",
      "        [0.1399],\n",
      "        [0.0533],\n",
      "        [0.1046],\n",
      "        [0.0946],\n",
      "        [0.0918],\n",
      "        [0.1161],\n",
      "        [0.1224],\n",
      "        [0.1643],\n",
      "        [0.1366],\n",
      "        [0.0835],\n",
      "        [0.1809],\n",
      "        [0.1825]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0059],\n",
      "        [0.0155],\n",
      "        [0.0441],\n",
      "        [0.0467],\n",
      "        [0.0909],\n",
      "        [0.0553],\n",
      "        [0.0567],\n",
      "        [0.0474],\n",
      "        [0.0052],\n",
      "        [0.0085],\n",
      "        [0.0335],\n",
      "        [0.0056],\n",
      "        [0.0512],\n",
      "        [0.0315],\n",
      "        [0.0516],\n",
      "        [0.0062],\n",
      "        [0.0114],\n",
      "        [0.0219],\n",
      "        [0.0164],\n",
      "        [0.1841],\n",
      "        [0.1309],\n",
      "        [0.1386],\n",
      "        [0.1701],\n",
      "        [0.1767],\n",
      "        [0.0316],\n",
      "        [0.1787],\n",
      "        [0.0452],\n",
      "        [0.0482],\n",
      "        [0.0544],\n",
      "        [0.1179],\n",
      "        [0.0694],\n",
      "        [0.1374],\n",
      "        [0.2020],\n",
      "        [0.0854],\n",
      "        [0.0912],\n",
      "        [0.0945],\n",
      "        [0.0998],\n",
      "        [0.1038],\n",
      "        [0.1079],\n",
      "        [0.1116],\n",
      "        [0.1161],\n",
      "        [0.1219],\n",
      "        [0.1704],\n",
      "        [0.2014],\n",
      "        [0.1458],\n",
      "        [0.1541],\n",
      "        [0.1689],\n",
      "        [0.1693],\n",
      "        [0.1775],\n",
      "        [0.1810],\n",
      "        [0.1820],\n",
      "        [0.1853],\n",
      "        [0.1947],\n",
      "        [0.1992],\n",
      "        [0.1550],\n",
      "        [0.0420],\n",
      "        [0.1613],\n",
      "        [0.0445],\n",
      "        [0.1572],\n",
      "        [0.0636],\n",
      "        [0.0655],\n",
      "        [0.0763],\n",
      "        [0.0853],\n",
      "        [0.1223],\n",
      "        [0.0901],\n",
      "        [0.0376],\n",
      "        [0.0375],\n",
      "        [0.0044],\n",
      "        [0.0186],\n",
      "        [0.0942],\n",
      "        [0.1213],\n",
      "        [0.1263],\n",
      "        [0.1541],\n",
      "        [0.1551],\n",
      "        [0.1675],\n",
      "        [0.1694],\n",
      "        [0.1703],\n",
      "        [0.1815],\n",
      "        [0.1647],\n",
      "        [0.1911],\n",
      "        [0.1765],\n",
      "        [0.1488],\n",
      "        [0.1908],\n",
      "        [0.2074],\n",
      "        [0.2092],\n",
      "        [0.0519],\n",
      "        [0.1186],\n",
      "        [0.0525],\n",
      "        [0.0374],\n",
      "        [0.1014],\n",
      "        [0.1073],\n",
      "        [0.1088],\n",
      "        [0.1260],\n",
      "        [0.1389],\n",
      "        [0.1390],\n",
      "        [0.1396],\n",
      "        [0.1506],\n",
      "        [0.1527],\n",
      "        [0.1565],\n",
      "        [0.1569],\n",
      "        [0.1616],\n",
      "        [0.1620],\n",
      "        [0.1742],\n",
      "        [0.1383],\n",
      "        [0.0517],\n",
      "        [0.1030],\n",
      "        [0.0930],\n",
      "        [0.0903],\n",
      "        [0.1145],\n",
      "        [0.1209],\n",
      "        [0.1627],\n",
      "        [0.1350],\n",
      "        [0.0819],\n",
      "        [0.1793],\n",
      "        [0.1810]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 24.419057369232178\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 116\n",
      "剩餘X 資料 torch.Size([44, 18])\n",
      "剩餘Y 資料 torch.Size([44, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.033144738525152206, 15)\n",
      "The second_loss value of k: (0.03507493808865547, 26)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.7056])\n",
      "目前模型的Data狀態 torch.Size([116, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.7980],\n",
      "        [0.8877]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0059],\n",
      "        [0.0155],\n",
      "        [0.0441],\n",
      "        [0.0467],\n",
      "        [0.0909],\n",
      "        [0.0553],\n",
      "        [0.0567],\n",
      "        [0.0474],\n",
      "        [0.0052],\n",
      "        [0.0085],\n",
      "        [0.0335],\n",
      "        [0.0056],\n",
      "        [0.0512],\n",
      "        [0.0315],\n",
      "        [0.0516],\n",
      "        [0.0062],\n",
      "        [0.0114],\n",
      "        [0.0219],\n",
      "        [0.0164],\n",
      "        [0.1841],\n",
      "        [0.1309],\n",
      "        [0.1386],\n",
      "        [0.1701],\n",
      "        [0.1767],\n",
      "        [0.0316],\n",
      "        [0.1787],\n",
      "        [0.0452],\n",
      "        [0.0482],\n",
      "        [0.0544],\n",
      "        [0.1179],\n",
      "        [0.0694],\n",
      "        [0.1374],\n",
      "        [0.2020],\n",
      "        [0.0854],\n",
      "        [0.0912],\n",
      "        [0.0945],\n",
      "        [0.0998],\n",
      "        [0.1038],\n",
      "        [0.1079],\n",
      "        [0.1116],\n",
      "        [0.1161],\n",
      "        [0.1219],\n",
      "        [0.1704],\n",
      "        [0.2014],\n",
      "        [0.1458],\n",
      "        [0.1541],\n",
      "        [0.1689],\n",
      "        [0.1693],\n",
      "        [0.1775],\n",
      "        [0.1810],\n",
      "        [0.1820],\n",
      "        [0.1853],\n",
      "        [0.1947],\n",
      "        [0.1992],\n",
      "        [0.1550],\n",
      "        [0.0420],\n",
      "        [0.1613],\n",
      "        [0.0445],\n",
      "        [0.1572],\n",
      "        [0.0636],\n",
      "        [0.0655],\n",
      "        [0.0763],\n",
      "        [0.0853],\n",
      "        [0.1223],\n",
      "        [0.0901],\n",
      "        [0.0376],\n",
      "        [0.0375],\n",
      "        [0.0044],\n",
      "        [0.0186],\n",
      "        [0.0942],\n",
      "        [0.1213],\n",
      "        [0.1263],\n",
      "        [0.1541],\n",
      "        [0.1551],\n",
      "        [0.1675],\n",
      "        [0.1694],\n",
      "        [0.1703],\n",
      "        [0.1815],\n",
      "        [0.1647],\n",
      "        [0.1911],\n",
      "        [0.1765],\n",
      "        [0.1488],\n",
      "        [0.1908],\n",
      "        [0.2074],\n",
      "        [0.2092],\n",
      "        [0.0519],\n",
      "        [0.1186],\n",
      "        [0.0525],\n",
      "        [0.0374],\n",
      "        [0.1014],\n",
      "        [0.1073],\n",
      "        [0.1088],\n",
      "        [0.1260],\n",
      "        [0.1389],\n",
      "        [0.1390],\n",
      "        [0.1396],\n",
      "        [0.1506],\n",
      "        [0.1527],\n",
      "        [0.1565],\n",
      "        [0.1569],\n",
      "        [0.1616],\n",
      "        [0.1620],\n",
      "        [0.1742],\n",
      "        [0.1383],\n",
      "        [0.0517],\n",
      "        [0.1030],\n",
      "        [0.0930],\n",
      "        [0.0903],\n",
      "        [0.1145],\n",
      "        [0.1209],\n",
      "        [0.1627],\n",
      "        [0.1350],\n",
      "        [0.0819],\n",
      "        [0.1793],\n",
      "        [0.1810],\n",
      "        [0.1821]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 71\n",
      "Number of shrink: 29\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0067],\n",
      "        [0.0163],\n",
      "        [0.0449],\n",
      "        [0.0475],\n",
      "        [0.0917],\n",
      "        [0.0561],\n",
      "        [0.0575],\n",
      "        [0.0482],\n",
      "        [0.0044],\n",
      "        [0.0093],\n",
      "        [0.0343],\n",
      "        [0.0064],\n",
      "        [0.0520],\n",
      "        [0.0323],\n",
      "        [0.0524],\n",
      "        [0.0070],\n",
      "        [0.0122],\n",
      "        [0.0227],\n",
      "        [0.0172],\n",
      "        [0.1849],\n",
      "        [0.1317],\n",
      "        [0.1394],\n",
      "        [0.1709],\n",
      "        [0.1775],\n",
      "        [0.0324],\n",
      "        [0.1795],\n",
      "        [0.0461],\n",
      "        [0.0490],\n",
      "        [0.0552],\n",
      "        [0.1187],\n",
      "        [0.0702],\n",
      "        [0.1382],\n",
      "        [0.2028],\n",
      "        [0.0862],\n",
      "        [0.0920],\n",
      "        [0.0953],\n",
      "        [0.1006],\n",
      "        [0.1046],\n",
      "        [0.1087],\n",
      "        [0.1124],\n",
      "        [0.1169],\n",
      "        [0.1227],\n",
      "        [0.1712],\n",
      "        [0.2022],\n",
      "        [0.1466],\n",
      "        [0.1549],\n",
      "        [0.1697],\n",
      "        [0.1701],\n",
      "        [0.1783],\n",
      "        [0.1818],\n",
      "        [0.1828],\n",
      "        [0.1861],\n",
      "        [0.1955],\n",
      "        [0.2000],\n",
      "        [0.1558],\n",
      "        [0.0428],\n",
      "        [0.1622],\n",
      "        [0.0453],\n",
      "        [0.1580],\n",
      "        [0.0644],\n",
      "        [0.0663],\n",
      "        [0.0771],\n",
      "        [0.0861],\n",
      "        [0.1231],\n",
      "        [0.0893],\n",
      "        [0.0384],\n",
      "        [0.0383],\n",
      "        [0.0036],\n",
      "        [0.0194],\n",
      "        [0.0934],\n",
      "        [0.1205],\n",
      "        [0.1255],\n",
      "        [0.1533],\n",
      "        [0.1543],\n",
      "        [0.1667],\n",
      "        [0.1686],\n",
      "        [0.1695],\n",
      "        [0.1807],\n",
      "        [0.1639],\n",
      "        [0.1903],\n",
      "        [0.1756],\n",
      "        [0.1480],\n",
      "        [0.1900],\n",
      "        [0.2066],\n",
      "        [0.2084],\n",
      "        [0.0511],\n",
      "        [0.1178],\n",
      "        [0.0517],\n",
      "        [0.0366],\n",
      "        [0.1006],\n",
      "        [0.1065],\n",
      "        [0.1080],\n",
      "        [0.1252],\n",
      "        [0.1381],\n",
      "        [0.1382],\n",
      "        [0.1387],\n",
      "        [0.1498],\n",
      "        [0.1518],\n",
      "        [0.1557],\n",
      "        [0.1561],\n",
      "        [0.1608],\n",
      "        [0.1612],\n",
      "        [0.1734],\n",
      "        [0.1375],\n",
      "        [0.0509],\n",
      "        [0.1022],\n",
      "        [0.0922],\n",
      "        [0.0895],\n",
      "        [0.1137],\n",
      "        [0.1201],\n",
      "        [0.1619],\n",
      "        [0.1342],\n",
      "        [0.0811],\n",
      "        [0.1785],\n",
      "        [0.1801],\n",
      "        [0.0916]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 24.694546937942505\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 117\n",
      "剩餘X 資料 torch.Size([43, 18])\n",
      "剩餘Y 資料 torch.Size([43, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.014457679353654385, 1)\n",
      "The second_loss value of k: (0.026810098439455032, 5)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.7218])\n",
      "目前模型的Data狀態 torch.Size([117, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.7972],\n",
      "        [0.8420]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0067],\n",
      "        [0.0163],\n",
      "        [0.0449],\n",
      "        [0.0475],\n",
      "        [0.0917],\n",
      "        [0.0561],\n",
      "        [0.0575],\n",
      "        [0.0482],\n",
      "        [0.0044],\n",
      "        [0.0093],\n",
      "        [0.0343],\n",
      "        [0.0064],\n",
      "        [0.0520],\n",
      "        [0.0323],\n",
      "        [0.0524],\n",
      "        [0.0070],\n",
      "        [0.0122],\n",
      "        [0.0227],\n",
      "        [0.0172],\n",
      "        [0.1849],\n",
      "        [0.1317],\n",
      "        [0.1394],\n",
      "        [0.1709],\n",
      "        [0.1775],\n",
      "        [0.0324],\n",
      "        [0.1795],\n",
      "        [0.0461],\n",
      "        [0.0490],\n",
      "        [0.0552],\n",
      "        [0.1187],\n",
      "        [0.0702],\n",
      "        [0.1382],\n",
      "        [0.2028],\n",
      "        [0.0862],\n",
      "        [0.0920],\n",
      "        [0.0953],\n",
      "        [0.1006],\n",
      "        [0.1046],\n",
      "        [0.1087],\n",
      "        [0.1124],\n",
      "        [0.1169],\n",
      "        [0.1227],\n",
      "        [0.1712],\n",
      "        [0.2022],\n",
      "        [0.1466],\n",
      "        [0.1549],\n",
      "        [0.1697],\n",
      "        [0.1701],\n",
      "        [0.1783],\n",
      "        [0.1818],\n",
      "        [0.1828],\n",
      "        [0.1861],\n",
      "        [0.1955],\n",
      "        [0.2000],\n",
      "        [0.1558],\n",
      "        [0.0428],\n",
      "        [0.1622],\n",
      "        [0.0453],\n",
      "        [0.1580],\n",
      "        [0.0644],\n",
      "        [0.0663],\n",
      "        [0.0771],\n",
      "        [0.0861],\n",
      "        [0.1231],\n",
      "        [0.0893],\n",
      "        [0.0384],\n",
      "        [0.0383],\n",
      "        [0.0036],\n",
      "        [0.0194],\n",
      "        [0.0934],\n",
      "        [0.1205],\n",
      "        [0.1255],\n",
      "        [0.1533],\n",
      "        [0.1543],\n",
      "        [0.1667],\n",
      "        [0.1686],\n",
      "        [0.1695],\n",
      "        [0.1807],\n",
      "        [0.1639],\n",
      "        [0.1903],\n",
      "        [0.1756],\n",
      "        [0.1480],\n",
      "        [0.1900],\n",
      "        [0.2066],\n",
      "        [0.2084],\n",
      "        [0.0511],\n",
      "        [0.1178],\n",
      "        [0.0517],\n",
      "        [0.0366],\n",
      "        [0.1006],\n",
      "        [0.1065],\n",
      "        [0.1080],\n",
      "        [0.1252],\n",
      "        [0.1381],\n",
      "        [0.1382],\n",
      "        [0.1387],\n",
      "        [0.1498],\n",
      "        [0.1518],\n",
      "        [0.1557],\n",
      "        [0.1561],\n",
      "        [0.1608],\n",
      "        [0.1612],\n",
      "        [0.1734],\n",
      "        [0.1375],\n",
      "        [0.0509],\n",
      "        [0.1022],\n",
      "        [0.0922],\n",
      "        [0.0895],\n",
      "        [0.1137],\n",
      "        [0.1201],\n",
      "        [0.1619],\n",
      "        [0.1342],\n",
      "        [0.0811],\n",
      "        [0.1785],\n",
      "        [0.1801],\n",
      "        [0.0916],\n",
      "        [0.1202]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0074],\n",
      "        [0.0170],\n",
      "        [0.0456],\n",
      "        [0.0481],\n",
      "        [0.0924],\n",
      "        [0.0568],\n",
      "        [0.0582],\n",
      "        [0.0489],\n",
      "        [0.0037],\n",
      "        [0.0100],\n",
      "        [0.0349],\n",
      "        [0.0071],\n",
      "        [0.0526],\n",
      "        [0.0330],\n",
      "        [0.0530],\n",
      "        [0.0077],\n",
      "        [0.0129],\n",
      "        [0.0234],\n",
      "        [0.0179],\n",
      "        [0.1855],\n",
      "        [0.1324],\n",
      "        [0.1401],\n",
      "        [0.1716],\n",
      "        [0.1782],\n",
      "        [0.0331],\n",
      "        [0.1801],\n",
      "        [0.0467],\n",
      "        [0.0497],\n",
      "        [0.0558],\n",
      "        [0.1193],\n",
      "        [0.0708],\n",
      "        [0.1389],\n",
      "        [0.2034],\n",
      "        [0.0869],\n",
      "        [0.0926],\n",
      "        [0.0960],\n",
      "        [0.1013],\n",
      "        [0.1053],\n",
      "        [0.1093],\n",
      "        [0.1130],\n",
      "        [0.1175],\n",
      "        [0.1234],\n",
      "        [0.1719],\n",
      "        [0.2028],\n",
      "        [0.1472],\n",
      "        [0.1556],\n",
      "        [0.1704],\n",
      "        [0.1707],\n",
      "        [0.1789],\n",
      "        [0.1825],\n",
      "        [0.1834],\n",
      "        [0.1868],\n",
      "        [0.1962],\n",
      "        [0.2007],\n",
      "        [0.1564],\n",
      "        [0.0434],\n",
      "        [0.1628],\n",
      "        [0.0460],\n",
      "        [0.1587],\n",
      "        [0.0651],\n",
      "        [0.0670],\n",
      "        [0.0778],\n",
      "        [0.0867],\n",
      "        [0.1237],\n",
      "        [0.0887],\n",
      "        [0.0391],\n",
      "        [0.0389],\n",
      "        [0.0030],\n",
      "        [0.0201],\n",
      "        [0.0927],\n",
      "        [0.1198],\n",
      "        [0.1249],\n",
      "        [0.1526],\n",
      "        [0.1536],\n",
      "        [0.1660],\n",
      "        [0.1679],\n",
      "        [0.1688],\n",
      "        [0.1800],\n",
      "        [0.1632],\n",
      "        [0.1896],\n",
      "        [0.1750],\n",
      "        [0.1473],\n",
      "        [0.1893],\n",
      "        [0.2059],\n",
      "        [0.2077],\n",
      "        [0.0504],\n",
      "        [0.1171],\n",
      "        [0.0511],\n",
      "        [0.0360],\n",
      "        [0.1000],\n",
      "        [0.1059],\n",
      "        [0.1073],\n",
      "        [0.1245],\n",
      "        [0.1374],\n",
      "        [0.1376],\n",
      "        [0.1381],\n",
      "        [0.1491],\n",
      "        [0.1512],\n",
      "        [0.1550],\n",
      "        [0.1554],\n",
      "        [0.1602],\n",
      "        [0.1606],\n",
      "        [0.1727],\n",
      "        [0.1368],\n",
      "        [0.0503],\n",
      "        [0.1016],\n",
      "        [0.0916],\n",
      "        [0.0888],\n",
      "        [0.1130],\n",
      "        [0.1194],\n",
      "        [0.1613],\n",
      "        [0.1336],\n",
      "        [0.0805],\n",
      "        [0.1779],\n",
      "        [0.1795],\n",
      "        [0.0909],\n",
      "        [0.0748]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 24.93659734725952\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 118\n",
      "剩餘X 資料 torch.Size([42, 18])\n",
      "剩餘Y 資料 torch.Size([42, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.016750195994973183, 0)\n",
      "The second_loss value of k: (0.02659923955798149, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.7586])\n",
      "目前模型的Data狀態 torch.Size([118, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.7966],\n",
      "        [0.8880]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0074],\n",
      "        [0.0170],\n",
      "        [0.0456],\n",
      "        [0.0481],\n",
      "        [0.0924],\n",
      "        [0.0568],\n",
      "        [0.0582],\n",
      "        [0.0489],\n",
      "        [0.0037],\n",
      "        [0.0100],\n",
      "        [0.0349],\n",
      "        [0.0071],\n",
      "        [0.0526],\n",
      "        [0.0330],\n",
      "        [0.0530],\n",
      "        [0.0077],\n",
      "        [0.0129],\n",
      "        [0.0234],\n",
      "        [0.0179],\n",
      "        [0.1855],\n",
      "        [0.1324],\n",
      "        [0.1401],\n",
      "        [0.1716],\n",
      "        [0.1782],\n",
      "        [0.0331],\n",
      "        [0.1801],\n",
      "        [0.0467],\n",
      "        [0.0497],\n",
      "        [0.0558],\n",
      "        [0.1193],\n",
      "        [0.0708],\n",
      "        [0.1389],\n",
      "        [0.2034],\n",
      "        [0.0869],\n",
      "        [0.0926],\n",
      "        [0.0960],\n",
      "        [0.1013],\n",
      "        [0.1053],\n",
      "        [0.1093],\n",
      "        [0.1130],\n",
      "        [0.1175],\n",
      "        [0.1234],\n",
      "        [0.1719],\n",
      "        [0.2028],\n",
      "        [0.1472],\n",
      "        [0.1556],\n",
      "        [0.1704],\n",
      "        [0.1707],\n",
      "        [0.1789],\n",
      "        [0.1825],\n",
      "        [0.1834],\n",
      "        [0.1868],\n",
      "        [0.1962],\n",
      "        [0.2007],\n",
      "        [0.1564],\n",
      "        [0.0434],\n",
      "        [0.1628],\n",
      "        [0.0460],\n",
      "        [0.1587],\n",
      "        [0.0651],\n",
      "        [0.0670],\n",
      "        [0.0778],\n",
      "        [0.0867],\n",
      "        [0.1237],\n",
      "        [0.0887],\n",
      "        [0.0391],\n",
      "        [0.0389],\n",
      "        [0.0030],\n",
      "        [0.0201],\n",
      "        [0.0927],\n",
      "        [0.1198],\n",
      "        [0.1249],\n",
      "        [0.1526],\n",
      "        [0.1536],\n",
      "        [0.1660],\n",
      "        [0.1679],\n",
      "        [0.1688],\n",
      "        [0.1800],\n",
      "        [0.1632],\n",
      "        [0.1896],\n",
      "        [0.1750],\n",
      "        [0.1473],\n",
      "        [0.1893],\n",
      "        [0.2059],\n",
      "        [0.2077],\n",
      "        [0.0504],\n",
      "        [0.1171],\n",
      "        [0.0511],\n",
      "        [0.0360],\n",
      "        [0.1000],\n",
      "        [0.1059],\n",
      "        [0.1073],\n",
      "        [0.1245],\n",
      "        [0.1374],\n",
      "        [0.1376],\n",
      "        [0.1381],\n",
      "        [0.1491],\n",
      "        [0.1512],\n",
      "        [0.1550],\n",
      "        [0.1554],\n",
      "        [0.1602],\n",
      "        [0.1606],\n",
      "        [0.1727],\n",
      "        [0.1368],\n",
      "        [0.0503],\n",
      "        [0.1016],\n",
      "        [0.0916],\n",
      "        [0.0888],\n",
      "        [0.1130],\n",
      "        [0.1194],\n",
      "        [0.1613],\n",
      "        [0.1336],\n",
      "        [0.0805],\n",
      "        [0.1779],\n",
      "        [0.1795],\n",
      "        [0.0909],\n",
      "        [0.0748],\n",
      "        [0.1294]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0077],\n",
      "        [0.0173],\n",
      "        [0.0459],\n",
      "        [0.0484],\n",
      "        [0.0927],\n",
      "        [0.0571],\n",
      "        [0.0585],\n",
      "        [0.0492],\n",
      "        [0.0034],\n",
      "        [0.0103],\n",
      "        [0.0353],\n",
      "        [0.0074],\n",
      "        [0.0530],\n",
      "        [0.0333],\n",
      "        [0.0533],\n",
      "        [0.0080],\n",
      "        [0.0132],\n",
      "        [0.0237],\n",
      "        [0.0182],\n",
      "        [0.1859],\n",
      "        [0.1327],\n",
      "        [0.1404],\n",
      "        [0.1719],\n",
      "        [0.1785],\n",
      "        [0.0334],\n",
      "        [0.1804],\n",
      "        [0.0470],\n",
      "        [0.0500],\n",
      "        [0.0562],\n",
      "        [0.1197],\n",
      "        [0.0712],\n",
      "        [0.1392],\n",
      "        [0.2038],\n",
      "        [0.0872],\n",
      "        [0.0930],\n",
      "        [0.0963],\n",
      "        [0.1016],\n",
      "        [0.1056],\n",
      "        [0.1097],\n",
      "        [0.1134],\n",
      "        [0.1179],\n",
      "        [0.1237],\n",
      "        [0.1722],\n",
      "        [0.2032],\n",
      "        [0.1475],\n",
      "        [0.1559],\n",
      "        [0.1707],\n",
      "        [0.1711],\n",
      "        [0.1793],\n",
      "        [0.1828],\n",
      "        [0.1838],\n",
      "        [0.1871],\n",
      "        [0.1965],\n",
      "        [0.2010],\n",
      "        [0.1568],\n",
      "        [0.0438],\n",
      "        [0.1631],\n",
      "        [0.0463],\n",
      "        [0.1590],\n",
      "        [0.0654],\n",
      "        [0.0673],\n",
      "        [0.0781],\n",
      "        [0.0871],\n",
      "        [0.1240],\n",
      "        [0.0883],\n",
      "        [0.0394],\n",
      "        [0.0392],\n",
      "        [0.0026],\n",
      "        [0.0204],\n",
      "        [0.0924],\n",
      "        [0.1195],\n",
      "        [0.1245],\n",
      "        [0.1523],\n",
      "        [0.1533],\n",
      "        [0.1657],\n",
      "        [0.1676],\n",
      "        [0.1685],\n",
      "        [0.1797],\n",
      "        [0.1629],\n",
      "        [0.1893],\n",
      "        [0.1747],\n",
      "        [0.1470],\n",
      "        [0.1890],\n",
      "        [0.2056],\n",
      "        [0.2074],\n",
      "        [0.0501],\n",
      "        [0.1168],\n",
      "        [0.0507],\n",
      "        [0.0356],\n",
      "        [0.0997],\n",
      "        [0.1055],\n",
      "        [0.1070],\n",
      "        [0.1242],\n",
      "        [0.1371],\n",
      "        [0.1373],\n",
      "        [0.1378],\n",
      "        [0.1488],\n",
      "        [0.1509],\n",
      "        [0.1547],\n",
      "        [0.1551],\n",
      "        [0.1599],\n",
      "        [0.1602],\n",
      "        [0.1724],\n",
      "        [0.1365],\n",
      "        [0.0499],\n",
      "        [0.1012],\n",
      "        [0.0912],\n",
      "        [0.0885],\n",
      "        [0.1127],\n",
      "        [0.1191],\n",
      "        [0.1609],\n",
      "        [0.1332],\n",
      "        [0.0802],\n",
      "        [0.1775],\n",
      "        [0.1792],\n",
      "        [0.0906],\n",
      "        [0.0745],\n",
      "        [0.0376]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 25.17973780632019\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 119\n",
      "剩餘X 資料 torch.Size([41, 18])\n",
      "剩餘Y 資料 torch.Size([41, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.010987909510731697, 12)\n",
      "The second_loss value of k: (0.019863413646817207, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引12，y= tensor([0.6914])\n",
      "目前模型的Data狀態 torch.Size([119, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962],\n",
      "        [0.7962]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0077],\n",
      "        [0.0173],\n",
      "        [0.0459],\n",
      "        [0.0484],\n",
      "        [0.0927],\n",
      "        [0.0571],\n",
      "        [0.0585],\n",
      "        [0.0492],\n",
      "        [0.0034],\n",
      "        [0.0103],\n",
      "        [0.0353],\n",
      "        [0.0074],\n",
      "        [0.0530],\n",
      "        [0.0333],\n",
      "        [0.0533],\n",
      "        [0.0080],\n",
      "        [0.0132],\n",
      "        [0.0237],\n",
      "        [0.0182],\n",
      "        [0.1859],\n",
      "        [0.1327],\n",
      "        [0.1404],\n",
      "        [0.1719],\n",
      "        [0.1785],\n",
      "        [0.0334],\n",
      "        [0.1804],\n",
      "        [0.0470],\n",
      "        [0.0500],\n",
      "        [0.0562],\n",
      "        [0.1197],\n",
      "        [0.0712],\n",
      "        [0.1392],\n",
      "        [0.2038],\n",
      "        [0.0872],\n",
      "        [0.0930],\n",
      "        [0.0963],\n",
      "        [0.1016],\n",
      "        [0.1056],\n",
      "        [0.1097],\n",
      "        [0.1134],\n",
      "        [0.1179],\n",
      "        [0.1237],\n",
      "        [0.1722],\n",
      "        [0.2032],\n",
      "        [0.1475],\n",
      "        [0.1559],\n",
      "        [0.1707],\n",
      "        [0.1711],\n",
      "        [0.1793],\n",
      "        [0.1828],\n",
      "        [0.1838],\n",
      "        [0.1871],\n",
      "        [0.1965],\n",
      "        [0.2010],\n",
      "        [0.1568],\n",
      "        [0.0438],\n",
      "        [0.1631],\n",
      "        [0.0463],\n",
      "        [0.1590],\n",
      "        [0.0654],\n",
      "        [0.0673],\n",
      "        [0.0781],\n",
      "        [0.0871],\n",
      "        [0.1240],\n",
      "        [0.0883],\n",
      "        [0.0394],\n",
      "        [0.0392],\n",
      "        [0.0026],\n",
      "        [0.0204],\n",
      "        [0.0924],\n",
      "        [0.1195],\n",
      "        [0.1245],\n",
      "        [0.1523],\n",
      "        [0.1533],\n",
      "        [0.1657],\n",
      "        [0.1676],\n",
      "        [0.1685],\n",
      "        [0.1797],\n",
      "        [0.1629],\n",
      "        [0.1893],\n",
      "        [0.1747],\n",
      "        [0.1470],\n",
      "        [0.1890],\n",
      "        [0.2056],\n",
      "        [0.2074],\n",
      "        [0.0501],\n",
      "        [0.1168],\n",
      "        [0.0507],\n",
      "        [0.0356],\n",
      "        [0.0997],\n",
      "        [0.1055],\n",
      "        [0.1070],\n",
      "        [0.1242],\n",
      "        [0.1371],\n",
      "        [0.1373],\n",
      "        [0.1378],\n",
      "        [0.1488],\n",
      "        [0.1509],\n",
      "        [0.1547],\n",
      "        [0.1551],\n",
      "        [0.1599],\n",
      "        [0.1602],\n",
      "        [0.1724],\n",
      "        [0.1365],\n",
      "        [0.0499],\n",
      "        [0.1012],\n",
      "        [0.0912],\n",
      "        [0.0885],\n",
      "        [0.1127],\n",
      "        [0.1191],\n",
      "        [0.1609],\n",
      "        [0.1332],\n",
      "        [0.0802],\n",
      "        [0.1775],\n",
      "        [0.1792],\n",
      "        [0.0906],\n",
      "        [0.0745],\n",
      "        [0.0376],\n",
      "        [0.1048]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0086],\n",
      "        [0.0182],\n",
      "        [0.0468],\n",
      "        [0.0493],\n",
      "        [0.0936],\n",
      "        [0.0580],\n",
      "        [0.0594],\n",
      "        [0.0501],\n",
      "        [0.0025],\n",
      "        [0.0112],\n",
      "        [0.0362],\n",
      "        [0.0083],\n",
      "        [0.0538],\n",
      "        [0.0342],\n",
      "        [0.0542],\n",
      "        [0.0089],\n",
      "        [0.0141],\n",
      "        [0.0246],\n",
      "        [0.0191],\n",
      "        [0.1867],\n",
      "        [0.1336],\n",
      "        [0.1413],\n",
      "        [0.1728],\n",
      "        [0.1794],\n",
      "        [0.0343],\n",
      "        [0.1813],\n",
      "        [0.0479],\n",
      "        [0.0509],\n",
      "        [0.0570],\n",
      "        [0.1205],\n",
      "        [0.0720],\n",
      "        [0.1401],\n",
      "        [0.2046],\n",
      "        [0.0881],\n",
      "        [0.0938],\n",
      "        [0.0972],\n",
      "        [0.1025],\n",
      "        [0.1065],\n",
      "        [0.1105],\n",
      "        [0.1142],\n",
      "        [0.1187],\n",
      "        [0.1246],\n",
      "        [0.1731],\n",
      "        [0.2040],\n",
      "        [0.1484],\n",
      "        [0.1568],\n",
      "        [0.1716],\n",
      "        [0.1720],\n",
      "        [0.1801],\n",
      "        [0.1837],\n",
      "        [0.1846],\n",
      "        [0.1880],\n",
      "        [0.1974],\n",
      "        [0.2019],\n",
      "        [0.1576],\n",
      "        [0.0447],\n",
      "        [0.1640],\n",
      "        [0.0472],\n",
      "        [0.1599],\n",
      "        [0.0663],\n",
      "        [0.0682],\n",
      "        [0.0790],\n",
      "        [0.0879],\n",
      "        [0.1249],\n",
      "        [0.0875],\n",
      "        [0.0403],\n",
      "        [0.0401],\n",
      "        [0.0018],\n",
      "        [0.0213],\n",
      "        [0.0915],\n",
      "        [0.1186],\n",
      "        [0.1237],\n",
      "        [0.1514],\n",
      "        [0.1524],\n",
      "        [0.1648],\n",
      "        [0.1667],\n",
      "        [0.1676],\n",
      "        [0.1788],\n",
      "        [0.1620],\n",
      "        [0.1884],\n",
      "        [0.1738],\n",
      "        [0.1461],\n",
      "        [0.1881],\n",
      "        [0.2047],\n",
      "        [0.2065],\n",
      "        [0.0492],\n",
      "        [0.1159],\n",
      "        [0.0499],\n",
      "        [0.0348],\n",
      "        [0.0988],\n",
      "        [0.1047],\n",
      "        [0.1061],\n",
      "        [0.1233],\n",
      "        [0.1362],\n",
      "        [0.1364],\n",
      "        [0.1369],\n",
      "        [0.1479],\n",
      "        [0.1500],\n",
      "        [0.1538],\n",
      "        [0.1542],\n",
      "        [0.1590],\n",
      "        [0.1594],\n",
      "        [0.1715],\n",
      "        [0.1356],\n",
      "        [0.0491],\n",
      "        [0.1004],\n",
      "        [0.0904],\n",
      "        [0.0876],\n",
      "        [0.1118],\n",
      "        [0.1182],\n",
      "        [0.1601],\n",
      "        [0.1324],\n",
      "        [0.0793],\n",
      "        [0.1767],\n",
      "        [0.1783],\n",
      "        [0.0897],\n",
      "        [0.0736],\n",
      "        [0.0368],\n",
      "        [0.1040]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 25.42137861251831\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 120\n",
      "剩餘X 資料 torch.Size([40, 18])\n",
      "剩餘Y 資料 torch.Size([40, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.019620679318904877, 4)\n",
      "The second_loss value of k: (0.02620878629386425, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引4，y= tensor([0.6553])\n",
      "目前模型的Data狀態 torch.Size([120, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954],\n",
      "        [0.7954]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0086],\n",
      "        [0.0182],\n",
      "        [0.0468],\n",
      "        [0.0493],\n",
      "        [0.0936],\n",
      "        [0.0580],\n",
      "        [0.0594],\n",
      "        [0.0501],\n",
      "        [0.0025],\n",
      "        [0.0112],\n",
      "        [0.0362],\n",
      "        [0.0083],\n",
      "        [0.0538],\n",
      "        [0.0342],\n",
      "        [0.0542],\n",
      "        [0.0089],\n",
      "        [0.0141],\n",
      "        [0.0246],\n",
      "        [0.0191],\n",
      "        [0.1867],\n",
      "        [0.1336],\n",
      "        [0.1413],\n",
      "        [0.1728],\n",
      "        [0.1794],\n",
      "        [0.0343],\n",
      "        [0.1813],\n",
      "        [0.0479],\n",
      "        [0.0509],\n",
      "        [0.0570],\n",
      "        [0.1205],\n",
      "        [0.0720],\n",
      "        [0.1401],\n",
      "        [0.2046],\n",
      "        [0.0881],\n",
      "        [0.0938],\n",
      "        [0.0972],\n",
      "        [0.1025],\n",
      "        [0.1065],\n",
      "        [0.1105],\n",
      "        [0.1142],\n",
      "        [0.1187],\n",
      "        [0.1246],\n",
      "        [0.1731],\n",
      "        [0.2040],\n",
      "        [0.1484],\n",
      "        [0.1568],\n",
      "        [0.1716],\n",
      "        [0.1720],\n",
      "        [0.1801],\n",
      "        [0.1837],\n",
      "        [0.1846],\n",
      "        [0.1880],\n",
      "        [0.1974],\n",
      "        [0.2019],\n",
      "        [0.1576],\n",
      "        [0.0447],\n",
      "        [0.1640],\n",
      "        [0.0472],\n",
      "        [0.1599],\n",
      "        [0.0663],\n",
      "        [0.0682],\n",
      "        [0.0790],\n",
      "        [0.0879],\n",
      "        [0.1249],\n",
      "        [0.0875],\n",
      "        [0.0403],\n",
      "        [0.0401],\n",
      "        [0.0018],\n",
      "        [0.0213],\n",
      "        [0.0915],\n",
      "        [0.1186],\n",
      "        [0.1237],\n",
      "        [0.1514],\n",
      "        [0.1524],\n",
      "        [0.1648],\n",
      "        [0.1667],\n",
      "        [0.1676],\n",
      "        [0.1788],\n",
      "        [0.1620],\n",
      "        [0.1884],\n",
      "        [0.1738],\n",
      "        [0.1461],\n",
      "        [0.1881],\n",
      "        [0.2047],\n",
      "        [0.2065],\n",
      "        [0.0492],\n",
      "        [0.1159],\n",
      "        [0.0499],\n",
      "        [0.0348],\n",
      "        [0.0988],\n",
      "        [0.1047],\n",
      "        [0.1061],\n",
      "        [0.1233],\n",
      "        [0.1362],\n",
      "        [0.1364],\n",
      "        [0.1369],\n",
      "        [0.1479],\n",
      "        [0.1500],\n",
      "        [0.1538],\n",
      "        [0.1542],\n",
      "        [0.1590],\n",
      "        [0.1594],\n",
      "        [0.1715],\n",
      "        [0.1356],\n",
      "        [0.0491],\n",
      "        [0.1004],\n",
      "        [0.0904],\n",
      "        [0.0876],\n",
      "        [0.1118],\n",
      "        [0.1182],\n",
      "        [0.1601],\n",
      "        [0.1324],\n",
      "        [0.0793],\n",
      "        [0.1767],\n",
      "        [0.1783],\n",
      "        [0.0897],\n",
      "        [0.0736],\n",
      "        [0.0368],\n",
      "        [0.1040],\n",
      "        [0.1401]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0097],\n",
      "        [0.0193],\n",
      "        [0.0479],\n",
      "        [0.0505],\n",
      "        [0.0947],\n",
      "        [0.0591],\n",
      "        [0.0606],\n",
      "        [0.0512],\n",
      "        [0.0013],\n",
      "        [0.0123],\n",
      "        [0.0373],\n",
      "        [0.0094],\n",
      "        [0.0550],\n",
      "        [0.0353],\n",
      "        [0.0554],\n",
      "        [0.0101],\n",
      "        [0.0152],\n",
      "        [0.0258],\n",
      "        [0.0202],\n",
      "        [0.1879],\n",
      "        [0.1348],\n",
      "        [0.1425],\n",
      "        [0.1740],\n",
      "        [0.1805],\n",
      "        [0.0354],\n",
      "        [0.1825],\n",
      "        [0.0491],\n",
      "        [0.0521],\n",
      "        [0.0582],\n",
      "        [0.1217],\n",
      "        [0.0732],\n",
      "        [0.1412],\n",
      "        [0.2058],\n",
      "        [0.0893],\n",
      "        [0.0950],\n",
      "        [0.0984],\n",
      "        [0.1036],\n",
      "        [0.1076],\n",
      "        [0.1117],\n",
      "        [0.1154],\n",
      "        [0.1199],\n",
      "        [0.1258],\n",
      "        [0.1743],\n",
      "        [0.2052],\n",
      "        [0.1496],\n",
      "        [0.1579],\n",
      "        [0.1727],\n",
      "        [0.1731],\n",
      "        [0.1813],\n",
      "        [0.1848],\n",
      "        [0.1858],\n",
      "        [0.1891],\n",
      "        [0.1985],\n",
      "        [0.2031],\n",
      "        [0.1588],\n",
      "        [0.0458],\n",
      "        [0.1652],\n",
      "        [0.0483],\n",
      "        [0.1610],\n",
      "        [0.0674],\n",
      "        [0.0693],\n",
      "        [0.0801],\n",
      "        [0.0891],\n",
      "        [0.1261],\n",
      "        [0.0863],\n",
      "        [0.0414],\n",
      "        [0.0413],\n",
      "        [0.0006],\n",
      "        [0.0224],\n",
      "        [0.0904],\n",
      "        [0.1175],\n",
      "        [0.1225],\n",
      "        [0.1503],\n",
      "        [0.1512],\n",
      "        [0.1637],\n",
      "        [0.1656],\n",
      "        [0.1664],\n",
      "        [0.1776],\n",
      "        [0.1609],\n",
      "        [0.1872],\n",
      "        [0.1726],\n",
      "        [0.1450],\n",
      "        [0.1870],\n",
      "        [0.2035],\n",
      "        [0.2054],\n",
      "        [0.0481],\n",
      "        [0.1148],\n",
      "        [0.0487],\n",
      "        [0.0336],\n",
      "        [0.0976],\n",
      "        [0.1035],\n",
      "        [0.1049],\n",
      "        [0.1222],\n",
      "        [0.1351],\n",
      "        [0.1352],\n",
      "        [0.1357],\n",
      "        [0.1468],\n",
      "        [0.1488],\n",
      "        [0.1527],\n",
      "        [0.1530],\n",
      "        [0.1578],\n",
      "        [0.1582],\n",
      "        [0.1704],\n",
      "        [0.1345],\n",
      "        [0.0479],\n",
      "        [0.0992],\n",
      "        [0.0892],\n",
      "        [0.0864],\n",
      "        [0.1107],\n",
      "        [0.1170],\n",
      "        [0.1589],\n",
      "        [0.1312],\n",
      "        [0.0781],\n",
      "        [0.1755],\n",
      "        [0.1771],\n",
      "        [0.0886],\n",
      "        [0.0724],\n",
      "        [0.0356],\n",
      "        [0.1028],\n",
      "        [0.1389]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 25.666173219680786\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 121\n",
      "剩餘X 資料 torch.Size([39, 18])\n",
      "剩餘Y 資料 torch.Size([39, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.02582976594567299, 3)\n",
      "The second_loss value of k: (0.028751548379659653, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.6335])\n",
      "目前模型的Data狀態 torch.Size([121, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942],\n",
      "        [0.7942]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0097],\n",
      "        [0.0193],\n",
      "        [0.0479],\n",
      "        [0.0505],\n",
      "        [0.0947],\n",
      "        [0.0591],\n",
      "        [0.0606],\n",
      "        [0.0512],\n",
      "        [0.0013],\n",
      "        [0.0123],\n",
      "        [0.0373],\n",
      "        [0.0094],\n",
      "        [0.0550],\n",
      "        [0.0353],\n",
      "        [0.0554],\n",
      "        [0.0101],\n",
      "        [0.0152],\n",
      "        [0.0258],\n",
      "        [0.0202],\n",
      "        [0.1879],\n",
      "        [0.1348],\n",
      "        [0.1425],\n",
      "        [0.1740],\n",
      "        [0.1805],\n",
      "        [0.0354],\n",
      "        [0.1825],\n",
      "        [0.0491],\n",
      "        [0.0521],\n",
      "        [0.0582],\n",
      "        [0.1217],\n",
      "        [0.0732],\n",
      "        [0.1412],\n",
      "        [0.2058],\n",
      "        [0.0893],\n",
      "        [0.0950],\n",
      "        [0.0984],\n",
      "        [0.1036],\n",
      "        [0.1076],\n",
      "        [0.1117],\n",
      "        [0.1154],\n",
      "        [0.1199],\n",
      "        [0.1258],\n",
      "        [0.1743],\n",
      "        [0.2052],\n",
      "        [0.1496],\n",
      "        [0.1579],\n",
      "        [0.1727],\n",
      "        [0.1731],\n",
      "        [0.1813],\n",
      "        [0.1848],\n",
      "        [0.1858],\n",
      "        [0.1891],\n",
      "        [0.1985],\n",
      "        [0.2031],\n",
      "        [0.1588],\n",
      "        [0.0458],\n",
      "        [0.1652],\n",
      "        [0.0483],\n",
      "        [0.1610],\n",
      "        [0.0674],\n",
      "        [0.0693],\n",
      "        [0.0801],\n",
      "        [0.0891],\n",
      "        [0.1261],\n",
      "        [0.0863],\n",
      "        [0.0414],\n",
      "        [0.0413],\n",
      "        [0.0006],\n",
      "        [0.0224],\n",
      "        [0.0904],\n",
      "        [0.1175],\n",
      "        [0.1225],\n",
      "        [0.1503],\n",
      "        [0.1512],\n",
      "        [0.1637],\n",
      "        [0.1656],\n",
      "        [0.1664],\n",
      "        [0.1776],\n",
      "        [0.1609],\n",
      "        [0.1872],\n",
      "        [0.1726],\n",
      "        [0.1450],\n",
      "        [0.1870],\n",
      "        [0.2035],\n",
      "        [0.2054],\n",
      "        [0.0481],\n",
      "        [0.1148],\n",
      "        [0.0487],\n",
      "        [0.0336],\n",
      "        [0.0976],\n",
      "        [0.1035],\n",
      "        [0.1049],\n",
      "        [0.1222],\n",
      "        [0.1351],\n",
      "        [0.1352],\n",
      "        [0.1357],\n",
      "        [0.1468],\n",
      "        [0.1488],\n",
      "        [0.1527],\n",
      "        [0.1530],\n",
      "        [0.1578],\n",
      "        [0.1582],\n",
      "        [0.1704],\n",
      "        [0.1345],\n",
      "        [0.0479],\n",
      "        [0.0992],\n",
      "        [0.0892],\n",
      "        [0.0864],\n",
      "        [0.1107],\n",
      "        [0.1170],\n",
      "        [0.1589],\n",
      "        [0.1312],\n",
      "        [0.0781],\n",
      "        [0.1755],\n",
      "        [0.1771],\n",
      "        [0.0886],\n",
      "        [0.0724],\n",
      "        [0.0356],\n",
      "        [0.1028],\n",
      "        [0.1389],\n",
      "        [0.1607]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0111],\n",
      "        [    0.0207],\n",
      "        [    0.0493],\n",
      "        [    0.0518],\n",
      "        [    0.0961],\n",
      "        [    0.0605],\n",
      "        [    0.0619],\n",
      "        [    0.0526],\n",
      "        [    0.0000],\n",
      "        [    0.0137],\n",
      "        [    0.0386],\n",
      "        [    0.0108],\n",
      "        [    0.0563],\n",
      "        [    0.0366],\n",
      "        [    0.0567],\n",
      "        [    0.0114],\n",
      "        [    0.0166],\n",
      "        [    0.0271],\n",
      "        [    0.0216],\n",
      "        [    0.1892],\n",
      "        [    0.1361],\n",
      "        [    0.1438],\n",
      "        [    0.1753],\n",
      "        [    0.1818],\n",
      "        [    0.0368],\n",
      "        [    0.1838],\n",
      "        [    0.0504],\n",
      "        [    0.0534],\n",
      "        [    0.0595],\n",
      "        [    0.1230],\n",
      "        [    0.0745],\n",
      "        [    0.1426],\n",
      "        [    0.2071],\n",
      "        [    0.0906],\n",
      "        [    0.0963],\n",
      "        [    0.0997],\n",
      "        [    0.1050],\n",
      "        [    0.1090],\n",
      "        [    0.1130],\n",
      "        [    0.1167],\n",
      "        [    0.1212],\n",
      "        [    0.1271],\n",
      "        [    0.1756],\n",
      "        [    0.2065],\n",
      "        [    0.1509],\n",
      "        [    0.1592],\n",
      "        [    0.1741],\n",
      "        [    0.1744],\n",
      "        [    0.1826],\n",
      "        [    0.1862],\n",
      "        [    0.1871],\n",
      "        [    0.1905],\n",
      "        [    0.1999],\n",
      "        [    0.2044],\n",
      "        [    0.1601],\n",
      "        [    0.0471],\n",
      "        [    0.1665],\n",
      "        [    0.0497],\n",
      "        [    0.1624],\n",
      "        [    0.0688],\n",
      "        [    0.0706],\n",
      "        [    0.0815],\n",
      "        [    0.0904],\n",
      "        [    0.1274],\n",
      "        [    0.0850],\n",
      "        [    0.0428],\n",
      "        [    0.0426],\n",
      "        [    0.0007],\n",
      "        [    0.0238],\n",
      "        [    0.0890],\n",
      "        [    0.1162],\n",
      "        [    0.1212],\n",
      "        [    0.1489],\n",
      "        [    0.1499],\n",
      "        [    0.1623],\n",
      "        [    0.1643],\n",
      "        [    0.1651],\n",
      "        [    0.1763],\n",
      "        [    0.1596],\n",
      "        [    0.1859],\n",
      "        [    0.1713],\n",
      "        [    0.1436],\n",
      "        [    0.1856],\n",
      "        [    0.2022],\n",
      "        [    0.2040],\n",
      "        [    0.0467],\n",
      "        [    0.1134],\n",
      "        [    0.0474],\n",
      "        [    0.0323],\n",
      "        [    0.0963],\n",
      "        [    0.1022],\n",
      "        [    0.1036],\n",
      "        [    0.1209],\n",
      "        [    0.1337],\n",
      "        [    0.1339],\n",
      "        [    0.1344],\n",
      "        [    0.1455],\n",
      "        [    0.1475],\n",
      "        [    0.1513],\n",
      "        [    0.1517],\n",
      "        [    0.1565],\n",
      "        [    0.1569],\n",
      "        [    0.1690],\n",
      "        [    0.1332],\n",
      "        [    0.0466],\n",
      "        [    0.0979],\n",
      "        [    0.0879],\n",
      "        [    0.0851],\n",
      "        [    0.1093],\n",
      "        [    0.1157],\n",
      "        [    0.1576],\n",
      "        [    0.1299],\n",
      "        [    0.0768],\n",
      "        [    0.1742],\n",
      "        [    0.1758],\n",
      "        [    0.0872],\n",
      "        [    0.0711],\n",
      "        [    0.0343],\n",
      "        [    0.1015],\n",
      "        [    0.1376],\n",
      "        [    0.1594]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 25.905765295028687\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 122\n",
      "剩餘X 資料 torch.Size([38, 18])\n",
      "剩餘Y 資料 torch.Size([38, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.028305422514677048, 9)\n",
      "The second_loss value of k: (0.029425406828522682, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.7034])\n",
      "目前模型的Data狀態 torch.Size([122, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.7929],\n",
      "        [0.8716]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0111],\n",
      "        [    0.0207],\n",
      "        [    0.0493],\n",
      "        [    0.0518],\n",
      "        [    0.0961],\n",
      "        [    0.0605],\n",
      "        [    0.0619],\n",
      "        [    0.0526],\n",
      "        [    0.0000],\n",
      "        [    0.0137],\n",
      "        [    0.0386],\n",
      "        [    0.0108],\n",
      "        [    0.0563],\n",
      "        [    0.0366],\n",
      "        [    0.0567],\n",
      "        [    0.0114],\n",
      "        [    0.0166],\n",
      "        [    0.0271],\n",
      "        [    0.0216],\n",
      "        [    0.1892],\n",
      "        [    0.1361],\n",
      "        [    0.1438],\n",
      "        [    0.1753],\n",
      "        [    0.1818],\n",
      "        [    0.0368],\n",
      "        [    0.1838],\n",
      "        [    0.0504],\n",
      "        [    0.0534],\n",
      "        [    0.0595],\n",
      "        [    0.1230],\n",
      "        [    0.0745],\n",
      "        [    0.1426],\n",
      "        [    0.2071],\n",
      "        [    0.0906],\n",
      "        [    0.0963],\n",
      "        [    0.0997],\n",
      "        [    0.1050],\n",
      "        [    0.1090],\n",
      "        [    0.1130],\n",
      "        [    0.1167],\n",
      "        [    0.1212],\n",
      "        [    0.1271],\n",
      "        [    0.1756],\n",
      "        [    0.2065],\n",
      "        [    0.1509],\n",
      "        [    0.1592],\n",
      "        [    0.1741],\n",
      "        [    0.1744],\n",
      "        [    0.1826],\n",
      "        [    0.1862],\n",
      "        [    0.1871],\n",
      "        [    0.1905],\n",
      "        [    0.1999],\n",
      "        [    0.2044],\n",
      "        [    0.1601],\n",
      "        [    0.0471],\n",
      "        [    0.1665],\n",
      "        [    0.0497],\n",
      "        [    0.1624],\n",
      "        [    0.0688],\n",
      "        [    0.0706],\n",
      "        [    0.0815],\n",
      "        [    0.0904],\n",
      "        [    0.1274],\n",
      "        [    0.0850],\n",
      "        [    0.0428],\n",
      "        [    0.0426],\n",
      "        [    0.0007],\n",
      "        [    0.0238],\n",
      "        [    0.0890],\n",
      "        [    0.1162],\n",
      "        [    0.1212],\n",
      "        [    0.1489],\n",
      "        [    0.1499],\n",
      "        [    0.1623],\n",
      "        [    0.1643],\n",
      "        [    0.1651],\n",
      "        [    0.1763],\n",
      "        [    0.1596],\n",
      "        [    0.1859],\n",
      "        [    0.1713],\n",
      "        [    0.1436],\n",
      "        [    0.1856],\n",
      "        [    0.2022],\n",
      "        [    0.2040],\n",
      "        [    0.0467],\n",
      "        [    0.1134],\n",
      "        [    0.0474],\n",
      "        [    0.0323],\n",
      "        [    0.0963],\n",
      "        [    0.1022],\n",
      "        [    0.1036],\n",
      "        [    0.1209],\n",
      "        [    0.1337],\n",
      "        [    0.1339],\n",
      "        [    0.1344],\n",
      "        [    0.1455],\n",
      "        [    0.1475],\n",
      "        [    0.1513],\n",
      "        [    0.1517],\n",
      "        [    0.1565],\n",
      "        [    0.1569],\n",
      "        [    0.1690],\n",
      "        [    0.1332],\n",
      "        [    0.0466],\n",
      "        [    0.0979],\n",
      "        [    0.0879],\n",
      "        [    0.0851],\n",
      "        [    0.1093],\n",
      "        [    0.1157],\n",
      "        [    0.1576],\n",
      "        [    0.1299],\n",
      "        [    0.0768],\n",
      "        [    0.1742],\n",
      "        [    0.1758],\n",
      "        [    0.0872],\n",
      "        [    0.0711],\n",
      "        [    0.0343],\n",
      "        [    0.1015],\n",
      "        [    0.1376],\n",
      "        [    0.1594],\n",
      "        [    0.1682]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 80\n",
      "Number of shrink: 20\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0118],\n",
      "        [0.0214],\n",
      "        [0.0500],\n",
      "        [0.0525],\n",
      "        [0.0968],\n",
      "        [0.0612],\n",
      "        [0.0626],\n",
      "        [0.0533],\n",
      "        [0.0007],\n",
      "        [0.0144],\n",
      "        [0.0394],\n",
      "        [0.0115],\n",
      "        [0.0570],\n",
      "        [0.0374],\n",
      "        [0.0574],\n",
      "        [0.0121],\n",
      "        [0.0173],\n",
      "        [0.0278],\n",
      "        [0.0223],\n",
      "        [0.1900],\n",
      "        [0.1368],\n",
      "        [0.1445],\n",
      "        [0.1760],\n",
      "        [0.1826],\n",
      "        [0.0375],\n",
      "        [0.1845],\n",
      "        [0.0511],\n",
      "        [0.0541],\n",
      "        [0.0603],\n",
      "        [0.1238],\n",
      "        [0.0753],\n",
      "        [0.1433],\n",
      "        [0.2078],\n",
      "        [0.0913],\n",
      "        [0.0971],\n",
      "        [0.1004],\n",
      "        [0.1057],\n",
      "        [0.1097],\n",
      "        [0.1138],\n",
      "        [0.1174],\n",
      "        [0.1219],\n",
      "        [0.1278],\n",
      "        [0.1763],\n",
      "        [0.2073],\n",
      "        [0.1516],\n",
      "        [0.1600],\n",
      "        [0.1748],\n",
      "        [0.1752],\n",
      "        [0.1834],\n",
      "        [0.1869],\n",
      "        [0.1879],\n",
      "        [0.1912],\n",
      "        [0.2006],\n",
      "        [0.2051],\n",
      "        [0.1608],\n",
      "        [0.0479],\n",
      "        [0.1672],\n",
      "        [0.0504],\n",
      "        [0.1631],\n",
      "        [0.0695],\n",
      "        [0.0714],\n",
      "        [0.0822],\n",
      "        [0.0912],\n",
      "        [0.1281],\n",
      "        [0.0842],\n",
      "        [0.0435],\n",
      "        [0.0433],\n",
      "        [0.0015],\n",
      "        [0.0245],\n",
      "        [0.0883],\n",
      "        [0.1154],\n",
      "        [0.1204],\n",
      "        [0.1482],\n",
      "        [0.1492],\n",
      "        [0.1616],\n",
      "        [0.1635],\n",
      "        [0.1644],\n",
      "        [0.1756],\n",
      "        [0.1588],\n",
      "        [0.1852],\n",
      "        [0.1706],\n",
      "        [0.1429],\n",
      "        [0.1849],\n",
      "        [0.2015],\n",
      "        [0.2033],\n",
      "        [0.0460],\n",
      "        [0.1127],\n",
      "        [0.0466],\n",
      "        [0.0316],\n",
      "        [0.0956],\n",
      "        [0.1014],\n",
      "        [0.1029],\n",
      "        [0.1201],\n",
      "        [0.1330],\n",
      "        [0.1332],\n",
      "        [0.1337],\n",
      "        [0.1447],\n",
      "        [0.1468],\n",
      "        [0.1506],\n",
      "        [0.1510],\n",
      "        [0.1558],\n",
      "        [0.1562],\n",
      "        [0.1683],\n",
      "        [0.1324],\n",
      "        [0.0459],\n",
      "        [0.0971],\n",
      "        [0.0871],\n",
      "        [0.0844],\n",
      "        [0.1086],\n",
      "        [0.1150],\n",
      "        [0.1568],\n",
      "        [0.1291],\n",
      "        [0.0761],\n",
      "        [0.1734],\n",
      "        [0.1751],\n",
      "        [0.0865],\n",
      "        [0.0704],\n",
      "        [0.0336],\n",
      "        [0.1007],\n",
      "        [0.1368],\n",
      "        [0.1587],\n",
      "        [0.0888]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 26.141737461090088\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 123\n",
      "剩餘X 資料 torch.Size([37, 18])\n",
      "剩餘Y 資料 torch.Size([37, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.02841630019247532, 3)\n",
      "The second_loss value of k: (0.029174014925956726, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.6236])\n",
      "目前模型的Data狀態 torch.Size([123, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922],\n",
      "        [0.7922]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0118],\n",
      "        [0.0214],\n",
      "        [0.0500],\n",
      "        [0.0525],\n",
      "        [0.0968],\n",
      "        [0.0612],\n",
      "        [0.0626],\n",
      "        [0.0533],\n",
      "        [0.0007],\n",
      "        [0.0144],\n",
      "        [0.0394],\n",
      "        [0.0115],\n",
      "        [0.0570],\n",
      "        [0.0374],\n",
      "        [0.0574],\n",
      "        [0.0121],\n",
      "        [0.0173],\n",
      "        [0.0278],\n",
      "        [0.0223],\n",
      "        [0.1900],\n",
      "        [0.1368],\n",
      "        [0.1445],\n",
      "        [0.1760],\n",
      "        [0.1826],\n",
      "        [0.0375],\n",
      "        [0.1845],\n",
      "        [0.0511],\n",
      "        [0.0541],\n",
      "        [0.0603],\n",
      "        [0.1238],\n",
      "        [0.0753],\n",
      "        [0.1433],\n",
      "        [0.2078],\n",
      "        [0.0913],\n",
      "        [0.0971],\n",
      "        [0.1004],\n",
      "        [0.1057],\n",
      "        [0.1097],\n",
      "        [0.1138],\n",
      "        [0.1174],\n",
      "        [0.1219],\n",
      "        [0.1278],\n",
      "        [0.1763],\n",
      "        [0.2073],\n",
      "        [0.1516],\n",
      "        [0.1600],\n",
      "        [0.1748],\n",
      "        [0.1752],\n",
      "        [0.1834],\n",
      "        [0.1869],\n",
      "        [0.1879],\n",
      "        [0.1912],\n",
      "        [0.2006],\n",
      "        [0.2051],\n",
      "        [0.1608],\n",
      "        [0.0479],\n",
      "        [0.1672],\n",
      "        [0.0504],\n",
      "        [0.1631],\n",
      "        [0.0695],\n",
      "        [0.0714],\n",
      "        [0.0822],\n",
      "        [0.0912],\n",
      "        [0.1281],\n",
      "        [0.0842],\n",
      "        [0.0435],\n",
      "        [0.0433],\n",
      "        [0.0015],\n",
      "        [0.0245],\n",
      "        [0.0883],\n",
      "        [0.1154],\n",
      "        [0.1204],\n",
      "        [0.1482],\n",
      "        [0.1492],\n",
      "        [0.1616],\n",
      "        [0.1635],\n",
      "        [0.1644],\n",
      "        [0.1756],\n",
      "        [0.1588],\n",
      "        [0.1852],\n",
      "        [0.1706],\n",
      "        [0.1429],\n",
      "        [0.1849],\n",
      "        [0.2015],\n",
      "        [0.2033],\n",
      "        [0.0460],\n",
      "        [0.1127],\n",
      "        [0.0466],\n",
      "        [0.0316],\n",
      "        [0.0956],\n",
      "        [0.1014],\n",
      "        [0.1029],\n",
      "        [0.1201],\n",
      "        [0.1330],\n",
      "        [0.1332],\n",
      "        [0.1337],\n",
      "        [0.1447],\n",
      "        [0.1468],\n",
      "        [0.1506],\n",
      "        [0.1510],\n",
      "        [0.1558],\n",
      "        [0.1562],\n",
      "        [0.1683],\n",
      "        [0.1324],\n",
      "        [0.0459],\n",
      "        [0.0971],\n",
      "        [0.0871],\n",
      "        [0.0844],\n",
      "        [0.1086],\n",
      "        [0.1150],\n",
      "        [0.1568],\n",
      "        [0.1291],\n",
      "        [0.0761],\n",
      "        [0.1734],\n",
      "        [0.1751],\n",
      "        [0.0865],\n",
      "        [0.0704],\n",
      "        [0.0336],\n",
      "        [0.1007],\n",
      "        [0.1368],\n",
      "        [0.1587],\n",
      "        [0.0888],\n",
      "        [0.1686]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0132],\n",
      "        [0.0228],\n",
      "        [0.0514],\n",
      "        [0.0539],\n",
      "        [0.0982],\n",
      "        [0.0626],\n",
      "        [0.0640],\n",
      "        [0.0547],\n",
      "        [0.0021],\n",
      "        [0.0158],\n",
      "        [0.0408],\n",
      "        [0.0129],\n",
      "        [0.0584],\n",
      "        [0.0388],\n",
      "        [0.0588],\n",
      "        [0.0135],\n",
      "        [0.0187],\n",
      "        [0.0292],\n",
      "        [0.0237],\n",
      "        [0.1913],\n",
      "        [0.1382],\n",
      "        [0.1459],\n",
      "        [0.1774],\n",
      "        [0.1840],\n",
      "        [0.0389],\n",
      "        [0.1859],\n",
      "        [0.0525],\n",
      "        [0.0555],\n",
      "        [0.0616],\n",
      "        [0.1251],\n",
      "        [0.0766],\n",
      "        [0.1447],\n",
      "        [0.2092],\n",
      "        [0.0927],\n",
      "        [0.0985],\n",
      "        [0.1018],\n",
      "        [0.1071],\n",
      "        [0.1111],\n",
      "        [0.1151],\n",
      "        [0.1188],\n",
      "        [0.1233],\n",
      "        [0.1292],\n",
      "        [0.1777],\n",
      "        [0.2086],\n",
      "        [0.1530],\n",
      "        [0.1614],\n",
      "        [0.1762],\n",
      "        [0.1766],\n",
      "        [0.1848],\n",
      "        [0.1883],\n",
      "        [0.1893],\n",
      "        [0.1926],\n",
      "        [0.2020],\n",
      "        [0.2065],\n",
      "        [0.1622],\n",
      "        [0.0493],\n",
      "        [0.1686],\n",
      "        [0.0518],\n",
      "        [0.1645],\n",
      "        [0.0709],\n",
      "        [0.0728],\n",
      "        [0.0836],\n",
      "        [0.0925],\n",
      "        [0.1295],\n",
      "        [0.0829],\n",
      "        [0.0449],\n",
      "        [0.0447],\n",
      "        [0.0028],\n",
      "        [0.0259],\n",
      "        [0.0869],\n",
      "        [0.1140],\n",
      "        [0.1190],\n",
      "        [0.1468],\n",
      "        [0.1478],\n",
      "        [0.1602],\n",
      "        [0.1621],\n",
      "        [0.1630],\n",
      "        [0.1742],\n",
      "        [0.1574],\n",
      "        [0.1838],\n",
      "        [0.1692],\n",
      "        [0.1415],\n",
      "        [0.1835],\n",
      "        [0.2001],\n",
      "        [0.2019],\n",
      "        [0.0446],\n",
      "        [0.1113],\n",
      "        [0.0453],\n",
      "        [0.0302],\n",
      "        [0.0942],\n",
      "        [0.1001],\n",
      "        [0.1015],\n",
      "        [0.1187],\n",
      "        [0.1316],\n",
      "        [0.1318],\n",
      "        [0.1323],\n",
      "        [0.1433],\n",
      "        [0.1454],\n",
      "        [0.1492],\n",
      "        [0.1496],\n",
      "        [0.1544],\n",
      "        [0.1548],\n",
      "        [0.1669],\n",
      "        [0.1310],\n",
      "        [0.0445],\n",
      "        [0.0957],\n",
      "        [0.0858],\n",
      "        [0.0830],\n",
      "        [0.1072],\n",
      "        [0.1136],\n",
      "        [0.1554],\n",
      "        [0.1277],\n",
      "        [0.0747],\n",
      "        [0.1720],\n",
      "        [0.1737],\n",
      "        [0.0851],\n",
      "        [0.0690],\n",
      "        [0.0322],\n",
      "        [0.0993],\n",
      "        [0.1355],\n",
      "        [0.1573],\n",
      "        [0.0874],\n",
      "        [0.1672]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 26.383809804916382\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 124\n",
      "剩餘X 資料 torch.Size([36, 18])\n",
      "剩餘Y 資料 torch.Size([36, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.02870340086519718, 1)\n",
      "The second_loss value of k: (0.0311282966285944, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.6213])\n",
      "目前模型的Data狀態 torch.Size([124, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908],\n",
      "        [0.7908]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0132],\n",
      "        [0.0228],\n",
      "        [0.0514],\n",
      "        [0.0539],\n",
      "        [0.0982],\n",
      "        [0.0626],\n",
      "        [0.0640],\n",
      "        [0.0547],\n",
      "        [0.0021],\n",
      "        [0.0158],\n",
      "        [0.0408],\n",
      "        [0.0129],\n",
      "        [0.0584],\n",
      "        [0.0388],\n",
      "        [0.0588],\n",
      "        [0.0135],\n",
      "        [0.0187],\n",
      "        [0.0292],\n",
      "        [0.0237],\n",
      "        [0.1913],\n",
      "        [0.1382],\n",
      "        [0.1459],\n",
      "        [0.1774],\n",
      "        [0.1840],\n",
      "        [0.0389],\n",
      "        [0.1859],\n",
      "        [0.0525],\n",
      "        [0.0555],\n",
      "        [0.0616],\n",
      "        [0.1251],\n",
      "        [0.0766],\n",
      "        [0.1447],\n",
      "        [0.2092],\n",
      "        [0.0927],\n",
      "        [0.0985],\n",
      "        [0.1018],\n",
      "        [0.1071],\n",
      "        [0.1111],\n",
      "        [0.1151],\n",
      "        [0.1188],\n",
      "        [0.1233],\n",
      "        [0.1292],\n",
      "        [0.1777],\n",
      "        [0.2086],\n",
      "        [0.1530],\n",
      "        [0.1614],\n",
      "        [0.1762],\n",
      "        [0.1766],\n",
      "        [0.1848],\n",
      "        [0.1883],\n",
      "        [0.1893],\n",
      "        [0.1926],\n",
      "        [0.2020],\n",
      "        [0.2065],\n",
      "        [0.1622],\n",
      "        [0.0493],\n",
      "        [0.1686],\n",
      "        [0.0518],\n",
      "        [0.1645],\n",
      "        [0.0709],\n",
      "        [0.0728],\n",
      "        [0.0836],\n",
      "        [0.0925],\n",
      "        [0.1295],\n",
      "        [0.0829],\n",
      "        [0.0449],\n",
      "        [0.0447],\n",
      "        [0.0028],\n",
      "        [0.0259],\n",
      "        [0.0869],\n",
      "        [0.1140],\n",
      "        [0.1190],\n",
      "        [0.1468],\n",
      "        [0.1478],\n",
      "        [0.1602],\n",
      "        [0.1621],\n",
      "        [0.1630],\n",
      "        [0.1742],\n",
      "        [0.1574],\n",
      "        [0.1838],\n",
      "        [0.1692],\n",
      "        [0.1415],\n",
      "        [0.1835],\n",
      "        [0.2001],\n",
      "        [0.2019],\n",
      "        [0.0446],\n",
      "        [0.1113],\n",
      "        [0.0453],\n",
      "        [0.0302],\n",
      "        [0.0942],\n",
      "        [0.1001],\n",
      "        [0.1015],\n",
      "        [0.1187],\n",
      "        [0.1316],\n",
      "        [0.1318],\n",
      "        [0.1323],\n",
      "        [0.1433],\n",
      "        [0.1454],\n",
      "        [0.1492],\n",
      "        [0.1496],\n",
      "        [0.1544],\n",
      "        [0.1548],\n",
      "        [0.1669],\n",
      "        [0.1310],\n",
      "        [0.0445],\n",
      "        [0.0957],\n",
      "        [0.0858],\n",
      "        [0.0830],\n",
      "        [0.1072],\n",
      "        [0.1136],\n",
      "        [0.1554],\n",
      "        [0.1277],\n",
      "        [0.0747],\n",
      "        [0.1720],\n",
      "        [0.1737],\n",
      "        [0.0851],\n",
      "        [0.0690],\n",
      "        [0.0322],\n",
      "        [0.0993],\n",
      "        [0.1355],\n",
      "        [0.1573],\n",
      "        [0.0874],\n",
      "        [0.1672],\n",
      "        [0.1694]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0145],\n",
      "        [0.0241],\n",
      "        [0.0527],\n",
      "        [0.0553],\n",
      "        [0.0995],\n",
      "        [0.0639],\n",
      "        [0.0654],\n",
      "        [0.0560],\n",
      "        [0.0035],\n",
      "        [0.0171],\n",
      "        [0.0421],\n",
      "        [0.0142],\n",
      "        [0.0598],\n",
      "        [0.0401],\n",
      "        [0.0602],\n",
      "        [0.0149],\n",
      "        [0.0200],\n",
      "        [0.0306],\n",
      "        [0.0250],\n",
      "        [0.1927],\n",
      "        [0.1396],\n",
      "        [0.1473],\n",
      "        [0.1788],\n",
      "        [0.1853],\n",
      "        [0.0402],\n",
      "        [0.1873],\n",
      "        [0.0539],\n",
      "        [0.0569],\n",
      "        [0.0630],\n",
      "        [0.1265],\n",
      "        [0.0780],\n",
      "        [0.1460],\n",
      "        [0.2106],\n",
      "        [0.0941],\n",
      "        [0.0998],\n",
      "        [0.1032],\n",
      "        [0.1084],\n",
      "        [0.1124],\n",
      "        [0.1165],\n",
      "        [0.1202],\n",
      "        [0.1247],\n",
      "        [0.1306],\n",
      "        [0.1791],\n",
      "        [0.2100],\n",
      "        [0.1544],\n",
      "        [0.1627],\n",
      "        [0.1775],\n",
      "        [0.1779],\n",
      "        [0.1861],\n",
      "        [0.1896],\n",
      "        [0.1906],\n",
      "        [0.1939],\n",
      "        [0.2033],\n",
      "        [0.2079],\n",
      "        [0.1636],\n",
      "        [0.0506],\n",
      "        [0.1700],\n",
      "        [0.0531],\n",
      "        [0.1658],\n",
      "        [0.0722],\n",
      "        [0.0741],\n",
      "        [0.0849],\n",
      "        [0.0939],\n",
      "        [0.1309],\n",
      "        [0.0815],\n",
      "        [0.0462],\n",
      "        [0.0461],\n",
      "        [0.0042],\n",
      "        [0.0272],\n",
      "        [0.0856],\n",
      "        [0.1127],\n",
      "        [0.1177],\n",
      "        [0.1455],\n",
      "        [0.1464],\n",
      "        [0.1589],\n",
      "        [0.1608],\n",
      "        [0.1616],\n",
      "        [0.1728],\n",
      "        [0.1561],\n",
      "        [0.1824],\n",
      "        [0.1678],\n",
      "        [0.1402],\n",
      "        [0.1822],\n",
      "        [0.1987],\n",
      "        [0.2006],\n",
      "        [0.0433],\n",
      "        [0.1100],\n",
      "        [0.0439],\n",
      "        [0.0288],\n",
      "        [0.0928],\n",
      "        [0.0987],\n",
      "        [0.1001],\n",
      "        [0.1174],\n",
      "        [0.1303],\n",
      "        [0.1304],\n",
      "        [0.1309],\n",
      "        [0.1420],\n",
      "        [0.1440],\n",
      "        [0.1479],\n",
      "        [0.1482],\n",
      "        [0.1530],\n",
      "        [0.1534],\n",
      "        [0.1656],\n",
      "        [0.1297],\n",
      "        [0.0431],\n",
      "        [0.0944],\n",
      "        [0.0844],\n",
      "        [0.0816],\n",
      "        [0.1059],\n",
      "        [0.1122],\n",
      "        [0.1541],\n",
      "        [0.1264],\n",
      "        [0.0733],\n",
      "        [0.1707],\n",
      "        [0.1723],\n",
      "        [0.0838],\n",
      "        [0.0676],\n",
      "        [0.0308],\n",
      "        [0.0980],\n",
      "        [0.1341],\n",
      "        [0.1559],\n",
      "        [0.0860],\n",
      "        [0.1658],\n",
      "        [0.1681]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 26.6235568523407\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 125\n",
      "剩餘X 資料 torch.Size([35, 18])\n",
      "剩餘Y 資料 torch.Size([35, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.030649855732917786, 1)\n",
      "The second_loss value of k: (0.031917620450258255, 17)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.6143])\n",
      "目前模型的Data狀態 torch.Size([125, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894],\n",
      "        [0.7894]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0145],\n",
      "        [0.0241],\n",
      "        [0.0527],\n",
      "        [0.0553],\n",
      "        [0.0995],\n",
      "        [0.0639],\n",
      "        [0.0654],\n",
      "        [0.0560],\n",
      "        [0.0035],\n",
      "        [0.0171],\n",
      "        [0.0421],\n",
      "        [0.0142],\n",
      "        [0.0598],\n",
      "        [0.0401],\n",
      "        [0.0602],\n",
      "        [0.0149],\n",
      "        [0.0200],\n",
      "        [0.0306],\n",
      "        [0.0250],\n",
      "        [0.1927],\n",
      "        [0.1396],\n",
      "        [0.1473],\n",
      "        [0.1788],\n",
      "        [0.1853],\n",
      "        [0.0402],\n",
      "        [0.1873],\n",
      "        [0.0539],\n",
      "        [0.0569],\n",
      "        [0.0630],\n",
      "        [0.1265],\n",
      "        [0.0780],\n",
      "        [0.1460],\n",
      "        [0.2106],\n",
      "        [0.0941],\n",
      "        [0.0998],\n",
      "        [0.1032],\n",
      "        [0.1084],\n",
      "        [0.1124],\n",
      "        [0.1165],\n",
      "        [0.1202],\n",
      "        [0.1247],\n",
      "        [0.1306],\n",
      "        [0.1791],\n",
      "        [0.2100],\n",
      "        [0.1544],\n",
      "        [0.1627],\n",
      "        [0.1775],\n",
      "        [0.1779],\n",
      "        [0.1861],\n",
      "        [0.1896],\n",
      "        [0.1906],\n",
      "        [0.1939],\n",
      "        [0.2033],\n",
      "        [0.2079],\n",
      "        [0.1636],\n",
      "        [0.0506],\n",
      "        [0.1700],\n",
      "        [0.0531],\n",
      "        [0.1658],\n",
      "        [0.0722],\n",
      "        [0.0741],\n",
      "        [0.0849],\n",
      "        [0.0939],\n",
      "        [0.1309],\n",
      "        [0.0815],\n",
      "        [0.0462],\n",
      "        [0.0461],\n",
      "        [0.0042],\n",
      "        [0.0272],\n",
      "        [0.0856],\n",
      "        [0.1127],\n",
      "        [0.1177],\n",
      "        [0.1455],\n",
      "        [0.1464],\n",
      "        [0.1589],\n",
      "        [0.1608],\n",
      "        [0.1616],\n",
      "        [0.1728],\n",
      "        [0.1561],\n",
      "        [0.1824],\n",
      "        [0.1678],\n",
      "        [0.1402],\n",
      "        [0.1822],\n",
      "        [0.1987],\n",
      "        [0.2006],\n",
      "        [0.0433],\n",
      "        [0.1100],\n",
      "        [0.0439],\n",
      "        [0.0288],\n",
      "        [0.0928],\n",
      "        [0.0987],\n",
      "        [0.1001],\n",
      "        [0.1174],\n",
      "        [0.1303],\n",
      "        [0.1304],\n",
      "        [0.1309],\n",
      "        [0.1420],\n",
      "        [0.1440],\n",
      "        [0.1479],\n",
      "        [0.1482],\n",
      "        [0.1530],\n",
      "        [0.1534],\n",
      "        [0.1656],\n",
      "        [0.1297],\n",
      "        [0.0431],\n",
      "        [0.0944],\n",
      "        [0.0844],\n",
      "        [0.0816],\n",
      "        [0.1059],\n",
      "        [0.1122],\n",
      "        [0.1541],\n",
      "        [0.1264],\n",
      "        [0.0733],\n",
      "        [0.1707],\n",
      "        [0.1723],\n",
      "        [0.0838],\n",
      "        [0.0676],\n",
      "        [0.0308],\n",
      "        [0.0980],\n",
      "        [0.1341],\n",
      "        [0.1559],\n",
      "        [0.0860],\n",
      "        [0.1658],\n",
      "        [0.1681],\n",
      "        [0.1751]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0159],\n",
      "        [0.0255],\n",
      "        [0.0541],\n",
      "        [0.0567],\n",
      "        [0.1009],\n",
      "        [0.0653],\n",
      "        [0.0668],\n",
      "        [0.0574],\n",
      "        [0.0049],\n",
      "        [0.0185],\n",
      "        [0.0435],\n",
      "        [0.0156],\n",
      "        [0.0612],\n",
      "        [0.0415],\n",
      "        [0.0616],\n",
      "        [0.0163],\n",
      "        [0.0214],\n",
      "        [0.0320],\n",
      "        [0.0264],\n",
      "        [0.1941],\n",
      "        [0.1410],\n",
      "        [0.1487],\n",
      "        [0.1802],\n",
      "        [0.1867],\n",
      "        [0.0416],\n",
      "        [0.1887],\n",
      "        [0.0553],\n",
      "        [0.0583],\n",
      "        [0.0644],\n",
      "        [0.1279],\n",
      "        [0.0794],\n",
      "        [0.1474],\n",
      "        [0.2120],\n",
      "        [0.0955],\n",
      "        [0.1012],\n",
      "        [0.1046],\n",
      "        [0.1098],\n",
      "        [0.1138],\n",
      "        [0.1179],\n",
      "        [0.1216],\n",
      "        [0.1261],\n",
      "        [0.1320],\n",
      "        [0.1805],\n",
      "        [0.2114],\n",
      "        [0.1558],\n",
      "        [0.1641],\n",
      "        [0.1789],\n",
      "        [0.1793],\n",
      "        [0.1875],\n",
      "        [0.1910],\n",
      "        [0.1920],\n",
      "        [0.1953],\n",
      "        [0.2047],\n",
      "        [0.2092],\n",
      "        [0.1650],\n",
      "        [0.0520],\n",
      "        [0.1714],\n",
      "        [0.0545],\n",
      "        [0.1672],\n",
      "        [0.0736],\n",
      "        [0.0755],\n",
      "        [0.0863],\n",
      "        [0.0953],\n",
      "        [0.1323],\n",
      "        [0.0801],\n",
      "        [0.0476],\n",
      "        [0.0475],\n",
      "        [0.0056],\n",
      "        [0.0286],\n",
      "        [0.0842],\n",
      "        [0.1113],\n",
      "        [0.1163],\n",
      "        [0.1441],\n",
      "        [0.1450],\n",
      "        [0.1575],\n",
      "        [0.1594],\n",
      "        [0.1602],\n",
      "        [0.1714],\n",
      "        [0.1547],\n",
      "        [0.1810],\n",
      "        [0.1664],\n",
      "        [0.1388],\n",
      "        [0.1808],\n",
      "        [0.1973],\n",
      "        [0.1992],\n",
      "        [0.0419],\n",
      "        [0.1086],\n",
      "        [0.0425],\n",
      "        [0.0274],\n",
      "        [0.0914],\n",
      "        [0.0973],\n",
      "        [0.0987],\n",
      "        [0.1160],\n",
      "        [0.1289],\n",
      "        [0.1290],\n",
      "        [0.1295],\n",
      "        [0.1406],\n",
      "        [0.1426],\n",
      "        [0.1465],\n",
      "        [0.1468],\n",
      "        [0.1516],\n",
      "        [0.1520],\n",
      "        [0.1642],\n",
      "        [0.1283],\n",
      "        [0.0417],\n",
      "        [0.0930],\n",
      "        [0.0830],\n",
      "        [0.0802],\n",
      "        [0.1045],\n",
      "        [0.1108],\n",
      "        [0.1527],\n",
      "        [0.1250],\n",
      "        [0.0719],\n",
      "        [0.1693],\n",
      "        [0.1709],\n",
      "        [0.0824],\n",
      "        [0.0662],\n",
      "        [0.0294],\n",
      "        [0.0966],\n",
      "        [0.1327],\n",
      "        [0.1545],\n",
      "        [0.0846],\n",
      "        [0.1644],\n",
      "        [0.1667],\n",
      "        [0.1737]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 26.862608194351196\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 126\n",
      "剩餘X 資料 torch.Size([34, 18])\n",
      "剩餘Y 資料 torch.Size([34, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03141981363296509, 16)\n",
      "The second_loss value of k: (0.0315101258456707, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.6108])\n",
      "目前模型的Data狀態 torch.Size([126, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880],\n",
      "        [0.7880]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0159],\n",
      "        [0.0255],\n",
      "        [0.0541],\n",
      "        [0.0567],\n",
      "        [0.1009],\n",
      "        [0.0653],\n",
      "        [0.0668],\n",
      "        [0.0574],\n",
      "        [0.0049],\n",
      "        [0.0185],\n",
      "        [0.0435],\n",
      "        [0.0156],\n",
      "        [0.0612],\n",
      "        [0.0415],\n",
      "        [0.0616],\n",
      "        [0.0163],\n",
      "        [0.0214],\n",
      "        [0.0320],\n",
      "        [0.0264],\n",
      "        [0.1941],\n",
      "        [0.1410],\n",
      "        [0.1487],\n",
      "        [0.1802],\n",
      "        [0.1867],\n",
      "        [0.0416],\n",
      "        [0.1887],\n",
      "        [0.0553],\n",
      "        [0.0583],\n",
      "        [0.0644],\n",
      "        [0.1279],\n",
      "        [0.0794],\n",
      "        [0.1474],\n",
      "        [0.2120],\n",
      "        [0.0955],\n",
      "        [0.1012],\n",
      "        [0.1046],\n",
      "        [0.1098],\n",
      "        [0.1138],\n",
      "        [0.1179],\n",
      "        [0.1216],\n",
      "        [0.1261],\n",
      "        [0.1320],\n",
      "        [0.1805],\n",
      "        [0.2114],\n",
      "        [0.1558],\n",
      "        [0.1641],\n",
      "        [0.1789],\n",
      "        [0.1793],\n",
      "        [0.1875],\n",
      "        [0.1910],\n",
      "        [0.1920],\n",
      "        [0.1953],\n",
      "        [0.2047],\n",
      "        [0.2092],\n",
      "        [0.1650],\n",
      "        [0.0520],\n",
      "        [0.1714],\n",
      "        [0.0545],\n",
      "        [0.1672],\n",
      "        [0.0736],\n",
      "        [0.0755],\n",
      "        [0.0863],\n",
      "        [0.0953],\n",
      "        [0.1323],\n",
      "        [0.0801],\n",
      "        [0.0476],\n",
      "        [0.0475],\n",
      "        [0.0056],\n",
      "        [0.0286],\n",
      "        [0.0842],\n",
      "        [0.1113],\n",
      "        [0.1163],\n",
      "        [0.1441],\n",
      "        [0.1450],\n",
      "        [0.1575],\n",
      "        [0.1594],\n",
      "        [0.1602],\n",
      "        [0.1714],\n",
      "        [0.1547],\n",
      "        [0.1810],\n",
      "        [0.1664],\n",
      "        [0.1388],\n",
      "        [0.1808],\n",
      "        [0.1973],\n",
      "        [0.1992],\n",
      "        [0.0419],\n",
      "        [0.1086],\n",
      "        [0.0425],\n",
      "        [0.0274],\n",
      "        [0.0914],\n",
      "        [0.0973],\n",
      "        [0.0987],\n",
      "        [0.1160],\n",
      "        [0.1289],\n",
      "        [0.1290],\n",
      "        [0.1295],\n",
      "        [0.1406],\n",
      "        [0.1426],\n",
      "        [0.1465],\n",
      "        [0.1468],\n",
      "        [0.1516],\n",
      "        [0.1520],\n",
      "        [0.1642],\n",
      "        [0.1283],\n",
      "        [0.0417],\n",
      "        [0.0930],\n",
      "        [0.0830],\n",
      "        [0.0802],\n",
      "        [0.1045],\n",
      "        [0.1108],\n",
      "        [0.1527],\n",
      "        [0.1250],\n",
      "        [0.0719],\n",
      "        [0.1693],\n",
      "        [0.1709],\n",
      "        [0.0824],\n",
      "        [0.0662],\n",
      "        [0.0294],\n",
      "        [0.0966],\n",
      "        [0.1327],\n",
      "        [0.1545],\n",
      "        [0.0846],\n",
      "        [0.1644],\n",
      "        [0.1667],\n",
      "        [0.1737],\n",
      "        [0.1773]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0173],\n",
      "        [0.0269],\n",
      "        [0.0555],\n",
      "        [0.0581],\n",
      "        [0.1023],\n",
      "        [0.0667],\n",
      "        [0.0681],\n",
      "        [0.0588],\n",
      "        [0.0063],\n",
      "        [0.0199],\n",
      "        [0.0449],\n",
      "        [0.0170],\n",
      "        [0.0626],\n",
      "        [0.0429],\n",
      "        [0.0630],\n",
      "        [0.0177],\n",
      "        [0.0228],\n",
      "        [0.0334],\n",
      "        [0.0278],\n",
      "        [0.1955],\n",
      "        [0.1424],\n",
      "        [0.1501],\n",
      "        [0.1816],\n",
      "        [0.1881],\n",
      "        [0.0430],\n",
      "        [0.1901],\n",
      "        [0.0567],\n",
      "        [0.0596],\n",
      "        [0.0658],\n",
      "        [0.1293],\n",
      "        [0.0808],\n",
      "        [0.1488],\n",
      "        [0.2134],\n",
      "        [0.0969],\n",
      "        [0.1026],\n",
      "        [0.1060],\n",
      "        [0.1112],\n",
      "        [0.1152],\n",
      "        [0.1193],\n",
      "        [0.1230],\n",
      "        [0.1275],\n",
      "        [0.1334],\n",
      "        [0.1819],\n",
      "        [0.2128],\n",
      "        [0.1572],\n",
      "        [0.1655],\n",
      "        [0.1803],\n",
      "        [0.1807],\n",
      "        [0.1889],\n",
      "        [0.1924],\n",
      "        [0.1934],\n",
      "        [0.1967],\n",
      "        [0.2061],\n",
      "        [0.2106],\n",
      "        [0.1664],\n",
      "        [0.0534],\n",
      "        [0.1728],\n",
      "        [0.0559],\n",
      "        [0.1686],\n",
      "        [0.0750],\n",
      "        [0.0769],\n",
      "        [0.0877],\n",
      "        [0.0967],\n",
      "        [0.1337],\n",
      "        [0.0787],\n",
      "        [0.0490],\n",
      "        [0.0489],\n",
      "        [0.0070],\n",
      "        [0.0300],\n",
      "        [0.0828],\n",
      "        [0.1099],\n",
      "        [0.1149],\n",
      "        [0.1427],\n",
      "        [0.1436],\n",
      "        [0.1561],\n",
      "        [0.1580],\n",
      "        [0.1588],\n",
      "        [0.1700],\n",
      "        [0.1533],\n",
      "        [0.1796],\n",
      "        [0.1650],\n",
      "        [0.1374],\n",
      "        [0.1794],\n",
      "        [0.1959],\n",
      "        [0.1978],\n",
      "        [0.0405],\n",
      "        [0.1072],\n",
      "        [0.0411],\n",
      "        [0.0260],\n",
      "        [0.0900],\n",
      "        [0.0959],\n",
      "        [0.0973],\n",
      "        [0.1146],\n",
      "        [0.1275],\n",
      "        [0.1276],\n",
      "        [0.1281],\n",
      "        [0.1392],\n",
      "        [0.1412],\n",
      "        [0.1451],\n",
      "        [0.1454],\n",
      "        [0.1502],\n",
      "        [0.1506],\n",
      "        [0.1628],\n",
      "        [0.1269],\n",
      "        [0.0403],\n",
      "        [0.0916],\n",
      "        [0.0816],\n",
      "        [0.0788],\n",
      "        [0.1031],\n",
      "        [0.1094],\n",
      "        [0.1513],\n",
      "        [0.1236],\n",
      "        [0.0705],\n",
      "        [0.1679],\n",
      "        [0.1695],\n",
      "        [0.0810],\n",
      "        [0.0648],\n",
      "        [0.0280],\n",
      "        [0.0952],\n",
      "        [0.1313],\n",
      "        [0.1531],\n",
      "        [0.0832],\n",
      "        [0.1630],\n",
      "        [0.1653],\n",
      "        [0.1723],\n",
      "        [0.1759]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 27.102563619613647\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 127\n",
      "剩餘X 資料 torch.Size([33, 18])\n",
      "剩餘Y 資料 torch.Size([33, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.03101579286158085, 14)\n",
      "The second_loss value of k: (0.04060020670294762, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.6105])\n",
      "目前模型的Data狀態 torch.Size([127, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866],\n",
      "        [0.7866]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0173],\n",
      "        [0.0269],\n",
      "        [0.0555],\n",
      "        [0.0581],\n",
      "        [0.1023],\n",
      "        [0.0667],\n",
      "        [0.0681],\n",
      "        [0.0588],\n",
      "        [0.0063],\n",
      "        [0.0199],\n",
      "        [0.0449],\n",
      "        [0.0170],\n",
      "        [0.0626],\n",
      "        [0.0429],\n",
      "        [0.0630],\n",
      "        [0.0177],\n",
      "        [0.0228],\n",
      "        [0.0334],\n",
      "        [0.0278],\n",
      "        [0.1955],\n",
      "        [0.1424],\n",
      "        [0.1501],\n",
      "        [0.1816],\n",
      "        [0.1881],\n",
      "        [0.0430],\n",
      "        [0.1901],\n",
      "        [0.0567],\n",
      "        [0.0596],\n",
      "        [0.0658],\n",
      "        [0.1293],\n",
      "        [0.0808],\n",
      "        [0.1488],\n",
      "        [0.2134],\n",
      "        [0.0969],\n",
      "        [0.1026],\n",
      "        [0.1060],\n",
      "        [0.1112],\n",
      "        [0.1152],\n",
      "        [0.1193],\n",
      "        [0.1230],\n",
      "        [0.1275],\n",
      "        [0.1334],\n",
      "        [0.1819],\n",
      "        [0.2128],\n",
      "        [0.1572],\n",
      "        [0.1655],\n",
      "        [0.1803],\n",
      "        [0.1807],\n",
      "        [0.1889],\n",
      "        [0.1924],\n",
      "        [0.1934],\n",
      "        [0.1967],\n",
      "        [0.2061],\n",
      "        [0.2106],\n",
      "        [0.1664],\n",
      "        [0.0534],\n",
      "        [0.1728],\n",
      "        [0.0559],\n",
      "        [0.1686],\n",
      "        [0.0750],\n",
      "        [0.0769],\n",
      "        [0.0877],\n",
      "        [0.0967],\n",
      "        [0.1337],\n",
      "        [0.0787],\n",
      "        [0.0490],\n",
      "        [0.0489],\n",
      "        [0.0070],\n",
      "        [0.0300],\n",
      "        [0.0828],\n",
      "        [0.1099],\n",
      "        [0.1149],\n",
      "        [0.1427],\n",
      "        [0.1436],\n",
      "        [0.1561],\n",
      "        [0.1580],\n",
      "        [0.1588],\n",
      "        [0.1700],\n",
      "        [0.1533],\n",
      "        [0.1796],\n",
      "        [0.1650],\n",
      "        [0.1374],\n",
      "        [0.1794],\n",
      "        [0.1959],\n",
      "        [0.1978],\n",
      "        [0.0405],\n",
      "        [0.1072],\n",
      "        [0.0411],\n",
      "        [0.0260],\n",
      "        [0.0900],\n",
      "        [0.0959],\n",
      "        [0.0973],\n",
      "        [0.1146],\n",
      "        [0.1275],\n",
      "        [0.1276],\n",
      "        [0.1281],\n",
      "        [0.1392],\n",
      "        [0.1412],\n",
      "        [0.1451],\n",
      "        [0.1454],\n",
      "        [0.1502],\n",
      "        [0.1506],\n",
      "        [0.1628],\n",
      "        [0.1269],\n",
      "        [0.0403],\n",
      "        [0.0916],\n",
      "        [0.0816],\n",
      "        [0.0788],\n",
      "        [0.1031],\n",
      "        [0.1094],\n",
      "        [0.1513],\n",
      "        [0.1236],\n",
      "        [0.0705],\n",
      "        [0.1679],\n",
      "        [0.1695],\n",
      "        [0.0810],\n",
      "        [0.0648],\n",
      "        [0.0280],\n",
      "        [0.0952],\n",
      "        [0.1313],\n",
      "        [0.1531],\n",
      "        [0.0832],\n",
      "        [0.1630],\n",
      "        [0.1653],\n",
      "        [0.1723],\n",
      "        [0.1759],\n",
      "        [0.1761]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0187],\n",
      "        [0.0283],\n",
      "        [0.0569],\n",
      "        [0.0595],\n",
      "        [0.1037],\n",
      "        [0.0681],\n",
      "        [0.0695],\n",
      "        [0.0602],\n",
      "        [0.0077],\n",
      "        [0.0213],\n",
      "        [0.0463],\n",
      "        [0.0184],\n",
      "        [0.0640],\n",
      "        [0.0443],\n",
      "        [0.0644],\n",
      "        [0.0191],\n",
      "        [0.0242],\n",
      "        [0.0348],\n",
      "        [0.0292],\n",
      "        [0.1969],\n",
      "        [0.1438],\n",
      "        [0.1515],\n",
      "        [0.1830],\n",
      "        [0.1895],\n",
      "        [0.0444],\n",
      "        [0.1915],\n",
      "        [0.0581],\n",
      "        [0.0610],\n",
      "        [0.0672],\n",
      "        [0.1307],\n",
      "        [0.0822],\n",
      "        [0.1502],\n",
      "        [0.2148],\n",
      "        [0.0983],\n",
      "        [0.1040],\n",
      "        [0.1074],\n",
      "        [0.1126],\n",
      "        [0.1166],\n",
      "        [0.1207],\n",
      "        [0.1244],\n",
      "        [0.1289],\n",
      "        [0.1348],\n",
      "        [0.1833],\n",
      "        [0.2142],\n",
      "        [0.1586],\n",
      "        [0.1669],\n",
      "        [0.1817],\n",
      "        [0.1821],\n",
      "        [0.1903],\n",
      "        [0.1938],\n",
      "        [0.1948],\n",
      "        [0.1981],\n",
      "        [0.2075],\n",
      "        [0.2120],\n",
      "        [0.1678],\n",
      "        [0.0548],\n",
      "        [0.1742],\n",
      "        [0.0573],\n",
      "        [0.1700],\n",
      "        [0.0764],\n",
      "        [0.0783],\n",
      "        [0.0891],\n",
      "        [0.0981],\n",
      "        [0.1351],\n",
      "        [0.0773],\n",
      "        [0.0504],\n",
      "        [0.0503],\n",
      "        [0.0084],\n",
      "        [0.0314],\n",
      "        [0.0814],\n",
      "        [0.1085],\n",
      "        [0.1135],\n",
      "        [0.1413],\n",
      "        [0.1422],\n",
      "        [0.1547],\n",
      "        [0.1566],\n",
      "        [0.1574],\n",
      "        [0.1686],\n",
      "        [0.1519],\n",
      "        [0.1782],\n",
      "        [0.1636],\n",
      "        [0.1360],\n",
      "        [0.1780],\n",
      "        [0.1945],\n",
      "        [0.1964],\n",
      "        [0.0391],\n",
      "        [0.1058],\n",
      "        [0.0397],\n",
      "        [0.0246],\n",
      "        [0.0886],\n",
      "        [0.0945],\n",
      "        [0.0959],\n",
      "        [0.1132],\n",
      "        [0.1261],\n",
      "        [0.1262],\n",
      "        [0.1267],\n",
      "        [0.1378],\n",
      "        [0.1398],\n",
      "        [0.1437],\n",
      "        [0.1440],\n",
      "        [0.1488],\n",
      "        [0.1492],\n",
      "        [0.1614],\n",
      "        [0.1255],\n",
      "        [0.0389],\n",
      "        [0.0902],\n",
      "        [0.0802],\n",
      "        [0.0774],\n",
      "        [0.1017],\n",
      "        [0.1080],\n",
      "        [0.1499],\n",
      "        [0.1222],\n",
      "        [0.0691],\n",
      "        [0.1665],\n",
      "        [0.1681],\n",
      "        [0.0796],\n",
      "        [0.0634],\n",
      "        [0.0266],\n",
      "        [0.0938],\n",
      "        [0.1299],\n",
      "        [0.1517],\n",
      "        [0.0818],\n",
      "        [0.1616],\n",
      "        [0.1639],\n",
      "        [0.1709],\n",
      "        [0.1745],\n",
      "        [0.1747]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 27.342499256134033\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 128\n",
      "剩餘X 資料 torch.Size([32, 18])\n",
      "剩餘Y 資料 torch.Size([32, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.040038079023361206, 14)\n",
      "The second_loss value of k: (0.04182898625731468, 16)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.5851])\n",
      "目前模型的Data狀態 torch.Size([128, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852],\n",
      "        [0.7852]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0187],\n",
      "        [0.0283],\n",
      "        [0.0569],\n",
      "        [0.0595],\n",
      "        [0.1037],\n",
      "        [0.0681],\n",
      "        [0.0695],\n",
      "        [0.0602],\n",
      "        [0.0077],\n",
      "        [0.0213],\n",
      "        [0.0463],\n",
      "        [0.0184],\n",
      "        [0.0640],\n",
      "        [0.0443],\n",
      "        [0.0644],\n",
      "        [0.0191],\n",
      "        [0.0242],\n",
      "        [0.0348],\n",
      "        [0.0292],\n",
      "        [0.1969],\n",
      "        [0.1438],\n",
      "        [0.1515],\n",
      "        [0.1830],\n",
      "        [0.1895],\n",
      "        [0.0444],\n",
      "        [0.1915],\n",
      "        [0.0581],\n",
      "        [0.0610],\n",
      "        [0.0672],\n",
      "        [0.1307],\n",
      "        [0.0822],\n",
      "        [0.1502],\n",
      "        [0.2148],\n",
      "        [0.0983],\n",
      "        [0.1040],\n",
      "        [0.1074],\n",
      "        [0.1126],\n",
      "        [0.1166],\n",
      "        [0.1207],\n",
      "        [0.1244],\n",
      "        [0.1289],\n",
      "        [0.1348],\n",
      "        [0.1833],\n",
      "        [0.2142],\n",
      "        [0.1586],\n",
      "        [0.1669],\n",
      "        [0.1817],\n",
      "        [0.1821],\n",
      "        [0.1903],\n",
      "        [0.1938],\n",
      "        [0.1948],\n",
      "        [0.1981],\n",
      "        [0.2075],\n",
      "        [0.2120],\n",
      "        [0.1678],\n",
      "        [0.0548],\n",
      "        [0.1742],\n",
      "        [0.0573],\n",
      "        [0.1700],\n",
      "        [0.0764],\n",
      "        [0.0783],\n",
      "        [0.0891],\n",
      "        [0.0981],\n",
      "        [0.1351],\n",
      "        [0.0773],\n",
      "        [0.0504],\n",
      "        [0.0503],\n",
      "        [0.0084],\n",
      "        [0.0314],\n",
      "        [0.0814],\n",
      "        [0.1085],\n",
      "        [0.1135],\n",
      "        [0.1413],\n",
      "        [0.1422],\n",
      "        [0.1547],\n",
      "        [0.1566],\n",
      "        [0.1574],\n",
      "        [0.1686],\n",
      "        [0.1519],\n",
      "        [0.1782],\n",
      "        [0.1636],\n",
      "        [0.1360],\n",
      "        [0.1780],\n",
      "        [0.1945],\n",
      "        [0.1964],\n",
      "        [0.0391],\n",
      "        [0.1058],\n",
      "        [0.0397],\n",
      "        [0.0246],\n",
      "        [0.0886],\n",
      "        [0.0945],\n",
      "        [0.0959],\n",
      "        [0.1132],\n",
      "        [0.1261],\n",
      "        [0.1262],\n",
      "        [0.1267],\n",
      "        [0.1378],\n",
      "        [0.1398],\n",
      "        [0.1437],\n",
      "        [0.1440],\n",
      "        [0.1488],\n",
      "        [0.1492],\n",
      "        [0.1614],\n",
      "        [0.1255],\n",
      "        [0.0389],\n",
      "        [0.0902],\n",
      "        [0.0802],\n",
      "        [0.0774],\n",
      "        [0.1017],\n",
      "        [0.1080],\n",
      "        [0.1499],\n",
      "        [0.1222],\n",
      "        [0.0691],\n",
      "        [0.1665],\n",
      "        [0.1681],\n",
      "        [0.0796],\n",
      "        [0.0634],\n",
      "        [0.0266],\n",
      "        [0.0938],\n",
      "        [0.1299],\n",
      "        [0.1517],\n",
      "        [0.0818],\n",
      "        [0.1616],\n",
      "        [0.1639],\n",
      "        [0.1709],\n",
      "        [0.1745],\n",
      "        [0.1747],\n",
      "        [0.2001]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0203],\n",
      "        [0.0299],\n",
      "        [0.0585],\n",
      "        [0.0610],\n",
      "        [0.1053],\n",
      "        [0.0697],\n",
      "        [0.0711],\n",
      "        [0.0618],\n",
      "        [0.0092],\n",
      "        [0.0229],\n",
      "        [0.0479],\n",
      "        [0.0200],\n",
      "        [0.0655],\n",
      "        [0.0459],\n",
      "        [0.0659],\n",
      "        [0.0206],\n",
      "        [0.0258],\n",
      "        [0.0363],\n",
      "        [0.0308],\n",
      "        [0.1985],\n",
      "        [0.1453],\n",
      "        [0.1530],\n",
      "        [0.1845],\n",
      "        [0.1911],\n",
      "        [0.0460],\n",
      "        [0.1930],\n",
      "        [0.0596],\n",
      "        [0.0626],\n",
      "        [0.0688],\n",
      "        [0.1322],\n",
      "        [0.0838],\n",
      "        [0.1518],\n",
      "        [0.2163],\n",
      "        [0.0998],\n",
      "        [0.1056],\n",
      "        [0.1089],\n",
      "        [0.1142],\n",
      "        [0.1182],\n",
      "        [0.1223],\n",
      "        [0.1259],\n",
      "        [0.1304],\n",
      "        [0.1363],\n",
      "        [0.1848],\n",
      "        [0.2158],\n",
      "        [0.1601],\n",
      "        [0.1685],\n",
      "        [0.1833],\n",
      "        [0.1837],\n",
      "        [0.1919],\n",
      "        [0.1954],\n",
      "        [0.1964],\n",
      "        [0.1997],\n",
      "        [0.2091],\n",
      "        [0.2136],\n",
      "        [0.1693],\n",
      "        [0.0564],\n",
      "        [0.1757],\n",
      "        [0.0589],\n",
      "        [0.1716],\n",
      "        [0.0780],\n",
      "        [0.0799],\n",
      "        [0.0907],\n",
      "        [0.0997],\n",
      "        [0.1366],\n",
      "        [0.0757],\n",
      "        [0.0520],\n",
      "        [0.0518],\n",
      "        [0.0100],\n",
      "        [0.0330],\n",
      "        [0.0798],\n",
      "        [0.1069],\n",
      "        [0.1119],\n",
      "        [0.1397],\n",
      "        [0.1407],\n",
      "        [0.1531],\n",
      "        [0.1550],\n",
      "        [0.1559],\n",
      "        [0.1671],\n",
      "        [0.1503],\n",
      "        [0.1767],\n",
      "        [0.1621],\n",
      "        [0.1344],\n",
      "        [0.1764],\n",
      "        [0.1930],\n",
      "        [0.1948],\n",
      "        [0.0375],\n",
      "        [0.1042],\n",
      "        [0.0381],\n",
      "        [0.0231],\n",
      "        [0.0871],\n",
      "        [0.0929],\n",
      "        [0.0944],\n",
      "        [0.1116],\n",
      "        [0.1245],\n",
      "        [0.1247],\n",
      "        [0.1252],\n",
      "        [0.1362],\n",
      "        [0.1383],\n",
      "        [0.1421],\n",
      "        [0.1425],\n",
      "        [0.1473],\n",
      "        [0.1477],\n",
      "        [0.1598],\n",
      "        [0.1239],\n",
      "        [0.0374],\n",
      "        [0.0886],\n",
      "        [0.0786],\n",
      "        [0.0759],\n",
      "        [0.1001],\n",
      "        [0.1065],\n",
      "        [0.1483],\n",
      "        [0.1206],\n",
      "        [0.0676],\n",
      "        [0.1649],\n",
      "        [0.1666],\n",
      "        [0.0780],\n",
      "        [0.0619],\n",
      "        [0.0251],\n",
      "        [0.0922],\n",
      "        [0.1284],\n",
      "        [0.1502],\n",
      "        [0.0803],\n",
      "        [0.1601],\n",
      "        [0.1623],\n",
      "        [0.1693],\n",
      "        [0.1729],\n",
      "        [0.1732],\n",
      "        [0.1985]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 27.581157684326172\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 129\n",
      "剩餘X 資料 torch.Size([31, 18])\n",
      "剩餘Y 資料 torch.Size([31, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.04119639843702316, 15)\n",
      "The second_loss value of k: (0.044950854033231735, 5)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.5807])\n",
      "目前模型的Data狀態 torch.Size([129, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837],\n",
      "        [0.7837]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0203],\n",
      "        [0.0299],\n",
      "        [0.0585],\n",
      "        [0.0610],\n",
      "        [0.1053],\n",
      "        [0.0697],\n",
      "        [0.0711],\n",
      "        [0.0618],\n",
      "        [0.0092],\n",
      "        [0.0229],\n",
      "        [0.0479],\n",
      "        [0.0200],\n",
      "        [0.0655],\n",
      "        [0.0459],\n",
      "        [0.0659],\n",
      "        [0.0206],\n",
      "        [0.0258],\n",
      "        [0.0363],\n",
      "        [0.0308],\n",
      "        [0.1985],\n",
      "        [0.1453],\n",
      "        [0.1530],\n",
      "        [0.1845],\n",
      "        [0.1911],\n",
      "        [0.0460],\n",
      "        [0.1930],\n",
      "        [0.0596],\n",
      "        [0.0626],\n",
      "        [0.0688],\n",
      "        [0.1322],\n",
      "        [0.0838],\n",
      "        [0.1518],\n",
      "        [0.2163],\n",
      "        [0.0998],\n",
      "        [0.1056],\n",
      "        [0.1089],\n",
      "        [0.1142],\n",
      "        [0.1182],\n",
      "        [0.1223],\n",
      "        [0.1259],\n",
      "        [0.1304],\n",
      "        [0.1363],\n",
      "        [0.1848],\n",
      "        [0.2158],\n",
      "        [0.1601],\n",
      "        [0.1685],\n",
      "        [0.1833],\n",
      "        [0.1837],\n",
      "        [0.1919],\n",
      "        [0.1954],\n",
      "        [0.1964],\n",
      "        [0.1997],\n",
      "        [0.2091],\n",
      "        [0.2136],\n",
      "        [0.1693],\n",
      "        [0.0564],\n",
      "        [0.1757],\n",
      "        [0.0589],\n",
      "        [0.1716],\n",
      "        [0.0780],\n",
      "        [0.0799],\n",
      "        [0.0907],\n",
      "        [0.0997],\n",
      "        [0.1366],\n",
      "        [0.0757],\n",
      "        [0.0520],\n",
      "        [0.0518],\n",
      "        [0.0100],\n",
      "        [0.0330],\n",
      "        [0.0798],\n",
      "        [0.1069],\n",
      "        [0.1119],\n",
      "        [0.1397],\n",
      "        [0.1407],\n",
      "        [0.1531],\n",
      "        [0.1550],\n",
      "        [0.1559],\n",
      "        [0.1671],\n",
      "        [0.1503],\n",
      "        [0.1767],\n",
      "        [0.1621],\n",
      "        [0.1344],\n",
      "        [0.1764],\n",
      "        [0.1930],\n",
      "        [0.1948],\n",
      "        [0.0375],\n",
      "        [0.1042],\n",
      "        [0.0381],\n",
      "        [0.0231],\n",
      "        [0.0871],\n",
      "        [0.0929],\n",
      "        [0.0944],\n",
      "        [0.1116],\n",
      "        [0.1245],\n",
      "        [0.1247],\n",
      "        [0.1252],\n",
      "        [0.1362],\n",
      "        [0.1383],\n",
      "        [0.1421],\n",
      "        [0.1425],\n",
      "        [0.1473],\n",
      "        [0.1477],\n",
      "        [0.1598],\n",
      "        [0.1239],\n",
      "        [0.0374],\n",
      "        [0.0886],\n",
      "        [0.0786],\n",
      "        [0.0759],\n",
      "        [0.1001],\n",
      "        [0.1065],\n",
      "        [0.1483],\n",
      "        [0.1206],\n",
      "        [0.0676],\n",
      "        [0.1649],\n",
      "        [0.1666],\n",
      "        [0.0780],\n",
      "        [0.0619],\n",
      "        [0.0251],\n",
      "        [0.0922],\n",
      "        [0.1284],\n",
      "        [0.1502],\n",
      "        [0.0803],\n",
      "        [0.1601],\n",
      "        [0.1623],\n",
      "        [0.1693],\n",
      "        [0.1729],\n",
      "        [0.1732],\n",
      "        [0.1985],\n",
      "        [0.2030]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 80\n",
      "Number of shrink: 20\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0219],\n",
      "        [0.0315],\n",
      "        [0.0601],\n",
      "        [0.0626],\n",
      "        [0.1069],\n",
      "        [0.0713],\n",
      "        [0.0727],\n",
      "        [0.0634],\n",
      "        [0.0108],\n",
      "        [0.0245],\n",
      "        [0.0494],\n",
      "        [0.0216],\n",
      "        [0.0671],\n",
      "        [0.0474],\n",
      "        [0.0675],\n",
      "        [0.0222],\n",
      "        [0.0274],\n",
      "        [0.0379],\n",
      "        [0.0324],\n",
      "        [0.2000],\n",
      "        [0.1469],\n",
      "        [0.1546],\n",
      "        [0.1861],\n",
      "        [0.1927],\n",
      "        [0.0476],\n",
      "        [0.1946],\n",
      "        [0.0612],\n",
      "        [0.0642],\n",
      "        [0.0703],\n",
      "        [0.1338],\n",
      "        [0.0853],\n",
      "        [0.1534],\n",
      "        [0.2179],\n",
      "        [0.1014],\n",
      "        [0.1071],\n",
      "        [0.1105],\n",
      "        [0.1158],\n",
      "        [0.1198],\n",
      "        [0.1238],\n",
      "        [0.1275],\n",
      "        [0.1320],\n",
      "        [0.1379],\n",
      "        [0.1864],\n",
      "        [0.2173],\n",
      "        [0.1617],\n",
      "        [0.1700],\n",
      "        [0.1849],\n",
      "        [0.1852],\n",
      "        [0.1934],\n",
      "        [0.1970],\n",
      "        [0.1979],\n",
      "        [0.2013],\n",
      "        [0.2107],\n",
      "        [0.2152],\n",
      "        [0.1709],\n",
      "        [0.0579],\n",
      "        [0.1773],\n",
      "        [0.0605],\n",
      "        [0.1732],\n",
      "        [0.0796],\n",
      "        [0.0814],\n",
      "        [0.0923],\n",
      "        [0.1012],\n",
      "        [0.1382],\n",
      "        [0.0742],\n",
      "        [0.0536],\n",
      "        [0.0534],\n",
      "        [0.0115],\n",
      "        [0.0346],\n",
      "        [0.0782],\n",
      "        [0.1054],\n",
      "        [0.1104],\n",
      "        [0.1381],\n",
      "        [0.1391],\n",
      "        [0.1515],\n",
      "        [0.1535],\n",
      "        [0.1543],\n",
      "        [0.1655],\n",
      "        [0.1488],\n",
      "        [0.1751],\n",
      "        [0.1605],\n",
      "        [0.1328],\n",
      "        [0.1748],\n",
      "        [0.1914],\n",
      "        [0.1932],\n",
      "        [0.0359],\n",
      "        [0.1026],\n",
      "        [0.0366],\n",
      "        [0.0215],\n",
      "        [0.0855],\n",
      "        [0.0914],\n",
      "        [0.0928],\n",
      "        [0.1101],\n",
      "        [0.1229],\n",
      "        [0.1231],\n",
      "        [0.1236],\n",
      "        [0.1347],\n",
      "        [0.1367],\n",
      "        [0.1405],\n",
      "        [0.1409],\n",
      "        [0.1457],\n",
      "        [0.1461],\n",
      "        [0.1582],\n",
      "        [0.1224],\n",
      "        [0.0358],\n",
      "        [0.0871],\n",
      "        [0.0771],\n",
      "        [0.0743],\n",
      "        [0.0985],\n",
      "        [0.1049],\n",
      "        [0.1468],\n",
      "        [0.1191],\n",
      "        [0.0660],\n",
      "        [0.1634],\n",
      "        [0.1650],\n",
      "        [0.0764],\n",
      "        [0.0603],\n",
      "        [0.0235],\n",
      "        [0.0907],\n",
      "        [0.1268],\n",
      "        [0.1486],\n",
      "        [0.0787],\n",
      "        [0.1585],\n",
      "        [0.1607],\n",
      "        [0.1677],\n",
      "        [0.1713],\n",
      "        [0.1716],\n",
      "        [0.1970],\n",
      "        [0.2014]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 27.81748628616333\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 130\n",
      "剩餘X 資料 torch.Size([30, 18])\n",
      "剩餘Y 資料 torch.Size([30, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.04428611323237419, 5)\n",
      "The second_loss value of k: (0.04630378261208534, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.7118])\n",
      "目前模型的Data狀態 torch.Size([130, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.7821],\n",
      "        [0.9222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0219],\n",
      "        [0.0315],\n",
      "        [0.0601],\n",
      "        [0.0626],\n",
      "        [0.1069],\n",
      "        [0.0713],\n",
      "        [0.0727],\n",
      "        [0.0634],\n",
      "        [0.0108],\n",
      "        [0.0245],\n",
      "        [0.0494],\n",
      "        [0.0216],\n",
      "        [0.0671],\n",
      "        [0.0474],\n",
      "        [0.0675],\n",
      "        [0.0222],\n",
      "        [0.0274],\n",
      "        [0.0379],\n",
      "        [0.0324],\n",
      "        [0.2000],\n",
      "        [0.1469],\n",
      "        [0.1546],\n",
      "        [0.1861],\n",
      "        [0.1927],\n",
      "        [0.0476],\n",
      "        [0.1946],\n",
      "        [0.0612],\n",
      "        [0.0642],\n",
      "        [0.0703],\n",
      "        [0.1338],\n",
      "        [0.0853],\n",
      "        [0.1534],\n",
      "        [0.2179],\n",
      "        [0.1014],\n",
      "        [0.1071],\n",
      "        [0.1105],\n",
      "        [0.1158],\n",
      "        [0.1198],\n",
      "        [0.1238],\n",
      "        [0.1275],\n",
      "        [0.1320],\n",
      "        [0.1379],\n",
      "        [0.1864],\n",
      "        [0.2173],\n",
      "        [0.1617],\n",
      "        [0.1700],\n",
      "        [0.1849],\n",
      "        [0.1852],\n",
      "        [0.1934],\n",
      "        [0.1970],\n",
      "        [0.1979],\n",
      "        [0.2013],\n",
      "        [0.2107],\n",
      "        [0.2152],\n",
      "        [0.1709],\n",
      "        [0.0579],\n",
      "        [0.1773],\n",
      "        [0.0605],\n",
      "        [0.1732],\n",
      "        [0.0796],\n",
      "        [0.0814],\n",
      "        [0.0923],\n",
      "        [0.1012],\n",
      "        [0.1382],\n",
      "        [0.0742],\n",
      "        [0.0536],\n",
      "        [0.0534],\n",
      "        [0.0115],\n",
      "        [0.0346],\n",
      "        [0.0782],\n",
      "        [0.1054],\n",
      "        [0.1104],\n",
      "        [0.1381],\n",
      "        [0.1391],\n",
      "        [0.1515],\n",
      "        [0.1535],\n",
      "        [0.1543],\n",
      "        [0.1655],\n",
      "        [0.1488],\n",
      "        [0.1751],\n",
      "        [0.1605],\n",
      "        [0.1328],\n",
      "        [0.1748],\n",
      "        [0.1914],\n",
      "        [0.1932],\n",
      "        [0.0359],\n",
      "        [0.1026],\n",
      "        [0.0366],\n",
      "        [0.0215],\n",
      "        [0.0855],\n",
      "        [0.0914],\n",
      "        [0.0928],\n",
      "        [0.1101],\n",
      "        [0.1229],\n",
      "        [0.1231],\n",
      "        [0.1236],\n",
      "        [0.1347],\n",
      "        [0.1367],\n",
      "        [0.1405],\n",
      "        [0.1409],\n",
      "        [0.1457],\n",
      "        [0.1461],\n",
      "        [0.1582],\n",
      "        [0.1224],\n",
      "        [0.0358],\n",
      "        [0.0871],\n",
      "        [0.0771],\n",
      "        [0.0743],\n",
      "        [0.0985],\n",
      "        [0.1049],\n",
      "        [0.1468],\n",
      "        [0.1191],\n",
      "        [0.0660],\n",
      "        [0.1634],\n",
      "        [0.1650],\n",
      "        [0.0764],\n",
      "        [0.0603],\n",
      "        [0.0235],\n",
      "        [0.0907],\n",
      "        [0.1268],\n",
      "        [0.1486],\n",
      "        [0.0787],\n",
      "        [0.1585],\n",
      "        [0.1607],\n",
      "        [0.1677],\n",
      "        [0.1713],\n",
      "        [0.1716],\n",
      "        [0.1970],\n",
      "        [0.2014],\n",
      "        [0.2104]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0224],\n",
      "        [0.0320],\n",
      "        [0.0606],\n",
      "        [0.0631],\n",
      "        [0.1074],\n",
      "        [0.0718],\n",
      "        [0.0732],\n",
      "        [0.0639],\n",
      "        [0.0113],\n",
      "        [0.0250],\n",
      "        [0.0500],\n",
      "        [0.0221],\n",
      "        [0.0677],\n",
      "        [0.0480],\n",
      "        [0.0680],\n",
      "        [0.0227],\n",
      "        [0.0279],\n",
      "        [0.0384],\n",
      "        [0.0329],\n",
      "        [0.2006],\n",
      "        [0.1474],\n",
      "        [0.1551],\n",
      "        [0.1866],\n",
      "        [0.1932],\n",
      "        [0.0481],\n",
      "        [0.1952],\n",
      "        [0.0617],\n",
      "        [0.0647],\n",
      "        [0.0709],\n",
      "        [0.1344],\n",
      "        [0.0859],\n",
      "        [0.1539],\n",
      "        [0.2185],\n",
      "        [0.1019],\n",
      "        [0.1077],\n",
      "        [0.1110],\n",
      "        [0.1163],\n",
      "        [0.1203],\n",
      "        [0.1244],\n",
      "        [0.1281],\n",
      "        [0.1326],\n",
      "        [0.1384],\n",
      "        [0.1869],\n",
      "        [0.2179],\n",
      "        [0.1622],\n",
      "        [0.1706],\n",
      "        [0.1854],\n",
      "        [0.1858],\n",
      "        [0.1940],\n",
      "        [0.1975],\n",
      "        [0.1985],\n",
      "        [0.2018],\n",
      "        [0.2112],\n",
      "        [0.2157],\n",
      "        [0.1715],\n",
      "        [0.0585],\n",
      "        [0.1778],\n",
      "        [0.0610],\n",
      "        [0.1737],\n",
      "        [0.0801],\n",
      "        [0.0820],\n",
      "        [0.0928],\n",
      "        [0.1018],\n",
      "        [0.1387],\n",
      "        [0.0736],\n",
      "        [0.0541],\n",
      "        [0.0539],\n",
      "        [0.0121],\n",
      "        [0.0351],\n",
      "        [0.0777],\n",
      "        [0.1048],\n",
      "        [0.1098],\n",
      "        [0.1376],\n",
      "        [0.1386],\n",
      "        [0.1510],\n",
      "        [0.1529],\n",
      "        [0.1538],\n",
      "        [0.1650],\n",
      "        [0.1482],\n",
      "        [0.1746],\n",
      "        [0.1600],\n",
      "        [0.1323],\n",
      "        [0.1743],\n",
      "        [0.1909],\n",
      "        [0.1927],\n",
      "        [0.0354],\n",
      "        [0.1021],\n",
      "        [0.0360],\n",
      "        [0.0209],\n",
      "        [0.0850],\n",
      "        [0.0908],\n",
      "        [0.0923],\n",
      "        [0.1095],\n",
      "        [0.1224],\n",
      "        [0.1226],\n",
      "        [0.1231],\n",
      "        [0.1341],\n",
      "        [0.1362],\n",
      "        [0.1400],\n",
      "        [0.1404],\n",
      "        [0.1452],\n",
      "        [0.1455],\n",
      "        [0.1577],\n",
      "        [0.1218],\n",
      "        [0.0352],\n",
      "        [0.0865],\n",
      "        [0.0765],\n",
      "        [0.0738],\n",
      "        [0.0980],\n",
      "        [0.1044],\n",
      "        [0.1462],\n",
      "        [0.1185],\n",
      "        [0.0655],\n",
      "        [0.1628],\n",
      "        [0.1645],\n",
      "        [0.0759],\n",
      "        [0.0598],\n",
      "        [0.0229],\n",
      "        [0.0901],\n",
      "        [0.1262],\n",
      "        [0.1481],\n",
      "        [0.0782],\n",
      "        [0.1580],\n",
      "        [0.1602],\n",
      "        [0.1672],\n",
      "        [0.1708],\n",
      "        [0.1710],\n",
      "        [0.1964],\n",
      "        [0.2009],\n",
      "        [0.0698]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 28.05748152732849\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 131\n",
      "剩餘X 資料 torch.Size([29, 18])\n",
      "剩餘Y 資料 torch.Size([29, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.007380388677120209, 3)\n",
      "The second_loss value of k: (0.011122840456664562, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.7355])\n",
      "目前模型的Data狀態 torch.Size([131, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.7815],\n",
      "        [0.8214]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0224],\n",
      "        [0.0320],\n",
      "        [0.0606],\n",
      "        [0.0631],\n",
      "        [0.1074],\n",
      "        [0.0718],\n",
      "        [0.0732],\n",
      "        [0.0639],\n",
      "        [0.0113],\n",
      "        [0.0250],\n",
      "        [0.0500],\n",
      "        [0.0221],\n",
      "        [0.0677],\n",
      "        [0.0480],\n",
      "        [0.0680],\n",
      "        [0.0227],\n",
      "        [0.0279],\n",
      "        [0.0384],\n",
      "        [0.0329],\n",
      "        [0.2006],\n",
      "        [0.1474],\n",
      "        [0.1551],\n",
      "        [0.1866],\n",
      "        [0.1932],\n",
      "        [0.0481],\n",
      "        [0.1952],\n",
      "        [0.0617],\n",
      "        [0.0647],\n",
      "        [0.0709],\n",
      "        [0.1344],\n",
      "        [0.0859],\n",
      "        [0.1539],\n",
      "        [0.2185],\n",
      "        [0.1019],\n",
      "        [0.1077],\n",
      "        [0.1110],\n",
      "        [0.1163],\n",
      "        [0.1203],\n",
      "        [0.1244],\n",
      "        [0.1281],\n",
      "        [0.1326],\n",
      "        [0.1384],\n",
      "        [0.1869],\n",
      "        [0.2179],\n",
      "        [0.1622],\n",
      "        [0.1706],\n",
      "        [0.1854],\n",
      "        [0.1858],\n",
      "        [0.1940],\n",
      "        [0.1975],\n",
      "        [0.1985],\n",
      "        [0.2018],\n",
      "        [0.2112],\n",
      "        [0.2157],\n",
      "        [0.1715],\n",
      "        [0.0585],\n",
      "        [0.1778],\n",
      "        [0.0610],\n",
      "        [0.1737],\n",
      "        [0.0801],\n",
      "        [0.0820],\n",
      "        [0.0928],\n",
      "        [0.1018],\n",
      "        [0.1387],\n",
      "        [0.0736],\n",
      "        [0.0541],\n",
      "        [0.0539],\n",
      "        [0.0121],\n",
      "        [0.0351],\n",
      "        [0.0777],\n",
      "        [0.1048],\n",
      "        [0.1098],\n",
      "        [0.1376],\n",
      "        [0.1386],\n",
      "        [0.1510],\n",
      "        [0.1529],\n",
      "        [0.1538],\n",
      "        [0.1650],\n",
      "        [0.1482],\n",
      "        [0.1746],\n",
      "        [0.1600],\n",
      "        [0.1323],\n",
      "        [0.1743],\n",
      "        [0.1909],\n",
      "        [0.1927],\n",
      "        [0.0354],\n",
      "        [0.1021],\n",
      "        [0.0360],\n",
      "        [0.0209],\n",
      "        [0.0850],\n",
      "        [0.0908],\n",
      "        [0.0923],\n",
      "        [0.1095],\n",
      "        [0.1224],\n",
      "        [0.1226],\n",
      "        [0.1231],\n",
      "        [0.1341],\n",
      "        [0.1362],\n",
      "        [0.1400],\n",
      "        [0.1404],\n",
      "        [0.1452],\n",
      "        [0.1455],\n",
      "        [0.1577],\n",
      "        [0.1218],\n",
      "        [0.0352],\n",
      "        [0.0865],\n",
      "        [0.0765],\n",
      "        [0.0738],\n",
      "        [0.0980],\n",
      "        [0.1044],\n",
      "        [0.1462],\n",
      "        [0.1185],\n",
      "        [0.0655],\n",
      "        [0.1628],\n",
      "        [0.1645],\n",
      "        [0.0759],\n",
      "        [0.0598],\n",
      "        [0.0229],\n",
      "        [0.0901],\n",
      "        [0.1262],\n",
      "        [0.1481],\n",
      "        [0.0782],\n",
      "        [0.1580],\n",
      "        [0.1602],\n",
      "        [0.1672],\n",
      "        [0.1708],\n",
      "        [0.1710],\n",
      "        [0.1964],\n",
      "        [0.2009],\n",
      "        [0.0698],\n",
      "        [0.0859]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 65\n",
      "Number of shrink: 35\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0228],\n",
      "        [0.0324],\n",
      "        [0.0610],\n",
      "        [0.0635],\n",
      "        [0.1078],\n",
      "        [0.0722],\n",
      "        [0.0736],\n",
      "        [0.0643],\n",
      "        [0.0117],\n",
      "        [0.0254],\n",
      "        [0.0504],\n",
      "        [0.0225],\n",
      "        [0.0680],\n",
      "        [0.0484],\n",
      "        [0.0684],\n",
      "        [0.0231],\n",
      "        [0.0283],\n",
      "        [0.0388],\n",
      "        [0.0333],\n",
      "        [0.2009],\n",
      "        [0.1478],\n",
      "        [0.1555],\n",
      "        [0.1870],\n",
      "        [0.1936],\n",
      "        [0.0485],\n",
      "        [0.1955],\n",
      "        [0.0621],\n",
      "        [0.0651],\n",
      "        [0.0712],\n",
      "        [0.1347],\n",
      "        [0.0862],\n",
      "        [0.1543],\n",
      "        [0.2188],\n",
      "        [0.1023],\n",
      "        [0.1081],\n",
      "        [0.1114],\n",
      "        [0.1167],\n",
      "        [0.1207],\n",
      "        [0.1247],\n",
      "        [0.1284],\n",
      "        [0.1329],\n",
      "        [0.1388],\n",
      "        [0.1873],\n",
      "        [0.2182],\n",
      "        [0.1626],\n",
      "        [0.1710],\n",
      "        [0.1858],\n",
      "        [0.1862],\n",
      "        [0.1944],\n",
      "        [0.1979],\n",
      "        [0.1989],\n",
      "        [0.2022],\n",
      "        [0.2116],\n",
      "        [0.2161],\n",
      "        [0.1718],\n",
      "        [0.0589],\n",
      "        [0.1782],\n",
      "        [0.0614],\n",
      "        [0.1741],\n",
      "        [0.0805],\n",
      "        [0.0824],\n",
      "        [0.0932],\n",
      "        [0.1021],\n",
      "        [0.1391],\n",
      "        [0.0733],\n",
      "        [0.0545],\n",
      "        [0.0543],\n",
      "        [0.0124],\n",
      "        [0.0355],\n",
      "        [0.0773],\n",
      "        [0.1044],\n",
      "        [0.1094],\n",
      "        [0.1372],\n",
      "        [0.1382],\n",
      "        [0.1506],\n",
      "        [0.1525],\n",
      "        [0.1534],\n",
      "        [0.1646],\n",
      "        [0.1478],\n",
      "        [0.1742],\n",
      "        [0.1596],\n",
      "        [0.1319],\n",
      "        [0.1739],\n",
      "        [0.1905],\n",
      "        [0.1923],\n",
      "        [0.0350],\n",
      "        [0.1017],\n",
      "        [0.0357],\n",
      "        [0.0206],\n",
      "        [0.0846],\n",
      "        [0.0905],\n",
      "        [0.0919],\n",
      "        [0.1091],\n",
      "        [0.1220],\n",
      "        [0.1222],\n",
      "        [0.1227],\n",
      "        [0.1337],\n",
      "        [0.1358],\n",
      "        [0.1396],\n",
      "        [0.1400],\n",
      "        [0.1448],\n",
      "        [0.1452],\n",
      "        [0.1573],\n",
      "        [0.1214],\n",
      "        [0.0349],\n",
      "        [0.0861],\n",
      "        [0.0762],\n",
      "        [0.0734],\n",
      "        [0.0976],\n",
      "        [0.1040],\n",
      "        [0.1458],\n",
      "        [0.1181],\n",
      "        [0.0651],\n",
      "        [0.1624],\n",
      "        [0.1641],\n",
      "        [0.0755],\n",
      "        [0.0594],\n",
      "        [0.0226],\n",
      "        [0.0897],\n",
      "        [0.1259],\n",
      "        [0.1477],\n",
      "        [0.0778],\n",
      "        [0.1576],\n",
      "        [0.1598],\n",
      "        [0.1668],\n",
      "        [0.1704],\n",
      "        [0.1707],\n",
      "        [0.1961],\n",
      "        [0.2005],\n",
      "        [0.0694],\n",
      "        [0.0457]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 28.29787802696228\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 132\n",
      "剩餘X 資料 torch.Size([28, 18])\n",
      "剩餘Y 資料 torch.Size([28, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.003930531907826662, 3)\n",
      "The second_loss value of k: (0.011776157654821873, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.7342])\n",
      "目前模型的Data狀態 torch.Size([132, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7812],\n",
      "        [0.7969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0228],\n",
      "        [0.0324],\n",
      "        [0.0610],\n",
      "        [0.0635],\n",
      "        [0.1078],\n",
      "        [0.0722],\n",
      "        [0.0736],\n",
      "        [0.0643],\n",
      "        [0.0117],\n",
      "        [0.0254],\n",
      "        [0.0504],\n",
      "        [0.0225],\n",
      "        [0.0680],\n",
      "        [0.0484],\n",
      "        [0.0684],\n",
      "        [0.0231],\n",
      "        [0.0283],\n",
      "        [0.0388],\n",
      "        [0.0333],\n",
      "        [0.2009],\n",
      "        [0.1478],\n",
      "        [0.1555],\n",
      "        [0.1870],\n",
      "        [0.1936],\n",
      "        [0.0485],\n",
      "        [0.1955],\n",
      "        [0.0621],\n",
      "        [0.0651],\n",
      "        [0.0712],\n",
      "        [0.1347],\n",
      "        [0.0862],\n",
      "        [0.1543],\n",
      "        [0.2188],\n",
      "        [0.1023],\n",
      "        [0.1081],\n",
      "        [0.1114],\n",
      "        [0.1167],\n",
      "        [0.1207],\n",
      "        [0.1247],\n",
      "        [0.1284],\n",
      "        [0.1329],\n",
      "        [0.1388],\n",
      "        [0.1873],\n",
      "        [0.2182],\n",
      "        [0.1626],\n",
      "        [0.1710],\n",
      "        [0.1858],\n",
      "        [0.1862],\n",
      "        [0.1944],\n",
      "        [0.1979],\n",
      "        [0.1989],\n",
      "        [0.2022],\n",
      "        [0.2116],\n",
      "        [0.2161],\n",
      "        [0.1718],\n",
      "        [0.0589],\n",
      "        [0.1782],\n",
      "        [0.0614],\n",
      "        [0.1741],\n",
      "        [0.0805],\n",
      "        [0.0824],\n",
      "        [0.0932],\n",
      "        [0.1021],\n",
      "        [0.1391],\n",
      "        [0.0733],\n",
      "        [0.0545],\n",
      "        [0.0543],\n",
      "        [0.0124],\n",
      "        [0.0355],\n",
      "        [0.0773],\n",
      "        [0.1044],\n",
      "        [0.1094],\n",
      "        [0.1372],\n",
      "        [0.1382],\n",
      "        [0.1506],\n",
      "        [0.1525],\n",
      "        [0.1534],\n",
      "        [0.1646],\n",
      "        [0.1478],\n",
      "        [0.1742],\n",
      "        [0.1596],\n",
      "        [0.1319],\n",
      "        [0.1739],\n",
      "        [0.1905],\n",
      "        [0.1923],\n",
      "        [0.0350],\n",
      "        [0.1017],\n",
      "        [0.0357],\n",
      "        [0.0206],\n",
      "        [0.0846],\n",
      "        [0.0905],\n",
      "        [0.0919],\n",
      "        [0.1091],\n",
      "        [0.1220],\n",
      "        [0.1222],\n",
      "        [0.1227],\n",
      "        [0.1337],\n",
      "        [0.1358],\n",
      "        [0.1396],\n",
      "        [0.1400],\n",
      "        [0.1448],\n",
      "        [0.1452],\n",
      "        [0.1573],\n",
      "        [0.1214],\n",
      "        [0.0349],\n",
      "        [0.0861],\n",
      "        [0.0762],\n",
      "        [0.0734],\n",
      "        [0.0976],\n",
      "        [0.1040],\n",
      "        [0.1458],\n",
      "        [0.1181],\n",
      "        [0.0651],\n",
      "        [0.1624],\n",
      "        [0.1641],\n",
      "        [0.0755],\n",
      "        [0.0594],\n",
      "        [0.0226],\n",
      "        [0.0897],\n",
      "        [0.1259],\n",
      "        [0.1477],\n",
      "        [0.0778],\n",
      "        [0.1576],\n",
      "        [0.1598],\n",
      "        [0.1668],\n",
      "        [0.1704],\n",
      "        [0.1707],\n",
      "        [0.1961],\n",
      "        [0.2005],\n",
      "        [0.0694],\n",
      "        [0.0457],\n",
      "        [0.0627]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0231],\n",
      "        [0.0327],\n",
      "        [0.0613],\n",
      "        [0.0639],\n",
      "        [0.1081],\n",
      "        [0.0725],\n",
      "        [0.0739],\n",
      "        [0.0646],\n",
      "        [0.0120],\n",
      "        [0.0257],\n",
      "        [0.0507],\n",
      "        [0.0228],\n",
      "        [0.0684],\n",
      "        [0.0487],\n",
      "        [0.0688],\n",
      "        [0.0234],\n",
      "        [0.0286],\n",
      "        [0.0391],\n",
      "        [0.0336],\n",
      "        [0.2013],\n",
      "        [0.1481],\n",
      "        [0.1558],\n",
      "        [0.1873],\n",
      "        [0.1939],\n",
      "        [0.0488],\n",
      "        [0.1959],\n",
      "        [0.0624],\n",
      "        [0.0654],\n",
      "        [0.0716],\n",
      "        [0.1351],\n",
      "        [0.0866],\n",
      "        [0.1546],\n",
      "        [0.2192],\n",
      "        [0.1026],\n",
      "        [0.1084],\n",
      "        [0.1117],\n",
      "        [0.1170],\n",
      "        [0.1210],\n",
      "        [0.1251],\n",
      "        [0.1288],\n",
      "        [0.1333],\n",
      "        [0.1391],\n",
      "        [0.1876],\n",
      "        [0.2186],\n",
      "        [0.1630],\n",
      "        [0.1713],\n",
      "        [0.1861],\n",
      "        [0.1865],\n",
      "        [0.1947],\n",
      "        [0.1982],\n",
      "        [0.1992],\n",
      "        [0.2025],\n",
      "        [0.2119],\n",
      "        [0.2164],\n",
      "        [0.1722],\n",
      "        [0.0592],\n",
      "        [0.1785],\n",
      "        [0.0617],\n",
      "        [0.1744],\n",
      "        [0.0808],\n",
      "        [0.0827],\n",
      "        [0.0935],\n",
      "        [0.1025],\n",
      "        [0.1395],\n",
      "        [0.0729],\n",
      "        [0.0548],\n",
      "        [0.0547],\n",
      "        [0.0128],\n",
      "        [0.0358],\n",
      "        [0.0770],\n",
      "        [0.1041],\n",
      "        [0.1091],\n",
      "        [0.1369],\n",
      "        [0.1379],\n",
      "        [0.1503],\n",
      "        [0.1522],\n",
      "        [0.1531],\n",
      "        [0.1643],\n",
      "        [0.1475],\n",
      "        [0.1739],\n",
      "        [0.1593],\n",
      "        [0.1316],\n",
      "        [0.1736],\n",
      "        [0.1902],\n",
      "        [0.1920],\n",
      "        [0.0347],\n",
      "        [0.1014],\n",
      "        [0.0353],\n",
      "        [0.0202],\n",
      "        [0.0842],\n",
      "        [0.0901],\n",
      "        [0.0916],\n",
      "        [0.1088],\n",
      "        [0.1217],\n",
      "        [0.1218],\n",
      "        [0.1224],\n",
      "        [0.1334],\n",
      "        [0.1354],\n",
      "        [0.1393],\n",
      "        [0.1397],\n",
      "        [0.1444],\n",
      "        [0.1448],\n",
      "        [0.1570],\n",
      "        [0.1211],\n",
      "        [0.0345],\n",
      "        [0.0858],\n",
      "        [0.0758],\n",
      "        [0.0731],\n",
      "        [0.0973],\n",
      "        [0.1037],\n",
      "        [0.1455],\n",
      "        [0.1178],\n",
      "        [0.0647],\n",
      "        [0.1621],\n",
      "        [0.1638],\n",
      "        [0.0752],\n",
      "        [0.0591],\n",
      "        [0.0222],\n",
      "        [0.0894],\n",
      "        [0.1255],\n",
      "        [0.1473],\n",
      "        [0.0775],\n",
      "        [0.1573],\n",
      "        [0.1595],\n",
      "        [0.1665],\n",
      "        [0.1701],\n",
      "        [0.1703],\n",
      "        [0.1957],\n",
      "        [0.2001],\n",
      "        [0.0691],\n",
      "        [0.0453],\n",
      "        [0.0466]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 28.535165786743164\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 133\n",
      "剩餘X 資料 torch.Size([27, 18])\n",
      "剩餘Y 資料 torch.Size([27, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.008622533641755581, 2)\n",
      "The second_loss value of k: (0.014344222843647003, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.7168])\n",
      "目前模型的Data狀態 torch.Size([133, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.7808],\n",
      "        [0.8096]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0231],\n",
      "        [0.0327],\n",
      "        [0.0613],\n",
      "        [0.0639],\n",
      "        [0.1081],\n",
      "        [0.0725],\n",
      "        [0.0739],\n",
      "        [0.0646],\n",
      "        [0.0120],\n",
      "        [0.0257],\n",
      "        [0.0507],\n",
      "        [0.0228],\n",
      "        [0.0684],\n",
      "        [0.0487],\n",
      "        [0.0688],\n",
      "        [0.0234],\n",
      "        [0.0286],\n",
      "        [0.0391],\n",
      "        [0.0336],\n",
      "        [0.2013],\n",
      "        [0.1481],\n",
      "        [0.1558],\n",
      "        [0.1873],\n",
      "        [0.1939],\n",
      "        [0.0488],\n",
      "        [0.1959],\n",
      "        [0.0624],\n",
      "        [0.0654],\n",
      "        [0.0716],\n",
      "        [0.1351],\n",
      "        [0.0866],\n",
      "        [0.1546],\n",
      "        [0.2192],\n",
      "        [0.1026],\n",
      "        [0.1084],\n",
      "        [0.1117],\n",
      "        [0.1170],\n",
      "        [0.1210],\n",
      "        [0.1251],\n",
      "        [0.1288],\n",
      "        [0.1333],\n",
      "        [0.1391],\n",
      "        [0.1876],\n",
      "        [0.2186],\n",
      "        [0.1630],\n",
      "        [0.1713],\n",
      "        [0.1861],\n",
      "        [0.1865],\n",
      "        [0.1947],\n",
      "        [0.1982],\n",
      "        [0.1992],\n",
      "        [0.2025],\n",
      "        [0.2119],\n",
      "        [0.2164],\n",
      "        [0.1722],\n",
      "        [0.0592],\n",
      "        [0.1785],\n",
      "        [0.0617],\n",
      "        [0.1744],\n",
      "        [0.0808],\n",
      "        [0.0827],\n",
      "        [0.0935],\n",
      "        [0.1025],\n",
      "        [0.1395],\n",
      "        [0.0729],\n",
      "        [0.0548],\n",
      "        [0.0547],\n",
      "        [0.0128],\n",
      "        [0.0358],\n",
      "        [0.0770],\n",
      "        [0.1041],\n",
      "        [0.1091],\n",
      "        [0.1369],\n",
      "        [0.1379],\n",
      "        [0.1503],\n",
      "        [0.1522],\n",
      "        [0.1531],\n",
      "        [0.1643],\n",
      "        [0.1475],\n",
      "        [0.1739],\n",
      "        [0.1593],\n",
      "        [0.1316],\n",
      "        [0.1736],\n",
      "        [0.1902],\n",
      "        [0.1920],\n",
      "        [0.0347],\n",
      "        [0.1014],\n",
      "        [0.0353],\n",
      "        [0.0202],\n",
      "        [0.0842],\n",
      "        [0.0901],\n",
      "        [0.0916],\n",
      "        [0.1088],\n",
      "        [0.1217],\n",
      "        [0.1218],\n",
      "        [0.1224],\n",
      "        [0.1334],\n",
      "        [0.1354],\n",
      "        [0.1393],\n",
      "        [0.1397],\n",
      "        [0.1444],\n",
      "        [0.1448],\n",
      "        [0.1570],\n",
      "        [0.1211],\n",
      "        [0.0345],\n",
      "        [0.0858],\n",
      "        [0.0758],\n",
      "        [0.0731],\n",
      "        [0.0973],\n",
      "        [0.1037],\n",
      "        [0.1455],\n",
      "        [0.1178],\n",
      "        [0.0647],\n",
      "        [0.1621],\n",
      "        [0.1638],\n",
      "        [0.0752],\n",
      "        [0.0591],\n",
      "        [0.0222],\n",
      "        [0.0894],\n",
      "        [0.1255],\n",
      "        [0.1473],\n",
      "        [0.0775],\n",
      "        [0.1573],\n",
      "        [0.1595],\n",
      "        [0.1665],\n",
      "        [0.1701],\n",
      "        [0.1703],\n",
      "        [0.1957],\n",
      "        [0.2001],\n",
      "        [0.0691],\n",
      "        [0.0453],\n",
      "        [0.0466],\n",
      "        [0.0929]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 73\n",
      "Number of shrink: 27\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0236],\n",
      "        [0.0332],\n",
      "        [0.0618],\n",
      "        [0.0643],\n",
      "        [0.1086],\n",
      "        [0.0730],\n",
      "        [0.0744],\n",
      "        [0.0651],\n",
      "        [0.0125],\n",
      "        [0.0262],\n",
      "        [0.0512],\n",
      "        [0.0233],\n",
      "        [0.0688],\n",
      "        [0.0492],\n",
      "        [0.0692],\n",
      "        [0.0239],\n",
      "        [0.0291],\n",
      "        [0.0396],\n",
      "        [0.0341],\n",
      "        [0.2018],\n",
      "        [0.1486],\n",
      "        [0.1563],\n",
      "        [0.1878],\n",
      "        [0.1944],\n",
      "        [0.0493],\n",
      "        [0.1963],\n",
      "        [0.0629],\n",
      "        [0.0659],\n",
      "        [0.0721],\n",
      "        [0.1355],\n",
      "        [0.0871],\n",
      "        [0.1551],\n",
      "        [0.2196],\n",
      "        [0.1031],\n",
      "        [0.1089],\n",
      "        [0.1122],\n",
      "        [0.1175],\n",
      "        [0.1215],\n",
      "        [0.1256],\n",
      "        [0.1292],\n",
      "        [0.1337],\n",
      "        [0.1396],\n",
      "        [0.1881],\n",
      "        [0.2191],\n",
      "        [0.1634],\n",
      "        [0.1718],\n",
      "        [0.1866],\n",
      "        [0.1870],\n",
      "        [0.1952],\n",
      "        [0.1987],\n",
      "        [0.1997],\n",
      "        [0.2030],\n",
      "        [0.2124],\n",
      "        [0.2169],\n",
      "        [0.1726],\n",
      "        [0.0597],\n",
      "        [0.1790],\n",
      "        [0.0622],\n",
      "        [0.1749],\n",
      "        [0.0813],\n",
      "        [0.0832],\n",
      "        [0.0940],\n",
      "        [0.1030],\n",
      "        [0.1399],\n",
      "        [0.0724],\n",
      "        [0.0553],\n",
      "        [0.0551],\n",
      "        [0.0133],\n",
      "        [0.0363],\n",
      "        [0.0765],\n",
      "        [0.1036],\n",
      "        [0.1086],\n",
      "        [0.1364],\n",
      "        [0.1374],\n",
      "        [0.1498],\n",
      "        [0.1517],\n",
      "        [0.1526],\n",
      "        [0.1638],\n",
      "        [0.1470],\n",
      "        [0.1734],\n",
      "        [0.1588],\n",
      "        [0.1311],\n",
      "        [0.1731],\n",
      "        [0.1897],\n",
      "        [0.1915],\n",
      "        [0.0342],\n",
      "        [0.1009],\n",
      "        [0.0348],\n",
      "        [0.0198],\n",
      "        [0.0838],\n",
      "        [0.0896],\n",
      "        [0.0911],\n",
      "        [0.1083],\n",
      "        [0.1212],\n",
      "        [0.1214],\n",
      "        [0.1219],\n",
      "        [0.1329],\n",
      "        [0.1350],\n",
      "        [0.1388],\n",
      "        [0.1392],\n",
      "        [0.1440],\n",
      "        [0.1444],\n",
      "        [0.1565],\n",
      "        [0.1206],\n",
      "        [0.0341],\n",
      "        [0.0853],\n",
      "        [0.0753],\n",
      "        [0.0726],\n",
      "        [0.0968],\n",
      "        [0.1032],\n",
      "        [0.1450],\n",
      "        [0.1173],\n",
      "        [0.0643],\n",
      "        [0.1616],\n",
      "        [0.1633],\n",
      "        [0.0747],\n",
      "        [0.0586],\n",
      "        [0.0218],\n",
      "        [0.0889],\n",
      "        [0.1250],\n",
      "        [0.1469],\n",
      "        [0.0770],\n",
      "        [0.1568],\n",
      "        [0.1590],\n",
      "        [0.1660],\n",
      "        [0.1696],\n",
      "        [0.1699],\n",
      "        [0.1952],\n",
      "        [0.1997],\n",
      "        [0.0686],\n",
      "        [0.0449],\n",
      "        [0.0461],\n",
      "        [0.0636]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 28.77277421951294\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 134\n",
      "剩餘X 資料 torch.Size([26, 18])\n",
      "剩餘Y 資料 torch.Size([26, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.01422969065606594, 1)\n",
      "The second_loss value of k: (0.04556174948811531, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.6611])\n",
      "目前模型的Data狀態 torch.Size([134, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804],\n",
      "        [0.7804]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0236],\n",
      "        [0.0332],\n",
      "        [0.0618],\n",
      "        [0.0643],\n",
      "        [0.1086],\n",
      "        [0.0730],\n",
      "        [0.0744],\n",
      "        [0.0651],\n",
      "        [0.0125],\n",
      "        [0.0262],\n",
      "        [0.0512],\n",
      "        [0.0233],\n",
      "        [0.0688],\n",
      "        [0.0492],\n",
      "        [0.0692],\n",
      "        [0.0239],\n",
      "        [0.0291],\n",
      "        [0.0396],\n",
      "        [0.0341],\n",
      "        [0.2018],\n",
      "        [0.1486],\n",
      "        [0.1563],\n",
      "        [0.1878],\n",
      "        [0.1944],\n",
      "        [0.0493],\n",
      "        [0.1963],\n",
      "        [0.0629],\n",
      "        [0.0659],\n",
      "        [0.0721],\n",
      "        [0.1355],\n",
      "        [0.0871],\n",
      "        [0.1551],\n",
      "        [0.2196],\n",
      "        [0.1031],\n",
      "        [0.1089],\n",
      "        [0.1122],\n",
      "        [0.1175],\n",
      "        [0.1215],\n",
      "        [0.1256],\n",
      "        [0.1292],\n",
      "        [0.1337],\n",
      "        [0.1396],\n",
      "        [0.1881],\n",
      "        [0.2191],\n",
      "        [0.1634],\n",
      "        [0.1718],\n",
      "        [0.1866],\n",
      "        [0.1870],\n",
      "        [0.1952],\n",
      "        [0.1987],\n",
      "        [0.1997],\n",
      "        [0.2030],\n",
      "        [0.2124],\n",
      "        [0.2169],\n",
      "        [0.1726],\n",
      "        [0.0597],\n",
      "        [0.1790],\n",
      "        [0.0622],\n",
      "        [0.1749],\n",
      "        [0.0813],\n",
      "        [0.0832],\n",
      "        [0.0940],\n",
      "        [0.1030],\n",
      "        [0.1399],\n",
      "        [0.0724],\n",
      "        [0.0553],\n",
      "        [0.0551],\n",
      "        [0.0133],\n",
      "        [0.0363],\n",
      "        [0.0765],\n",
      "        [0.1036],\n",
      "        [0.1086],\n",
      "        [0.1364],\n",
      "        [0.1374],\n",
      "        [0.1498],\n",
      "        [0.1517],\n",
      "        [0.1526],\n",
      "        [0.1638],\n",
      "        [0.1470],\n",
      "        [0.1734],\n",
      "        [0.1588],\n",
      "        [0.1311],\n",
      "        [0.1731],\n",
      "        [0.1897],\n",
      "        [0.1915],\n",
      "        [0.0342],\n",
      "        [0.1009],\n",
      "        [0.0348],\n",
      "        [0.0198],\n",
      "        [0.0838],\n",
      "        [0.0896],\n",
      "        [0.0911],\n",
      "        [0.1083],\n",
      "        [0.1212],\n",
      "        [0.1214],\n",
      "        [0.1219],\n",
      "        [0.1329],\n",
      "        [0.1350],\n",
      "        [0.1388],\n",
      "        [0.1392],\n",
      "        [0.1440],\n",
      "        [0.1444],\n",
      "        [0.1565],\n",
      "        [0.1206],\n",
      "        [0.0341],\n",
      "        [0.0853],\n",
      "        [0.0753],\n",
      "        [0.0726],\n",
      "        [0.0968],\n",
      "        [0.1032],\n",
      "        [0.1450],\n",
      "        [0.1173],\n",
      "        [0.0643],\n",
      "        [0.1616],\n",
      "        [0.1633],\n",
      "        [0.0747],\n",
      "        [0.0586],\n",
      "        [0.0218],\n",
      "        [0.0889],\n",
      "        [0.1250],\n",
      "        [0.1469],\n",
      "        [0.0770],\n",
      "        [0.1568],\n",
      "        [0.1590],\n",
      "        [0.1660],\n",
      "        [0.1696],\n",
      "        [0.1699],\n",
      "        [0.1952],\n",
      "        [0.1997],\n",
      "        [0.0686],\n",
      "        [0.0449],\n",
      "        [0.0461],\n",
      "        [0.0636],\n",
      "        [0.1193]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0245],\n",
      "        [0.0341],\n",
      "        [0.0627],\n",
      "        [0.0652],\n",
      "        [0.1095],\n",
      "        [0.0739],\n",
      "        [0.0753],\n",
      "        [0.0660],\n",
      "        [0.0134],\n",
      "        [0.0271],\n",
      "        [0.0521],\n",
      "        [0.0242],\n",
      "        [0.0697],\n",
      "        [0.0501],\n",
      "        [0.0701],\n",
      "        [0.0248],\n",
      "        [0.0300],\n",
      "        [0.0405],\n",
      "        [0.0350],\n",
      "        [0.2026],\n",
      "        [0.1495],\n",
      "        [0.1572],\n",
      "        [0.1887],\n",
      "        [0.1953],\n",
      "        [0.0502],\n",
      "        [0.1972],\n",
      "        [0.0638],\n",
      "        [0.0668],\n",
      "        [0.0729],\n",
      "        [0.1364],\n",
      "        [0.0879],\n",
      "        [0.1560],\n",
      "        [0.2205],\n",
      "        [0.1040],\n",
      "        [0.1098],\n",
      "        [0.1131],\n",
      "        [0.1184],\n",
      "        [0.1224],\n",
      "        [0.1265],\n",
      "        [0.1301],\n",
      "        [0.1346],\n",
      "        [0.1405],\n",
      "        [0.1890],\n",
      "        [0.2199],\n",
      "        [0.1643],\n",
      "        [0.1727],\n",
      "        [0.1875],\n",
      "        [0.1879],\n",
      "        [0.1961],\n",
      "        [0.1996],\n",
      "        [0.2006],\n",
      "        [0.2039],\n",
      "        [0.2133],\n",
      "        [0.2178],\n",
      "        [0.1735],\n",
      "        [0.0606],\n",
      "        [0.1799],\n",
      "        [0.0631],\n",
      "        [0.1758],\n",
      "        [0.0822],\n",
      "        [0.0841],\n",
      "        [0.0949],\n",
      "        [0.1038],\n",
      "        [0.1408],\n",
      "        [0.0716],\n",
      "        [0.0562],\n",
      "        [0.0560],\n",
      "        [0.0142],\n",
      "        [0.0372],\n",
      "        [0.0756],\n",
      "        [0.1027],\n",
      "        [0.1077],\n",
      "        [0.1355],\n",
      "        [0.1365],\n",
      "        [0.1489],\n",
      "        [0.1508],\n",
      "        [0.1517],\n",
      "        [0.1629],\n",
      "        [0.1461],\n",
      "        [0.1725],\n",
      "        [0.1579],\n",
      "        [0.1302],\n",
      "        [0.1722],\n",
      "        [0.1888],\n",
      "        [0.1906],\n",
      "        [0.0333],\n",
      "        [0.1000],\n",
      "        [0.0339],\n",
      "        [0.0189],\n",
      "        [0.0829],\n",
      "        [0.0887],\n",
      "        [0.0902],\n",
      "        [0.1074],\n",
      "        [0.1203],\n",
      "        [0.1205],\n",
      "        [0.1210],\n",
      "        [0.1320],\n",
      "        [0.1341],\n",
      "        [0.1379],\n",
      "        [0.1383],\n",
      "        [0.1431],\n",
      "        [0.1435],\n",
      "        [0.1556],\n",
      "        [0.1197],\n",
      "        [0.0332],\n",
      "        [0.0844],\n",
      "        [0.0745],\n",
      "        [0.0717],\n",
      "        [0.0959],\n",
      "        [0.1023],\n",
      "        [0.1441],\n",
      "        [0.1164],\n",
      "        [0.0634],\n",
      "        [0.1607],\n",
      "        [0.1624],\n",
      "        [0.0738],\n",
      "        [0.0577],\n",
      "        [0.0209],\n",
      "        [0.0880],\n",
      "        [0.1242],\n",
      "        [0.1460],\n",
      "        [0.0761],\n",
      "        [0.1559],\n",
      "        [0.1581],\n",
      "        [0.1651],\n",
      "        [0.1687],\n",
      "        [0.1690],\n",
      "        [0.1943],\n",
      "        [0.1988],\n",
      "        [0.0677],\n",
      "        [0.0440],\n",
      "        [0.0452],\n",
      "        [0.0627],\n",
      "        [0.1184]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 29.008683681488037\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 135\n",
      "剩餘X 資料 torch.Size([25, 18])\n",
      "剩餘Y 資料 torch.Size([25, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.045182742178440094, 1)\n",
      "The second_loss value of k: (0.04652472957968712, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.5669])\n",
      "目前模型的Data狀態 torch.Size([135, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795],\n",
      "        [0.7795]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0245],\n",
      "        [0.0341],\n",
      "        [0.0627],\n",
      "        [0.0652],\n",
      "        [0.1095],\n",
      "        [0.0739],\n",
      "        [0.0753],\n",
      "        [0.0660],\n",
      "        [0.0134],\n",
      "        [0.0271],\n",
      "        [0.0521],\n",
      "        [0.0242],\n",
      "        [0.0697],\n",
      "        [0.0501],\n",
      "        [0.0701],\n",
      "        [0.0248],\n",
      "        [0.0300],\n",
      "        [0.0405],\n",
      "        [0.0350],\n",
      "        [0.2026],\n",
      "        [0.1495],\n",
      "        [0.1572],\n",
      "        [0.1887],\n",
      "        [0.1953],\n",
      "        [0.0502],\n",
      "        [0.1972],\n",
      "        [0.0638],\n",
      "        [0.0668],\n",
      "        [0.0729],\n",
      "        [0.1364],\n",
      "        [0.0879],\n",
      "        [0.1560],\n",
      "        [0.2205],\n",
      "        [0.1040],\n",
      "        [0.1098],\n",
      "        [0.1131],\n",
      "        [0.1184],\n",
      "        [0.1224],\n",
      "        [0.1265],\n",
      "        [0.1301],\n",
      "        [0.1346],\n",
      "        [0.1405],\n",
      "        [0.1890],\n",
      "        [0.2199],\n",
      "        [0.1643],\n",
      "        [0.1727],\n",
      "        [0.1875],\n",
      "        [0.1879],\n",
      "        [0.1961],\n",
      "        [0.1996],\n",
      "        [0.2006],\n",
      "        [0.2039],\n",
      "        [0.2133],\n",
      "        [0.2178],\n",
      "        [0.1735],\n",
      "        [0.0606],\n",
      "        [0.1799],\n",
      "        [0.0631],\n",
      "        [0.1758],\n",
      "        [0.0822],\n",
      "        [0.0841],\n",
      "        [0.0949],\n",
      "        [0.1038],\n",
      "        [0.1408],\n",
      "        [0.0716],\n",
      "        [0.0562],\n",
      "        [0.0560],\n",
      "        [0.0142],\n",
      "        [0.0372],\n",
      "        [0.0756],\n",
      "        [0.1027],\n",
      "        [0.1077],\n",
      "        [0.1355],\n",
      "        [0.1365],\n",
      "        [0.1489],\n",
      "        [0.1508],\n",
      "        [0.1517],\n",
      "        [0.1629],\n",
      "        [0.1461],\n",
      "        [0.1725],\n",
      "        [0.1579],\n",
      "        [0.1302],\n",
      "        [0.1722],\n",
      "        [0.1888],\n",
      "        [0.1906],\n",
      "        [0.0333],\n",
      "        [0.1000],\n",
      "        [0.0339],\n",
      "        [0.0189],\n",
      "        [0.0829],\n",
      "        [0.0887],\n",
      "        [0.0902],\n",
      "        [0.1074],\n",
      "        [0.1203],\n",
      "        [0.1205],\n",
      "        [0.1210],\n",
      "        [0.1320],\n",
      "        [0.1341],\n",
      "        [0.1379],\n",
      "        [0.1383],\n",
      "        [0.1431],\n",
      "        [0.1435],\n",
      "        [0.1556],\n",
      "        [0.1197],\n",
      "        [0.0332],\n",
      "        [0.0844],\n",
      "        [0.0745],\n",
      "        [0.0717],\n",
      "        [0.0959],\n",
      "        [0.1023],\n",
      "        [0.1441],\n",
      "        [0.1164],\n",
      "        [0.0634],\n",
      "        [0.1607],\n",
      "        [0.1624],\n",
      "        [0.0738],\n",
      "        [0.0577],\n",
      "        [0.0209],\n",
      "        [0.0880],\n",
      "        [0.1242],\n",
      "        [0.1460],\n",
      "        [0.0761],\n",
      "        [0.1559],\n",
      "        [0.1581],\n",
      "        [0.1651],\n",
      "        [0.1687],\n",
      "        [0.1690],\n",
      "        [0.1943],\n",
      "        [0.1988],\n",
      "        [0.0677],\n",
      "        [0.0440],\n",
      "        [0.0452],\n",
      "        [0.0627],\n",
      "        [0.1184],\n",
      "        [0.2126]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 65\n",
      "Number of shrink: 35\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0261],\n",
      "        [0.0357],\n",
      "        [0.0643],\n",
      "        [0.0668],\n",
      "        [0.1111],\n",
      "        [0.0755],\n",
      "        [0.0769],\n",
      "        [0.0676],\n",
      "        [0.0150],\n",
      "        [0.0287],\n",
      "        [0.0537],\n",
      "        [0.0258],\n",
      "        [0.0713],\n",
      "        [0.0517],\n",
      "        [0.0717],\n",
      "        [0.0264],\n",
      "        [0.0316],\n",
      "        [0.0421],\n",
      "        [0.0366],\n",
      "        [0.2043],\n",
      "        [0.1511],\n",
      "        [0.1588],\n",
      "        [0.1903],\n",
      "        [0.1969],\n",
      "        [0.0518],\n",
      "        [0.1988],\n",
      "        [0.0654],\n",
      "        [0.0684],\n",
      "        [0.0745],\n",
      "        [0.1380],\n",
      "        [0.0896],\n",
      "        [0.1576],\n",
      "        [0.2221],\n",
      "        [0.1056],\n",
      "        [0.1114],\n",
      "        [0.1147],\n",
      "        [0.1200],\n",
      "        [0.1240],\n",
      "        [0.1281],\n",
      "        [0.1317],\n",
      "        [0.1362],\n",
      "        [0.1421],\n",
      "        [0.1906],\n",
      "        [0.2216],\n",
      "        [0.1659],\n",
      "        [0.1743],\n",
      "        [0.1891],\n",
      "        [0.1895],\n",
      "        [0.1977],\n",
      "        [0.2012],\n",
      "        [0.2022],\n",
      "        [0.2055],\n",
      "        [0.2149],\n",
      "        [0.2194],\n",
      "        [0.1751],\n",
      "        [0.0622],\n",
      "        [0.1815],\n",
      "        [0.0647],\n",
      "        [0.1774],\n",
      "        [0.0838],\n",
      "        [0.0857],\n",
      "        [0.0965],\n",
      "        [0.1055],\n",
      "        [0.1424],\n",
      "        [0.0699],\n",
      "        [0.0578],\n",
      "        [0.0576],\n",
      "        [0.0158],\n",
      "        [0.0388],\n",
      "        [0.0740],\n",
      "        [0.1011],\n",
      "        [0.1061],\n",
      "        [0.1339],\n",
      "        [0.1349],\n",
      "        [0.1473],\n",
      "        [0.1492],\n",
      "        [0.1501],\n",
      "        [0.1613],\n",
      "        [0.1445],\n",
      "        [0.1709],\n",
      "        [0.1563],\n",
      "        [0.1286],\n",
      "        [0.1706],\n",
      "        [0.1872],\n",
      "        [0.1890],\n",
      "        [0.0317],\n",
      "        [0.0984],\n",
      "        [0.0323],\n",
      "        [0.0173],\n",
      "        [0.0813],\n",
      "        [0.0871],\n",
      "        [0.0886],\n",
      "        [0.1058],\n",
      "        [0.1187],\n",
      "        [0.1189],\n",
      "        [0.1194],\n",
      "        [0.1304],\n",
      "        [0.1325],\n",
      "        [0.1363],\n",
      "        [0.1367],\n",
      "        [0.1415],\n",
      "        [0.1419],\n",
      "        [0.1540],\n",
      "        [0.1181],\n",
      "        [0.0316],\n",
      "        [0.0828],\n",
      "        [0.0728],\n",
      "        [0.0701],\n",
      "        [0.0943],\n",
      "        [0.1007],\n",
      "        [0.1425],\n",
      "        [0.1148],\n",
      "        [0.0618],\n",
      "        [0.1591],\n",
      "        [0.1608],\n",
      "        [0.0722],\n",
      "        [0.0561],\n",
      "        [0.0193],\n",
      "        [0.0864],\n",
      "        [0.1226],\n",
      "        [0.1444],\n",
      "        [0.0745],\n",
      "        [0.1543],\n",
      "        [0.1565],\n",
      "        [0.1635],\n",
      "        [0.1671],\n",
      "        [0.1674],\n",
      "        [0.1927],\n",
      "        [0.1972],\n",
      "        [0.0661],\n",
      "        [0.0424],\n",
      "        [0.0436],\n",
      "        [0.0611],\n",
      "        [0.1168],\n",
      "        [0.2110]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 29.253615617752075\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 136\n",
      "剩餘X 資料 torch.Size([24, 18])\n",
      "剩餘Y 資料 torch.Size([24, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.045835595577955246, 8)\n",
      "The second_loss value of k: (0.047000136226415634, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.5638])\n",
      "目前模型的Data狀態 torch.Size([136, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779],\n",
      "        [0.7779]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0261],\n",
      "        [0.0357],\n",
      "        [0.0643],\n",
      "        [0.0668],\n",
      "        [0.1111],\n",
      "        [0.0755],\n",
      "        [0.0769],\n",
      "        [0.0676],\n",
      "        [0.0150],\n",
      "        [0.0287],\n",
      "        [0.0537],\n",
      "        [0.0258],\n",
      "        [0.0713],\n",
      "        [0.0517],\n",
      "        [0.0717],\n",
      "        [0.0264],\n",
      "        [0.0316],\n",
      "        [0.0421],\n",
      "        [0.0366],\n",
      "        [0.2043],\n",
      "        [0.1511],\n",
      "        [0.1588],\n",
      "        [0.1903],\n",
      "        [0.1969],\n",
      "        [0.0518],\n",
      "        [0.1988],\n",
      "        [0.0654],\n",
      "        [0.0684],\n",
      "        [0.0745],\n",
      "        [0.1380],\n",
      "        [0.0896],\n",
      "        [0.1576],\n",
      "        [0.2221],\n",
      "        [0.1056],\n",
      "        [0.1114],\n",
      "        [0.1147],\n",
      "        [0.1200],\n",
      "        [0.1240],\n",
      "        [0.1281],\n",
      "        [0.1317],\n",
      "        [0.1362],\n",
      "        [0.1421],\n",
      "        [0.1906],\n",
      "        [0.2216],\n",
      "        [0.1659],\n",
      "        [0.1743],\n",
      "        [0.1891],\n",
      "        [0.1895],\n",
      "        [0.1977],\n",
      "        [0.2012],\n",
      "        [0.2022],\n",
      "        [0.2055],\n",
      "        [0.2149],\n",
      "        [0.2194],\n",
      "        [0.1751],\n",
      "        [0.0622],\n",
      "        [0.1815],\n",
      "        [0.0647],\n",
      "        [0.1774],\n",
      "        [0.0838],\n",
      "        [0.0857],\n",
      "        [0.0965],\n",
      "        [0.1055],\n",
      "        [0.1424],\n",
      "        [0.0699],\n",
      "        [0.0578],\n",
      "        [0.0576],\n",
      "        [0.0158],\n",
      "        [0.0388],\n",
      "        [0.0740],\n",
      "        [0.1011],\n",
      "        [0.1061],\n",
      "        [0.1339],\n",
      "        [0.1349],\n",
      "        [0.1473],\n",
      "        [0.1492],\n",
      "        [0.1501],\n",
      "        [0.1613],\n",
      "        [0.1445],\n",
      "        [0.1709],\n",
      "        [0.1563],\n",
      "        [0.1286],\n",
      "        [0.1706],\n",
      "        [0.1872],\n",
      "        [0.1890],\n",
      "        [0.0317],\n",
      "        [0.0984],\n",
      "        [0.0323],\n",
      "        [0.0173],\n",
      "        [0.0813],\n",
      "        [0.0871],\n",
      "        [0.0886],\n",
      "        [0.1058],\n",
      "        [0.1187],\n",
      "        [0.1189],\n",
      "        [0.1194],\n",
      "        [0.1304],\n",
      "        [0.1325],\n",
      "        [0.1363],\n",
      "        [0.1367],\n",
      "        [0.1415],\n",
      "        [0.1419],\n",
      "        [0.1540],\n",
      "        [0.1181],\n",
      "        [0.0316],\n",
      "        [0.0828],\n",
      "        [0.0728],\n",
      "        [0.0701],\n",
      "        [0.0943],\n",
      "        [0.1007],\n",
      "        [0.1425],\n",
      "        [0.1148],\n",
      "        [0.0618],\n",
      "        [0.1591],\n",
      "        [0.1608],\n",
      "        [0.0722],\n",
      "        [0.0561],\n",
      "        [0.0193],\n",
      "        [0.0864],\n",
      "        [0.1226],\n",
      "        [0.1444],\n",
      "        [0.0745],\n",
      "        [0.1543],\n",
      "        [0.1565],\n",
      "        [0.1635],\n",
      "        [0.1671],\n",
      "        [0.1674],\n",
      "        [0.1927],\n",
      "        [0.1972],\n",
      "        [0.0661],\n",
      "        [0.0424],\n",
      "        [0.0436],\n",
      "        [0.0611],\n",
      "        [0.1168],\n",
      "        [0.2110],\n",
      "        [0.2141]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 74\n",
      "Number of shrink: 26\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0276],\n",
      "        [0.0372],\n",
      "        [0.0658],\n",
      "        [0.0684],\n",
      "        [0.1126],\n",
      "        [0.0770],\n",
      "        [0.0784],\n",
      "        [0.0691],\n",
      "        [0.0166],\n",
      "        [0.0302],\n",
      "        [0.0552],\n",
      "        [0.0273],\n",
      "        [0.0729],\n",
      "        [0.0532],\n",
      "        [0.0733],\n",
      "        [0.0280],\n",
      "        [0.0331],\n",
      "        [0.0437],\n",
      "        [0.0381],\n",
      "        [0.2058],\n",
      "        [0.1527],\n",
      "        [0.1603],\n",
      "        [0.1919],\n",
      "        [0.1984],\n",
      "        [0.0533],\n",
      "        [0.2004],\n",
      "        [0.0670],\n",
      "        [0.0699],\n",
      "        [0.0761],\n",
      "        [0.1396],\n",
      "        [0.0911],\n",
      "        [0.1591],\n",
      "        [0.2237],\n",
      "        [0.1072],\n",
      "        [0.1129],\n",
      "        [0.1163],\n",
      "        [0.1215],\n",
      "        [0.1255],\n",
      "        [0.1296],\n",
      "        [0.1333],\n",
      "        [0.1378],\n",
      "        [0.1437],\n",
      "        [0.1922],\n",
      "        [0.2231],\n",
      "        [0.1675],\n",
      "        [0.1758],\n",
      "        [0.1906],\n",
      "        [0.1910],\n",
      "        [0.1992],\n",
      "        [0.2027],\n",
      "        [0.2037],\n",
      "        [0.2070],\n",
      "        [0.2164],\n",
      "        [0.2209],\n",
      "        [0.1767],\n",
      "        [0.0637],\n",
      "        [0.1831],\n",
      "        [0.0662],\n",
      "        [0.1789],\n",
      "        [0.0853],\n",
      "        [0.0872],\n",
      "        [0.0980],\n",
      "        [0.1070],\n",
      "        [0.1440],\n",
      "        [0.0684],\n",
      "        [0.0593],\n",
      "        [0.0592],\n",
      "        [0.0173],\n",
      "        [0.0403],\n",
      "        [0.0725],\n",
      "        [0.0996],\n",
      "        [0.1046],\n",
      "        [0.1324],\n",
      "        [0.1333],\n",
      "        [0.1458],\n",
      "        [0.1477],\n",
      "        [0.1485],\n",
      "        [0.1597],\n",
      "        [0.1430],\n",
      "        [0.1693],\n",
      "        [0.1547],\n",
      "        [0.1271],\n",
      "        [0.1691],\n",
      "        [0.1856],\n",
      "        [0.1875],\n",
      "        [0.0302],\n",
      "        [0.0969],\n",
      "        [0.0308],\n",
      "        [0.0157],\n",
      "        [0.0797],\n",
      "        [0.0856],\n",
      "        [0.0870],\n",
      "        [0.1043],\n",
      "        [0.1172],\n",
      "        [0.1173],\n",
      "        [0.1178],\n",
      "        [0.1289],\n",
      "        [0.1309],\n",
      "        [0.1348],\n",
      "        [0.1351],\n",
      "        [0.1399],\n",
      "        [0.1403],\n",
      "        [0.1525],\n",
      "        [0.1166],\n",
      "        [0.0300],\n",
      "        [0.0813],\n",
      "        [0.0713],\n",
      "        [0.0685],\n",
      "        [0.0928],\n",
      "        [0.0992],\n",
      "        [0.1410],\n",
      "        [0.1133],\n",
      "        [0.0602],\n",
      "        [0.1576],\n",
      "        [0.1592],\n",
      "        [0.0707],\n",
      "        [0.0545],\n",
      "        [0.0177],\n",
      "        [0.0849],\n",
      "        [0.1210],\n",
      "        [0.1428],\n",
      "        [0.0729],\n",
      "        [0.1527],\n",
      "        [0.1550],\n",
      "        [0.1620],\n",
      "        [0.1656],\n",
      "        [0.1658],\n",
      "        [0.1912],\n",
      "        [0.1956],\n",
      "        [0.0646],\n",
      "        [0.0408],\n",
      "        [0.0421],\n",
      "        [0.0596],\n",
      "        [0.1152],\n",
      "        [0.2094],\n",
      "        [0.2125]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 29.49192237854004\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 137\n",
      "剩餘X 資料 torch.Size([23, 18])\n",
      "剩餘Y 資料 torch.Size([23, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.04633244127035141, 9)\n",
      "The second_loss value of k: (0.04723048210144043, 8)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.5611])\n",
      "目前模型的Data狀態 torch.Size([137, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763],\n",
      "        [0.7763]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0276],\n",
      "        [0.0372],\n",
      "        [0.0658],\n",
      "        [0.0684],\n",
      "        [0.1126],\n",
      "        [0.0770],\n",
      "        [0.0784],\n",
      "        [0.0691],\n",
      "        [0.0166],\n",
      "        [0.0302],\n",
      "        [0.0552],\n",
      "        [0.0273],\n",
      "        [0.0729],\n",
      "        [0.0532],\n",
      "        [0.0733],\n",
      "        [0.0280],\n",
      "        [0.0331],\n",
      "        [0.0437],\n",
      "        [0.0381],\n",
      "        [0.2058],\n",
      "        [0.1527],\n",
      "        [0.1603],\n",
      "        [0.1919],\n",
      "        [0.1984],\n",
      "        [0.0533],\n",
      "        [0.2004],\n",
      "        [0.0670],\n",
      "        [0.0699],\n",
      "        [0.0761],\n",
      "        [0.1396],\n",
      "        [0.0911],\n",
      "        [0.1591],\n",
      "        [0.2237],\n",
      "        [0.1072],\n",
      "        [0.1129],\n",
      "        [0.1163],\n",
      "        [0.1215],\n",
      "        [0.1255],\n",
      "        [0.1296],\n",
      "        [0.1333],\n",
      "        [0.1378],\n",
      "        [0.1437],\n",
      "        [0.1922],\n",
      "        [0.2231],\n",
      "        [0.1675],\n",
      "        [0.1758],\n",
      "        [0.1906],\n",
      "        [0.1910],\n",
      "        [0.1992],\n",
      "        [0.2027],\n",
      "        [0.2037],\n",
      "        [0.2070],\n",
      "        [0.2164],\n",
      "        [0.2209],\n",
      "        [0.1767],\n",
      "        [0.0637],\n",
      "        [0.1831],\n",
      "        [0.0662],\n",
      "        [0.1789],\n",
      "        [0.0853],\n",
      "        [0.0872],\n",
      "        [0.0980],\n",
      "        [0.1070],\n",
      "        [0.1440],\n",
      "        [0.0684],\n",
      "        [0.0593],\n",
      "        [0.0592],\n",
      "        [0.0173],\n",
      "        [0.0403],\n",
      "        [0.0725],\n",
      "        [0.0996],\n",
      "        [0.1046],\n",
      "        [0.1324],\n",
      "        [0.1333],\n",
      "        [0.1458],\n",
      "        [0.1477],\n",
      "        [0.1485],\n",
      "        [0.1597],\n",
      "        [0.1430],\n",
      "        [0.1693],\n",
      "        [0.1547],\n",
      "        [0.1271],\n",
      "        [0.1691],\n",
      "        [0.1856],\n",
      "        [0.1875],\n",
      "        [0.0302],\n",
      "        [0.0969],\n",
      "        [0.0308],\n",
      "        [0.0157],\n",
      "        [0.0797],\n",
      "        [0.0856],\n",
      "        [0.0870],\n",
      "        [0.1043],\n",
      "        [0.1172],\n",
      "        [0.1173],\n",
      "        [0.1178],\n",
      "        [0.1289],\n",
      "        [0.1309],\n",
      "        [0.1348],\n",
      "        [0.1351],\n",
      "        [0.1399],\n",
      "        [0.1403],\n",
      "        [0.1525],\n",
      "        [0.1166],\n",
      "        [0.0300],\n",
      "        [0.0813],\n",
      "        [0.0713],\n",
      "        [0.0685],\n",
      "        [0.0928],\n",
      "        [0.0992],\n",
      "        [0.1410],\n",
      "        [0.1133],\n",
      "        [0.0602],\n",
      "        [0.1576],\n",
      "        [0.1592],\n",
      "        [0.0707],\n",
      "        [0.0545],\n",
      "        [0.0177],\n",
      "        [0.0849],\n",
      "        [0.1210],\n",
      "        [0.1428],\n",
      "        [0.0729],\n",
      "        [0.1527],\n",
      "        [0.1550],\n",
      "        [0.1620],\n",
      "        [0.1656],\n",
      "        [0.1658],\n",
      "        [0.1912],\n",
      "        [0.1956],\n",
      "        [0.0646],\n",
      "        [0.0408],\n",
      "        [0.0421],\n",
      "        [0.0596],\n",
      "        [0.1152],\n",
      "        [0.2094],\n",
      "        [0.2125],\n",
      "        [0.2152]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0292],\n",
      "        [0.0388],\n",
      "        [0.0674],\n",
      "        [0.0699],\n",
      "        [0.1142],\n",
      "        [0.0786],\n",
      "        [0.0800],\n",
      "        [0.0707],\n",
      "        [0.0181],\n",
      "        [0.0318],\n",
      "        [0.0568],\n",
      "        [0.0289],\n",
      "        [0.0745],\n",
      "        [0.0548],\n",
      "        [0.0748],\n",
      "        [0.0295],\n",
      "        [0.0347],\n",
      "        [0.0452],\n",
      "        [0.0397],\n",
      "        [0.2074],\n",
      "        [0.1542],\n",
      "        [0.1619],\n",
      "        [0.1934],\n",
      "        [0.2000],\n",
      "        [0.0549],\n",
      "        [0.2020],\n",
      "        [0.0685],\n",
      "        [0.0715],\n",
      "        [0.0777],\n",
      "        [0.1412],\n",
      "        [0.0927],\n",
      "        [0.1607],\n",
      "        [0.2253],\n",
      "        [0.1087],\n",
      "        [0.1145],\n",
      "        [0.1178],\n",
      "        [0.1231],\n",
      "        [0.1271],\n",
      "        [0.1312],\n",
      "        [0.1349],\n",
      "        [0.1394],\n",
      "        [0.1452],\n",
      "        [0.1937],\n",
      "        [0.2247],\n",
      "        [0.1690],\n",
      "        [0.1774],\n",
      "        [0.1922],\n",
      "        [0.1926],\n",
      "        [0.2008],\n",
      "        [0.2043],\n",
      "        [0.2053],\n",
      "        [0.2086],\n",
      "        [0.2180],\n",
      "        [0.2225],\n",
      "        [0.1783],\n",
      "        [0.0653],\n",
      "        [0.1846],\n",
      "        [0.0678],\n",
      "        [0.1805],\n",
      "        [0.0869],\n",
      "        [0.0888],\n",
      "        [0.0996],\n",
      "        [0.1086],\n",
      "        [0.1455],\n",
      "        [0.0668],\n",
      "        [0.0609],\n",
      "        [0.0607],\n",
      "        [0.0189],\n",
      "        [0.0419],\n",
      "        [0.0709],\n",
      "        [0.0980],\n",
      "        [0.1030],\n",
      "        [0.1308],\n",
      "        [0.1318],\n",
      "        [0.1442],\n",
      "        [0.1461],\n",
      "        [0.1470],\n",
      "        [0.1582],\n",
      "        [0.1414],\n",
      "        [0.1678],\n",
      "        [0.1532],\n",
      "        [0.1255],\n",
      "        [0.1675],\n",
      "        [0.1841],\n",
      "        [0.1859],\n",
      "        [0.0286],\n",
      "        [0.0953],\n",
      "        [0.0292],\n",
      "        [0.0141],\n",
      "        [0.0782],\n",
      "        [0.0840],\n",
      "        [0.0855],\n",
      "        [0.1027],\n",
      "        [0.1156],\n",
      "        [0.1158],\n",
      "        [0.1163],\n",
      "        [0.1273],\n",
      "        [0.1294],\n",
      "        [0.1332],\n",
      "        [0.1336],\n",
      "        [0.1384],\n",
      "        [0.1387],\n",
      "        [0.1509],\n",
      "        [0.1150],\n",
      "        [0.0284],\n",
      "        [0.0797],\n",
      "        [0.0697],\n",
      "        [0.0670],\n",
      "        [0.0912],\n",
      "        [0.0976],\n",
      "        [0.1394],\n",
      "        [0.1117],\n",
      "        [0.0587],\n",
      "        [0.1560],\n",
      "        [0.1577],\n",
      "        [0.0691],\n",
      "        [0.0530],\n",
      "        [0.0161],\n",
      "        [0.0833],\n",
      "        [0.1194],\n",
      "        [0.1413],\n",
      "        [0.0714],\n",
      "        [0.1512],\n",
      "        [0.1534],\n",
      "        [0.1604],\n",
      "        [0.1640],\n",
      "        [0.1642],\n",
      "        [0.1896],\n",
      "        [0.1941],\n",
      "        [0.0630],\n",
      "        [0.0393],\n",
      "        [0.0405],\n",
      "        [0.0580],\n",
      "        [0.1137],\n",
      "        [0.2078],\n",
      "        [0.2110],\n",
      "        [0.2137]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 29.728942394256592\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 138\n",
      "剩餘X 資料 torch.Size([22, 18])\n",
      "剩餘Y 資料 torch.Size([22, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.04654993116855621, 8)\n",
      "The second_loss value of k: (0.047920484095811844, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.5590])\n",
      "目前模型的Data狀態 torch.Size([138, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747],\n",
      "        [0.7747]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0292],\n",
      "        [0.0388],\n",
      "        [0.0674],\n",
      "        [0.0699],\n",
      "        [0.1142],\n",
      "        [0.0786],\n",
      "        [0.0800],\n",
      "        [0.0707],\n",
      "        [0.0181],\n",
      "        [0.0318],\n",
      "        [0.0568],\n",
      "        [0.0289],\n",
      "        [0.0745],\n",
      "        [0.0548],\n",
      "        [0.0748],\n",
      "        [0.0295],\n",
      "        [0.0347],\n",
      "        [0.0452],\n",
      "        [0.0397],\n",
      "        [0.2074],\n",
      "        [0.1542],\n",
      "        [0.1619],\n",
      "        [0.1934],\n",
      "        [0.2000],\n",
      "        [0.0549],\n",
      "        [0.2020],\n",
      "        [0.0685],\n",
      "        [0.0715],\n",
      "        [0.0777],\n",
      "        [0.1412],\n",
      "        [0.0927],\n",
      "        [0.1607],\n",
      "        [0.2253],\n",
      "        [0.1087],\n",
      "        [0.1145],\n",
      "        [0.1178],\n",
      "        [0.1231],\n",
      "        [0.1271],\n",
      "        [0.1312],\n",
      "        [0.1349],\n",
      "        [0.1394],\n",
      "        [0.1452],\n",
      "        [0.1937],\n",
      "        [0.2247],\n",
      "        [0.1690],\n",
      "        [0.1774],\n",
      "        [0.1922],\n",
      "        [0.1926],\n",
      "        [0.2008],\n",
      "        [0.2043],\n",
      "        [0.2053],\n",
      "        [0.2086],\n",
      "        [0.2180],\n",
      "        [0.2225],\n",
      "        [0.1783],\n",
      "        [0.0653],\n",
      "        [0.1846],\n",
      "        [0.0678],\n",
      "        [0.1805],\n",
      "        [0.0869],\n",
      "        [0.0888],\n",
      "        [0.0996],\n",
      "        [0.1086],\n",
      "        [0.1455],\n",
      "        [0.0668],\n",
      "        [0.0609],\n",
      "        [0.0607],\n",
      "        [0.0189],\n",
      "        [0.0419],\n",
      "        [0.0709],\n",
      "        [0.0980],\n",
      "        [0.1030],\n",
      "        [0.1308],\n",
      "        [0.1318],\n",
      "        [0.1442],\n",
      "        [0.1461],\n",
      "        [0.1470],\n",
      "        [0.1582],\n",
      "        [0.1414],\n",
      "        [0.1678],\n",
      "        [0.1532],\n",
      "        [0.1255],\n",
      "        [0.1675],\n",
      "        [0.1841],\n",
      "        [0.1859],\n",
      "        [0.0286],\n",
      "        [0.0953],\n",
      "        [0.0292],\n",
      "        [0.0141],\n",
      "        [0.0782],\n",
      "        [0.0840],\n",
      "        [0.0855],\n",
      "        [0.1027],\n",
      "        [0.1156],\n",
      "        [0.1158],\n",
      "        [0.1163],\n",
      "        [0.1273],\n",
      "        [0.1294],\n",
      "        [0.1332],\n",
      "        [0.1336],\n",
      "        [0.1384],\n",
      "        [0.1387],\n",
      "        [0.1509],\n",
      "        [0.1150],\n",
      "        [0.0284],\n",
      "        [0.0797],\n",
      "        [0.0697],\n",
      "        [0.0670],\n",
      "        [0.0912],\n",
      "        [0.0976],\n",
      "        [0.1394],\n",
      "        [0.1117],\n",
      "        [0.0587],\n",
      "        [0.1560],\n",
      "        [0.1577],\n",
      "        [0.0691],\n",
      "        [0.0530],\n",
      "        [0.0161],\n",
      "        [0.0833],\n",
      "        [0.1194],\n",
      "        [0.1413],\n",
      "        [0.0714],\n",
      "        [0.1512],\n",
      "        [0.1534],\n",
      "        [0.1604],\n",
      "        [0.1640],\n",
      "        [0.1642],\n",
      "        [0.1896],\n",
      "        [0.1941],\n",
      "        [0.0630],\n",
      "        [0.0393],\n",
      "        [0.0405],\n",
      "        [0.0580],\n",
      "        [0.1137],\n",
      "        [0.2078],\n",
      "        [0.2110],\n",
      "        [0.2137],\n",
      "        [0.2158]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0308],\n",
      "        [0.0404],\n",
      "        [0.0690],\n",
      "        [0.0715],\n",
      "        [0.1158],\n",
      "        [0.0802],\n",
      "        [0.0816],\n",
      "        [0.0723],\n",
      "        [0.0197],\n",
      "        [0.0334],\n",
      "        [0.0584],\n",
      "        [0.0305],\n",
      "        [0.0760],\n",
      "        [0.0564],\n",
      "        [0.0764],\n",
      "        [0.0311],\n",
      "        [0.0363],\n",
      "        [0.0468],\n",
      "        [0.0413],\n",
      "        [0.2089],\n",
      "        [0.1558],\n",
      "        [0.1635],\n",
      "        [0.1950],\n",
      "        [0.2016],\n",
      "        [0.0565],\n",
      "        [0.2035],\n",
      "        [0.0701],\n",
      "        [0.0731],\n",
      "        [0.0792],\n",
      "        [0.1427],\n",
      "        [0.0942],\n",
      "        [0.1623],\n",
      "        [0.2268],\n",
      "        [0.1103],\n",
      "        [0.1160],\n",
      "        [0.1194],\n",
      "        [0.1247],\n",
      "        [0.1287],\n",
      "        [0.1327],\n",
      "        [0.1364],\n",
      "        [0.1409],\n",
      "        [0.1468],\n",
      "        [0.1953],\n",
      "        [0.2262],\n",
      "        [0.1706],\n",
      "        [0.1790],\n",
      "        [0.1938],\n",
      "        [0.1942],\n",
      "        [0.2023],\n",
      "        [0.2059],\n",
      "        [0.2068],\n",
      "        [0.2102],\n",
      "        [0.2196],\n",
      "        [0.2241],\n",
      "        [0.1798],\n",
      "        [0.0669],\n",
      "        [0.1862],\n",
      "        [0.0694],\n",
      "        [0.1821],\n",
      "        [0.0885],\n",
      "        [0.0904],\n",
      "        [0.1012],\n",
      "        [0.1101],\n",
      "        [0.1471],\n",
      "        [0.0653],\n",
      "        [0.0625],\n",
      "        [0.0623],\n",
      "        [0.0204],\n",
      "        [0.0435],\n",
      "        [0.0693],\n",
      "        [0.0964],\n",
      "        [0.1015],\n",
      "        [0.1292],\n",
      "        [0.1302],\n",
      "        [0.1426],\n",
      "        [0.1445],\n",
      "        [0.1454],\n",
      "        [0.1566],\n",
      "        [0.1398],\n",
      "        [0.1662],\n",
      "        [0.1516],\n",
      "        [0.1239],\n",
      "        [0.1659],\n",
      "        [0.1825],\n",
      "        [0.1843],\n",
      "        [0.0270],\n",
      "        [0.0937],\n",
      "        [0.0277],\n",
      "        [0.0126],\n",
      "        [0.0766],\n",
      "        [0.0825],\n",
      "        [0.0839],\n",
      "        [0.1011],\n",
      "        [0.1140],\n",
      "        [0.1142],\n",
      "        [0.1147],\n",
      "        [0.1257],\n",
      "        [0.1278],\n",
      "        [0.1316],\n",
      "        [0.1320],\n",
      "        [0.1368],\n",
      "        [0.1372],\n",
      "        [0.1493],\n",
      "        [0.1134],\n",
      "        [0.0269],\n",
      "        [0.0782],\n",
      "        [0.0682],\n",
      "        [0.0654],\n",
      "        [0.0896],\n",
      "        [0.0960],\n",
      "        [0.1378],\n",
      "        [0.1102],\n",
      "        [0.0571],\n",
      "        [0.1545],\n",
      "        [0.1561],\n",
      "        [0.0675],\n",
      "        [0.0514],\n",
      "        [0.0146],\n",
      "        [0.0818],\n",
      "        [0.1179],\n",
      "        [0.1397],\n",
      "        [0.0698],\n",
      "        [0.1496],\n",
      "        [0.1518],\n",
      "        [0.1588],\n",
      "        [0.1624],\n",
      "        [0.1627],\n",
      "        [0.1881],\n",
      "        [0.1925],\n",
      "        [0.0614],\n",
      "        [0.0377],\n",
      "        [0.0389],\n",
      "        [0.0564],\n",
      "        [0.1121],\n",
      "        [0.2063],\n",
      "        [0.2094],\n",
      "        [0.2121],\n",
      "        [0.2142]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 29.964812994003296\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 139\n",
      "剩餘X 資料 torch.Size([21, 18])\n",
      "剩餘Y 資料 torch.Size([21, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.047238435596227646, 8)\n",
      "The second_loss value of k: (0.04917333647608757, 7)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.5558])\n",
      "目前模型的Data狀態 torch.Size([139, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732],\n",
      "        [0.7732]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0308],\n",
      "        [0.0404],\n",
      "        [0.0690],\n",
      "        [0.0715],\n",
      "        [0.1158],\n",
      "        [0.0802],\n",
      "        [0.0816],\n",
      "        [0.0723],\n",
      "        [0.0197],\n",
      "        [0.0334],\n",
      "        [0.0584],\n",
      "        [0.0305],\n",
      "        [0.0760],\n",
      "        [0.0564],\n",
      "        [0.0764],\n",
      "        [0.0311],\n",
      "        [0.0363],\n",
      "        [0.0468],\n",
      "        [0.0413],\n",
      "        [0.2089],\n",
      "        [0.1558],\n",
      "        [0.1635],\n",
      "        [0.1950],\n",
      "        [0.2016],\n",
      "        [0.0565],\n",
      "        [0.2035],\n",
      "        [0.0701],\n",
      "        [0.0731],\n",
      "        [0.0792],\n",
      "        [0.1427],\n",
      "        [0.0942],\n",
      "        [0.1623],\n",
      "        [0.2268],\n",
      "        [0.1103],\n",
      "        [0.1160],\n",
      "        [0.1194],\n",
      "        [0.1247],\n",
      "        [0.1287],\n",
      "        [0.1327],\n",
      "        [0.1364],\n",
      "        [0.1409],\n",
      "        [0.1468],\n",
      "        [0.1953],\n",
      "        [0.2262],\n",
      "        [0.1706],\n",
      "        [0.1790],\n",
      "        [0.1938],\n",
      "        [0.1942],\n",
      "        [0.2023],\n",
      "        [0.2059],\n",
      "        [0.2068],\n",
      "        [0.2102],\n",
      "        [0.2196],\n",
      "        [0.2241],\n",
      "        [0.1798],\n",
      "        [0.0669],\n",
      "        [0.1862],\n",
      "        [0.0694],\n",
      "        [0.1821],\n",
      "        [0.0885],\n",
      "        [0.0904],\n",
      "        [0.1012],\n",
      "        [0.1101],\n",
      "        [0.1471],\n",
      "        [0.0653],\n",
      "        [0.0625],\n",
      "        [0.0623],\n",
      "        [0.0204],\n",
      "        [0.0435],\n",
      "        [0.0693],\n",
      "        [0.0964],\n",
      "        [0.1015],\n",
      "        [0.1292],\n",
      "        [0.1302],\n",
      "        [0.1426],\n",
      "        [0.1445],\n",
      "        [0.1454],\n",
      "        [0.1566],\n",
      "        [0.1398],\n",
      "        [0.1662],\n",
      "        [0.1516],\n",
      "        [0.1239],\n",
      "        [0.1659],\n",
      "        [0.1825],\n",
      "        [0.1843],\n",
      "        [0.0270],\n",
      "        [0.0937],\n",
      "        [0.0277],\n",
      "        [0.0126],\n",
      "        [0.0766],\n",
      "        [0.0825],\n",
      "        [0.0839],\n",
      "        [0.1011],\n",
      "        [0.1140],\n",
      "        [0.1142],\n",
      "        [0.1147],\n",
      "        [0.1257],\n",
      "        [0.1278],\n",
      "        [0.1316],\n",
      "        [0.1320],\n",
      "        [0.1368],\n",
      "        [0.1372],\n",
      "        [0.1493],\n",
      "        [0.1134],\n",
      "        [0.0269],\n",
      "        [0.0782],\n",
      "        [0.0682],\n",
      "        [0.0654],\n",
      "        [0.0896],\n",
      "        [0.0960],\n",
      "        [0.1378],\n",
      "        [0.1102],\n",
      "        [0.0571],\n",
      "        [0.1545],\n",
      "        [0.1561],\n",
      "        [0.0675],\n",
      "        [0.0514],\n",
      "        [0.0146],\n",
      "        [0.0818],\n",
      "        [0.1179],\n",
      "        [0.1397],\n",
      "        [0.0698],\n",
      "        [0.1496],\n",
      "        [0.1518],\n",
      "        [0.1588],\n",
      "        [0.1624],\n",
      "        [0.1627],\n",
      "        [0.1881],\n",
      "        [0.1925],\n",
      "        [0.0614],\n",
      "        [0.0377],\n",
      "        [0.0389],\n",
      "        [0.0564],\n",
      "        [0.1121],\n",
      "        [0.2063],\n",
      "        [0.2094],\n",
      "        [0.2121],\n",
      "        [0.2142],\n",
      "        [0.2173]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0324],\n",
      "        [0.0420],\n",
      "        [0.0705],\n",
      "        [0.0731],\n",
      "        [0.1174],\n",
      "        [0.0817],\n",
      "        [0.0832],\n",
      "        [0.0738],\n",
      "        [0.0213],\n",
      "        [0.0349],\n",
      "        [0.0599],\n",
      "        [0.0320],\n",
      "        [0.0776],\n",
      "        [0.0579],\n",
      "        [0.0780],\n",
      "        [0.0327],\n",
      "        [0.0378],\n",
      "        [0.0484],\n",
      "        [0.0429],\n",
      "        [0.2105],\n",
      "        [0.1574],\n",
      "        [0.1651],\n",
      "        [0.1966],\n",
      "        [0.2031],\n",
      "        [0.0581],\n",
      "        [0.2051],\n",
      "        [0.0717],\n",
      "        [0.0747],\n",
      "        [0.0808],\n",
      "        [0.1443],\n",
      "        [0.0958],\n",
      "        [0.1638],\n",
      "        [0.2284],\n",
      "        [0.1119],\n",
      "        [0.1176],\n",
      "        [0.1210],\n",
      "        [0.1262],\n",
      "        [0.1302],\n",
      "        [0.1343],\n",
      "        [0.1380],\n",
      "        [0.1425],\n",
      "        [0.1484],\n",
      "        [0.1969],\n",
      "        [0.2278],\n",
      "        [0.1722],\n",
      "        [0.1805],\n",
      "        [0.1953],\n",
      "        [0.1957],\n",
      "        [0.2039],\n",
      "        [0.2074],\n",
      "        [0.2084],\n",
      "        [0.2118],\n",
      "        [0.2212],\n",
      "        [0.2257],\n",
      "        [0.1814],\n",
      "        [0.0684],\n",
      "        [0.1878],\n",
      "        [0.0709],\n",
      "        [0.1836],\n",
      "        [0.0901],\n",
      "        [0.0919],\n",
      "        [0.1027],\n",
      "        [0.1117],\n",
      "        [0.1487],\n",
      "        [0.0637],\n",
      "        [0.0640],\n",
      "        [0.0639],\n",
      "        [0.0220],\n",
      "        [0.0450],\n",
      "        [0.0678],\n",
      "        [0.0949],\n",
      "        [0.0999],\n",
      "        [0.1277],\n",
      "        [0.1286],\n",
      "        [0.1410],\n",
      "        [0.1430],\n",
      "        [0.1438],\n",
      "        [0.1550],\n",
      "        [0.1383],\n",
      "        [0.1646],\n",
      "        [0.1500],\n",
      "        [0.1224],\n",
      "        [0.1644],\n",
      "        [0.1809],\n",
      "        [0.1828],\n",
      "        [0.0255],\n",
      "        [0.0922],\n",
      "        [0.0261],\n",
      "        [0.0110],\n",
      "        [0.0750],\n",
      "        [0.0809],\n",
      "        [0.0823],\n",
      "        [0.0996],\n",
      "        [0.1125],\n",
      "        [0.1126],\n",
      "        [0.1131],\n",
      "        [0.1242],\n",
      "        [0.1262],\n",
      "        [0.1300],\n",
      "        [0.1304],\n",
      "        [0.1352],\n",
      "        [0.1356],\n",
      "        [0.1477],\n",
      "        [0.1119],\n",
      "        [0.0253],\n",
      "        [0.0766],\n",
      "        [0.0666],\n",
      "        [0.0638],\n",
      "        [0.0881],\n",
      "        [0.0944],\n",
      "        [0.1363],\n",
      "        [0.1086],\n",
      "        [0.0555],\n",
      "        [0.1529],\n",
      "        [0.1545],\n",
      "        [0.0660],\n",
      "        [0.0498],\n",
      "        [0.0130],\n",
      "        [0.0802],\n",
      "        [0.1163],\n",
      "        [0.1381],\n",
      "        [0.0682],\n",
      "        [0.1480],\n",
      "        [0.1503],\n",
      "        [0.1573],\n",
      "        [0.1608],\n",
      "        [0.1611],\n",
      "        [0.1865],\n",
      "        [0.1909],\n",
      "        [0.0598],\n",
      "        [0.0361],\n",
      "        [0.0374],\n",
      "        [0.0548],\n",
      "        [0.1105],\n",
      "        [0.2047],\n",
      "        [0.2078],\n",
      "        [0.2105],\n",
      "        [0.2126],\n",
      "        [0.2158]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 30.209690809249878\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 140\n",
      "剩餘X 資料 torch.Size([20, 18])\n",
      "剩餘Y 資料 torch.Size([20, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.048475153744220734, 7)\n",
      "The second_loss value of k: (0.05002237856388092, 8)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引7，y= tensor([0.5514])\n",
      "目前模型的Data狀態 torch.Size([140, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716],\n",
      "        [0.7716]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0324],\n",
      "        [0.0420],\n",
      "        [0.0705],\n",
      "        [0.0731],\n",
      "        [0.1174],\n",
      "        [0.0817],\n",
      "        [0.0832],\n",
      "        [0.0738],\n",
      "        [0.0213],\n",
      "        [0.0349],\n",
      "        [0.0599],\n",
      "        [0.0320],\n",
      "        [0.0776],\n",
      "        [0.0579],\n",
      "        [0.0780],\n",
      "        [0.0327],\n",
      "        [0.0378],\n",
      "        [0.0484],\n",
      "        [0.0429],\n",
      "        [0.2105],\n",
      "        [0.1574],\n",
      "        [0.1651],\n",
      "        [0.1966],\n",
      "        [0.2031],\n",
      "        [0.0581],\n",
      "        [0.2051],\n",
      "        [0.0717],\n",
      "        [0.0747],\n",
      "        [0.0808],\n",
      "        [0.1443],\n",
      "        [0.0958],\n",
      "        [0.1638],\n",
      "        [0.2284],\n",
      "        [0.1119],\n",
      "        [0.1176],\n",
      "        [0.1210],\n",
      "        [0.1262],\n",
      "        [0.1302],\n",
      "        [0.1343],\n",
      "        [0.1380],\n",
      "        [0.1425],\n",
      "        [0.1484],\n",
      "        [0.1969],\n",
      "        [0.2278],\n",
      "        [0.1722],\n",
      "        [0.1805],\n",
      "        [0.1953],\n",
      "        [0.1957],\n",
      "        [0.2039],\n",
      "        [0.2074],\n",
      "        [0.2084],\n",
      "        [0.2118],\n",
      "        [0.2212],\n",
      "        [0.2257],\n",
      "        [0.1814],\n",
      "        [0.0684],\n",
      "        [0.1878],\n",
      "        [0.0709],\n",
      "        [0.1836],\n",
      "        [0.0901],\n",
      "        [0.0919],\n",
      "        [0.1027],\n",
      "        [0.1117],\n",
      "        [0.1487],\n",
      "        [0.0637],\n",
      "        [0.0640],\n",
      "        [0.0639],\n",
      "        [0.0220],\n",
      "        [0.0450],\n",
      "        [0.0678],\n",
      "        [0.0949],\n",
      "        [0.0999],\n",
      "        [0.1277],\n",
      "        [0.1286],\n",
      "        [0.1410],\n",
      "        [0.1430],\n",
      "        [0.1438],\n",
      "        [0.1550],\n",
      "        [0.1383],\n",
      "        [0.1646],\n",
      "        [0.1500],\n",
      "        [0.1224],\n",
      "        [0.1644],\n",
      "        [0.1809],\n",
      "        [0.1828],\n",
      "        [0.0255],\n",
      "        [0.0922],\n",
      "        [0.0261],\n",
      "        [0.0110],\n",
      "        [0.0750],\n",
      "        [0.0809],\n",
      "        [0.0823],\n",
      "        [0.0996],\n",
      "        [0.1125],\n",
      "        [0.1126],\n",
      "        [0.1131],\n",
      "        [0.1242],\n",
      "        [0.1262],\n",
      "        [0.1300],\n",
      "        [0.1304],\n",
      "        [0.1352],\n",
      "        [0.1356],\n",
      "        [0.1477],\n",
      "        [0.1119],\n",
      "        [0.0253],\n",
      "        [0.0766],\n",
      "        [0.0666],\n",
      "        [0.0638],\n",
      "        [0.0881],\n",
      "        [0.0944],\n",
      "        [0.1363],\n",
      "        [0.1086],\n",
      "        [0.0555],\n",
      "        [0.1529],\n",
      "        [0.1545],\n",
      "        [0.0660],\n",
      "        [0.0498],\n",
      "        [0.0130],\n",
      "        [0.0802],\n",
      "        [0.1163],\n",
      "        [0.1381],\n",
      "        [0.0682],\n",
      "        [0.1480],\n",
      "        [0.1503],\n",
      "        [0.1573],\n",
      "        [0.1608],\n",
      "        [0.1611],\n",
      "        [0.1865],\n",
      "        [0.1909],\n",
      "        [0.0598],\n",
      "        [0.0361],\n",
      "        [0.0374],\n",
      "        [0.0548],\n",
      "        [0.1105],\n",
      "        [0.2047],\n",
      "        [0.2078],\n",
      "        [0.2105],\n",
      "        [0.2126],\n",
      "        [0.2158],\n",
      "        [0.2202]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0339],\n",
      "        [0.0435],\n",
      "        [0.0721],\n",
      "        [0.0746],\n",
      "        [0.1189],\n",
      "        [0.0833],\n",
      "        [0.0847],\n",
      "        [0.0754],\n",
      "        [0.0228],\n",
      "        [0.0365],\n",
      "        [0.0615],\n",
      "        [0.0336],\n",
      "        [0.0792],\n",
      "        [0.0595],\n",
      "        [0.0795],\n",
      "        [0.0342],\n",
      "        [0.0394],\n",
      "        [0.0499],\n",
      "        [0.0444],\n",
      "        [0.2121],\n",
      "        [0.1589],\n",
      "        [0.1666],\n",
      "        [0.1981],\n",
      "        [0.2047],\n",
      "        [0.0596],\n",
      "        [0.2066],\n",
      "        [0.0732],\n",
      "        [0.0762],\n",
      "        [0.0824],\n",
      "        [0.1459],\n",
      "        [0.0974],\n",
      "        [0.1654],\n",
      "        [0.2300],\n",
      "        [0.1134],\n",
      "        [0.1192],\n",
      "        [0.1225],\n",
      "        [0.1278],\n",
      "        [0.1318],\n",
      "        [0.1359],\n",
      "        [0.1396],\n",
      "        [0.1441],\n",
      "        [0.1499],\n",
      "        [0.1984],\n",
      "        [0.2294],\n",
      "        [0.1737],\n",
      "        [0.1821],\n",
      "        [0.1969],\n",
      "        [0.1973],\n",
      "        [0.2055],\n",
      "        [0.2090],\n",
      "        [0.2100],\n",
      "        [0.2133],\n",
      "        [0.2227],\n",
      "        [0.2272],\n",
      "        [0.1830],\n",
      "        [0.0700],\n",
      "        [0.1893],\n",
      "        [0.0725],\n",
      "        [0.1852],\n",
      "        [0.0916],\n",
      "        [0.0935],\n",
      "        [0.1043],\n",
      "        [0.1133],\n",
      "        [0.1502],\n",
      "        [0.0621],\n",
      "        [0.0656],\n",
      "        [0.0654],\n",
      "        [0.0236],\n",
      "        [0.0466],\n",
      "        [0.0662],\n",
      "        [0.0933],\n",
      "        [0.0983],\n",
      "        [0.1261],\n",
      "        [0.1271],\n",
      "        [0.1395],\n",
      "        [0.1414],\n",
      "        [0.1423],\n",
      "        [0.1535],\n",
      "        [0.1367],\n",
      "        [0.1631],\n",
      "        [0.1485],\n",
      "        [0.1208],\n",
      "        [0.1628],\n",
      "        [0.1794],\n",
      "        [0.1812],\n",
      "        [0.0239],\n",
      "        [0.0906],\n",
      "        [0.0245],\n",
      "        [0.0095],\n",
      "        [0.0735],\n",
      "        [0.0793],\n",
      "        [0.0808],\n",
      "        [0.0980],\n",
      "        [0.1109],\n",
      "        [0.1111],\n",
      "        [0.1116],\n",
      "        [0.1226],\n",
      "        [0.1247],\n",
      "        [0.1285],\n",
      "        [0.1289],\n",
      "        [0.1337],\n",
      "        [0.1340],\n",
      "        [0.1462],\n",
      "        [0.1103],\n",
      "        [0.0237],\n",
      "        [0.0750],\n",
      "        [0.0650],\n",
      "        [0.0623],\n",
      "        [0.0865],\n",
      "        [0.0929],\n",
      "        [0.1347],\n",
      "        [0.1070],\n",
      "        [0.0540],\n",
      "        [0.1513],\n",
      "        [0.1530],\n",
      "        [0.0644],\n",
      "        [0.0483],\n",
      "        [0.0114],\n",
      "        [0.0786],\n",
      "        [0.1147],\n",
      "        [0.1366],\n",
      "        [0.0667],\n",
      "        [0.1465],\n",
      "        [0.1487],\n",
      "        [0.1557],\n",
      "        [0.1593],\n",
      "        [0.1595],\n",
      "        [0.1849],\n",
      "        [0.1894],\n",
      "        [0.0583],\n",
      "        [0.0346],\n",
      "        [0.0358],\n",
      "        [0.0533],\n",
      "        [0.1090],\n",
      "        [0.2031],\n",
      "        [0.2063],\n",
      "        [0.2090],\n",
      "        [0.2111],\n",
      "        [0.2142],\n",
      "        [0.2186]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 30.448710203170776\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 141\n",
      "剩餘X 資料 torch.Size([19, 18])\n",
      "剩餘Y 資料 torch.Size([19, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.04932929202914238, 7)\n",
      "The second_loss value of k: (0.05265457183122635, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引7，y= tensor([0.5479])\n",
      "目前模型的Data狀態 torch.Size([141, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700],\n",
      "        [0.7700]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0339],\n",
      "        [0.0435],\n",
      "        [0.0721],\n",
      "        [0.0746],\n",
      "        [0.1189],\n",
      "        [0.0833],\n",
      "        [0.0847],\n",
      "        [0.0754],\n",
      "        [0.0228],\n",
      "        [0.0365],\n",
      "        [0.0615],\n",
      "        [0.0336],\n",
      "        [0.0792],\n",
      "        [0.0595],\n",
      "        [0.0795],\n",
      "        [0.0342],\n",
      "        [0.0394],\n",
      "        [0.0499],\n",
      "        [0.0444],\n",
      "        [0.2121],\n",
      "        [0.1589],\n",
      "        [0.1666],\n",
      "        [0.1981],\n",
      "        [0.2047],\n",
      "        [0.0596],\n",
      "        [0.2066],\n",
      "        [0.0732],\n",
      "        [0.0762],\n",
      "        [0.0824],\n",
      "        [0.1459],\n",
      "        [0.0974],\n",
      "        [0.1654],\n",
      "        [0.2300],\n",
      "        [0.1134],\n",
      "        [0.1192],\n",
      "        [0.1225],\n",
      "        [0.1278],\n",
      "        [0.1318],\n",
      "        [0.1359],\n",
      "        [0.1396],\n",
      "        [0.1441],\n",
      "        [0.1499],\n",
      "        [0.1984],\n",
      "        [0.2294],\n",
      "        [0.1737],\n",
      "        [0.1821],\n",
      "        [0.1969],\n",
      "        [0.1973],\n",
      "        [0.2055],\n",
      "        [0.2090],\n",
      "        [0.2100],\n",
      "        [0.2133],\n",
      "        [0.2227],\n",
      "        [0.2272],\n",
      "        [0.1830],\n",
      "        [0.0700],\n",
      "        [0.1893],\n",
      "        [0.0725],\n",
      "        [0.1852],\n",
      "        [0.0916],\n",
      "        [0.0935],\n",
      "        [0.1043],\n",
      "        [0.1133],\n",
      "        [0.1502],\n",
      "        [0.0621],\n",
      "        [0.0656],\n",
      "        [0.0654],\n",
      "        [0.0236],\n",
      "        [0.0466],\n",
      "        [0.0662],\n",
      "        [0.0933],\n",
      "        [0.0983],\n",
      "        [0.1261],\n",
      "        [0.1271],\n",
      "        [0.1395],\n",
      "        [0.1414],\n",
      "        [0.1423],\n",
      "        [0.1535],\n",
      "        [0.1367],\n",
      "        [0.1631],\n",
      "        [0.1485],\n",
      "        [0.1208],\n",
      "        [0.1628],\n",
      "        [0.1794],\n",
      "        [0.1812],\n",
      "        [0.0239],\n",
      "        [0.0906],\n",
      "        [0.0245],\n",
      "        [0.0095],\n",
      "        [0.0735],\n",
      "        [0.0793],\n",
      "        [0.0808],\n",
      "        [0.0980],\n",
      "        [0.1109],\n",
      "        [0.1111],\n",
      "        [0.1116],\n",
      "        [0.1226],\n",
      "        [0.1247],\n",
      "        [0.1285],\n",
      "        [0.1289],\n",
      "        [0.1337],\n",
      "        [0.1340],\n",
      "        [0.1462],\n",
      "        [0.1103],\n",
      "        [0.0237],\n",
      "        [0.0750],\n",
      "        [0.0650],\n",
      "        [0.0623],\n",
      "        [0.0865],\n",
      "        [0.0929],\n",
      "        [0.1347],\n",
      "        [0.1070],\n",
      "        [0.0540],\n",
      "        [0.1513],\n",
      "        [0.1530],\n",
      "        [0.0644],\n",
      "        [0.0483],\n",
      "        [0.0114],\n",
      "        [0.0786],\n",
      "        [0.1147],\n",
      "        [0.1366],\n",
      "        [0.0667],\n",
      "        [0.1465],\n",
      "        [0.1487],\n",
      "        [0.1557],\n",
      "        [0.1593],\n",
      "        [0.1595],\n",
      "        [0.1849],\n",
      "        [0.1894],\n",
      "        [0.0583],\n",
      "        [0.0346],\n",
      "        [0.0358],\n",
      "        [0.0533],\n",
      "        [0.1090],\n",
      "        [0.2031],\n",
      "        [0.2063],\n",
      "        [0.2090],\n",
      "        [0.2111],\n",
      "        [0.2142],\n",
      "        [0.2186],\n",
      "        [0.2221]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0355],\n",
      "        [0.0451],\n",
      "        [0.0737],\n",
      "        [0.0762],\n",
      "        [0.1205],\n",
      "        [0.0849],\n",
      "        [0.0863],\n",
      "        [0.0770],\n",
      "        [0.0244],\n",
      "        [0.0381],\n",
      "        [0.0631],\n",
      "        [0.0352],\n",
      "        [0.0807],\n",
      "        [0.0611],\n",
      "        [0.0811],\n",
      "        [0.0358],\n",
      "        [0.0410],\n",
      "        [0.0515],\n",
      "        [0.0460],\n",
      "        [0.2136],\n",
      "        [0.1605],\n",
      "        [0.1682],\n",
      "        [0.1997],\n",
      "        [0.2063],\n",
      "        [0.0612],\n",
      "        [0.2082],\n",
      "        [0.0748],\n",
      "        [0.0778],\n",
      "        [0.0839],\n",
      "        [0.1474],\n",
      "        [0.0989],\n",
      "        [0.1670],\n",
      "        [0.2315],\n",
      "        [0.1150],\n",
      "        [0.1208],\n",
      "        [0.1241],\n",
      "        [0.1294],\n",
      "        [0.1334],\n",
      "        [0.1374],\n",
      "        [0.1411],\n",
      "        [0.1456],\n",
      "        [0.1515],\n",
      "        [0.2000],\n",
      "        [0.2309],\n",
      "        [0.1753],\n",
      "        [0.1837],\n",
      "        [0.1985],\n",
      "        [0.1989],\n",
      "        [0.2070],\n",
      "        [0.2106],\n",
      "        [0.2116],\n",
      "        [0.2149],\n",
      "        [0.2243],\n",
      "        [0.2288],\n",
      "        [0.1845],\n",
      "        [0.0716],\n",
      "        [0.1909],\n",
      "        [0.0741],\n",
      "        [0.1868],\n",
      "        [0.0932],\n",
      "        [0.0951],\n",
      "        [0.1059],\n",
      "        [0.1148],\n",
      "        [0.1518],\n",
      "        [0.0606],\n",
      "        [0.0672],\n",
      "        [0.0670],\n",
      "        [0.0251],\n",
      "        [0.0482],\n",
      "        [0.0646],\n",
      "        [0.0917],\n",
      "        [0.0968],\n",
      "        [0.1245],\n",
      "        [0.1255],\n",
      "        [0.1379],\n",
      "        [0.1398],\n",
      "        [0.1407],\n",
      "        [0.1519],\n",
      "        [0.1351],\n",
      "        [0.1615],\n",
      "        [0.1469],\n",
      "        [0.1192],\n",
      "        [0.1612],\n",
      "        [0.1778],\n",
      "        [0.1796],\n",
      "        [0.0223],\n",
      "        [0.0890],\n",
      "        [0.0230],\n",
      "        [0.0079],\n",
      "        [0.0719],\n",
      "        [0.0778],\n",
      "        [0.0792],\n",
      "        [0.0964],\n",
      "        [0.1093],\n",
      "        [0.1095],\n",
      "        [0.1100],\n",
      "        [0.1210],\n",
      "        [0.1231],\n",
      "        [0.1269],\n",
      "        [0.1273],\n",
      "        [0.1321],\n",
      "        [0.1325],\n",
      "        [0.1446],\n",
      "        [0.1087],\n",
      "        [0.0222],\n",
      "        [0.0734],\n",
      "        [0.0635],\n",
      "        [0.0607],\n",
      "        [0.0849],\n",
      "        [0.0913],\n",
      "        [0.1331],\n",
      "        [0.1054],\n",
      "        [0.0524],\n",
      "        [0.1497],\n",
      "        [0.1514],\n",
      "        [0.0628],\n",
      "        [0.0467],\n",
      "        [0.0099],\n",
      "        [0.0770],\n",
      "        [0.1132],\n",
      "        [0.1350],\n",
      "        [0.0651],\n",
      "        [0.1449],\n",
      "        [0.1471],\n",
      "        [0.1541],\n",
      "        [0.1577],\n",
      "        [0.1580],\n",
      "        [0.1834],\n",
      "        [0.1878],\n",
      "        [0.0567],\n",
      "        [0.0330],\n",
      "        [0.0342],\n",
      "        [0.0517],\n",
      "        [0.1074],\n",
      "        [0.2016],\n",
      "        [0.2047],\n",
      "        [0.2074],\n",
      "        [0.2095],\n",
      "        [0.2126],\n",
      "        [0.2170],\n",
      "        [0.2205]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 30.685791015625\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 142\n",
      "剩餘X 資料 torch.Size([18, 18])\n",
      "剩餘Y 資料 torch.Size([18, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05193434655666351, 8)\n",
      "The second_loss value of k: (0.05375313758850098, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.5406])\n",
      "目前模型的Data狀態 torch.Size([142, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685],\n",
      "        [0.7685]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0355],\n",
      "        [0.0451],\n",
      "        [0.0737],\n",
      "        [0.0762],\n",
      "        [0.1205],\n",
      "        [0.0849],\n",
      "        [0.0863],\n",
      "        [0.0770],\n",
      "        [0.0244],\n",
      "        [0.0381],\n",
      "        [0.0631],\n",
      "        [0.0352],\n",
      "        [0.0807],\n",
      "        [0.0611],\n",
      "        [0.0811],\n",
      "        [0.0358],\n",
      "        [0.0410],\n",
      "        [0.0515],\n",
      "        [0.0460],\n",
      "        [0.2136],\n",
      "        [0.1605],\n",
      "        [0.1682],\n",
      "        [0.1997],\n",
      "        [0.2063],\n",
      "        [0.0612],\n",
      "        [0.2082],\n",
      "        [0.0748],\n",
      "        [0.0778],\n",
      "        [0.0839],\n",
      "        [0.1474],\n",
      "        [0.0989],\n",
      "        [0.1670],\n",
      "        [0.2315],\n",
      "        [0.1150],\n",
      "        [0.1208],\n",
      "        [0.1241],\n",
      "        [0.1294],\n",
      "        [0.1334],\n",
      "        [0.1374],\n",
      "        [0.1411],\n",
      "        [0.1456],\n",
      "        [0.1515],\n",
      "        [0.2000],\n",
      "        [0.2309],\n",
      "        [0.1753],\n",
      "        [0.1837],\n",
      "        [0.1985],\n",
      "        [0.1989],\n",
      "        [0.2070],\n",
      "        [0.2106],\n",
      "        [0.2116],\n",
      "        [0.2149],\n",
      "        [0.2243],\n",
      "        [0.2288],\n",
      "        [0.1845],\n",
      "        [0.0716],\n",
      "        [0.1909],\n",
      "        [0.0741],\n",
      "        [0.1868],\n",
      "        [0.0932],\n",
      "        [0.0951],\n",
      "        [0.1059],\n",
      "        [0.1148],\n",
      "        [0.1518],\n",
      "        [0.0606],\n",
      "        [0.0672],\n",
      "        [0.0670],\n",
      "        [0.0251],\n",
      "        [0.0482],\n",
      "        [0.0646],\n",
      "        [0.0917],\n",
      "        [0.0968],\n",
      "        [0.1245],\n",
      "        [0.1255],\n",
      "        [0.1379],\n",
      "        [0.1398],\n",
      "        [0.1407],\n",
      "        [0.1519],\n",
      "        [0.1351],\n",
      "        [0.1615],\n",
      "        [0.1469],\n",
      "        [0.1192],\n",
      "        [0.1612],\n",
      "        [0.1778],\n",
      "        [0.1796],\n",
      "        [0.0223],\n",
      "        [0.0890],\n",
      "        [0.0230],\n",
      "        [0.0079],\n",
      "        [0.0719],\n",
      "        [0.0778],\n",
      "        [0.0792],\n",
      "        [0.0964],\n",
      "        [0.1093],\n",
      "        [0.1095],\n",
      "        [0.1100],\n",
      "        [0.1210],\n",
      "        [0.1231],\n",
      "        [0.1269],\n",
      "        [0.1273],\n",
      "        [0.1321],\n",
      "        [0.1325],\n",
      "        [0.1446],\n",
      "        [0.1087],\n",
      "        [0.0222],\n",
      "        [0.0734],\n",
      "        [0.0635],\n",
      "        [0.0607],\n",
      "        [0.0849],\n",
      "        [0.0913],\n",
      "        [0.1331],\n",
      "        [0.1054],\n",
      "        [0.0524],\n",
      "        [0.1497],\n",
      "        [0.1514],\n",
      "        [0.0628],\n",
      "        [0.0467],\n",
      "        [0.0099],\n",
      "        [0.0770],\n",
      "        [0.1132],\n",
      "        [0.1350],\n",
      "        [0.0651],\n",
      "        [0.1449],\n",
      "        [0.1471],\n",
      "        [0.1541],\n",
      "        [0.1577],\n",
      "        [0.1580],\n",
      "        [0.1834],\n",
      "        [0.1878],\n",
      "        [0.0567],\n",
      "        [0.0330],\n",
      "        [0.0342],\n",
      "        [0.0517],\n",
      "        [0.1074],\n",
      "        [0.2016],\n",
      "        [0.2047],\n",
      "        [0.2074],\n",
      "        [0.2095],\n",
      "        [0.2126],\n",
      "        [0.2170],\n",
      "        [0.2205],\n",
      "        [0.2279]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0371],\n",
      "        [0.0467],\n",
      "        [0.0753],\n",
      "        [0.0778],\n",
      "        [0.1221],\n",
      "        [0.0865],\n",
      "        [0.0879],\n",
      "        [0.0786],\n",
      "        [0.0260],\n",
      "        [0.0397],\n",
      "        [0.0647],\n",
      "        [0.0368],\n",
      "        [0.0823],\n",
      "        [0.0627],\n",
      "        [0.0827],\n",
      "        [0.0374],\n",
      "        [0.0426],\n",
      "        [0.0531],\n",
      "        [0.0476],\n",
      "        [0.2152],\n",
      "        [0.1621],\n",
      "        [0.1698],\n",
      "        [0.2013],\n",
      "        [0.2079],\n",
      "        [0.0628],\n",
      "        [0.2098],\n",
      "        [0.0764],\n",
      "        [0.0794],\n",
      "        [0.0855],\n",
      "        [0.1490],\n",
      "        [0.1005],\n",
      "        [0.1686],\n",
      "        [0.2331],\n",
      "        [0.1166],\n",
      "        [0.1224],\n",
      "        [0.1257],\n",
      "        [0.1310],\n",
      "        [0.1350],\n",
      "        [0.1391],\n",
      "        [0.1427],\n",
      "        [0.1472],\n",
      "        [0.1531],\n",
      "        [0.2016],\n",
      "        [0.2325],\n",
      "        [0.1769],\n",
      "        [0.1853],\n",
      "        [0.2001],\n",
      "        [0.2005],\n",
      "        [0.2087],\n",
      "        [0.2122],\n",
      "        [0.2132],\n",
      "        [0.2165],\n",
      "        [0.2259],\n",
      "        [0.2304],\n",
      "        [0.1861],\n",
      "        [0.0732],\n",
      "        [0.1925],\n",
      "        [0.0757],\n",
      "        [0.1884],\n",
      "        [0.0948],\n",
      "        [0.0967],\n",
      "        [0.1075],\n",
      "        [0.1164],\n",
      "        [0.1534],\n",
      "        [0.0590],\n",
      "        [0.0688],\n",
      "        [0.0686],\n",
      "        [0.0268],\n",
      "        [0.0498],\n",
      "        [0.0630],\n",
      "        [0.0901],\n",
      "        [0.0951],\n",
      "        [0.1229],\n",
      "        [0.1239],\n",
      "        [0.1363],\n",
      "        [0.1382],\n",
      "        [0.1391],\n",
      "        [0.1503],\n",
      "        [0.1335],\n",
      "        [0.1599],\n",
      "        [0.1453],\n",
      "        [0.1176],\n",
      "        [0.1596],\n",
      "        [0.1762],\n",
      "        [0.1780],\n",
      "        [0.0207],\n",
      "        [0.0874],\n",
      "        [0.0213],\n",
      "        [0.0063],\n",
      "        [0.0703],\n",
      "        [0.0761],\n",
      "        [0.0776],\n",
      "        [0.0948],\n",
      "        [0.1077],\n",
      "        [0.1079],\n",
      "        [0.1084],\n",
      "        [0.1194],\n",
      "        [0.1215],\n",
      "        [0.1253],\n",
      "        [0.1257],\n",
      "        [0.1305],\n",
      "        [0.1309],\n",
      "        [0.1430],\n",
      "        [0.1071],\n",
      "        [0.0206],\n",
      "        [0.0718],\n",
      "        [0.0619],\n",
      "        [0.0591],\n",
      "        [0.0833],\n",
      "        [0.0897],\n",
      "        [0.1315],\n",
      "        [0.1038],\n",
      "        [0.0508],\n",
      "        [0.1481],\n",
      "        [0.1498],\n",
      "        [0.0612],\n",
      "        [0.0451],\n",
      "        [0.0083],\n",
      "        [0.0754],\n",
      "        [0.1116],\n",
      "        [0.1334],\n",
      "        [0.0635],\n",
      "        [0.1433],\n",
      "        [0.1455],\n",
      "        [0.1525],\n",
      "        [0.1561],\n",
      "        [0.1564],\n",
      "        [0.1817],\n",
      "        [0.1862],\n",
      "        [0.0551],\n",
      "        [0.0314],\n",
      "        [0.0326],\n",
      "        [0.0501],\n",
      "        [0.1058],\n",
      "        [0.2000],\n",
      "        [0.2031],\n",
      "        [0.2058],\n",
      "        [0.2079],\n",
      "        [0.2110],\n",
      "        [0.2154],\n",
      "        [0.2189],\n",
      "        [0.2263]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 30.92458462715149\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 143\n",
      "剩餘X 資料 torch.Size([17, 18])\n",
      "剩餘Y 資料 torch.Size([17, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05301059037446976, 8)\n",
      "The second_loss value of k: (0.05422595888376236, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.5366])\n",
      "目前模型的Data狀態 torch.Size([143, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669],\n",
      "        [0.7669]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0371],\n",
      "        [0.0467],\n",
      "        [0.0753],\n",
      "        [0.0778],\n",
      "        [0.1221],\n",
      "        [0.0865],\n",
      "        [0.0879],\n",
      "        [0.0786],\n",
      "        [0.0260],\n",
      "        [0.0397],\n",
      "        [0.0647],\n",
      "        [0.0368],\n",
      "        [0.0823],\n",
      "        [0.0627],\n",
      "        [0.0827],\n",
      "        [0.0374],\n",
      "        [0.0426],\n",
      "        [0.0531],\n",
      "        [0.0476],\n",
      "        [0.2152],\n",
      "        [0.1621],\n",
      "        [0.1698],\n",
      "        [0.2013],\n",
      "        [0.2079],\n",
      "        [0.0628],\n",
      "        [0.2098],\n",
      "        [0.0764],\n",
      "        [0.0794],\n",
      "        [0.0855],\n",
      "        [0.1490],\n",
      "        [0.1005],\n",
      "        [0.1686],\n",
      "        [0.2331],\n",
      "        [0.1166],\n",
      "        [0.1224],\n",
      "        [0.1257],\n",
      "        [0.1310],\n",
      "        [0.1350],\n",
      "        [0.1391],\n",
      "        [0.1427],\n",
      "        [0.1472],\n",
      "        [0.1531],\n",
      "        [0.2016],\n",
      "        [0.2325],\n",
      "        [0.1769],\n",
      "        [0.1853],\n",
      "        [0.2001],\n",
      "        [0.2005],\n",
      "        [0.2087],\n",
      "        [0.2122],\n",
      "        [0.2132],\n",
      "        [0.2165],\n",
      "        [0.2259],\n",
      "        [0.2304],\n",
      "        [0.1861],\n",
      "        [0.0732],\n",
      "        [0.1925],\n",
      "        [0.0757],\n",
      "        [0.1884],\n",
      "        [0.0948],\n",
      "        [0.0967],\n",
      "        [0.1075],\n",
      "        [0.1164],\n",
      "        [0.1534],\n",
      "        [0.0590],\n",
      "        [0.0688],\n",
      "        [0.0686],\n",
      "        [0.0268],\n",
      "        [0.0498],\n",
      "        [0.0630],\n",
      "        [0.0901],\n",
      "        [0.0951],\n",
      "        [0.1229],\n",
      "        [0.1239],\n",
      "        [0.1363],\n",
      "        [0.1382],\n",
      "        [0.1391],\n",
      "        [0.1503],\n",
      "        [0.1335],\n",
      "        [0.1599],\n",
      "        [0.1453],\n",
      "        [0.1176],\n",
      "        [0.1596],\n",
      "        [0.1762],\n",
      "        [0.1780],\n",
      "        [0.0207],\n",
      "        [0.0874],\n",
      "        [0.0213],\n",
      "        [0.0063],\n",
      "        [0.0703],\n",
      "        [0.0761],\n",
      "        [0.0776],\n",
      "        [0.0948],\n",
      "        [0.1077],\n",
      "        [0.1079],\n",
      "        [0.1084],\n",
      "        [0.1194],\n",
      "        [0.1215],\n",
      "        [0.1253],\n",
      "        [0.1257],\n",
      "        [0.1305],\n",
      "        [0.1309],\n",
      "        [0.1430],\n",
      "        [0.1071],\n",
      "        [0.0206],\n",
      "        [0.0718],\n",
      "        [0.0619],\n",
      "        [0.0591],\n",
      "        [0.0833],\n",
      "        [0.0897],\n",
      "        [0.1315],\n",
      "        [0.1038],\n",
      "        [0.0508],\n",
      "        [0.1481],\n",
      "        [0.1498],\n",
      "        [0.0612],\n",
      "        [0.0451],\n",
      "        [0.0083],\n",
      "        [0.0754],\n",
      "        [0.1116],\n",
      "        [0.1334],\n",
      "        [0.0635],\n",
      "        [0.1433],\n",
      "        [0.1455],\n",
      "        [0.1525],\n",
      "        [0.1561],\n",
      "        [0.1564],\n",
      "        [0.1817],\n",
      "        [0.1862],\n",
      "        [0.0551],\n",
      "        [0.0314],\n",
      "        [0.0326],\n",
      "        [0.0501],\n",
      "        [0.1058],\n",
      "        [0.2000],\n",
      "        [0.2031],\n",
      "        [0.2058],\n",
      "        [0.2079],\n",
      "        [0.2110],\n",
      "        [0.2154],\n",
      "        [0.2189],\n",
      "        [0.2263],\n",
      "        [0.2302]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0387],\n",
      "        [0.0483],\n",
      "        [0.0769],\n",
      "        [0.0794],\n",
      "        [0.1237],\n",
      "        [0.0881],\n",
      "        [0.0895],\n",
      "        [0.0802],\n",
      "        [0.0276],\n",
      "        [0.0413],\n",
      "        [0.0663],\n",
      "        [0.0384],\n",
      "        [0.0839],\n",
      "        [0.0643],\n",
      "        [0.0843],\n",
      "        [0.0390],\n",
      "        [0.0442],\n",
      "        [0.0547],\n",
      "        [0.0492],\n",
      "        [0.2169],\n",
      "        [0.1637],\n",
      "        [0.1714],\n",
      "        [0.2029],\n",
      "        [0.2095],\n",
      "        [0.0644],\n",
      "        [0.2114],\n",
      "        [0.0780],\n",
      "        [0.0810],\n",
      "        [0.0872],\n",
      "        [0.1506],\n",
      "        [0.1022],\n",
      "        [0.1702],\n",
      "        [0.2347],\n",
      "        [0.1182],\n",
      "        [0.1240],\n",
      "        [0.1273],\n",
      "        [0.1326],\n",
      "        [0.1366],\n",
      "        [0.1407],\n",
      "        [0.1443],\n",
      "        [0.1488],\n",
      "        [0.1547],\n",
      "        [0.2032],\n",
      "        [0.2342],\n",
      "        [0.1785],\n",
      "        [0.1869],\n",
      "        [0.2017],\n",
      "        [0.2021],\n",
      "        [0.2103],\n",
      "        [0.2138],\n",
      "        [0.2148],\n",
      "        [0.2181],\n",
      "        [0.2275],\n",
      "        [0.2320],\n",
      "        [0.1877],\n",
      "        [0.0748],\n",
      "        [0.1941],\n",
      "        [0.0773],\n",
      "        [0.1900],\n",
      "        [0.0964],\n",
      "        [0.0983],\n",
      "        [0.1091],\n",
      "        [0.1181],\n",
      "        [0.1550],\n",
      "        [0.0573],\n",
      "        [0.0704],\n",
      "        [0.0702],\n",
      "        [0.0284],\n",
      "        [0.0514],\n",
      "        [0.0614],\n",
      "        [0.0885],\n",
      "        [0.0935],\n",
      "        [0.1213],\n",
      "        [0.1223],\n",
      "        [0.1347],\n",
      "        [0.1366],\n",
      "        [0.1375],\n",
      "        [0.1487],\n",
      "        [0.1319],\n",
      "        [0.1583],\n",
      "        [0.1437],\n",
      "        [0.1160],\n",
      "        [0.1580],\n",
      "        [0.1746],\n",
      "        [0.1764],\n",
      "        [0.0191],\n",
      "        [0.0858],\n",
      "        [0.0197],\n",
      "        [0.0047],\n",
      "        [0.0687],\n",
      "        [0.0745],\n",
      "        [0.0760],\n",
      "        [0.0932],\n",
      "        [0.1061],\n",
      "        [0.1063],\n",
      "        [0.1068],\n",
      "        [0.1178],\n",
      "        [0.1199],\n",
      "        [0.1237],\n",
      "        [0.1241],\n",
      "        [0.1289],\n",
      "        [0.1293],\n",
      "        [0.1414],\n",
      "        [0.1055],\n",
      "        [0.0190],\n",
      "        [0.0702],\n",
      "        [0.0602],\n",
      "        [0.0575],\n",
      "        [0.0817],\n",
      "        [0.0881],\n",
      "        [0.1299],\n",
      "        [0.1022],\n",
      "        [0.0492],\n",
      "        [0.1465],\n",
      "        [0.1482],\n",
      "        [0.0596],\n",
      "        [0.0435],\n",
      "        [0.0067],\n",
      "        [0.0738],\n",
      "        [0.1099],\n",
      "        [0.1318],\n",
      "        [0.0619],\n",
      "        [0.1417],\n",
      "        [0.1439],\n",
      "        [0.1509],\n",
      "        [0.1545],\n",
      "        [0.1548],\n",
      "        [0.1801],\n",
      "        [0.1846],\n",
      "        [0.0535],\n",
      "        [0.0298],\n",
      "        [0.0310],\n",
      "        [0.0485],\n",
      "        [0.1042],\n",
      "        [0.1984],\n",
      "        [0.2015],\n",
      "        [0.2042],\n",
      "        [0.2063],\n",
      "        [0.2094],\n",
      "        [0.2138],\n",
      "        [0.2173],\n",
      "        [0.2247],\n",
      "        [0.2286]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 31.161970853805542\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 144\n",
      "剩餘X 資料 torch.Size([16, 18])\n",
      "剩餘Y 資料 torch.Size([16, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05347870662808418, 8)\n",
      "The second_loss value of k: (0.05396895855665207, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.5340])\n",
      "目前模型的Data狀態 torch.Size([144, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653],\n",
      "        [0.7653]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0387],\n",
      "        [0.0483],\n",
      "        [0.0769],\n",
      "        [0.0794],\n",
      "        [0.1237],\n",
      "        [0.0881],\n",
      "        [0.0895],\n",
      "        [0.0802],\n",
      "        [0.0276],\n",
      "        [0.0413],\n",
      "        [0.0663],\n",
      "        [0.0384],\n",
      "        [0.0839],\n",
      "        [0.0643],\n",
      "        [0.0843],\n",
      "        [0.0390],\n",
      "        [0.0442],\n",
      "        [0.0547],\n",
      "        [0.0492],\n",
      "        [0.2169],\n",
      "        [0.1637],\n",
      "        [0.1714],\n",
      "        [0.2029],\n",
      "        [0.2095],\n",
      "        [0.0644],\n",
      "        [0.2114],\n",
      "        [0.0780],\n",
      "        [0.0810],\n",
      "        [0.0872],\n",
      "        [0.1506],\n",
      "        [0.1022],\n",
      "        [0.1702],\n",
      "        [0.2347],\n",
      "        [0.1182],\n",
      "        [0.1240],\n",
      "        [0.1273],\n",
      "        [0.1326],\n",
      "        [0.1366],\n",
      "        [0.1407],\n",
      "        [0.1443],\n",
      "        [0.1488],\n",
      "        [0.1547],\n",
      "        [0.2032],\n",
      "        [0.2342],\n",
      "        [0.1785],\n",
      "        [0.1869],\n",
      "        [0.2017],\n",
      "        [0.2021],\n",
      "        [0.2103],\n",
      "        [0.2138],\n",
      "        [0.2148],\n",
      "        [0.2181],\n",
      "        [0.2275],\n",
      "        [0.2320],\n",
      "        [0.1877],\n",
      "        [0.0748],\n",
      "        [0.1941],\n",
      "        [0.0773],\n",
      "        [0.1900],\n",
      "        [0.0964],\n",
      "        [0.0983],\n",
      "        [0.1091],\n",
      "        [0.1181],\n",
      "        [0.1550],\n",
      "        [0.0573],\n",
      "        [0.0704],\n",
      "        [0.0702],\n",
      "        [0.0284],\n",
      "        [0.0514],\n",
      "        [0.0614],\n",
      "        [0.0885],\n",
      "        [0.0935],\n",
      "        [0.1213],\n",
      "        [0.1223],\n",
      "        [0.1347],\n",
      "        [0.1366],\n",
      "        [0.1375],\n",
      "        [0.1487],\n",
      "        [0.1319],\n",
      "        [0.1583],\n",
      "        [0.1437],\n",
      "        [0.1160],\n",
      "        [0.1580],\n",
      "        [0.1746],\n",
      "        [0.1764],\n",
      "        [0.0191],\n",
      "        [0.0858],\n",
      "        [0.0197],\n",
      "        [0.0047],\n",
      "        [0.0687],\n",
      "        [0.0745],\n",
      "        [0.0760],\n",
      "        [0.0932],\n",
      "        [0.1061],\n",
      "        [0.1063],\n",
      "        [0.1068],\n",
      "        [0.1178],\n",
      "        [0.1199],\n",
      "        [0.1237],\n",
      "        [0.1241],\n",
      "        [0.1289],\n",
      "        [0.1293],\n",
      "        [0.1414],\n",
      "        [0.1055],\n",
      "        [0.0190],\n",
      "        [0.0702],\n",
      "        [0.0602],\n",
      "        [0.0575],\n",
      "        [0.0817],\n",
      "        [0.0881],\n",
      "        [0.1299],\n",
      "        [0.1022],\n",
      "        [0.0492],\n",
      "        [0.1465],\n",
      "        [0.1482],\n",
      "        [0.0596],\n",
      "        [0.0435],\n",
      "        [0.0067],\n",
      "        [0.0738],\n",
      "        [0.1099],\n",
      "        [0.1318],\n",
      "        [0.0619],\n",
      "        [0.1417],\n",
      "        [0.1439],\n",
      "        [0.1509],\n",
      "        [0.1545],\n",
      "        [0.1548],\n",
      "        [0.1801],\n",
      "        [0.1846],\n",
      "        [0.0535],\n",
      "        [0.0298],\n",
      "        [0.0310],\n",
      "        [0.0485],\n",
      "        [0.1042],\n",
      "        [0.1984],\n",
      "        [0.2015],\n",
      "        [0.2042],\n",
      "        [0.2063],\n",
      "        [0.2094],\n",
      "        [0.2138],\n",
      "        [0.2173],\n",
      "        [0.2247],\n",
      "        [0.2286],\n",
      "        [0.2313]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0403],\n",
      "        [0.0499],\n",
      "        [0.0785],\n",
      "        [0.0810],\n",
      "        [0.1253],\n",
      "        [0.0897],\n",
      "        [0.0911],\n",
      "        [0.0818],\n",
      "        [0.0292],\n",
      "        [0.0429],\n",
      "        [0.0679],\n",
      "        [0.0400],\n",
      "        [0.0856],\n",
      "        [0.0659],\n",
      "        [0.0859],\n",
      "        [0.0406],\n",
      "        [0.0458],\n",
      "        [0.0563],\n",
      "        [0.0508],\n",
      "        [0.2185],\n",
      "        [0.1653],\n",
      "        [0.1730],\n",
      "        [0.2045],\n",
      "        [0.2111],\n",
      "        [0.0660],\n",
      "        [0.2130],\n",
      "        [0.0796],\n",
      "        [0.0826],\n",
      "        [0.0888],\n",
      "        [0.1523],\n",
      "        [0.1038],\n",
      "        [0.1718],\n",
      "        [0.2364],\n",
      "        [0.1198],\n",
      "        [0.1256],\n",
      "        [0.1289],\n",
      "        [0.1342],\n",
      "        [0.1382],\n",
      "        [0.1423],\n",
      "        [0.1460],\n",
      "        [0.1505],\n",
      "        [0.1563],\n",
      "        [0.2048],\n",
      "        [0.2358],\n",
      "        [0.1801],\n",
      "        [0.1885],\n",
      "        [0.2033],\n",
      "        [0.2037],\n",
      "        [0.2119],\n",
      "        [0.2154],\n",
      "        [0.2164],\n",
      "        [0.2197],\n",
      "        [0.2291],\n",
      "        [0.2336],\n",
      "        [0.1893],\n",
      "        [0.0764],\n",
      "        [0.1957],\n",
      "        [0.0789],\n",
      "        [0.1916],\n",
      "        [0.0980],\n",
      "        [0.0999],\n",
      "        [0.1107],\n",
      "        [0.1197],\n",
      "        [0.1566],\n",
      "        [0.0557],\n",
      "        [0.0720],\n",
      "        [0.0718],\n",
      "        [0.0300],\n",
      "        [0.0530],\n",
      "        [0.0598],\n",
      "        [0.0869],\n",
      "        [0.0919],\n",
      "        [0.1197],\n",
      "        [0.1207],\n",
      "        [0.1331],\n",
      "        [0.1350],\n",
      "        [0.1359],\n",
      "        [0.1471],\n",
      "        [0.1303],\n",
      "        [0.1567],\n",
      "        [0.1421],\n",
      "        [0.1144],\n",
      "        [0.1564],\n",
      "        [0.1730],\n",
      "        [0.1748],\n",
      "        [0.0175],\n",
      "        [0.0842],\n",
      "        [0.0181],\n",
      "        [0.0031],\n",
      "        [0.0671],\n",
      "        [0.0729],\n",
      "        [0.0744],\n",
      "        [0.0916],\n",
      "        [0.1045],\n",
      "        [0.1047],\n",
      "        [0.1052],\n",
      "        [0.1162],\n",
      "        [0.1183],\n",
      "        [0.1221],\n",
      "        [0.1225],\n",
      "        [0.1273],\n",
      "        [0.1276],\n",
      "        [0.1398],\n",
      "        [0.1039],\n",
      "        [0.0173],\n",
      "        [0.0686],\n",
      "        [0.0586],\n",
      "        [0.0559],\n",
      "        [0.0801],\n",
      "        [0.0865],\n",
      "        [0.1283],\n",
      "        [0.1006],\n",
      "        [0.0476],\n",
      "        [0.1449],\n",
      "        [0.1466],\n",
      "        [0.0580],\n",
      "        [0.0419],\n",
      "        [0.0050],\n",
      "        [0.0722],\n",
      "        [0.1083],\n",
      "        [0.1302],\n",
      "        [0.0603],\n",
      "        [0.1401],\n",
      "        [0.1423],\n",
      "        [0.1493],\n",
      "        [0.1529],\n",
      "        [0.1531],\n",
      "        [0.1785],\n",
      "        [0.1830],\n",
      "        [0.0519],\n",
      "        [0.0282],\n",
      "        [0.0294],\n",
      "        [0.0469],\n",
      "        [0.1026],\n",
      "        [0.1967],\n",
      "        [0.1999],\n",
      "        [0.2026],\n",
      "        [0.2047],\n",
      "        [0.2078],\n",
      "        [0.2122],\n",
      "        [0.2157],\n",
      "        [0.2231],\n",
      "        [0.2270],\n",
      "        [0.2296]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 31.402052402496338\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 145\n",
      "剩餘X 資料 torch.Size([15, 18])\n",
      "剩餘Y 資料 torch.Size([15, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.053224753588438034, 8)\n",
      "The second_loss value of k: (0.05623003840446472, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.5329])\n",
      "目前模型的Data狀態 torch.Size([145, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636],\n",
      "        [0.7636]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0403],\n",
      "        [0.0499],\n",
      "        [0.0785],\n",
      "        [0.0810],\n",
      "        [0.1253],\n",
      "        [0.0897],\n",
      "        [0.0911],\n",
      "        [0.0818],\n",
      "        [0.0292],\n",
      "        [0.0429],\n",
      "        [0.0679],\n",
      "        [0.0400],\n",
      "        [0.0856],\n",
      "        [0.0659],\n",
      "        [0.0859],\n",
      "        [0.0406],\n",
      "        [0.0458],\n",
      "        [0.0563],\n",
      "        [0.0508],\n",
      "        [0.2185],\n",
      "        [0.1653],\n",
      "        [0.1730],\n",
      "        [0.2045],\n",
      "        [0.2111],\n",
      "        [0.0660],\n",
      "        [0.2130],\n",
      "        [0.0796],\n",
      "        [0.0826],\n",
      "        [0.0888],\n",
      "        [0.1523],\n",
      "        [0.1038],\n",
      "        [0.1718],\n",
      "        [0.2364],\n",
      "        [0.1198],\n",
      "        [0.1256],\n",
      "        [0.1289],\n",
      "        [0.1342],\n",
      "        [0.1382],\n",
      "        [0.1423],\n",
      "        [0.1460],\n",
      "        [0.1505],\n",
      "        [0.1563],\n",
      "        [0.2048],\n",
      "        [0.2358],\n",
      "        [0.1801],\n",
      "        [0.1885],\n",
      "        [0.2033],\n",
      "        [0.2037],\n",
      "        [0.2119],\n",
      "        [0.2154],\n",
      "        [0.2164],\n",
      "        [0.2197],\n",
      "        [0.2291],\n",
      "        [0.2336],\n",
      "        [0.1893],\n",
      "        [0.0764],\n",
      "        [0.1957],\n",
      "        [0.0789],\n",
      "        [0.1916],\n",
      "        [0.0980],\n",
      "        [0.0999],\n",
      "        [0.1107],\n",
      "        [0.1197],\n",
      "        [0.1566],\n",
      "        [0.0557],\n",
      "        [0.0720],\n",
      "        [0.0718],\n",
      "        [0.0300],\n",
      "        [0.0530],\n",
      "        [0.0598],\n",
      "        [0.0869],\n",
      "        [0.0919],\n",
      "        [0.1197],\n",
      "        [0.1207],\n",
      "        [0.1331],\n",
      "        [0.1350],\n",
      "        [0.1359],\n",
      "        [0.1471],\n",
      "        [0.1303],\n",
      "        [0.1567],\n",
      "        [0.1421],\n",
      "        [0.1144],\n",
      "        [0.1564],\n",
      "        [0.1730],\n",
      "        [0.1748],\n",
      "        [0.0175],\n",
      "        [0.0842],\n",
      "        [0.0181],\n",
      "        [0.0031],\n",
      "        [0.0671],\n",
      "        [0.0729],\n",
      "        [0.0744],\n",
      "        [0.0916],\n",
      "        [0.1045],\n",
      "        [0.1047],\n",
      "        [0.1052],\n",
      "        [0.1162],\n",
      "        [0.1183],\n",
      "        [0.1221],\n",
      "        [0.1225],\n",
      "        [0.1273],\n",
      "        [0.1276],\n",
      "        [0.1398],\n",
      "        [0.1039],\n",
      "        [0.0173],\n",
      "        [0.0686],\n",
      "        [0.0586],\n",
      "        [0.0559],\n",
      "        [0.0801],\n",
      "        [0.0865],\n",
      "        [0.1283],\n",
      "        [0.1006],\n",
      "        [0.0476],\n",
      "        [0.1449],\n",
      "        [0.1466],\n",
      "        [0.0580],\n",
      "        [0.0419],\n",
      "        [0.0050],\n",
      "        [0.0722],\n",
      "        [0.1083],\n",
      "        [0.1302],\n",
      "        [0.0603],\n",
      "        [0.1401],\n",
      "        [0.1423],\n",
      "        [0.1493],\n",
      "        [0.1529],\n",
      "        [0.1531],\n",
      "        [0.1785],\n",
      "        [0.1830],\n",
      "        [0.0519],\n",
      "        [0.0282],\n",
      "        [0.0294],\n",
      "        [0.0469],\n",
      "        [0.1026],\n",
      "        [0.1967],\n",
      "        [0.1999],\n",
      "        [0.2026],\n",
      "        [0.2047],\n",
      "        [0.2078],\n",
      "        [0.2122],\n",
      "        [0.2157],\n",
      "        [0.2231],\n",
      "        [0.2270],\n",
      "        [0.2296],\n",
      "        [0.2307]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0419],\n",
      "        [0.0515],\n",
      "        [0.0801],\n",
      "        [0.0826],\n",
      "        [0.1269],\n",
      "        [0.0913],\n",
      "        [0.0927],\n",
      "        [0.0834],\n",
      "        [0.0308],\n",
      "        [0.0445],\n",
      "        [0.0695],\n",
      "        [0.0416],\n",
      "        [0.0871],\n",
      "        [0.0675],\n",
      "        [0.0875],\n",
      "        [0.0422],\n",
      "        [0.0474],\n",
      "        [0.0579],\n",
      "        [0.0524],\n",
      "        [0.2201],\n",
      "        [0.1669],\n",
      "        [0.1746],\n",
      "        [0.2061],\n",
      "        [0.2127],\n",
      "        [0.0676],\n",
      "        [0.2146],\n",
      "        [0.0812],\n",
      "        [0.0842],\n",
      "        [0.0904],\n",
      "        [0.1538],\n",
      "        [0.1054],\n",
      "        [0.1734],\n",
      "        [0.2379],\n",
      "        [0.1214],\n",
      "        [0.1272],\n",
      "        [0.1305],\n",
      "        [0.1358],\n",
      "        [0.1398],\n",
      "        [0.1439],\n",
      "        [0.1475],\n",
      "        [0.1520],\n",
      "        [0.1579],\n",
      "        [0.2064],\n",
      "        [0.2374],\n",
      "        [0.1817],\n",
      "        [0.1901],\n",
      "        [0.2049],\n",
      "        [0.2053],\n",
      "        [0.2135],\n",
      "        [0.2170],\n",
      "        [0.2180],\n",
      "        [0.2213],\n",
      "        [0.2307],\n",
      "        [0.2352],\n",
      "        [0.1909],\n",
      "        [0.0780],\n",
      "        [0.1973],\n",
      "        [0.0805],\n",
      "        [0.1932],\n",
      "        [0.0996],\n",
      "        [0.1015],\n",
      "        [0.1123],\n",
      "        [0.1213],\n",
      "        [0.1582],\n",
      "        [0.0541],\n",
      "        [0.0736],\n",
      "        [0.0734],\n",
      "        [0.0316],\n",
      "        [0.0546],\n",
      "        [0.0582],\n",
      "        [0.0853],\n",
      "        [0.0903],\n",
      "        [0.1181],\n",
      "        [0.1191],\n",
      "        [0.1315],\n",
      "        [0.1334],\n",
      "        [0.1343],\n",
      "        [0.1455],\n",
      "        [0.1287],\n",
      "        [0.1551],\n",
      "        [0.1405],\n",
      "        [0.1128],\n",
      "        [0.1548],\n",
      "        [0.1714],\n",
      "        [0.1732],\n",
      "        [0.0159],\n",
      "        [0.0826],\n",
      "        [0.0165],\n",
      "        [0.0015],\n",
      "        [0.0655],\n",
      "        [0.0713],\n",
      "        [0.0728],\n",
      "        [0.0900],\n",
      "        [0.1029],\n",
      "        [0.1031],\n",
      "        [0.1036],\n",
      "        [0.1146],\n",
      "        [0.1167],\n",
      "        [0.1205],\n",
      "        [0.1209],\n",
      "        [0.1257],\n",
      "        [0.1261],\n",
      "        [0.1382],\n",
      "        [0.1023],\n",
      "        [0.0158],\n",
      "        [0.0670],\n",
      "        [0.0570],\n",
      "        [0.0543],\n",
      "        [0.0785],\n",
      "        [0.0849],\n",
      "        [0.1267],\n",
      "        [0.0990],\n",
      "        [0.0460],\n",
      "        [0.1433],\n",
      "        [0.1450],\n",
      "        [0.0564],\n",
      "        [0.0403],\n",
      "        [0.0035],\n",
      "        [0.0706],\n",
      "        [0.1067],\n",
      "        [0.1286],\n",
      "        [0.0587],\n",
      "        [0.1385],\n",
      "        [0.1407],\n",
      "        [0.1477],\n",
      "        [0.1513],\n",
      "        [0.1516],\n",
      "        [0.1769],\n",
      "        [0.1814],\n",
      "        [0.0503],\n",
      "        [0.0266],\n",
      "        [0.0278],\n",
      "        [0.0453],\n",
      "        [0.1010],\n",
      "        [0.1952],\n",
      "        [0.1983],\n",
      "        [0.2010],\n",
      "        [0.2031],\n",
      "        [0.2062],\n",
      "        [0.2106],\n",
      "        [0.2141],\n",
      "        [0.2215],\n",
      "        [0.2254],\n",
      "        [0.2281],\n",
      "        [0.2291]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 31.63503384590149\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 146\n",
      "剩餘X 資料 torch.Size([14, 18])\n",
      "剩餘Y 資料 torch.Size([14, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05547862872481346, 6)\n",
      "The second_loss value of k: (0.055515531450510025, 7)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.5265])\n",
      "目前模型的Data狀態 torch.Size([146, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621],\n",
      "        [0.7621]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0419],\n",
      "        [0.0515],\n",
      "        [0.0801],\n",
      "        [0.0826],\n",
      "        [0.1269],\n",
      "        [0.0913],\n",
      "        [0.0927],\n",
      "        [0.0834],\n",
      "        [0.0308],\n",
      "        [0.0445],\n",
      "        [0.0695],\n",
      "        [0.0416],\n",
      "        [0.0871],\n",
      "        [0.0675],\n",
      "        [0.0875],\n",
      "        [0.0422],\n",
      "        [0.0474],\n",
      "        [0.0579],\n",
      "        [0.0524],\n",
      "        [0.2201],\n",
      "        [0.1669],\n",
      "        [0.1746],\n",
      "        [0.2061],\n",
      "        [0.2127],\n",
      "        [0.0676],\n",
      "        [0.2146],\n",
      "        [0.0812],\n",
      "        [0.0842],\n",
      "        [0.0904],\n",
      "        [0.1538],\n",
      "        [0.1054],\n",
      "        [0.1734],\n",
      "        [0.2379],\n",
      "        [0.1214],\n",
      "        [0.1272],\n",
      "        [0.1305],\n",
      "        [0.1358],\n",
      "        [0.1398],\n",
      "        [0.1439],\n",
      "        [0.1475],\n",
      "        [0.1520],\n",
      "        [0.1579],\n",
      "        [0.2064],\n",
      "        [0.2374],\n",
      "        [0.1817],\n",
      "        [0.1901],\n",
      "        [0.2049],\n",
      "        [0.2053],\n",
      "        [0.2135],\n",
      "        [0.2170],\n",
      "        [0.2180],\n",
      "        [0.2213],\n",
      "        [0.2307],\n",
      "        [0.2352],\n",
      "        [0.1909],\n",
      "        [0.0780],\n",
      "        [0.1973],\n",
      "        [0.0805],\n",
      "        [0.1932],\n",
      "        [0.0996],\n",
      "        [0.1015],\n",
      "        [0.1123],\n",
      "        [0.1213],\n",
      "        [0.1582],\n",
      "        [0.0541],\n",
      "        [0.0736],\n",
      "        [0.0734],\n",
      "        [0.0316],\n",
      "        [0.0546],\n",
      "        [0.0582],\n",
      "        [0.0853],\n",
      "        [0.0903],\n",
      "        [0.1181],\n",
      "        [0.1191],\n",
      "        [0.1315],\n",
      "        [0.1334],\n",
      "        [0.1343],\n",
      "        [0.1455],\n",
      "        [0.1287],\n",
      "        [0.1551],\n",
      "        [0.1405],\n",
      "        [0.1128],\n",
      "        [0.1548],\n",
      "        [0.1714],\n",
      "        [0.1732],\n",
      "        [0.0159],\n",
      "        [0.0826],\n",
      "        [0.0165],\n",
      "        [0.0015],\n",
      "        [0.0655],\n",
      "        [0.0713],\n",
      "        [0.0728],\n",
      "        [0.0900],\n",
      "        [0.1029],\n",
      "        [0.1031],\n",
      "        [0.1036],\n",
      "        [0.1146],\n",
      "        [0.1167],\n",
      "        [0.1205],\n",
      "        [0.1209],\n",
      "        [0.1257],\n",
      "        [0.1261],\n",
      "        [0.1382],\n",
      "        [0.1023],\n",
      "        [0.0158],\n",
      "        [0.0670],\n",
      "        [0.0570],\n",
      "        [0.0543],\n",
      "        [0.0785],\n",
      "        [0.0849],\n",
      "        [0.1267],\n",
      "        [0.0990],\n",
      "        [0.0460],\n",
      "        [0.1433],\n",
      "        [0.1450],\n",
      "        [0.0564],\n",
      "        [0.0403],\n",
      "        [0.0035],\n",
      "        [0.0706],\n",
      "        [0.1067],\n",
      "        [0.1286],\n",
      "        [0.0587],\n",
      "        [0.1385],\n",
      "        [0.1407],\n",
      "        [0.1477],\n",
      "        [0.1513],\n",
      "        [0.1516],\n",
      "        [0.1769],\n",
      "        [0.1814],\n",
      "        [0.0503],\n",
      "        [0.0266],\n",
      "        [0.0278],\n",
      "        [0.0453],\n",
      "        [0.1010],\n",
      "        [0.1952],\n",
      "        [0.1983],\n",
      "        [0.2010],\n",
      "        [0.2031],\n",
      "        [0.2062],\n",
      "        [0.2106],\n",
      "        [0.2141],\n",
      "        [0.2215],\n",
      "        [0.2254],\n",
      "        [0.2281],\n",
      "        [0.2291],\n",
      "        [0.2355]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 78\n",
      "Number of shrink: 22\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0435],\n",
      "        [    0.0531],\n",
      "        [    0.0817],\n",
      "        [    0.0842],\n",
      "        [    0.1285],\n",
      "        [    0.0929],\n",
      "        [    0.0943],\n",
      "        [    0.0850],\n",
      "        [    0.0324],\n",
      "        [    0.0461],\n",
      "        [    0.0711],\n",
      "        [    0.0432],\n",
      "        [    0.0888],\n",
      "        [    0.0691],\n",
      "        [    0.0891],\n",
      "        [    0.0438],\n",
      "        [    0.0490],\n",
      "        [    0.0595],\n",
      "        [    0.0540],\n",
      "        [    0.2217],\n",
      "        [    0.1685],\n",
      "        [    0.1762],\n",
      "        [    0.2077],\n",
      "        [    0.2143],\n",
      "        [    0.0692],\n",
      "        [    0.2163],\n",
      "        [    0.0828],\n",
      "        [    0.0858],\n",
      "        [    0.0920],\n",
      "        [    0.1555],\n",
      "        [    0.1070],\n",
      "        [    0.1750],\n",
      "        [    0.2396],\n",
      "        [    0.1230],\n",
      "        [    0.1288],\n",
      "        [    0.1321],\n",
      "        [    0.1374],\n",
      "        [    0.1414],\n",
      "        [    0.1455],\n",
      "        [    0.1492],\n",
      "        [    0.1537],\n",
      "        [    0.1595],\n",
      "        [    0.2080],\n",
      "        [    0.2390],\n",
      "        [    0.1833],\n",
      "        [    0.1917],\n",
      "        [    0.2065],\n",
      "        [    0.2069],\n",
      "        [    0.2151],\n",
      "        [    0.2186],\n",
      "        [    0.2196],\n",
      "        [    0.2229],\n",
      "        [    0.2323],\n",
      "        [    0.2368],\n",
      "        [    0.1926],\n",
      "        [    0.0796],\n",
      "        [    0.1989],\n",
      "        [    0.0821],\n",
      "        [    0.1948],\n",
      "        [    0.1012],\n",
      "        [    0.1031],\n",
      "        [    0.1139],\n",
      "        [    0.1229],\n",
      "        [    0.1598],\n",
      "        [    0.0525],\n",
      "        [    0.0752],\n",
      "        [    0.0750],\n",
      "        [    0.0332],\n",
      "        [    0.0562],\n",
      "        [    0.0566],\n",
      "        [    0.0837],\n",
      "        [    0.0887],\n",
      "        [    0.1165],\n",
      "        [    0.1175],\n",
      "        [    0.1299],\n",
      "        [    0.1318],\n",
      "        [    0.1327],\n",
      "        [    0.1439],\n",
      "        [    0.1271],\n",
      "        [    0.1535],\n",
      "        [    0.1389],\n",
      "        [    0.1112],\n",
      "        [    0.1532],\n",
      "        [    0.1698],\n",
      "        [    0.1716],\n",
      "        [    0.0143],\n",
      "        [    0.0810],\n",
      "        [    0.0149],\n",
      "        [    0.0002],\n",
      "        [    0.0639],\n",
      "        [    0.0697],\n",
      "        [    0.0712],\n",
      "        [    0.0884],\n",
      "        [    0.1013],\n",
      "        [    0.1015],\n",
      "        [    0.1020],\n",
      "        [    0.1130],\n",
      "        [    0.1151],\n",
      "        [    0.1189],\n",
      "        [    0.1193],\n",
      "        [    0.1241],\n",
      "        [    0.1244],\n",
      "        [    0.1366],\n",
      "        [    0.1007],\n",
      "        [    0.0141],\n",
      "        [    0.0654],\n",
      "        [    0.0554],\n",
      "        [    0.0527],\n",
      "        [    0.0769],\n",
      "        [    0.0833],\n",
      "        [    0.1251],\n",
      "        [    0.0974],\n",
      "        [    0.0444],\n",
      "        [    0.1417],\n",
      "        [    0.1434],\n",
      "        [    0.0548],\n",
      "        [    0.0387],\n",
      "        [    0.0018],\n",
      "        [    0.0690],\n",
      "        [    0.1051],\n",
      "        [    0.1270],\n",
      "        [    0.0571],\n",
      "        [    0.1369],\n",
      "        [    0.1391],\n",
      "        [    0.1461],\n",
      "        [    0.1497],\n",
      "        [    0.1499],\n",
      "        [    0.1753],\n",
      "        [    0.1798],\n",
      "        [    0.0487],\n",
      "        [    0.0250],\n",
      "        [    0.0262],\n",
      "        [    0.0437],\n",
      "        [    0.0994],\n",
      "        [    0.1935],\n",
      "        [    0.1967],\n",
      "        [    0.1994],\n",
      "        [    0.2015],\n",
      "        [    0.2046],\n",
      "        [    0.2090],\n",
      "        [    0.2125],\n",
      "        [    0.2199],\n",
      "        [    0.2238],\n",
      "        [    0.2264],\n",
      "        [    0.2275],\n",
      "        [    0.2339]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 31.867966651916504\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 147\n",
      "剩餘X 資料 torch.Size([13, 18])\n",
      "剩餘Y 資料 torch.Size([13, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05475788190960884, 6)\n",
      "The second_loss value of k: (0.05636436119675636, 7)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.5264])\n",
      "目前模型的Data狀態 torch.Size([147, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604],\n",
      "        [0.7604]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0435],\n",
      "        [    0.0531],\n",
      "        [    0.0817],\n",
      "        [    0.0842],\n",
      "        [    0.1285],\n",
      "        [    0.0929],\n",
      "        [    0.0943],\n",
      "        [    0.0850],\n",
      "        [    0.0324],\n",
      "        [    0.0461],\n",
      "        [    0.0711],\n",
      "        [    0.0432],\n",
      "        [    0.0888],\n",
      "        [    0.0691],\n",
      "        [    0.0891],\n",
      "        [    0.0438],\n",
      "        [    0.0490],\n",
      "        [    0.0595],\n",
      "        [    0.0540],\n",
      "        [    0.2217],\n",
      "        [    0.1685],\n",
      "        [    0.1762],\n",
      "        [    0.2077],\n",
      "        [    0.2143],\n",
      "        [    0.0692],\n",
      "        [    0.2163],\n",
      "        [    0.0828],\n",
      "        [    0.0858],\n",
      "        [    0.0920],\n",
      "        [    0.1555],\n",
      "        [    0.1070],\n",
      "        [    0.1750],\n",
      "        [    0.2396],\n",
      "        [    0.1230],\n",
      "        [    0.1288],\n",
      "        [    0.1321],\n",
      "        [    0.1374],\n",
      "        [    0.1414],\n",
      "        [    0.1455],\n",
      "        [    0.1492],\n",
      "        [    0.1537],\n",
      "        [    0.1595],\n",
      "        [    0.2080],\n",
      "        [    0.2390],\n",
      "        [    0.1833],\n",
      "        [    0.1917],\n",
      "        [    0.2065],\n",
      "        [    0.2069],\n",
      "        [    0.2151],\n",
      "        [    0.2186],\n",
      "        [    0.2196],\n",
      "        [    0.2229],\n",
      "        [    0.2323],\n",
      "        [    0.2368],\n",
      "        [    0.1926],\n",
      "        [    0.0796],\n",
      "        [    0.1989],\n",
      "        [    0.0821],\n",
      "        [    0.1948],\n",
      "        [    0.1012],\n",
      "        [    0.1031],\n",
      "        [    0.1139],\n",
      "        [    0.1229],\n",
      "        [    0.1598],\n",
      "        [    0.0525],\n",
      "        [    0.0752],\n",
      "        [    0.0750],\n",
      "        [    0.0332],\n",
      "        [    0.0562],\n",
      "        [    0.0566],\n",
      "        [    0.0837],\n",
      "        [    0.0887],\n",
      "        [    0.1165],\n",
      "        [    0.1175],\n",
      "        [    0.1299],\n",
      "        [    0.1318],\n",
      "        [    0.1327],\n",
      "        [    0.1439],\n",
      "        [    0.1271],\n",
      "        [    0.1535],\n",
      "        [    0.1389],\n",
      "        [    0.1112],\n",
      "        [    0.1532],\n",
      "        [    0.1698],\n",
      "        [    0.1716],\n",
      "        [    0.0143],\n",
      "        [    0.0810],\n",
      "        [    0.0149],\n",
      "        [    0.0002],\n",
      "        [    0.0639],\n",
      "        [    0.0697],\n",
      "        [    0.0712],\n",
      "        [    0.0884],\n",
      "        [    0.1013],\n",
      "        [    0.1015],\n",
      "        [    0.1020],\n",
      "        [    0.1130],\n",
      "        [    0.1151],\n",
      "        [    0.1189],\n",
      "        [    0.1193],\n",
      "        [    0.1241],\n",
      "        [    0.1244],\n",
      "        [    0.1366],\n",
      "        [    0.1007],\n",
      "        [    0.0141],\n",
      "        [    0.0654],\n",
      "        [    0.0554],\n",
      "        [    0.0527],\n",
      "        [    0.0769],\n",
      "        [    0.0833],\n",
      "        [    0.1251],\n",
      "        [    0.0974],\n",
      "        [    0.0444],\n",
      "        [    0.1417],\n",
      "        [    0.1434],\n",
      "        [    0.0548],\n",
      "        [    0.0387],\n",
      "        [    0.0018],\n",
      "        [    0.0690],\n",
      "        [    0.1051],\n",
      "        [    0.1270],\n",
      "        [    0.0571],\n",
      "        [    0.1369],\n",
      "        [    0.1391],\n",
      "        [    0.1461],\n",
      "        [    0.1497],\n",
      "        [    0.1499],\n",
      "        [    0.1753],\n",
      "        [    0.1798],\n",
      "        [    0.0487],\n",
      "        [    0.0250],\n",
      "        [    0.0262],\n",
      "        [    0.0437],\n",
      "        [    0.0994],\n",
      "        [    0.1935],\n",
      "        [    0.1967],\n",
      "        [    0.1994],\n",
      "        [    0.2015],\n",
      "        [    0.2046],\n",
      "        [    0.2090],\n",
      "        [    0.2125],\n",
      "        [    0.2199],\n",
      "        [    0.2238],\n",
      "        [    0.2264],\n",
      "        [    0.2275],\n",
      "        [    0.2339],\n",
      "        [    0.2340]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0451],\n",
      "        [0.0547],\n",
      "        [0.0833],\n",
      "        [0.0858],\n",
      "        [0.1301],\n",
      "        [0.0945],\n",
      "        [0.0959],\n",
      "        [0.0866],\n",
      "        [0.0340],\n",
      "        [0.0477],\n",
      "        [0.0727],\n",
      "        [0.0448],\n",
      "        [0.0903],\n",
      "        [0.0707],\n",
      "        [0.0907],\n",
      "        [0.0454],\n",
      "        [0.0506],\n",
      "        [0.0611],\n",
      "        [0.0556],\n",
      "        [0.2233],\n",
      "        [0.1701],\n",
      "        [0.1778],\n",
      "        [0.2093],\n",
      "        [0.2159],\n",
      "        [0.0708],\n",
      "        [0.2178],\n",
      "        [0.0844],\n",
      "        [0.0874],\n",
      "        [0.0936],\n",
      "        [0.1571],\n",
      "        [0.1086],\n",
      "        [0.1766],\n",
      "        [0.2412],\n",
      "        [0.1246],\n",
      "        [0.1304],\n",
      "        [0.1337],\n",
      "        [0.1390],\n",
      "        [0.1430],\n",
      "        [0.1471],\n",
      "        [0.1507],\n",
      "        [0.1553],\n",
      "        [0.1611],\n",
      "        [0.2096],\n",
      "        [0.2406],\n",
      "        [0.1849],\n",
      "        [0.1933],\n",
      "        [0.2081],\n",
      "        [0.2085],\n",
      "        [0.2167],\n",
      "        [0.2202],\n",
      "        [0.2212],\n",
      "        [0.2245],\n",
      "        [0.2339],\n",
      "        [0.2384],\n",
      "        [0.1941],\n",
      "        [0.0812],\n",
      "        [0.2005],\n",
      "        [0.0837],\n",
      "        [0.1964],\n",
      "        [0.1028],\n",
      "        [0.1047],\n",
      "        [0.1155],\n",
      "        [0.1245],\n",
      "        [0.1614],\n",
      "        [0.0509],\n",
      "        [0.0768],\n",
      "        [0.0766],\n",
      "        [0.0348],\n",
      "        [0.0578],\n",
      "        [0.0550],\n",
      "        [0.0821],\n",
      "        [0.0871],\n",
      "        [0.1149],\n",
      "        [0.1159],\n",
      "        [0.1283],\n",
      "        [0.1302],\n",
      "        [0.1311],\n",
      "        [0.1423],\n",
      "        [0.1255],\n",
      "        [0.1519],\n",
      "        [0.1373],\n",
      "        [0.1096],\n",
      "        [0.1516],\n",
      "        [0.1682],\n",
      "        [0.1700],\n",
      "        [0.0127],\n",
      "        [0.0794],\n",
      "        [0.0133],\n",
      "        [0.0017],\n",
      "        [0.0623],\n",
      "        [0.0681],\n",
      "        [0.0696],\n",
      "        [0.0868],\n",
      "        [0.0997],\n",
      "        [0.0999],\n",
      "        [0.1004],\n",
      "        [0.1114],\n",
      "        [0.1135],\n",
      "        [0.1173],\n",
      "        [0.1177],\n",
      "        [0.1225],\n",
      "        [0.1229],\n",
      "        [0.1350],\n",
      "        [0.0991],\n",
      "        [0.0125],\n",
      "        [0.0638],\n",
      "        [0.0538],\n",
      "        [0.0511],\n",
      "        [0.0753],\n",
      "        [0.0817],\n",
      "        [0.1235],\n",
      "        [0.0958],\n",
      "        [0.0428],\n",
      "        [0.1401],\n",
      "        [0.1418],\n",
      "        [0.0532],\n",
      "        [0.0371],\n",
      "        [0.0003],\n",
      "        [0.0674],\n",
      "        [0.1035],\n",
      "        [0.1254],\n",
      "        [0.0555],\n",
      "        [0.1353],\n",
      "        [0.1375],\n",
      "        [0.1445],\n",
      "        [0.1481],\n",
      "        [0.1484],\n",
      "        [0.1737],\n",
      "        [0.1782],\n",
      "        [0.0471],\n",
      "        [0.0234],\n",
      "        [0.0246],\n",
      "        [0.0421],\n",
      "        [0.0978],\n",
      "        [0.1919],\n",
      "        [0.1951],\n",
      "        [0.1978],\n",
      "        [0.1999],\n",
      "        [0.2030],\n",
      "        [0.2074],\n",
      "        [0.2109],\n",
      "        [0.2183],\n",
      "        [0.2222],\n",
      "        [0.2248],\n",
      "        [0.2259],\n",
      "        [0.2323],\n",
      "        [0.2324]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 32.10783052444458\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 148\n",
      "剩餘X 資料 torch.Size([12, 18])\n",
      "剩餘Y 資料 torch.Size([12, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05560935661196709, 6)\n",
      "The second_loss value of k: (0.06125073507428169, 5)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.5230])\n",
      "目前模型的Data狀態 torch.Size([148, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588],\n",
      "        [0.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0451],\n",
      "        [0.0547],\n",
      "        [0.0833],\n",
      "        [0.0858],\n",
      "        [0.1301],\n",
      "        [0.0945],\n",
      "        [0.0959],\n",
      "        [0.0866],\n",
      "        [0.0340],\n",
      "        [0.0477],\n",
      "        [0.0727],\n",
      "        [0.0448],\n",
      "        [0.0903],\n",
      "        [0.0707],\n",
      "        [0.0907],\n",
      "        [0.0454],\n",
      "        [0.0506],\n",
      "        [0.0611],\n",
      "        [0.0556],\n",
      "        [0.2233],\n",
      "        [0.1701],\n",
      "        [0.1778],\n",
      "        [0.2093],\n",
      "        [0.2159],\n",
      "        [0.0708],\n",
      "        [0.2178],\n",
      "        [0.0844],\n",
      "        [0.0874],\n",
      "        [0.0936],\n",
      "        [0.1571],\n",
      "        [0.1086],\n",
      "        [0.1766],\n",
      "        [0.2412],\n",
      "        [0.1246],\n",
      "        [0.1304],\n",
      "        [0.1337],\n",
      "        [0.1390],\n",
      "        [0.1430],\n",
      "        [0.1471],\n",
      "        [0.1507],\n",
      "        [0.1553],\n",
      "        [0.1611],\n",
      "        [0.2096],\n",
      "        [0.2406],\n",
      "        [0.1849],\n",
      "        [0.1933],\n",
      "        [0.2081],\n",
      "        [0.2085],\n",
      "        [0.2167],\n",
      "        [0.2202],\n",
      "        [0.2212],\n",
      "        [0.2245],\n",
      "        [0.2339],\n",
      "        [0.2384],\n",
      "        [0.1941],\n",
      "        [0.0812],\n",
      "        [0.2005],\n",
      "        [0.0837],\n",
      "        [0.1964],\n",
      "        [0.1028],\n",
      "        [0.1047],\n",
      "        [0.1155],\n",
      "        [0.1245],\n",
      "        [0.1614],\n",
      "        [0.0509],\n",
      "        [0.0768],\n",
      "        [0.0766],\n",
      "        [0.0348],\n",
      "        [0.0578],\n",
      "        [0.0550],\n",
      "        [0.0821],\n",
      "        [0.0871],\n",
      "        [0.1149],\n",
      "        [0.1159],\n",
      "        [0.1283],\n",
      "        [0.1302],\n",
      "        [0.1311],\n",
      "        [0.1423],\n",
      "        [0.1255],\n",
      "        [0.1519],\n",
      "        [0.1373],\n",
      "        [0.1096],\n",
      "        [0.1516],\n",
      "        [0.1682],\n",
      "        [0.1700],\n",
      "        [0.0127],\n",
      "        [0.0794],\n",
      "        [0.0133],\n",
      "        [0.0017],\n",
      "        [0.0623],\n",
      "        [0.0681],\n",
      "        [0.0696],\n",
      "        [0.0868],\n",
      "        [0.0997],\n",
      "        [0.0999],\n",
      "        [0.1004],\n",
      "        [0.1114],\n",
      "        [0.1135],\n",
      "        [0.1173],\n",
      "        [0.1177],\n",
      "        [0.1225],\n",
      "        [0.1229],\n",
      "        [0.1350],\n",
      "        [0.0991],\n",
      "        [0.0125],\n",
      "        [0.0638],\n",
      "        [0.0538],\n",
      "        [0.0511],\n",
      "        [0.0753],\n",
      "        [0.0817],\n",
      "        [0.1235],\n",
      "        [0.0958],\n",
      "        [0.0428],\n",
      "        [0.1401],\n",
      "        [0.1418],\n",
      "        [0.0532],\n",
      "        [0.0371],\n",
      "        [0.0003],\n",
      "        [0.0674],\n",
      "        [0.1035],\n",
      "        [0.1254],\n",
      "        [0.0555],\n",
      "        [0.1353],\n",
      "        [0.1375],\n",
      "        [0.1445],\n",
      "        [0.1481],\n",
      "        [0.1484],\n",
      "        [0.1737],\n",
      "        [0.1782],\n",
      "        [0.0471],\n",
      "        [0.0234],\n",
      "        [0.0246],\n",
      "        [0.0421],\n",
      "        [0.0978],\n",
      "        [0.1919],\n",
      "        [0.1951],\n",
      "        [0.1978],\n",
      "        [0.1999],\n",
      "        [0.2030],\n",
      "        [0.2074],\n",
      "        [0.2109],\n",
      "        [0.2183],\n",
      "        [0.2222],\n",
      "        [0.2248],\n",
      "        [0.2259],\n",
      "        [0.2323],\n",
      "        [0.2324],\n",
      "        [0.2358]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0467],\n",
      "        [0.0563],\n",
      "        [0.0849],\n",
      "        [0.0874],\n",
      "        [0.1317],\n",
      "        [0.0961],\n",
      "        [0.0975],\n",
      "        [0.0882],\n",
      "        [0.0356],\n",
      "        [0.0493],\n",
      "        [0.0743],\n",
      "        [0.0464],\n",
      "        [0.0919],\n",
      "        [0.0723],\n",
      "        [0.0923],\n",
      "        [0.0470],\n",
      "        [0.0522],\n",
      "        [0.0627],\n",
      "        [0.0572],\n",
      "        [0.2249],\n",
      "        [0.1717],\n",
      "        [0.1794],\n",
      "        [0.2109],\n",
      "        [0.2175],\n",
      "        [0.0724],\n",
      "        [0.2194],\n",
      "        [0.0860],\n",
      "        [0.0890],\n",
      "        [0.0952],\n",
      "        [0.1586],\n",
      "        [0.1102],\n",
      "        [0.1782],\n",
      "        [0.2427],\n",
      "        [0.1262],\n",
      "        [0.1320],\n",
      "        [0.1353],\n",
      "        [0.1406],\n",
      "        [0.1446],\n",
      "        [0.1487],\n",
      "        [0.1523],\n",
      "        [0.1568],\n",
      "        [0.1627],\n",
      "        [0.2112],\n",
      "        [0.2422],\n",
      "        [0.1865],\n",
      "        [0.1949],\n",
      "        [0.2097],\n",
      "        [0.2101],\n",
      "        [0.2183],\n",
      "        [0.2218],\n",
      "        [0.2228],\n",
      "        [0.2261],\n",
      "        [0.2355],\n",
      "        [0.2400],\n",
      "        [0.1957],\n",
      "        [0.0828],\n",
      "        [0.2021],\n",
      "        [0.0853],\n",
      "        [0.1980],\n",
      "        [0.1044],\n",
      "        [0.1063],\n",
      "        [0.1171],\n",
      "        [0.1261],\n",
      "        [0.1630],\n",
      "        [0.0493],\n",
      "        [0.0784],\n",
      "        [0.0782],\n",
      "        [0.0364],\n",
      "        [0.0594],\n",
      "        [0.0534],\n",
      "        [0.0805],\n",
      "        [0.0855],\n",
      "        [0.1133],\n",
      "        [0.1143],\n",
      "        [0.1267],\n",
      "        [0.1286],\n",
      "        [0.1295],\n",
      "        [0.1407],\n",
      "        [0.1239],\n",
      "        [0.1503],\n",
      "        [0.1357],\n",
      "        [0.1080],\n",
      "        [0.1500],\n",
      "        [0.1666],\n",
      "        [0.1684],\n",
      "        [0.0111],\n",
      "        [0.0778],\n",
      "        [0.0117],\n",
      "        [0.0033],\n",
      "        [0.0607],\n",
      "        [0.0665],\n",
      "        [0.0680],\n",
      "        [0.0852],\n",
      "        [0.0981],\n",
      "        [0.0983],\n",
      "        [0.0988],\n",
      "        [0.1098],\n",
      "        [0.1119],\n",
      "        [0.1157],\n",
      "        [0.1161],\n",
      "        [0.1209],\n",
      "        [0.1213],\n",
      "        [0.1334],\n",
      "        [0.0975],\n",
      "        [0.0110],\n",
      "        [0.0622],\n",
      "        [0.0522],\n",
      "        [0.0495],\n",
      "        [0.0737],\n",
      "        [0.0801],\n",
      "        [0.1219],\n",
      "        [0.0942],\n",
      "        [0.0412],\n",
      "        [0.1385],\n",
      "        [0.1402],\n",
      "        [0.0516],\n",
      "        [0.0355],\n",
      "        [0.0013],\n",
      "        [0.0658],\n",
      "        [0.1020],\n",
      "        [0.1238],\n",
      "        [0.0539],\n",
      "        [0.1337],\n",
      "        [0.1359],\n",
      "        [0.1429],\n",
      "        [0.1465],\n",
      "        [0.1468],\n",
      "        [0.1721],\n",
      "        [0.1766],\n",
      "        [0.0455],\n",
      "        [0.0218],\n",
      "        [0.0230],\n",
      "        [0.0405],\n",
      "        [0.0962],\n",
      "        [0.1904],\n",
      "        [0.1935],\n",
      "        [0.1962],\n",
      "        [0.1983],\n",
      "        [0.2014],\n",
      "        [0.2058],\n",
      "        [0.2093],\n",
      "        [0.2167],\n",
      "        [0.2206],\n",
      "        [0.2233],\n",
      "        [0.2243],\n",
      "        [0.2307],\n",
      "        [0.2308],\n",
      "        [0.2342]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 32.34389281272888\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 149\n",
      "剩餘X 資料 torch.Size([11, 18])\n",
      "剩餘Y 資料 torch.Size([11, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.060466330498456955, 5)\n",
      "The second_loss value of k: (0.06860177963972092, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.5114])\n",
      "目前模型的Data狀態 torch.Size([149, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573],\n",
      "        [0.7573]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0467],\n",
      "        [0.0563],\n",
      "        [0.0849],\n",
      "        [0.0874],\n",
      "        [0.1317],\n",
      "        [0.0961],\n",
      "        [0.0975],\n",
      "        [0.0882],\n",
      "        [0.0356],\n",
      "        [0.0493],\n",
      "        [0.0743],\n",
      "        [0.0464],\n",
      "        [0.0919],\n",
      "        [0.0723],\n",
      "        [0.0923],\n",
      "        [0.0470],\n",
      "        [0.0522],\n",
      "        [0.0627],\n",
      "        [0.0572],\n",
      "        [0.2249],\n",
      "        [0.1717],\n",
      "        [0.1794],\n",
      "        [0.2109],\n",
      "        [0.2175],\n",
      "        [0.0724],\n",
      "        [0.2194],\n",
      "        [0.0860],\n",
      "        [0.0890],\n",
      "        [0.0952],\n",
      "        [0.1586],\n",
      "        [0.1102],\n",
      "        [0.1782],\n",
      "        [0.2427],\n",
      "        [0.1262],\n",
      "        [0.1320],\n",
      "        [0.1353],\n",
      "        [0.1406],\n",
      "        [0.1446],\n",
      "        [0.1487],\n",
      "        [0.1523],\n",
      "        [0.1568],\n",
      "        [0.1627],\n",
      "        [0.2112],\n",
      "        [0.2422],\n",
      "        [0.1865],\n",
      "        [0.1949],\n",
      "        [0.2097],\n",
      "        [0.2101],\n",
      "        [0.2183],\n",
      "        [0.2218],\n",
      "        [0.2228],\n",
      "        [0.2261],\n",
      "        [0.2355],\n",
      "        [0.2400],\n",
      "        [0.1957],\n",
      "        [0.0828],\n",
      "        [0.2021],\n",
      "        [0.0853],\n",
      "        [0.1980],\n",
      "        [0.1044],\n",
      "        [0.1063],\n",
      "        [0.1171],\n",
      "        [0.1261],\n",
      "        [0.1630],\n",
      "        [0.0493],\n",
      "        [0.0784],\n",
      "        [0.0782],\n",
      "        [0.0364],\n",
      "        [0.0594],\n",
      "        [0.0534],\n",
      "        [0.0805],\n",
      "        [0.0855],\n",
      "        [0.1133],\n",
      "        [0.1143],\n",
      "        [0.1267],\n",
      "        [0.1286],\n",
      "        [0.1295],\n",
      "        [0.1407],\n",
      "        [0.1239],\n",
      "        [0.1503],\n",
      "        [0.1357],\n",
      "        [0.1080],\n",
      "        [0.1500],\n",
      "        [0.1666],\n",
      "        [0.1684],\n",
      "        [0.0111],\n",
      "        [0.0778],\n",
      "        [0.0117],\n",
      "        [0.0033],\n",
      "        [0.0607],\n",
      "        [0.0665],\n",
      "        [0.0680],\n",
      "        [0.0852],\n",
      "        [0.0981],\n",
      "        [0.0983],\n",
      "        [0.0988],\n",
      "        [0.1098],\n",
      "        [0.1119],\n",
      "        [0.1157],\n",
      "        [0.1161],\n",
      "        [0.1209],\n",
      "        [0.1213],\n",
      "        [0.1334],\n",
      "        [0.0975],\n",
      "        [0.0110],\n",
      "        [0.0622],\n",
      "        [0.0522],\n",
      "        [0.0495],\n",
      "        [0.0737],\n",
      "        [0.0801],\n",
      "        [0.1219],\n",
      "        [0.0942],\n",
      "        [0.0412],\n",
      "        [0.1385],\n",
      "        [0.1402],\n",
      "        [0.0516],\n",
      "        [0.0355],\n",
      "        [0.0013],\n",
      "        [0.0658],\n",
      "        [0.1020],\n",
      "        [0.1238],\n",
      "        [0.0539],\n",
      "        [0.1337],\n",
      "        [0.1359],\n",
      "        [0.1429],\n",
      "        [0.1465],\n",
      "        [0.1468],\n",
      "        [0.1721],\n",
      "        [0.1766],\n",
      "        [0.0455],\n",
      "        [0.0218],\n",
      "        [0.0230],\n",
      "        [0.0405],\n",
      "        [0.0962],\n",
      "        [0.1904],\n",
      "        [0.1935],\n",
      "        [0.1962],\n",
      "        [0.1983],\n",
      "        [0.2014],\n",
      "        [0.2058],\n",
      "        [0.2093],\n",
      "        [0.2167],\n",
      "        [0.2206],\n",
      "        [0.2233],\n",
      "        [0.2243],\n",
      "        [0.2307],\n",
      "        [0.2308],\n",
      "        [0.2342],\n",
      "        [0.2459]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0483],\n",
      "        [0.0579],\n",
      "        [0.0865],\n",
      "        [0.0891],\n",
      "        [0.1333],\n",
      "        [0.0977],\n",
      "        [0.0991],\n",
      "        [0.0898],\n",
      "        [0.0373],\n",
      "        [0.0509],\n",
      "        [0.0759],\n",
      "        [0.0480],\n",
      "        [0.0936],\n",
      "        [0.0739],\n",
      "        [0.0940],\n",
      "        [0.0487],\n",
      "        [0.0538],\n",
      "        [0.0644],\n",
      "        [0.0588],\n",
      "        [0.2265],\n",
      "        [0.1734],\n",
      "        [0.1811],\n",
      "        [0.2126],\n",
      "        [0.2191],\n",
      "        [0.0740],\n",
      "        [0.2211],\n",
      "        [0.0877],\n",
      "        [0.0906],\n",
      "        [0.0968],\n",
      "        [0.1603],\n",
      "        [0.1118],\n",
      "        [0.1798],\n",
      "        [0.2444],\n",
      "        [0.1279],\n",
      "        [0.1336],\n",
      "        [0.1370],\n",
      "        [0.1422],\n",
      "        [0.1462],\n",
      "        [0.1503],\n",
      "        [0.1540],\n",
      "        [0.1585],\n",
      "        [0.1644],\n",
      "        [0.2129],\n",
      "        [0.2438],\n",
      "        [0.1882],\n",
      "        [0.1965],\n",
      "        [0.2113],\n",
      "        [0.2117],\n",
      "        [0.2199],\n",
      "        [0.2234],\n",
      "        [0.2244],\n",
      "        [0.2277],\n",
      "        [0.2371],\n",
      "        [0.2416],\n",
      "        [0.1974],\n",
      "        [0.0844],\n",
      "        [0.2038],\n",
      "        [0.0869],\n",
      "        [0.1996],\n",
      "        [0.1060],\n",
      "        [0.1079],\n",
      "        [0.1187],\n",
      "        [0.1277],\n",
      "        [0.1647],\n",
      "        [0.0477],\n",
      "        [0.0800],\n",
      "        [0.0799],\n",
      "        [0.0380],\n",
      "        [0.0610],\n",
      "        [0.0518],\n",
      "        [0.0789],\n",
      "        [0.0839],\n",
      "        [0.1117],\n",
      "        [0.1126],\n",
      "        [0.1251],\n",
      "        [0.1270],\n",
      "        [0.1278],\n",
      "        [0.1390],\n",
      "        [0.1223],\n",
      "        [0.1486],\n",
      "        [0.1340],\n",
      "        [0.1064],\n",
      "        [0.1484],\n",
      "        [0.1649],\n",
      "        [0.1668],\n",
      "        [0.0095],\n",
      "        [0.0762],\n",
      "        [0.0101],\n",
      "        [0.0050],\n",
      "        [0.0590],\n",
      "        [0.0649],\n",
      "        [0.0663],\n",
      "        [0.0836],\n",
      "        [0.0965],\n",
      "        [0.0966],\n",
      "        [0.0971],\n",
      "        [0.1082],\n",
      "        [0.1102],\n",
      "        [0.1141],\n",
      "        [0.1144],\n",
      "        [0.1192],\n",
      "        [0.1196],\n",
      "        [0.1318],\n",
      "        [0.0959],\n",
      "        [0.0093],\n",
      "        [0.0606],\n",
      "        [0.0506],\n",
      "        [0.0478],\n",
      "        [0.0721],\n",
      "        [0.0784],\n",
      "        [0.1203],\n",
      "        [0.0926],\n",
      "        [0.0395],\n",
      "        [0.1369],\n",
      "        [0.1385],\n",
      "        [0.0500],\n",
      "        [0.0338],\n",
      "        [0.0030],\n",
      "        [0.0642],\n",
      "        [0.1003],\n",
      "        [0.1221],\n",
      "        [0.0522],\n",
      "        [0.1320],\n",
      "        [0.1343],\n",
      "        [0.1413],\n",
      "        [0.1449],\n",
      "        [0.1451],\n",
      "        [0.1705],\n",
      "        [0.1749],\n",
      "        [0.0439],\n",
      "        [0.0201],\n",
      "        [0.0214],\n",
      "        [0.0388],\n",
      "        [0.0945],\n",
      "        [0.1887],\n",
      "        [0.1918],\n",
      "        [0.1945],\n",
      "        [0.1966],\n",
      "        [0.1998],\n",
      "        [0.2042],\n",
      "        [0.2077],\n",
      "        [0.2150],\n",
      "        [0.2190],\n",
      "        [0.2216],\n",
      "        [0.2227],\n",
      "        [0.2291],\n",
      "        [0.2292],\n",
      "        [0.2326],\n",
      "        [0.2443]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 32.581926584243774\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 150\n",
      "剩餘X 資料 torch.Size([10, 18])\n",
      "剩餘Y 資料 torch.Size([10, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0677412673830986, 4)\n",
      "The second_loss value of k: (0.07202762365341187, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引4，y= tensor([0.4953])\n",
      "目前模型的Data狀態 torch.Size([150, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556],\n",
      "        [0.7556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0483],\n",
      "        [0.0579],\n",
      "        [0.0865],\n",
      "        [0.0891],\n",
      "        [0.1333],\n",
      "        [0.0977],\n",
      "        [0.0991],\n",
      "        [0.0898],\n",
      "        [0.0373],\n",
      "        [0.0509],\n",
      "        [0.0759],\n",
      "        [0.0480],\n",
      "        [0.0936],\n",
      "        [0.0739],\n",
      "        [0.0940],\n",
      "        [0.0487],\n",
      "        [0.0538],\n",
      "        [0.0644],\n",
      "        [0.0588],\n",
      "        [0.2265],\n",
      "        [0.1734],\n",
      "        [0.1811],\n",
      "        [0.2126],\n",
      "        [0.2191],\n",
      "        [0.0740],\n",
      "        [0.2211],\n",
      "        [0.0877],\n",
      "        [0.0906],\n",
      "        [0.0968],\n",
      "        [0.1603],\n",
      "        [0.1118],\n",
      "        [0.1798],\n",
      "        [0.2444],\n",
      "        [0.1279],\n",
      "        [0.1336],\n",
      "        [0.1370],\n",
      "        [0.1422],\n",
      "        [0.1462],\n",
      "        [0.1503],\n",
      "        [0.1540],\n",
      "        [0.1585],\n",
      "        [0.1644],\n",
      "        [0.2129],\n",
      "        [0.2438],\n",
      "        [0.1882],\n",
      "        [0.1965],\n",
      "        [0.2113],\n",
      "        [0.2117],\n",
      "        [0.2199],\n",
      "        [0.2234],\n",
      "        [0.2244],\n",
      "        [0.2277],\n",
      "        [0.2371],\n",
      "        [0.2416],\n",
      "        [0.1974],\n",
      "        [0.0844],\n",
      "        [0.2038],\n",
      "        [0.0869],\n",
      "        [0.1996],\n",
      "        [0.1060],\n",
      "        [0.1079],\n",
      "        [0.1187],\n",
      "        [0.1277],\n",
      "        [0.1647],\n",
      "        [0.0477],\n",
      "        [0.0800],\n",
      "        [0.0799],\n",
      "        [0.0380],\n",
      "        [0.0610],\n",
      "        [0.0518],\n",
      "        [0.0789],\n",
      "        [0.0839],\n",
      "        [0.1117],\n",
      "        [0.1126],\n",
      "        [0.1251],\n",
      "        [0.1270],\n",
      "        [0.1278],\n",
      "        [0.1390],\n",
      "        [0.1223],\n",
      "        [0.1486],\n",
      "        [0.1340],\n",
      "        [0.1064],\n",
      "        [0.1484],\n",
      "        [0.1649],\n",
      "        [0.1668],\n",
      "        [0.0095],\n",
      "        [0.0762],\n",
      "        [0.0101],\n",
      "        [0.0050],\n",
      "        [0.0590],\n",
      "        [0.0649],\n",
      "        [0.0663],\n",
      "        [0.0836],\n",
      "        [0.0965],\n",
      "        [0.0966],\n",
      "        [0.0971],\n",
      "        [0.1082],\n",
      "        [0.1102],\n",
      "        [0.1141],\n",
      "        [0.1144],\n",
      "        [0.1192],\n",
      "        [0.1196],\n",
      "        [0.1318],\n",
      "        [0.0959],\n",
      "        [0.0093],\n",
      "        [0.0606],\n",
      "        [0.0506],\n",
      "        [0.0478],\n",
      "        [0.0721],\n",
      "        [0.0784],\n",
      "        [0.1203],\n",
      "        [0.0926],\n",
      "        [0.0395],\n",
      "        [0.1369],\n",
      "        [0.1385],\n",
      "        [0.0500],\n",
      "        [0.0338],\n",
      "        [0.0030],\n",
      "        [0.0642],\n",
      "        [0.1003],\n",
      "        [0.1221],\n",
      "        [0.0522],\n",
      "        [0.1320],\n",
      "        [0.1343],\n",
      "        [0.1413],\n",
      "        [0.1449],\n",
      "        [0.1451],\n",
      "        [0.1705],\n",
      "        [0.1749],\n",
      "        [0.0439],\n",
      "        [0.0201],\n",
      "        [0.0214],\n",
      "        [0.0388],\n",
      "        [0.0945],\n",
      "        [0.1887],\n",
      "        [0.1918],\n",
      "        [0.1945],\n",
      "        [0.1966],\n",
      "        [0.1998],\n",
      "        [0.2042],\n",
      "        [0.2077],\n",
      "        [0.2150],\n",
      "        [0.2190],\n",
      "        [0.2216],\n",
      "        [0.2227],\n",
      "        [0.2291],\n",
      "        [0.2292],\n",
      "        [0.2326],\n",
      "        [0.2443],\n",
      "        [0.2603]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0501],\n",
      "        [0.0597],\n",
      "        [0.0883],\n",
      "        [0.0908],\n",
      "        [0.1351],\n",
      "        [0.0995],\n",
      "        [0.1009],\n",
      "        [0.0916],\n",
      "        [0.0390],\n",
      "        [0.0527],\n",
      "        [0.0777],\n",
      "        [0.0498],\n",
      "        [0.0953],\n",
      "        [0.0757],\n",
      "        [0.0957],\n",
      "        [0.0504],\n",
      "        [0.0556],\n",
      "        [0.0661],\n",
      "        [0.0606],\n",
      "        [0.2283],\n",
      "        [0.1751],\n",
      "        [0.1828],\n",
      "        [0.2143],\n",
      "        [0.2209],\n",
      "        [0.0758],\n",
      "        [0.2228],\n",
      "        [0.0894],\n",
      "        [0.0924],\n",
      "        [0.0986],\n",
      "        [0.1620],\n",
      "        [0.1136],\n",
      "        [0.1816],\n",
      "        [0.2461],\n",
      "        [0.1296],\n",
      "        [0.1354],\n",
      "        [0.1387],\n",
      "        [0.1440],\n",
      "        [0.1480],\n",
      "        [0.1521],\n",
      "        [0.1557],\n",
      "        [0.1602],\n",
      "        [0.1661],\n",
      "        [0.2146],\n",
      "        [0.2456],\n",
      "        [0.1899],\n",
      "        [0.1983],\n",
      "        [0.2131],\n",
      "        [0.2135],\n",
      "        [0.2217],\n",
      "        [0.2252],\n",
      "        [0.2262],\n",
      "        [0.2295],\n",
      "        [0.2389],\n",
      "        [0.2434],\n",
      "        [0.1991],\n",
      "        [0.0862],\n",
      "        [0.2055],\n",
      "        [0.0887],\n",
      "        [0.2014],\n",
      "        [0.1078],\n",
      "        [0.1097],\n",
      "        [0.1205],\n",
      "        [0.1295],\n",
      "        [0.1664],\n",
      "        [0.0459],\n",
      "        [0.0818],\n",
      "        [0.0816],\n",
      "        [0.0398],\n",
      "        [0.0628],\n",
      "        [0.0500],\n",
      "        [0.0771],\n",
      "        [0.0821],\n",
      "        [0.1099],\n",
      "        [0.1109],\n",
      "        [0.1233],\n",
      "        [0.1252],\n",
      "        [0.1261],\n",
      "        [0.1373],\n",
      "        [0.1205],\n",
      "        [0.1469],\n",
      "        [0.1323],\n",
      "        [0.1046],\n",
      "        [0.1466],\n",
      "        [0.1632],\n",
      "        [0.1650],\n",
      "        [0.0077],\n",
      "        [0.0744],\n",
      "        [0.0083],\n",
      "        [0.0067],\n",
      "        [0.0573],\n",
      "        [0.0631],\n",
      "        [0.0646],\n",
      "        [0.0818],\n",
      "        [0.0947],\n",
      "        [0.0949],\n",
      "        [0.0954],\n",
      "        [0.1064],\n",
      "        [0.1085],\n",
      "        [0.1123],\n",
      "        [0.1127],\n",
      "        [0.1175],\n",
      "        [0.1179],\n",
      "        [0.1300],\n",
      "        [0.0941],\n",
      "        [0.0076],\n",
      "        [0.0588],\n",
      "        [0.0488],\n",
      "        [0.0461],\n",
      "        [0.0703],\n",
      "        [0.0767],\n",
      "        [0.1185],\n",
      "        [0.0908],\n",
      "        [0.0378],\n",
      "        [0.1351],\n",
      "        [0.1368],\n",
      "        [0.0482],\n",
      "        [0.0321],\n",
      "        [0.0047],\n",
      "        [0.0624],\n",
      "        [0.0985],\n",
      "        [0.1204],\n",
      "        [0.0505],\n",
      "        [0.1303],\n",
      "        [0.1325],\n",
      "        [0.1395],\n",
      "        [0.1431],\n",
      "        [0.1434],\n",
      "        [0.1687],\n",
      "        [0.1732],\n",
      "        [0.0421],\n",
      "        [0.0184],\n",
      "        [0.0196],\n",
      "        [0.0371],\n",
      "        [0.0928],\n",
      "        [0.1870],\n",
      "        [0.1901],\n",
      "        [0.1928],\n",
      "        [0.1949],\n",
      "        [0.1980],\n",
      "        [0.2024],\n",
      "        [0.2059],\n",
      "        [0.2133],\n",
      "        [0.2172],\n",
      "        [0.2199],\n",
      "        [0.2209],\n",
      "        [0.2273],\n",
      "        [0.2274],\n",
      "        [0.2308],\n",
      "        [0.2425],\n",
      "        [0.2585]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 32.85679364204407\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 151\n",
      "剩餘X 資料 torch.Size([9, 18])\n",
      "剩餘Y 資料 torch.Size([9, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.07108872383832932, 5)\n",
      "The second_loss value of k: (0.07834580540657043, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.4872])\n",
      "目前模型的Data狀態 torch.Size([151, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539],\n",
      "        [0.7539]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0501],\n",
      "        [0.0597],\n",
      "        [0.0883],\n",
      "        [0.0908],\n",
      "        [0.1351],\n",
      "        [0.0995],\n",
      "        [0.1009],\n",
      "        [0.0916],\n",
      "        [0.0390],\n",
      "        [0.0527],\n",
      "        [0.0777],\n",
      "        [0.0498],\n",
      "        [0.0953],\n",
      "        [0.0757],\n",
      "        [0.0957],\n",
      "        [0.0504],\n",
      "        [0.0556],\n",
      "        [0.0661],\n",
      "        [0.0606],\n",
      "        [0.2283],\n",
      "        [0.1751],\n",
      "        [0.1828],\n",
      "        [0.2143],\n",
      "        [0.2209],\n",
      "        [0.0758],\n",
      "        [0.2228],\n",
      "        [0.0894],\n",
      "        [0.0924],\n",
      "        [0.0986],\n",
      "        [0.1620],\n",
      "        [0.1136],\n",
      "        [0.1816],\n",
      "        [0.2461],\n",
      "        [0.1296],\n",
      "        [0.1354],\n",
      "        [0.1387],\n",
      "        [0.1440],\n",
      "        [0.1480],\n",
      "        [0.1521],\n",
      "        [0.1557],\n",
      "        [0.1602],\n",
      "        [0.1661],\n",
      "        [0.2146],\n",
      "        [0.2456],\n",
      "        [0.1899],\n",
      "        [0.1983],\n",
      "        [0.2131],\n",
      "        [0.2135],\n",
      "        [0.2217],\n",
      "        [0.2252],\n",
      "        [0.2262],\n",
      "        [0.2295],\n",
      "        [0.2389],\n",
      "        [0.2434],\n",
      "        [0.1991],\n",
      "        [0.0862],\n",
      "        [0.2055],\n",
      "        [0.0887],\n",
      "        [0.2014],\n",
      "        [0.1078],\n",
      "        [0.1097],\n",
      "        [0.1205],\n",
      "        [0.1295],\n",
      "        [0.1664],\n",
      "        [0.0459],\n",
      "        [0.0818],\n",
      "        [0.0816],\n",
      "        [0.0398],\n",
      "        [0.0628],\n",
      "        [0.0500],\n",
      "        [0.0771],\n",
      "        [0.0821],\n",
      "        [0.1099],\n",
      "        [0.1109],\n",
      "        [0.1233],\n",
      "        [0.1252],\n",
      "        [0.1261],\n",
      "        [0.1373],\n",
      "        [0.1205],\n",
      "        [0.1469],\n",
      "        [0.1323],\n",
      "        [0.1046],\n",
      "        [0.1466],\n",
      "        [0.1632],\n",
      "        [0.1650],\n",
      "        [0.0077],\n",
      "        [0.0744],\n",
      "        [0.0083],\n",
      "        [0.0067],\n",
      "        [0.0573],\n",
      "        [0.0631],\n",
      "        [0.0646],\n",
      "        [0.0818],\n",
      "        [0.0947],\n",
      "        [0.0949],\n",
      "        [0.0954],\n",
      "        [0.1064],\n",
      "        [0.1085],\n",
      "        [0.1123],\n",
      "        [0.1127],\n",
      "        [0.1175],\n",
      "        [0.1179],\n",
      "        [0.1300],\n",
      "        [0.0941],\n",
      "        [0.0076],\n",
      "        [0.0588],\n",
      "        [0.0488],\n",
      "        [0.0461],\n",
      "        [0.0703],\n",
      "        [0.0767],\n",
      "        [0.1185],\n",
      "        [0.0908],\n",
      "        [0.0378],\n",
      "        [0.1351],\n",
      "        [0.1368],\n",
      "        [0.0482],\n",
      "        [0.0321],\n",
      "        [0.0047],\n",
      "        [0.0624],\n",
      "        [0.0985],\n",
      "        [0.1204],\n",
      "        [0.0505],\n",
      "        [0.1303],\n",
      "        [0.1325],\n",
      "        [0.1395],\n",
      "        [0.1431],\n",
      "        [0.1434],\n",
      "        [0.1687],\n",
      "        [0.1732],\n",
      "        [0.0421],\n",
      "        [0.0184],\n",
      "        [0.0196],\n",
      "        [0.0371],\n",
      "        [0.0928],\n",
      "        [0.1870],\n",
      "        [0.1901],\n",
      "        [0.1928],\n",
      "        [0.1949],\n",
      "        [0.1980],\n",
      "        [0.2024],\n",
      "        [0.2059],\n",
      "        [0.2133],\n",
      "        [0.2172],\n",
      "        [0.2199],\n",
      "        [0.2209],\n",
      "        [0.2273],\n",
      "        [0.2274],\n",
      "        [0.2308],\n",
      "        [0.2425],\n",
      "        [0.2585],\n",
      "        [0.2666]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0519],\n",
      "        [0.0615],\n",
      "        [0.0901],\n",
      "        [0.0926],\n",
      "        [0.1369],\n",
      "        [0.1013],\n",
      "        [0.1027],\n",
      "        [0.0933],\n",
      "        [0.0408],\n",
      "        [0.0544],\n",
      "        [0.0794],\n",
      "        [0.0516],\n",
      "        [0.0971],\n",
      "        [0.0774],\n",
      "        [0.0975],\n",
      "        [0.0522],\n",
      "        [0.0573],\n",
      "        [0.0679],\n",
      "        [0.0624],\n",
      "        [0.2300],\n",
      "        [0.1769],\n",
      "        [0.1846],\n",
      "        [0.2161],\n",
      "        [0.2226],\n",
      "        [0.0776],\n",
      "        [0.2246],\n",
      "        [0.0912],\n",
      "        [0.0942],\n",
      "        [0.1003],\n",
      "        [0.1638],\n",
      "        [0.1153],\n",
      "        [0.1834],\n",
      "        [0.2479],\n",
      "        [0.1314],\n",
      "        [0.1371],\n",
      "        [0.1405],\n",
      "        [0.1458],\n",
      "        [0.1497],\n",
      "        [0.1538],\n",
      "        [0.1575],\n",
      "        [0.1620],\n",
      "        [0.1679],\n",
      "        [0.2164],\n",
      "        [0.2473],\n",
      "        [0.1917],\n",
      "        [0.2000],\n",
      "        [0.2148],\n",
      "        [0.2152],\n",
      "        [0.2234],\n",
      "        [0.2270],\n",
      "        [0.2279],\n",
      "        [0.2313],\n",
      "        [0.2407],\n",
      "        [0.2452],\n",
      "        [0.2009],\n",
      "        [0.0879],\n",
      "        [0.2073],\n",
      "        [0.0904],\n",
      "        [0.2032],\n",
      "        [0.1096],\n",
      "        [0.1114],\n",
      "        [0.1223],\n",
      "        [0.1312],\n",
      "        [0.1682],\n",
      "        [0.0442],\n",
      "        [0.0836],\n",
      "        [0.0834],\n",
      "        [0.0415],\n",
      "        [0.0646],\n",
      "        [0.0483],\n",
      "        [0.0754],\n",
      "        [0.0804],\n",
      "        [0.1081],\n",
      "        [0.1091],\n",
      "        [0.1215],\n",
      "        [0.1235],\n",
      "        [0.1243],\n",
      "        [0.1355],\n",
      "        [0.1188],\n",
      "        [0.1451],\n",
      "        [0.1305],\n",
      "        [0.1029],\n",
      "        [0.1448],\n",
      "        [0.1614],\n",
      "        [0.1633],\n",
      "        [0.0060],\n",
      "        [0.0727],\n",
      "        [0.0066],\n",
      "        [0.0085],\n",
      "        [0.0555],\n",
      "        [0.0614],\n",
      "        [0.0628],\n",
      "        [0.0801],\n",
      "        [0.0929],\n",
      "        [0.0931],\n",
      "        [0.0936],\n",
      "        [0.1047],\n",
      "        [0.1067],\n",
      "        [0.1105],\n",
      "        [0.1109],\n",
      "        [0.1157],\n",
      "        [0.1161],\n",
      "        [0.1282],\n",
      "        [0.0924],\n",
      "        [0.0058],\n",
      "        [0.0571],\n",
      "        [0.0471],\n",
      "        [0.0443],\n",
      "        [0.0685],\n",
      "        [0.0749],\n",
      "        [0.1168],\n",
      "        [0.0891],\n",
      "        [0.0360],\n",
      "        [0.1334],\n",
      "        [0.1350],\n",
      "        [0.0465],\n",
      "        [0.0303],\n",
      "        [0.0065],\n",
      "        [0.0607],\n",
      "        [0.0968],\n",
      "        [0.1186],\n",
      "        [0.0487],\n",
      "        [0.1285],\n",
      "        [0.1307],\n",
      "        [0.1378],\n",
      "        [0.1413],\n",
      "        [0.1416],\n",
      "        [0.1670],\n",
      "        [0.1714],\n",
      "        [0.0403],\n",
      "        [0.0166],\n",
      "        [0.0179],\n",
      "        [0.0353],\n",
      "        [0.0910],\n",
      "        [0.1852],\n",
      "        [0.1883],\n",
      "        [0.1910],\n",
      "        [0.1931],\n",
      "        [0.1963],\n",
      "        [0.2007],\n",
      "        [0.2041],\n",
      "        [0.2115],\n",
      "        [0.2155],\n",
      "        [0.2181],\n",
      "        [0.2192],\n",
      "        [0.2256],\n",
      "        [0.2257],\n",
      "        [0.2291],\n",
      "        [0.2407],\n",
      "        [0.2568],\n",
      "        [0.2649]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 33.136714696884155\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 152\n",
      "剩餘X 資料 torch.Size([8, 18])\n",
      "剩餘Y 資料 torch.Size([8, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.07736168056726456, 5)\n",
      "The second_loss value of k: (0.07849886268377304, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.4740])\n",
      "目前模型的Data狀態 torch.Size([152, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521],\n",
      "        [0.7521]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0519],\n",
      "        [0.0615],\n",
      "        [0.0901],\n",
      "        [0.0926],\n",
      "        [0.1369],\n",
      "        [0.1013],\n",
      "        [0.1027],\n",
      "        [0.0933],\n",
      "        [0.0408],\n",
      "        [0.0544],\n",
      "        [0.0794],\n",
      "        [0.0516],\n",
      "        [0.0971],\n",
      "        [0.0774],\n",
      "        [0.0975],\n",
      "        [0.0522],\n",
      "        [0.0573],\n",
      "        [0.0679],\n",
      "        [0.0624],\n",
      "        [0.2300],\n",
      "        [0.1769],\n",
      "        [0.1846],\n",
      "        [0.2161],\n",
      "        [0.2226],\n",
      "        [0.0776],\n",
      "        [0.2246],\n",
      "        [0.0912],\n",
      "        [0.0942],\n",
      "        [0.1003],\n",
      "        [0.1638],\n",
      "        [0.1153],\n",
      "        [0.1834],\n",
      "        [0.2479],\n",
      "        [0.1314],\n",
      "        [0.1371],\n",
      "        [0.1405],\n",
      "        [0.1458],\n",
      "        [0.1497],\n",
      "        [0.1538],\n",
      "        [0.1575],\n",
      "        [0.1620],\n",
      "        [0.1679],\n",
      "        [0.2164],\n",
      "        [0.2473],\n",
      "        [0.1917],\n",
      "        [0.2000],\n",
      "        [0.2148],\n",
      "        [0.2152],\n",
      "        [0.2234],\n",
      "        [0.2270],\n",
      "        [0.2279],\n",
      "        [0.2313],\n",
      "        [0.2407],\n",
      "        [0.2452],\n",
      "        [0.2009],\n",
      "        [0.0879],\n",
      "        [0.2073],\n",
      "        [0.0904],\n",
      "        [0.2032],\n",
      "        [0.1096],\n",
      "        [0.1114],\n",
      "        [0.1223],\n",
      "        [0.1312],\n",
      "        [0.1682],\n",
      "        [0.0442],\n",
      "        [0.0836],\n",
      "        [0.0834],\n",
      "        [0.0415],\n",
      "        [0.0646],\n",
      "        [0.0483],\n",
      "        [0.0754],\n",
      "        [0.0804],\n",
      "        [0.1081],\n",
      "        [0.1091],\n",
      "        [0.1215],\n",
      "        [0.1235],\n",
      "        [0.1243],\n",
      "        [0.1355],\n",
      "        [0.1188],\n",
      "        [0.1451],\n",
      "        [0.1305],\n",
      "        [0.1029],\n",
      "        [0.1448],\n",
      "        [0.1614],\n",
      "        [0.1633],\n",
      "        [0.0060],\n",
      "        [0.0727],\n",
      "        [0.0066],\n",
      "        [0.0085],\n",
      "        [0.0555],\n",
      "        [0.0614],\n",
      "        [0.0628],\n",
      "        [0.0801],\n",
      "        [0.0929],\n",
      "        [0.0931],\n",
      "        [0.0936],\n",
      "        [0.1047],\n",
      "        [0.1067],\n",
      "        [0.1105],\n",
      "        [0.1109],\n",
      "        [0.1157],\n",
      "        [0.1161],\n",
      "        [0.1282],\n",
      "        [0.0924],\n",
      "        [0.0058],\n",
      "        [0.0571],\n",
      "        [0.0471],\n",
      "        [0.0443],\n",
      "        [0.0685],\n",
      "        [0.0749],\n",
      "        [0.1168],\n",
      "        [0.0891],\n",
      "        [0.0360],\n",
      "        [0.1334],\n",
      "        [0.1350],\n",
      "        [0.0465],\n",
      "        [0.0303],\n",
      "        [0.0065],\n",
      "        [0.0607],\n",
      "        [0.0968],\n",
      "        [0.1186],\n",
      "        [0.0487],\n",
      "        [0.1285],\n",
      "        [0.1307],\n",
      "        [0.1378],\n",
      "        [0.1413],\n",
      "        [0.1416],\n",
      "        [0.1670],\n",
      "        [0.1714],\n",
      "        [0.0403],\n",
      "        [0.0166],\n",
      "        [0.0179],\n",
      "        [0.0353],\n",
      "        [0.0910],\n",
      "        [0.1852],\n",
      "        [0.1883],\n",
      "        [0.1910],\n",
      "        [0.1931],\n",
      "        [0.1963],\n",
      "        [0.2007],\n",
      "        [0.2041],\n",
      "        [0.2115],\n",
      "        [0.2155],\n",
      "        [0.2181],\n",
      "        [0.2192],\n",
      "        [0.2256],\n",
      "        [0.2257],\n",
      "        [0.2291],\n",
      "        [0.2407],\n",
      "        [0.2568],\n",
      "        [0.2649],\n",
      "        [0.2781]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0537],\n",
      "        [0.0633],\n",
      "        [0.0919],\n",
      "        [0.0944],\n",
      "        [0.1387],\n",
      "        [0.1031],\n",
      "        [0.1045],\n",
      "        [0.0952],\n",
      "        [0.0426],\n",
      "        [0.0563],\n",
      "        [0.0813],\n",
      "        [0.0534],\n",
      "        [0.0989],\n",
      "        [0.0793],\n",
      "        [0.0993],\n",
      "        [0.0540],\n",
      "        [0.0592],\n",
      "        [0.0697],\n",
      "        [0.0642],\n",
      "        [0.2318],\n",
      "        [0.1787],\n",
      "        [0.1864],\n",
      "        [0.2179],\n",
      "        [0.2245],\n",
      "        [0.0794],\n",
      "        [0.2264],\n",
      "        [0.0930],\n",
      "        [0.0960],\n",
      "        [0.1021],\n",
      "        [0.1656],\n",
      "        [0.1171],\n",
      "        [0.1852],\n",
      "        [0.2497],\n",
      "        [0.1332],\n",
      "        [0.1390],\n",
      "        [0.1423],\n",
      "        [0.1476],\n",
      "        [0.1516],\n",
      "        [0.1556],\n",
      "        [0.1593],\n",
      "        [0.1638],\n",
      "        [0.1697],\n",
      "        [0.2182],\n",
      "        [0.2491],\n",
      "        [0.1935],\n",
      "        [0.2019],\n",
      "        [0.2167],\n",
      "        [0.2171],\n",
      "        [0.2252],\n",
      "        [0.2288],\n",
      "        [0.2297],\n",
      "        [0.2331],\n",
      "        [0.2425],\n",
      "        [0.2470],\n",
      "        [0.2027],\n",
      "        [0.0898],\n",
      "        [0.2091],\n",
      "        [0.0923],\n",
      "        [0.2050],\n",
      "        [0.1114],\n",
      "        [0.1133],\n",
      "        [0.1241],\n",
      "        [0.1330],\n",
      "        [0.1700],\n",
      "        [0.0424],\n",
      "        [0.0854],\n",
      "        [0.0852],\n",
      "        [0.0433],\n",
      "        [0.0664],\n",
      "        [0.0464],\n",
      "        [0.0735],\n",
      "        [0.0786],\n",
      "        [0.1063],\n",
      "        [0.1073],\n",
      "        [0.1197],\n",
      "        [0.1216],\n",
      "        [0.1225],\n",
      "        [0.1337],\n",
      "        [0.1169],\n",
      "        [0.1433],\n",
      "        [0.1287],\n",
      "        [0.1010],\n",
      "        [0.1430],\n",
      "        [0.1596],\n",
      "        [0.1614],\n",
      "        [0.0041],\n",
      "        [0.0708],\n",
      "        [0.0048],\n",
      "        [0.0103],\n",
      "        [0.0537],\n",
      "        [0.0596],\n",
      "        [0.0610],\n",
      "        [0.0782],\n",
      "        [0.0911],\n",
      "        [0.0913],\n",
      "        [0.0918],\n",
      "        [0.1028],\n",
      "        [0.1049],\n",
      "        [0.1087],\n",
      "        [0.1091],\n",
      "        [0.1139],\n",
      "        [0.1143],\n",
      "        [0.1264],\n",
      "        [0.0905],\n",
      "        [0.0040],\n",
      "        [0.0553],\n",
      "        [0.0453],\n",
      "        [0.0425],\n",
      "        [0.0667],\n",
      "        [0.0731],\n",
      "        [0.1149],\n",
      "        [0.0873],\n",
      "        [0.0342],\n",
      "        [0.1316],\n",
      "        [0.1332],\n",
      "        [0.0446],\n",
      "        [0.0285],\n",
      "        [0.0083],\n",
      "        [0.0589],\n",
      "        [0.0950],\n",
      "        [0.1168],\n",
      "        [0.0469],\n",
      "        [0.1267],\n",
      "        [0.1289],\n",
      "        [0.1359],\n",
      "        [0.1395],\n",
      "        [0.1398],\n",
      "        [0.1652],\n",
      "        [0.1696],\n",
      "        [0.0385],\n",
      "        [0.0148],\n",
      "        [0.0160],\n",
      "        [0.0335],\n",
      "        [0.0892],\n",
      "        [0.1834],\n",
      "        [0.1865],\n",
      "        [0.1892],\n",
      "        [0.1913],\n",
      "        [0.1944],\n",
      "        [0.1988],\n",
      "        [0.2023],\n",
      "        [0.2097],\n",
      "        [0.2137],\n",
      "        [0.2163],\n",
      "        [0.2173],\n",
      "        [0.2238],\n",
      "        [0.2238],\n",
      "        [0.2272],\n",
      "        [0.2389],\n",
      "        [0.2549],\n",
      "        [0.2630],\n",
      "        [0.2763]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 33.36977434158325\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 153\n",
      "剩餘X 資料 torch.Size([7, 18])\n",
      "剩餘Y 資料 torch.Size([7, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.07748520374298096, 5)\n",
      "The second_loss value of k: (0.07803133130073547, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.4719])\n",
      "目前模型的Data狀態 torch.Size([153, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503],\n",
      "        [0.7503]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0537],\n",
      "        [0.0633],\n",
      "        [0.0919],\n",
      "        [0.0944],\n",
      "        [0.1387],\n",
      "        [0.1031],\n",
      "        [0.1045],\n",
      "        [0.0952],\n",
      "        [0.0426],\n",
      "        [0.0563],\n",
      "        [0.0813],\n",
      "        [0.0534],\n",
      "        [0.0989],\n",
      "        [0.0793],\n",
      "        [0.0993],\n",
      "        [0.0540],\n",
      "        [0.0592],\n",
      "        [0.0697],\n",
      "        [0.0642],\n",
      "        [0.2318],\n",
      "        [0.1787],\n",
      "        [0.1864],\n",
      "        [0.2179],\n",
      "        [0.2245],\n",
      "        [0.0794],\n",
      "        [0.2264],\n",
      "        [0.0930],\n",
      "        [0.0960],\n",
      "        [0.1021],\n",
      "        [0.1656],\n",
      "        [0.1171],\n",
      "        [0.1852],\n",
      "        [0.2497],\n",
      "        [0.1332],\n",
      "        [0.1390],\n",
      "        [0.1423],\n",
      "        [0.1476],\n",
      "        [0.1516],\n",
      "        [0.1556],\n",
      "        [0.1593],\n",
      "        [0.1638],\n",
      "        [0.1697],\n",
      "        [0.2182],\n",
      "        [0.2491],\n",
      "        [0.1935],\n",
      "        [0.2019],\n",
      "        [0.2167],\n",
      "        [0.2171],\n",
      "        [0.2252],\n",
      "        [0.2288],\n",
      "        [0.2297],\n",
      "        [0.2331],\n",
      "        [0.2425],\n",
      "        [0.2470],\n",
      "        [0.2027],\n",
      "        [0.0898],\n",
      "        [0.2091],\n",
      "        [0.0923],\n",
      "        [0.2050],\n",
      "        [0.1114],\n",
      "        [0.1133],\n",
      "        [0.1241],\n",
      "        [0.1330],\n",
      "        [0.1700],\n",
      "        [0.0424],\n",
      "        [0.0854],\n",
      "        [0.0852],\n",
      "        [0.0433],\n",
      "        [0.0664],\n",
      "        [0.0464],\n",
      "        [0.0735],\n",
      "        [0.0786],\n",
      "        [0.1063],\n",
      "        [0.1073],\n",
      "        [0.1197],\n",
      "        [0.1216],\n",
      "        [0.1225],\n",
      "        [0.1337],\n",
      "        [0.1169],\n",
      "        [0.1433],\n",
      "        [0.1287],\n",
      "        [0.1010],\n",
      "        [0.1430],\n",
      "        [0.1596],\n",
      "        [0.1614],\n",
      "        [0.0041],\n",
      "        [0.0708],\n",
      "        [0.0048],\n",
      "        [0.0103],\n",
      "        [0.0537],\n",
      "        [0.0596],\n",
      "        [0.0610],\n",
      "        [0.0782],\n",
      "        [0.0911],\n",
      "        [0.0913],\n",
      "        [0.0918],\n",
      "        [0.1028],\n",
      "        [0.1049],\n",
      "        [0.1087],\n",
      "        [0.1091],\n",
      "        [0.1139],\n",
      "        [0.1143],\n",
      "        [0.1264],\n",
      "        [0.0905],\n",
      "        [0.0040],\n",
      "        [0.0553],\n",
      "        [0.0453],\n",
      "        [0.0425],\n",
      "        [0.0667],\n",
      "        [0.0731],\n",
      "        [0.1149],\n",
      "        [0.0873],\n",
      "        [0.0342],\n",
      "        [0.1316],\n",
      "        [0.1332],\n",
      "        [0.0446],\n",
      "        [0.0285],\n",
      "        [0.0083],\n",
      "        [0.0589],\n",
      "        [0.0950],\n",
      "        [0.1168],\n",
      "        [0.0469],\n",
      "        [0.1267],\n",
      "        [0.1289],\n",
      "        [0.1359],\n",
      "        [0.1395],\n",
      "        [0.1398],\n",
      "        [0.1652],\n",
      "        [0.1696],\n",
      "        [0.0385],\n",
      "        [0.0148],\n",
      "        [0.0160],\n",
      "        [0.0335],\n",
      "        [0.0892],\n",
      "        [0.1834],\n",
      "        [0.1865],\n",
      "        [0.1892],\n",
      "        [0.1913],\n",
      "        [0.1944],\n",
      "        [0.1988],\n",
      "        [0.2023],\n",
      "        [0.2097],\n",
      "        [0.2137],\n",
      "        [0.2163],\n",
      "        [0.2173],\n",
      "        [0.2238],\n",
      "        [0.2238],\n",
      "        [0.2272],\n",
      "        [0.2389],\n",
      "        [0.2549],\n",
      "        [0.2630],\n",
      "        [0.2763],\n",
      "        [0.2784]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0555],\n",
      "        [0.0651],\n",
      "        [0.0937],\n",
      "        [0.0962],\n",
      "        [0.1405],\n",
      "        [0.1049],\n",
      "        [0.1063],\n",
      "        [0.0970],\n",
      "        [0.0444],\n",
      "        [0.0581],\n",
      "        [0.0831],\n",
      "        [0.0552],\n",
      "        [0.1007],\n",
      "        [0.0811],\n",
      "        [0.1011],\n",
      "        [0.0558],\n",
      "        [0.0610],\n",
      "        [0.0715],\n",
      "        [0.0660],\n",
      "        [0.2337],\n",
      "        [0.1805],\n",
      "        [0.1882],\n",
      "        [0.2197],\n",
      "        [0.2263],\n",
      "        [0.0812],\n",
      "        [0.2282],\n",
      "        [0.0948],\n",
      "        [0.0978],\n",
      "        [0.1040],\n",
      "        [0.1674],\n",
      "        [0.1190],\n",
      "        [0.1870],\n",
      "        [0.2515],\n",
      "        [0.1350],\n",
      "        [0.1408],\n",
      "        [0.1441],\n",
      "        [0.1494],\n",
      "        [0.1534],\n",
      "        [0.1575],\n",
      "        [0.1611],\n",
      "        [0.1656],\n",
      "        [0.1715],\n",
      "        [0.2200],\n",
      "        [0.2510],\n",
      "        [0.1953],\n",
      "        [0.2037],\n",
      "        [0.2185],\n",
      "        [0.2189],\n",
      "        [0.2271],\n",
      "        [0.2306],\n",
      "        [0.2316],\n",
      "        [0.2349],\n",
      "        [0.2443],\n",
      "        [0.2488],\n",
      "        [0.2045],\n",
      "        [0.0916],\n",
      "        [0.2109],\n",
      "        [0.0941],\n",
      "        [0.2068],\n",
      "        [0.1132],\n",
      "        [0.1151],\n",
      "        [0.1259],\n",
      "        [0.1349],\n",
      "        [0.1718],\n",
      "        [0.0405],\n",
      "        [0.0872],\n",
      "        [0.0870],\n",
      "        [0.0452],\n",
      "        [0.0682],\n",
      "        [0.0446],\n",
      "        [0.0717],\n",
      "        [0.0767],\n",
      "        [0.1045],\n",
      "        [0.1055],\n",
      "        [0.1179],\n",
      "        [0.1198],\n",
      "        [0.1207],\n",
      "        [0.1319],\n",
      "        [0.1151],\n",
      "        [0.1415],\n",
      "        [0.1269],\n",
      "        [0.0992],\n",
      "        [0.1412],\n",
      "        [0.1578],\n",
      "        [0.1596],\n",
      "        [0.0023],\n",
      "        [0.0690],\n",
      "        [0.0029],\n",
      "        [0.0121],\n",
      "        [0.0519],\n",
      "        [0.0577],\n",
      "        [0.0592],\n",
      "        [0.0764],\n",
      "        [0.0893],\n",
      "        [0.0895],\n",
      "        [0.0900],\n",
      "        [0.1010],\n",
      "        [0.1031],\n",
      "        [0.1069],\n",
      "        [0.1073],\n",
      "        [0.1121],\n",
      "        [0.1125],\n",
      "        [0.1246],\n",
      "        [0.0887],\n",
      "        [0.0022],\n",
      "        [0.0534],\n",
      "        [0.0434],\n",
      "        [0.0407],\n",
      "        [0.0649],\n",
      "        [0.0713],\n",
      "        [0.1131],\n",
      "        [0.0854],\n",
      "        [0.0324],\n",
      "        [0.1297],\n",
      "        [0.1314],\n",
      "        [0.0428],\n",
      "        [0.0267],\n",
      "        [0.0101],\n",
      "        [0.0570],\n",
      "        [0.0932],\n",
      "        [0.1150],\n",
      "        [0.0451],\n",
      "        [0.1249],\n",
      "        [0.1271],\n",
      "        [0.1341],\n",
      "        [0.1377],\n",
      "        [0.1380],\n",
      "        [0.1633],\n",
      "        [0.1678],\n",
      "        [0.0367],\n",
      "        [0.0130],\n",
      "        [0.0142],\n",
      "        [0.0317],\n",
      "        [0.0874],\n",
      "        [0.1816],\n",
      "        [0.1847],\n",
      "        [0.1874],\n",
      "        [0.1895],\n",
      "        [0.1926],\n",
      "        [0.1970],\n",
      "        [0.2005],\n",
      "        [0.2079],\n",
      "        [0.2118],\n",
      "        [0.2145],\n",
      "        [0.2155],\n",
      "        [0.2219],\n",
      "        [0.2220],\n",
      "        [0.2254],\n",
      "        [0.2371],\n",
      "        [0.2531],\n",
      "        [0.2612],\n",
      "        [0.2745],\n",
      "        [0.2765]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 33.60413455963135\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 1 個區塊累積花費時間(s) 33.60353183746338\n",
      "<<The performance of 1 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 33.60353183746338\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 3269.87\n",
      "The MAPE for l = 1: 0.06%\n",
      "The RMSE for l = 1: 3703.16\n",
      "The accuracy(2000) for l = 1: 25.49%\n",
      "The accuracy(3000) for l = 1: 50.33%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in toutlier>>\n",
      "The MAE for l = 1: 7629.46\n",
      "The MAPE for l = 1: 0.17%\n",
      "The RMSE for l = 1: 7646.41\n",
      "The accuracy(2000) for l = 1: 0.00%\n",
      "The accuracy(3000) for l = 1: 0.00%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 10332.6\n",
      "The MAPE for l = 1: 0.2%\n",
      "The RMSE for l = 1: 10438.0\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 0.0%\n",
      "------------------------------------------------------------\n",
      "0.2549019607843137\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<2>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.662565515900496e-06, 49)\n",
      "The second_loss value of k: (5.363846867112443e-06, 46)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [49, 46, 47, 50, 48, 61, 62, 51, 60, 67, 63, 19, 68, 66, 69, 43, 64, 41, 80, 70, 1, 5, 65, 81, 82, 6, 71, 8, 42, 79, 72, 7, 106, 83, 107, 3, 21, 0, 52, 40, 39, 59, 73, 77, 78, 45, 9, 57, 10, 20, 22, 44, 2, 75, 4, 85, 23, 114, 108, 84, 76, 86, 87, 74, 37, 56, 11, 110, 105, 27, 111, 115, 99, 58, 38, 109, 54, 101, 100, 113, 55, 36, 26, 102, 97, 24, 116, 112, 28, 25, 18, 30, 117, 88, 29, 98, 104, 31, 35, 89, 103, 32, 119, 118, 120, 96, 121, 17, 12, 34, 123, 33, 124, 125, 126, 16, 15, 14, 95, 122, 127, 94, 13, 93, 129, 130, 131]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0022],\n",
      "        [0.0023],\n",
      "        [0.0029],\n",
      "        [0.0101],\n",
      "        [0.0121],\n",
      "        [0.0130],\n",
      "        [0.0142],\n",
      "        [0.0267],\n",
      "        [0.0317],\n",
      "        [0.0324],\n",
      "        [0.0367],\n",
      "        [0.0405],\n",
      "        [0.0407],\n",
      "        [0.0428],\n",
      "        [0.0434],\n",
      "        [0.0446],\n",
      "        [0.0451],\n",
      "        [0.0452],\n",
      "        [0.0519],\n",
      "        [0.0534],\n",
      "        [0.0552],\n",
      "        [0.0558],\n",
      "        [0.0570],\n",
      "        [0.0577],\n",
      "        [0.0592],\n",
      "        [0.0610],\n",
      "        [0.0649],\n",
      "        [0.0660],\n",
      "        [0.0682],\n",
      "        [0.0690],\n",
      "        [0.0713],\n",
      "        [0.0715],\n",
      "        [0.0717],\n",
      "        [0.0764],\n",
      "        [0.0767],\n",
      "        [0.0811],\n",
      "        [0.0812],\n",
      "        [0.0831],\n",
      "        [0.0854],\n",
      "        [0.0870],\n",
      "        [0.0872],\n",
      "        [0.0874],\n",
      "        [0.0887],\n",
      "        [0.0893],\n",
      "        [0.0895],\n",
      "        [0.0900],\n",
      "        [0.0916],\n",
      "        [0.0932],\n",
      "        [0.0941],\n",
      "        [0.0948],\n",
      "        [0.0978],\n",
      "        [0.0992],\n",
      "        [0.1007],\n",
      "        [0.1010],\n",
      "        [0.1011],\n",
      "        [0.1031],\n",
      "        [0.1040],\n",
      "        [0.1045],\n",
      "        [0.1055],\n",
      "        [0.1069],\n",
      "        [0.1073],\n",
      "        [0.1121],\n",
      "        [0.1125],\n",
      "        [0.1131],\n",
      "        [0.1132],\n",
      "        [0.1150],\n",
      "        [0.1151],\n",
      "        [0.1151],\n",
      "        [0.1179],\n",
      "        [0.1190],\n",
      "        [0.1198],\n",
      "        [0.1207],\n",
      "        [0.1246],\n",
      "        [0.1249],\n",
      "        [0.1259],\n",
      "        [0.1269],\n",
      "        [0.1271],\n",
      "        [0.1297],\n",
      "        [0.1314],\n",
      "        [0.1319],\n",
      "        [0.1341],\n",
      "        [0.1349],\n",
      "        [0.1350],\n",
      "        [0.1377],\n",
      "        [0.1380],\n",
      "        [0.1408],\n",
      "        [0.1412],\n",
      "        [0.1415],\n",
      "        [0.1441],\n",
      "        [0.1494],\n",
      "        [0.1534],\n",
      "        [0.1575],\n",
      "        [0.1578],\n",
      "        [0.1596],\n",
      "        [0.1611],\n",
      "        [0.1633],\n",
      "        [0.1678],\n",
      "        [0.1715],\n",
      "        [0.1718],\n",
      "        [0.1816],\n",
      "        [0.1847],\n",
      "        [0.1870],\n",
      "        [0.1874],\n",
      "        [0.1895],\n",
      "        [0.1926],\n",
      "        [0.1970],\n",
      "        [0.2005],\n",
      "        [0.2037],\n",
      "        [0.2045],\n",
      "        [0.2068],\n",
      "        [0.2079],\n",
      "        [0.2109],\n",
      "        [0.2118],\n",
      "        [0.2145],\n",
      "        [0.2155],\n",
      "        [0.2185],\n",
      "        [0.2197],\n",
      "        [0.2200],\n",
      "        [0.2219],\n",
      "        [0.2220],\n",
      "        [0.2254],\n",
      "        [0.2371],\n",
      "        [0.2515],\n",
      "        [0.2531],\n",
      "        [0.2612],\n",
      "        [0.2745],\n",
      "        [0.2765]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 128\n",
      "剩餘X 資料 torch.Size([32, 18])\n",
      "剩餘Y 資料 torch.Size([32, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.07701826095581055, 4)\n",
      "The second_loss value of k: (0.08060277998447418, 5)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引4，y= tensor([0.4709])\n",
      "目前模型的Data狀態 torch.Size([128, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485],\n",
      "        [0.7485]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0022],\n",
      "        [0.0023],\n",
      "        [0.0029],\n",
      "        [0.0101],\n",
      "        [0.0121],\n",
      "        [0.0130],\n",
      "        [0.0142],\n",
      "        [0.0267],\n",
      "        [0.0317],\n",
      "        [0.0324],\n",
      "        [0.0367],\n",
      "        [0.0405],\n",
      "        [0.0407],\n",
      "        [0.0428],\n",
      "        [0.0434],\n",
      "        [0.0446],\n",
      "        [0.0451],\n",
      "        [0.0452],\n",
      "        [0.0519],\n",
      "        [0.0534],\n",
      "        [0.0552],\n",
      "        [0.0558],\n",
      "        [0.0570],\n",
      "        [0.0577],\n",
      "        [0.0592],\n",
      "        [0.0610],\n",
      "        [0.0649],\n",
      "        [0.0660],\n",
      "        [0.0682],\n",
      "        [0.0690],\n",
      "        [0.0713],\n",
      "        [0.0715],\n",
      "        [0.0717],\n",
      "        [0.0764],\n",
      "        [0.0767],\n",
      "        [0.0811],\n",
      "        [0.0812],\n",
      "        [0.0831],\n",
      "        [0.0854],\n",
      "        [0.0870],\n",
      "        [0.0872],\n",
      "        [0.0874],\n",
      "        [0.0887],\n",
      "        [0.0893],\n",
      "        [0.0895],\n",
      "        [0.0900],\n",
      "        [0.0916],\n",
      "        [0.0932],\n",
      "        [0.0941],\n",
      "        [0.0948],\n",
      "        [0.0978],\n",
      "        [0.0992],\n",
      "        [0.1007],\n",
      "        [0.1010],\n",
      "        [0.1011],\n",
      "        [0.1031],\n",
      "        [0.1040],\n",
      "        [0.1045],\n",
      "        [0.1055],\n",
      "        [0.1069],\n",
      "        [0.1073],\n",
      "        [0.1121],\n",
      "        [0.1125],\n",
      "        [0.1131],\n",
      "        [0.1132],\n",
      "        [0.1150],\n",
      "        [0.1151],\n",
      "        [0.1151],\n",
      "        [0.1179],\n",
      "        [0.1190],\n",
      "        [0.1198],\n",
      "        [0.1207],\n",
      "        [0.1246],\n",
      "        [0.1249],\n",
      "        [0.1259],\n",
      "        [0.1269],\n",
      "        [0.1271],\n",
      "        [0.1297],\n",
      "        [0.1314],\n",
      "        [0.1319],\n",
      "        [0.1341],\n",
      "        [0.1349],\n",
      "        [0.1350],\n",
      "        [0.1377],\n",
      "        [0.1380],\n",
      "        [0.1408],\n",
      "        [0.1412],\n",
      "        [0.1415],\n",
      "        [0.1441],\n",
      "        [0.1494],\n",
      "        [0.1534],\n",
      "        [0.1575],\n",
      "        [0.1578],\n",
      "        [0.1596],\n",
      "        [0.1611],\n",
      "        [0.1633],\n",
      "        [0.1678],\n",
      "        [0.1715],\n",
      "        [0.1718],\n",
      "        [0.1816],\n",
      "        [0.1847],\n",
      "        [0.1870],\n",
      "        [0.1874],\n",
      "        [0.1895],\n",
      "        [0.1926],\n",
      "        [0.1970],\n",
      "        [0.2005],\n",
      "        [0.2037],\n",
      "        [0.2045],\n",
      "        [0.2068],\n",
      "        [0.2079],\n",
      "        [0.2109],\n",
      "        [0.2118],\n",
      "        [0.2145],\n",
      "        [0.2155],\n",
      "        [0.2185],\n",
      "        [0.2197],\n",
      "        [0.2200],\n",
      "        [0.2219],\n",
      "        [0.2220],\n",
      "        [0.2254],\n",
      "        [0.2371],\n",
      "        [0.2515],\n",
      "        [0.2531],\n",
      "        [0.2612],\n",
      "        [0.2745],\n",
      "        [0.2765],\n",
      "        [0.2775]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0339],\n",
      "        [0.0337],\n",
      "        [0.0331],\n",
      "        [0.0462],\n",
      "        [0.0482],\n",
      "        [0.0231],\n",
      "        [0.0218],\n",
      "        [0.0094],\n",
      "        [0.0043],\n",
      "        [0.0037],\n",
      "        [0.0007],\n",
      "        [0.0045],\n",
      "        [0.0047],\n",
      "        [0.0068],\n",
      "        [0.0074],\n",
      "        [0.0086],\n",
      "        [0.0091],\n",
      "        [0.0812],\n",
      "        [0.0158],\n",
      "        [0.0174],\n",
      "        [0.0912],\n",
      "        [0.0918],\n",
      "        [0.0210],\n",
      "        [0.0217],\n",
      "        [0.0232],\n",
      "        [0.0970],\n",
      "        [0.0289],\n",
      "        [0.1020],\n",
      "        [0.1042],\n",
      "        [0.0330],\n",
      "        [0.0353],\n",
      "        [0.1075],\n",
      "        [0.0357],\n",
      "        [0.0404],\n",
      "        [0.0407],\n",
      "        [0.1171],\n",
      "        [0.1172],\n",
      "        [0.1191],\n",
      "        [0.0494],\n",
      "        [0.1231],\n",
      "        [0.1232],\n",
      "        [0.0514],\n",
      "        [0.0527],\n",
      "        [0.0533],\n",
      "        [0.0534],\n",
      "        [0.0539],\n",
      "        [0.1276],\n",
      "        [0.0571],\n",
      "        [0.1301],\n",
      "        [0.1309],\n",
      "        [0.1338],\n",
      "        [0.0632],\n",
      "        [0.1368],\n",
      "        [0.0650],\n",
      "        [0.1372],\n",
      "        [0.0670],\n",
      "        [0.1400],\n",
      "        [0.0685],\n",
      "        [0.0695],\n",
      "        [0.0709],\n",
      "        [0.0713],\n",
      "        [0.0760],\n",
      "        [0.0764],\n",
      "        [0.0771],\n",
      "        [0.1492],\n",
      "        [0.0789],\n",
      "        [0.1511],\n",
      "        [0.0791],\n",
      "        [0.0819],\n",
      "        [0.1550],\n",
      "        [0.0838],\n",
      "        [0.0847],\n",
      "        [0.0886],\n",
      "        [0.0888],\n",
      "        [0.1619],\n",
      "        [0.0908],\n",
      "        [0.0911],\n",
      "        [0.0937],\n",
      "        [0.0953],\n",
      "        [0.0959],\n",
      "        [0.0981],\n",
      "        [0.1709],\n",
      "        [0.1710],\n",
      "        [0.1017],\n",
      "        [0.1019],\n",
      "        [0.1768],\n",
      "        [0.1052],\n",
      "        [0.1055],\n",
      "        [0.1802],\n",
      "        [0.1854],\n",
      "        [0.1894],\n",
      "        [0.1935],\n",
      "        [0.1217],\n",
      "        [0.1236],\n",
      "        [0.1972],\n",
      "        [0.1273],\n",
      "        [0.1317],\n",
      "        [0.2075],\n",
      "        [0.2079],\n",
      "        [0.1455],\n",
      "        [0.1487],\n",
      "        [0.2230],\n",
      "        [0.1514],\n",
      "        [0.1534],\n",
      "        [0.1566],\n",
      "        [0.1610],\n",
      "        [0.1645],\n",
      "        [0.2397],\n",
      "        [0.2406],\n",
      "        [0.2428],\n",
      "        [0.1718],\n",
      "        [0.2470],\n",
      "        [0.1758],\n",
      "        [0.1784],\n",
      "        [0.1795],\n",
      "        [0.2545],\n",
      "        [0.2557],\n",
      "        [0.2560],\n",
      "        [0.1859],\n",
      "        [0.1860],\n",
      "        [0.1894],\n",
      "        [0.2011],\n",
      "        [0.2876],\n",
      "        [0.2171],\n",
      "        [0.2252],\n",
      "        [0.2385],\n",
      "        [0.2405],\n",
      "        [0.2415]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 34.130176305770874\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 129\n",
      "剩餘X 資料 torch.Size([31, 18])\n",
      "剩餘Y 資料 torch.Size([31, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06144218146800995, 4)\n",
      "The second_loss value of k: (0.06314318627119064, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引4，y= tensor([0.4646])\n",
      "目前模型的Data狀態 torch.Size([129, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124],\n",
      "        [0.7124]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0339],\n",
      "        [0.0337],\n",
      "        [0.0331],\n",
      "        [0.0462],\n",
      "        [0.0482],\n",
      "        [0.0231],\n",
      "        [0.0218],\n",
      "        [0.0094],\n",
      "        [0.0043],\n",
      "        [0.0037],\n",
      "        [0.0007],\n",
      "        [0.0045],\n",
      "        [0.0047],\n",
      "        [0.0068],\n",
      "        [0.0074],\n",
      "        [0.0086],\n",
      "        [0.0091],\n",
      "        [0.0812],\n",
      "        [0.0158],\n",
      "        [0.0174],\n",
      "        [0.0912],\n",
      "        [0.0918],\n",
      "        [0.0210],\n",
      "        [0.0217],\n",
      "        [0.0232],\n",
      "        [0.0970],\n",
      "        [0.0289],\n",
      "        [0.1020],\n",
      "        [0.1042],\n",
      "        [0.0330],\n",
      "        [0.0353],\n",
      "        [0.1075],\n",
      "        [0.0357],\n",
      "        [0.0404],\n",
      "        [0.0407],\n",
      "        [0.1171],\n",
      "        [0.1172],\n",
      "        [0.1191],\n",
      "        [0.0494],\n",
      "        [0.1231],\n",
      "        [0.1232],\n",
      "        [0.0514],\n",
      "        [0.0527],\n",
      "        [0.0533],\n",
      "        [0.0534],\n",
      "        [0.0539],\n",
      "        [0.1276],\n",
      "        [0.0571],\n",
      "        [0.1301],\n",
      "        [0.1309],\n",
      "        [0.1338],\n",
      "        [0.0632],\n",
      "        [0.1368],\n",
      "        [0.0650],\n",
      "        [0.1372],\n",
      "        [0.0670],\n",
      "        [0.1400],\n",
      "        [0.0685],\n",
      "        [0.0695],\n",
      "        [0.0709],\n",
      "        [0.0713],\n",
      "        [0.0760],\n",
      "        [0.0764],\n",
      "        [0.0771],\n",
      "        [0.1492],\n",
      "        [0.0789],\n",
      "        [0.1511],\n",
      "        [0.0791],\n",
      "        [0.0819],\n",
      "        [0.1550],\n",
      "        [0.0838],\n",
      "        [0.0847],\n",
      "        [0.0886],\n",
      "        [0.0888],\n",
      "        [0.1619],\n",
      "        [0.0908],\n",
      "        [0.0911],\n",
      "        [0.0937],\n",
      "        [0.0953],\n",
      "        [0.0959],\n",
      "        [0.0981],\n",
      "        [0.1709],\n",
      "        [0.1710],\n",
      "        [0.1017],\n",
      "        [0.1019],\n",
      "        [0.1768],\n",
      "        [0.1052],\n",
      "        [0.1055],\n",
      "        [0.1802],\n",
      "        [0.1854],\n",
      "        [0.1894],\n",
      "        [0.1935],\n",
      "        [0.1217],\n",
      "        [0.1236],\n",
      "        [0.1972],\n",
      "        [0.1273],\n",
      "        [0.1317],\n",
      "        [0.2075],\n",
      "        [0.2079],\n",
      "        [0.1455],\n",
      "        [0.1487],\n",
      "        [0.2230],\n",
      "        [0.1514],\n",
      "        [0.1534],\n",
      "        [0.1566],\n",
      "        [0.1610],\n",
      "        [0.1645],\n",
      "        [0.2397],\n",
      "        [0.2406],\n",
      "        [0.2428],\n",
      "        [0.1718],\n",
      "        [0.2470],\n",
      "        [0.1758],\n",
      "        [0.1784],\n",
      "        [0.1795],\n",
      "        [0.2545],\n",
      "        [0.2557],\n",
      "        [0.2560],\n",
      "        [0.1859],\n",
      "        [0.1860],\n",
      "        [0.1894],\n",
      "        [0.2011],\n",
      "        [0.2876],\n",
      "        [0.2171],\n",
      "        [0.2252],\n",
      "        [0.2385],\n",
      "        [0.2405],\n",
      "        [0.2415],\n",
      "        [0.2479]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0358],\n",
      "        [0.0356],\n",
      "        [0.0350],\n",
      "        [0.0481],\n",
      "        [0.0501],\n",
      "        [0.0250],\n",
      "        [0.0237],\n",
      "        [0.0113],\n",
      "        [0.0063],\n",
      "        [0.0056],\n",
      "        [0.0012],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0049],\n",
      "        [0.0055],\n",
      "        [0.0067],\n",
      "        [0.0071],\n",
      "        [0.0831],\n",
      "        [0.0139],\n",
      "        [0.0155],\n",
      "        [0.0931],\n",
      "        [0.0938],\n",
      "        [0.0191],\n",
      "        [0.0198],\n",
      "        [0.0212],\n",
      "        [0.0989],\n",
      "        [0.0270],\n",
      "        [0.1039],\n",
      "        [0.1061],\n",
      "        [0.0311],\n",
      "        [0.0333],\n",
      "        [0.1095],\n",
      "        [0.0338],\n",
      "        [0.0385],\n",
      "        [0.0388],\n",
      "        [0.1190],\n",
      "        [0.1191],\n",
      "        [0.1210],\n",
      "        [0.0475],\n",
      "        [0.1250],\n",
      "        [0.1251],\n",
      "        [0.0494],\n",
      "        [0.0508],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0520],\n",
      "        [0.1295],\n",
      "        [0.0552],\n",
      "        [0.1320],\n",
      "        [0.1328],\n",
      "        [0.1357],\n",
      "        [0.0613],\n",
      "        [0.1387],\n",
      "        [0.0631],\n",
      "        [0.1391],\n",
      "        [0.0651],\n",
      "        [0.1419],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0690],\n",
      "        [0.0693],\n",
      "        [0.0741],\n",
      "        [0.0745],\n",
      "        [0.0752],\n",
      "        [0.1511],\n",
      "        [0.0770],\n",
      "        [0.1530],\n",
      "        [0.0772],\n",
      "        [0.0800],\n",
      "        [0.1569],\n",
      "        [0.0819],\n",
      "        [0.0827],\n",
      "        [0.0867],\n",
      "        [0.0869],\n",
      "        [0.1638],\n",
      "        [0.0889],\n",
      "        [0.0892],\n",
      "        [0.0918],\n",
      "        [0.0934],\n",
      "        [0.0939],\n",
      "        [0.0962],\n",
      "        [0.1728],\n",
      "        [0.1730],\n",
      "        [0.0998],\n",
      "        [0.1000],\n",
      "        [0.1787],\n",
      "        [0.1033],\n",
      "        [0.1035],\n",
      "        [0.1821],\n",
      "        [0.1873],\n",
      "        [0.1913],\n",
      "        [0.1954],\n",
      "        [0.1198],\n",
      "        [0.1217],\n",
      "        [0.1991],\n",
      "        [0.1254],\n",
      "        [0.1298],\n",
      "        [0.2095],\n",
      "        [0.2098],\n",
      "        [0.1436],\n",
      "        [0.1467],\n",
      "        [0.2249],\n",
      "        [0.1494],\n",
      "        [0.1515],\n",
      "        [0.1547],\n",
      "        [0.1591],\n",
      "        [0.1626],\n",
      "        [0.2416],\n",
      "        [0.2425],\n",
      "        [0.2447],\n",
      "        [0.1699],\n",
      "        [0.2489],\n",
      "        [0.1739],\n",
      "        [0.1765],\n",
      "        [0.1776],\n",
      "        [0.2564],\n",
      "        [0.2577],\n",
      "        [0.2580],\n",
      "        [0.1840],\n",
      "        [0.1841],\n",
      "        [0.1875],\n",
      "        [0.1992],\n",
      "        [0.2895],\n",
      "        [0.2152],\n",
      "        [0.2233],\n",
      "        [0.2366],\n",
      "        [0.2386],\n",
      "        [0.2396],\n",
      "        [0.2460]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 34.36910653114319\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 130\n",
      "剩餘X 資料 torch.Size([30, 18])\n",
      "剩餘Y 資料 torch.Size([30, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06218394637107849, 0)\n",
      "The second_loss value of k: (0.0643315240740776, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.4611])\n",
      "目前模型的Data狀態 torch.Size([130, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105],\n",
      "        [0.7105]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0358],\n",
      "        [0.0356],\n",
      "        [0.0350],\n",
      "        [0.0481],\n",
      "        [0.0501],\n",
      "        [0.0250],\n",
      "        [0.0237],\n",
      "        [0.0113],\n",
      "        [0.0063],\n",
      "        [0.0056],\n",
      "        [0.0012],\n",
      "        [0.0026],\n",
      "        [0.0027],\n",
      "        [0.0049],\n",
      "        [0.0055],\n",
      "        [0.0067],\n",
      "        [0.0071],\n",
      "        [0.0831],\n",
      "        [0.0139],\n",
      "        [0.0155],\n",
      "        [0.0931],\n",
      "        [0.0938],\n",
      "        [0.0191],\n",
      "        [0.0198],\n",
      "        [0.0212],\n",
      "        [0.0989],\n",
      "        [0.0270],\n",
      "        [0.1039],\n",
      "        [0.1061],\n",
      "        [0.0311],\n",
      "        [0.0333],\n",
      "        [0.1095],\n",
      "        [0.0338],\n",
      "        [0.0385],\n",
      "        [0.0388],\n",
      "        [0.1190],\n",
      "        [0.1191],\n",
      "        [0.1210],\n",
      "        [0.0475],\n",
      "        [0.1250],\n",
      "        [0.1251],\n",
      "        [0.0494],\n",
      "        [0.0508],\n",
      "        [0.0514],\n",
      "        [0.0515],\n",
      "        [0.0520],\n",
      "        [0.1295],\n",
      "        [0.0552],\n",
      "        [0.1320],\n",
      "        [0.1328],\n",
      "        [0.1357],\n",
      "        [0.0613],\n",
      "        [0.1387],\n",
      "        [0.0631],\n",
      "        [0.1391],\n",
      "        [0.0651],\n",
      "        [0.1419],\n",
      "        [0.0666],\n",
      "        [0.0675],\n",
      "        [0.0690],\n",
      "        [0.0693],\n",
      "        [0.0741],\n",
      "        [0.0745],\n",
      "        [0.0752],\n",
      "        [0.1511],\n",
      "        [0.0770],\n",
      "        [0.1530],\n",
      "        [0.0772],\n",
      "        [0.0800],\n",
      "        [0.1569],\n",
      "        [0.0819],\n",
      "        [0.0827],\n",
      "        [0.0867],\n",
      "        [0.0869],\n",
      "        [0.1638],\n",
      "        [0.0889],\n",
      "        [0.0892],\n",
      "        [0.0918],\n",
      "        [0.0934],\n",
      "        [0.0939],\n",
      "        [0.0962],\n",
      "        [0.1728],\n",
      "        [0.1730],\n",
      "        [0.0998],\n",
      "        [0.1000],\n",
      "        [0.1787],\n",
      "        [0.1033],\n",
      "        [0.1035],\n",
      "        [0.1821],\n",
      "        [0.1873],\n",
      "        [0.1913],\n",
      "        [0.1954],\n",
      "        [0.1198],\n",
      "        [0.1217],\n",
      "        [0.1991],\n",
      "        [0.1254],\n",
      "        [0.1298],\n",
      "        [0.2095],\n",
      "        [0.2098],\n",
      "        [0.1436],\n",
      "        [0.1467],\n",
      "        [0.2249],\n",
      "        [0.1494],\n",
      "        [0.1515],\n",
      "        [0.1547],\n",
      "        [0.1591],\n",
      "        [0.1626],\n",
      "        [0.2416],\n",
      "        [0.2425],\n",
      "        [0.2447],\n",
      "        [0.1699],\n",
      "        [0.2489],\n",
      "        [0.1739],\n",
      "        [0.1765],\n",
      "        [0.1776],\n",
      "        [0.2564],\n",
      "        [0.2577],\n",
      "        [0.2580],\n",
      "        [0.1840],\n",
      "        [0.1841],\n",
      "        [0.1875],\n",
      "        [0.1992],\n",
      "        [0.2895],\n",
      "        [0.2152],\n",
      "        [0.2233],\n",
      "        [0.2366],\n",
      "        [0.2386],\n",
      "        [0.2396],\n",
      "        [0.2460],\n",
      "        [0.2494]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 65\n",
      "Number of shrink: 35\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0377],\n",
      "        [0.0376],\n",
      "        [0.0370],\n",
      "        [0.0500],\n",
      "        [0.0520],\n",
      "        [0.0269],\n",
      "        [0.0257],\n",
      "        [0.0132],\n",
      "        [0.0082],\n",
      "        [0.0075],\n",
      "        [0.0032],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0029],\n",
      "        [0.0036],\n",
      "        [0.0047],\n",
      "        [0.0052],\n",
      "        [0.0851],\n",
      "        [0.0120],\n",
      "        [0.0135],\n",
      "        [0.0951],\n",
      "        [0.0957],\n",
      "        [0.0171],\n",
      "        [0.0178],\n",
      "        [0.0193],\n",
      "        [0.1009],\n",
      "        [0.0250],\n",
      "        [0.1059],\n",
      "        [0.1081],\n",
      "        [0.0291],\n",
      "        [0.0314],\n",
      "        [0.1114],\n",
      "        [0.0318],\n",
      "        [0.0365],\n",
      "        [0.0368],\n",
      "        [0.1210],\n",
      "        [0.1211],\n",
      "        [0.1230],\n",
      "        [0.0455],\n",
      "        [0.1269],\n",
      "        [0.1271],\n",
      "        [0.0475],\n",
      "        [0.0488],\n",
      "        [0.0494],\n",
      "        [0.0496],\n",
      "        [0.0501],\n",
      "        [0.1315],\n",
      "        [0.0533],\n",
      "        [0.1340],\n",
      "        [0.1347],\n",
      "        [0.1377],\n",
      "        [0.0593],\n",
      "        [0.1406],\n",
      "        [0.0611],\n",
      "        [0.1410],\n",
      "        [0.0632],\n",
      "        [0.1438],\n",
      "        [0.0646],\n",
      "        [0.0656],\n",
      "        [0.0670],\n",
      "        [0.0674],\n",
      "        [0.0722],\n",
      "        [0.0726],\n",
      "        [0.0732],\n",
      "        [0.1531],\n",
      "        [0.0751],\n",
      "        [0.1550],\n",
      "        [0.0752],\n",
      "        [0.0780],\n",
      "        [0.1588],\n",
      "        [0.0799],\n",
      "        [0.0808],\n",
      "        [0.0847],\n",
      "        [0.0850],\n",
      "        [0.1658],\n",
      "        [0.0870],\n",
      "        [0.0872],\n",
      "        [0.0898],\n",
      "        [0.0915],\n",
      "        [0.0920],\n",
      "        [0.0942],\n",
      "        [0.1747],\n",
      "        [0.1749],\n",
      "        [0.0978],\n",
      "        [0.0981],\n",
      "        [0.1807],\n",
      "        [0.1013],\n",
      "        [0.1016],\n",
      "        [0.1840],\n",
      "        [0.1893],\n",
      "        [0.1933],\n",
      "        [0.1974],\n",
      "        [0.1179],\n",
      "        [0.1197],\n",
      "        [0.2010],\n",
      "        [0.1234],\n",
      "        [0.1279],\n",
      "        [0.2114],\n",
      "        [0.2117],\n",
      "        [0.1417],\n",
      "        [0.1448],\n",
      "        [0.2269],\n",
      "        [0.1475],\n",
      "        [0.1496],\n",
      "        [0.1527],\n",
      "        [0.1571],\n",
      "        [0.1606],\n",
      "        [0.2436],\n",
      "        [0.2444],\n",
      "        [0.2467],\n",
      "        [0.1680],\n",
      "        [0.2508],\n",
      "        [0.1719],\n",
      "        [0.1746],\n",
      "        [0.1756],\n",
      "        [0.2584],\n",
      "        [0.2596],\n",
      "        [0.2599],\n",
      "        [0.1820],\n",
      "        [0.1821],\n",
      "        [0.1855],\n",
      "        [0.1972],\n",
      "        [0.2914],\n",
      "        [0.2132],\n",
      "        [0.2213],\n",
      "        [0.2346],\n",
      "        [0.2366],\n",
      "        [0.2376],\n",
      "        [0.2440],\n",
      "        [0.2474]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 34.607808351516724\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 131\n",
      "剩餘X 資料 torch.Size([29, 18])\n",
      "剩餘Y 資料 torch.Size([29, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06334763020277023, 2)\n",
      "The second_loss value of k: (0.06573577225208282, 21)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.4569])\n",
      "目前模型的Data狀態 torch.Size([131, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086],\n",
      "        [0.7086]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0377],\n",
      "        [0.0376],\n",
      "        [0.0370],\n",
      "        [0.0500],\n",
      "        [0.0520],\n",
      "        [0.0269],\n",
      "        [0.0257],\n",
      "        [0.0132],\n",
      "        [0.0082],\n",
      "        [0.0075],\n",
      "        [0.0032],\n",
      "        [0.0007],\n",
      "        [0.0008],\n",
      "        [0.0029],\n",
      "        [0.0036],\n",
      "        [0.0047],\n",
      "        [0.0052],\n",
      "        [0.0851],\n",
      "        [0.0120],\n",
      "        [0.0135],\n",
      "        [0.0951],\n",
      "        [0.0957],\n",
      "        [0.0171],\n",
      "        [0.0178],\n",
      "        [0.0193],\n",
      "        [0.1009],\n",
      "        [0.0250],\n",
      "        [0.1059],\n",
      "        [0.1081],\n",
      "        [0.0291],\n",
      "        [0.0314],\n",
      "        [0.1114],\n",
      "        [0.0318],\n",
      "        [0.0365],\n",
      "        [0.0368],\n",
      "        [0.1210],\n",
      "        [0.1211],\n",
      "        [0.1230],\n",
      "        [0.0455],\n",
      "        [0.1269],\n",
      "        [0.1271],\n",
      "        [0.0475],\n",
      "        [0.0488],\n",
      "        [0.0494],\n",
      "        [0.0496],\n",
      "        [0.0501],\n",
      "        [0.1315],\n",
      "        [0.0533],\n",
      "        [0.1340],\n",
      "        [0.1347],\n",
      "        [0.1377],\n",
      "        [0.0593],\n",
      "        [0.1406],\n",
      "        [0.0611],\n",
      "        [0.1410],\n",
      "        [0.0632],\n",
      "        [0.1438],\n",
      "        [0.0646],\n",
      "        [0.0656],\n",
      "        [0.0670],\n",
      "        [0.0674],\n",
      "        [0.0722],\n",
      "        [0.0726],\n",
      "        [0.0732],\n",
      "        [0.1531],\n",
      "        [0.0751],\n",
      "        [0.1550],\n",
      "        [0.0752],\n",
      "        [0.0780],\n",
      "        [0.1588],\n",
      "        [0.0799],\n",
      "        [0.0808],\n",
      "        [0.0847],\n",
      "        [0.0850],\n",
      "        [0.1658],\n",
      "        [0.0870],\n",
      "        [0.0872],\n",
      "        [0.0898],\n",
      "        [0.0915],\n",
      "        [0.0920],\n",
      "        [0.0942],\n",
      "        [0.1747],\n",
      "        [0.1749],\n",
      "        [0.0978],\n",
      "        [0.0981],\n",
      "        [0.1807],\n",
      "        [0.1013],\n",
      "        [0.1016],\n",
      "        [0.1840],\n",
      "        [0.1893],\n",
      "        [0.1933],\n",
      "        [0.1974],\n",
      "        [0.1179],\n",
      "        [0.1197],\n",
      "        [0.2010],\n",
      "        [0.1234],\n",
      "        [0.1279],\n",
      "        [0.2114],\n",
      "        [0.2117],\n",
      "        [0.1417],\n",
      "        [0.1448],\n",
      "        [0.2269],\n",
      "        [0.1475],\n",
      "        [0.1496],\n",
      "        [0.1527],\n",
      "        [0.1571],\n",
      "        [0.1606],\n",
      "        [0.2436],\n",
      "        [0.2444],\n",
      "        [0.2467],\n",
      "        [0.1680],\n",
      "        [0.2508],\n",
      "        [0.1719],\n",
      "        [0.1746],\n",
      "        [0.1756],\n",
      "        [0.2584],\n",
      "        [0.2596],\n",
      "        [0.2599],\n",
      "        [0.1820],\n",
      "        [0.1821],\n",
      "        [0.1855],\n",
      "        [0.1972],\n",
      "        [0.2914],\n",
      "        [0.2132],\n",
      "        [0.2213],\n",
      "        [0.2346],\n",
      "        [0.2366],\n",
      "        [0.2376],\n",
      "        [0.2440],\n",
      "        [0.2474],\n",
      "        [0.2517]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 74\n",
      "Number of shrink: 26\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0396],\n",
      "        [0.0395],\n",
      "        [0.0388],\n",
      "        [0.0519],\n",
      "        [0.0539],\n",
      "        [0.0288],\n",
      "        [0.0276],\n",
      "        [0.0151],\n",
      "        [0.0101],\n",
      "        [0.0094],\n",
      "        [0.0051],\n",
      "        [0.0012],\n",
      "        [0.0011],\n",
      "        [0.0010],\n",
      "        [0.0017],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0869],\n",
      "        [0.0101],\n",
      "        [0.0116],\n",
      "        [0.0970],\n",
      "        [0.0976],\n",
      "        [0.0152],\n",
      "        [0.0160],\n",
      "        [0.0174],\n",
      "        [0.1028],\n",
      "        [0.0231],\n",
      "        [0.1078],\n",
      "        [0.1100],\n",
      "        [0.0272],\n",
      "        [0.0295],\n",
      "        [0.1133],\n",
      "        [0.0299],\n",
      "        [0.0346],\n",
      "        [0.0349],\n",
      "        [0.1229],\n",
      "        [0.1230],\n",
      "        [0.1249],\n",
      "        [0.0436],\n",
      "        [0.1288],\n",
      "        [0.1290],\n",
      "        [0.0456],\n",
      "        [0.0469],\n",
      "        [0.0475],\n",
      "        [0.0477],\n",
      "        [0.0482],\n",
      "        [0.1334],\n",
      "        [0.0514],\n",
      "        [0.1359],\n",
      "        [0.1366],\n",
      "        [0.1396],\n",
      "        [0.0574],\n",
      "        [0.1425],\n",
      "        [0.0592],\n",
      "        [0.1429],\n",
      "        [0.0613],\n",
      "        [0.1457],\n",
      "        [0.0627],\n",
      "        [0.0637],\n",
      "        [0.0651],\n",
      "        [0.0655],\n",
      "        [0.0703],\n",
      "        [0.0707],\n",
      "        [0.0713],\n",
      "        [0.1550],\n",
      "        [0.0732],\n",
      "        [0.1569],\n",
      "        [0.0733],\n",
      "        [0.0761],\n",
      "        [0.1607],\n",
      "        [0.0780],\n",
      "        [0.0789],\n",
      "        [0.0828],\n",
      "        [0.0831],\n",
      "        [0.1677],\n",
      "        [0.0851],\n",
      "        [0.0853],\n",
      "        [0.0879],\n",
      "        [0.0896],\n",
      "        [0.0901],\n",
      "        [0.0923],\n",
      "        [0.1766],\n",
      "        [0.1768],\n",
      "        [0.0959],\n",
      "        [0.0962],\n",
      "        [0.1826],\n",
      "        [0.0994],\n",
      "        [0.0997],\n",
      "        [0.1859],\n",
      "        [0.1912],\n",
      "        [0.1952],\n",
      "        [0.1992],\n",
      "        [0.1160],\n",
      "        [0.1178],\n",
      "        [0.2029],\n",
      "        [0.1216],\n",
      "        [0.1260],\n",
      "        [0.2133],\n",
      "        [0.2136],\n",
      "        [0.1398],\n",
      "        [0.1429],\n",
      "        [0.2288],\n",
      "        [0.1456],\n",
      "        [0.1477],\n",
      "        [0.1508],\n",
      "        [0.1552],\n",
      "        [0.1587],\n",
      "        [0.2455],\n",
      "        [0.2463],\n",
      "        [0.2486],\n",
      "        [0.1661],\n",
      "        [0.2527],\n",
      "        [0.1700],\n",
      "        [0.1727],\n",
      "        [0.1737],\n",
      "        [0.2603],\n",
      "        [0.2615],\n",
      "        [0.2618],\n",
      "        [0.1802],\n",
      "        [0.1802],\n",
      "        [0.1836],\n",
      "        [0.1953],\n",
      "        [0.2933],\n",
      "        [0.2113],\n",
      "        [0.2194],\n",
      "        [0.2327],\n",
      "        [0.2348],\n",
      "        [0.2357],\n",
      "        [0.2421],\n",
      "        [0.2455],\n",
      "        [0.2498]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 34.843116760253906\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 132\n",
      "剩餘X 資料 torch.Size([28, 18])\n",
      "剩餘Y 資料 torch.Size([28, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06476767361164093, 20)\n",
      "The second_loss value of k: (0.06534713506698608, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引20，y= tensor([0.4522])\n",
      "目前模型的Data狀態 torch.Size([132, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067],\n",
      "        [0.7067]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0396],\n",
      "        [0.0395],\n",
      "        [0.0388],\n",
      "        [0.0519],\n",
      "        [0.0539],\n",
      "        [0.0288],\n",
      "        [0.0276],\n",
      "        [0.0151],\n",
      "        [0.0101],\n",
      "        [0.0094],\n",
      "        [0.0051],\n",
      "        [0.0012],\n",
      "        [0.0011],\n",
      "        [0.0010],\n",
      "        [0.0017],\n",
      "        [0.0028],\n",
      "        [0.0033],\n",
      "        [0.0869],\n",
      "        [0.0101],\n",
      "        [0.0116],\n",
      "        [0.0970],\n",
      "        [0.0976],\n",
      "        [0.0152],\n",
      "        [0.0160],\n",
      "        [0.0174],\n",
      "        [0.1028],\n",
      "        [0.0231],\n",
      "        [0.1078],\n",
      "        [0.1100],\n",
      "        [0.0272],\n",
      "        [0.0295],\n",
      "        [0.1133],\n",
      "        [0.0299],\n",
      "        [0.0346],\n",
      "        [0.0349],\n",
      "        [0.1229],\n",
      "        [0.1230],\n",
      "        [0.1249],\n",
      "        [0.0436],\n",
      "        [0.1288],\n",
      "        [0.1290],\n",
      "        [0.0456],\n",
      "        [0.0469],\n",
      "        [0.0475],\n",
      "        [0.0477],\n",
      "        [0.0482],\n",
      "        [0.1334],\n",
      "        [0.0514],\n",
      "        [0.1359],\n",
      "        [0.1366],\n",
      "        [0.1396],\n",
      "        [0.0574],\n",
      "        [0.1425],\n",
      "        [0.0592],\n",
      "        [0.1429],\n",
      "        [0.0613],\n",
      "        [0.1457],\n",
      "        [0.0627],\n",
      "        [0.0637],\n",
      "        [0.0651],\n",
      "        [0.0655],\n",
      "        [0.0703],\n",
      "        [0.0707],\n",
      "        [0.0713],\n",
      "        [0.1550],\n",
      "        [0.0732],\n",
      "        [0.1569],\n",
      "        [0.0733],\n",
      "        [0.0761],\n",
      "        [0.1607],\n",
      "        [0.0780],\n",
      "        [0.0789],\n",
      "        [0.0828],\n",
      "        [0.0831],\n",
      "        [0.1677],\n",
      "        [0.0851],\n",
      "        [0.0853],\n",
      "        [0.0879],\n",
      "        [0.0896],\n",
      "        [0.0901],\n",
      "        [0.0923],\n",
      "        [0.1766],\n",
      "        [0.1768],\n",
      "        [0.0959],\n",
      "        [0.0962],\n",
      "        [0.1826],\n",
      "        [0.0994],\n",
      "        [0.0997],\n",
      "        [0.1859],\n",
      "        [0.1912],\n",
      "        [0.1952],\n",
      "        [0.1992],\n",
      "        [0.1160],\n",
      "        [0.1178],\n",
      "        [0.2029],\n",
      "        [0.1216],\n",
      "        [0.1260],\n",
      "        [0.2133],\n",
      "        [0.2136],\n",
      "        [0.1398],\n",
      "        [0.1429],\n",
      "        [0.2288],\n",
      "        [0.1456],\n",
      "        [0.1477],\n",
      "        [0.1508],\n",
      "        [0.1552],\n",
      "        [0.1587],\n",
      "        [0.2455],\n",
      "        [0.2463],\n",
      "        [0.2486],\n",
      "        [0.1661],\n",
      "        [0.2527],\n",
      "        [0.1700],\n",
      "        [0.1727],\n",
      "        [0.1737],\n",
      "        [0.2603],\n",
      "        [0.2615],\n",
      "        [0.2618],\n",
      "        [0.1802],\n",
      "        [0.1802],\n",
      "        [0.1836],\n",
      "        [0.1953],\n",
      "        [0.2933],\n",
      "        [0.2113],\n",
      "        [0.2194],\n",
      "        [0.2327],\n",
      "        [0.2348],\n",
      "        [0.2357],\n",
      "        [0.2421],\n",
      "        [0.2455],\n",
      "        [0.2498],\n",
      "        [0.2545]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0416],\n",
      "        [    0.0414],\n",
      "        [    0.0408],\n",
      "        [    0.0539],\n",
      "        [    0.0559],\n",
      "        [    0.0308],\n",
      "        [    0.0295],\n",
      "        [    0.0170],\n",
      "        [    0.0120],\n",
      "        [    0.0114],\n",
      "        [    0.0070],\n",
      "        [    0.0032],\n",
      "        [    0.0030],\n",
      "        [    0.0009],\n",
      "        [    0.0003],\n",
      "        [    0.0009],\n",
      "        [    0.0014],\n",
      "        [    0.0889],\n",
      "        [    0.0081],\n",
      "        [    0.0097],\n",
      "        [    0.0989],\n",
      "        [    0.0995],\n",
      "        [    0.0133],\n",
      "        [    0.0140],\n",
      "        [    0.0155],\n",
      "        [    0.1047],\n",
      "        [    0.0212],\n",
      "        [    0.1097],\n",
      "        [    0.1119],\n",
      "        [    0.0253],\n",
      "        [    0.0276],\n",
      "        [    0.1152],\n",
      "        [    0.0280],\n",
      "        [    0.0327],\n",
      "        [    0.0330],\n",
      "        [    0.1248],\n",
      "        [    0.1249],\n",
      "        [    0.1268],\n",
      "        [    0.0417],\n",
      "        [    0.1308],\n",
      "        [    0.1309],\n",
      "        [    0.0437],\n",
      "        [    0.0450],\n",
      "        [    0.0456],\n",
      "        [    0.0457],\n",
      "        [    0.0462],\n",
      "        [    0.1353],\n",
      "        [    0.0494],\n",
      "        [    0.1378],\n",
      "        [    0.1386],\n",
      "        [    0.1415],\n",
      "        [    0.0555],\n",
      "        [    0.1445],\n",
      "        [    0.0573],\n",
      "        [    0.1449],\n",
      "        [    0.0593],\n",
      "        [    0.1477],\n",
      "        [    0.0608],\n",
      "        [    0.0618],\n",
      "        [    0.0632],\n",
      "        [    0.0636],\n",
      "        [    0.0683],\n",
      "        [    0.0687],\n",
      "        [    0.0694],\n",
      "        [    0.1569],\n",
      "        [    0.0712],\n",
      "        [    0.1588],\n",
      "        [    0.0714],\n",
      "        [    0.0742],\n",
      "        [    0.1627],\n",
      "        [    0.0761],\n",
      "        [    0.0770],\n",
      "        [    0.0809],\n",
      "        [    0.0811],\n",
      "        [    0.1696],\n",
      "        [    0.0831],\n",
      "        [    0.0834],\n",
      "        [    0.0860],\n",
      "        [    0.0877],\n",
      "        [    0.0882],\n",
      "        [    0.0904],\n",
      "        [    0.1786],\n",
      "        [    0.1787],\n",
      "        [    0.0940],\n",
      "        [    0.0942],\n",
      "        [    0.1845],\n",
      "        [    0.0975],\n",
      "        [    0.0978],\n",
      "        [    0.1878],\n",
      "        [    0.1931],\n",
      "        [    0.1971],\n",
      "        [    0.2012],\n",
      "        [    0.1141],\n",
      "        [    0.1159],\n",
      "        [    0.2049],\n",
      "        [    0.1196],\n",
      "        [    0.1240],\n",
      "        [    0.2152],\n",
      "        [    0.2156],\n",
      "        [    0.1378],\n",
      "        [    0.1410],\n",
      "        [    0.2307],\n",
      "        [    0.1437],\n",
      "        [    0.1457],\n",
      "        [    0.1489],\n",
      "        [    0.1533],\n",
      "        [    0.1568],\n",
      "        [    0.2474],\n",
      "        [    0.2483],\n",
      "        [    0.2505],\n",
      "        [    0.1641],\n",
      "        [    0.2547],\n",
      "        [    0.1681],\n",
      "        [    0.1707],\n",
      "        [    0.1718],\n",
      "        [    0.2622],\n",
      "        [    0.2634],\n",
      "        [    0.2637],\n",
      "        [    0.1782],\n",
      "        [    0.1783],\n",
      "        [    0.1817],\n",
      "        [    0.1934],\n",
      "        [    0.2953],\n",
      "        [    0.2094],\n",
      "        [    0.2175],\n",
      "        [    0.2308],\n",
      "        [    0.2328],\n",
      "        [    0.2338],\n",
      "        [    0.2402],\n",
      "        [    0.2436],\n",
      "        [    0.2479],\n",
      "        [    0.2526]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 35.081955671310425\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 133\n",
      "剩餘X 資料 torch.Size([27, 18])\n",
      "剩餘Y 資料 torch.Size([27, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06435877084732056, 19)\n",
      "The second_loss value of k: (0.06985726952552795, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引19，y= tensor([0.4510])\n",
      "目前模型的Data狀態 torch.Size([133, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047],\n",
      "        [0.7047]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0416],\n",
      "        [    0.0414],\n",
      "        [    0.0408],\n",
      "        [    0.0539],\n",
      "        [    0.0559],\n",
      "        [    0.0308],\n",
      "        [    0.0295],\n",
      "        [    0.0170],\n",
      "        [    0.0120],\n",
      "        [    0.0114],\n",
      "        [    0.0070],\n",
      "        [    0.0032],\n",
      "        [    0.0030],\n",
      "        [    0.0009],\n",
      "        [    0.0003],\n",
      "        [    0.0009],\n",
      "        [    0.0014],\n",
      "        [    0.0889],\n",
      "        [    0.0081],\n",
      "        [    0.0097],\n",
      "        [    0.0989],\n",
      "        [    0.0995],\n",
      "        [    0.0133],\n",
      "        [    0.0140],\n",
      "        [    0.0155],\n",
      "        [    0.1047],\n",
      "        [    0.0212],\n",
      "        [    0.1097],\n",
      "        [    0.1119],\n",
      "        [    0.0253],\n",
      "        [    0.0276],\n",
      "        [    0.1152],\n",
      "        [    0.0280],\n",
      "        [    0.0327],\n",
      "        [    0.0330],\n",
      "        [    0.1248],\n",
      "        [    0.1249],\n",
      "        [    0.1268],\n",
      "        [    0.0417],\n",
      "        [    0.1308],\n",
      "        [    0.1309],\n",
      "        [    0.0437],\n",
      "        [    0.0450],\n",
      "        [    0.0456],\n",
      "        [    0.0457],\n",
      "        [    0.0462],\n",
      "        [    0.1353],\n",
      "        [    0.0494],\n",
      "        [    0.1378],\n",
      "        [    0.1386],\n",
      "        [    0.1415],\n",
      "        [    0.0555],\n",
      "        [    0.1445],\n",
      "        [    0.0573],\n",
      "        [    0.1449],\n",
      "        [    0.0593],\n",
      "        [    0.1477],\n",
      "        [    0.0608],\n",
      "        [    0.0618],\n",
      "        [    0.0632],\n",
      "        [    0.0636],\n",
      "        [    0.0683],\n",
      "        [    0.0687],\n",
      "        [    0.0694],\n",
      "        [    0.1569],\n",
      "        [    0.0712],\n",
      "        [    0.1588],\n",
      "        [    0.0714],\n",
      "        [    0.0742],\n",
      "        [    0.1627],\n",
      "        [    0.0761],\n",
      "        [    0.0770],\n",
      "        [    0.0809],\n",
      "        [    0.0811],\n",
      "        [    0.1696],\n",
      "        [    0.0831],\n",
      "        [    0.0834],\n",
      "        [    0.0860],\n",
      "        [    0.0877],\n",
      "        [    0.0882],\n",
      "        [    0.0904],\n",
      "        [    0.1786],\n",
      "        [    0.1787],\n",
      "        [    0.0940],\n",
      "        [    0.0942],\n",
      "        [    0.1845],\n",
      "        [    0.0975],\n",
      "        [    0.0978],\n",
      "        [    0.1878],\n",
      "        [    0.1931],\n",
      "        [    0.1971],\n",
      "        [    0.2012],\n",
      "        [    0.1141],\n",
      "        [    0.1159],\n",
      "        [    0.2049],\n",
      "        [    0.1196],\n",
      "        [    0.1240],\n",
      "        [    0.2152],\n",
      "        [    0.2156],\n",
      "        [    0.1378],\n",
      "        [    0.1410],\n",
      "        [    0.2307],\n",
      "        [    0.1437],\n",
      "        [    0.1457],\n",
      "        [    0.1489],\n",
      "        [    0.1533],\n",
      "        [    0.1568],\n",
      "        [    0.2474],\n",
      "        [    0.2483],\n",
      "        [    0.2505],\n",
      "        [    0.1641],\n",
      "        [    0.2547],\n",
      "        [    0.1681],\n",
      "        [    0.1707],\n",
      "        [    0.1718],\n",
      "        [    0.2622],\n",
      "        [    0.2634],\n",
      "        [    0.2637],\n",
      "        [    0.1782],\n",
      "        [    0.1783],\n",
      "        [    0.1817],\n",
      "        [    0.1934],\n",
      "        [    0.2953],\n",
      "        [    0.2094],\n",
      "        [    0.2175],\n",
      "        [    0.2308],\n",
      "        [    0.2328],\n",
      "        [    0.2338],\n",
      "        [    0.2402],\n",
      "        [    0.2436],\n",
      "        [    0.2479],\n",
      "        [    0.2526],\n",
      "        [    0.2537]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0435],\n",
      "        [0.0433],\n",
      "        [0.0427],\n",
      "        [0.0558],\n",
      "        [0.0578],\n",
      "        [0.0327],\n",
      "        [0.0314],\n",
      "        [0.0189],\n",
      "        [0.0139],\n",
      "        [0.0133],\n",
      "        [0.0089],\n",
      "        [0.0051],\n",
      "        [0.0049],\n",
      "        [0.0028],\n",
      "        [0.0022],\n",
      "        [0.0010],\n",
      "        [0.0005],\n",
      "        [0.0908],\n",
      "        [0.0062],\n",
      "        [0.0078],\n",
      "        [0.1008],\n",
      "        [0.1014],\n",
      "        [0.0114],\n",
      "        [0.0121],\n",
      "        [0.0136],\n",
      "        [0.1066],\n",
      "        [0.0193],\n",
      "        [0.1116],\n",
      "        [0.1138],\n",
      "        [0.0234],\n",
      "        [0.0257],\n",
      "        [0.1171],\n",
      "        [0.0261],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.1267],\n",
      "        [0.1268],\n",
      "        [0.1287],\n",
      "        [0.0398],\n",
      "        [0.1327],\n",
      "        [0.1328],\n",
      "        [0.0418],\n",
      "        [0.0431],\n",
      "        [0.0437],\n",
      "        [0.0438],\n",
      "        [0.0444],\n",
      "        [0.1372],\n",
      "        [0.0475],\n",
      "        [0.1397],\n",
      "        [0.1405],\n",
      "        [0.1434],\n",
      "        [0.0536],\n",
      "        [0.1464],\n",
      "        [0.0554],\n",
      "        [0.1468],\n",
      "        [0.0574],\n",
      "        [0.1496],\n",
      "        [0.0589],\n",
      "        [0.0599],\n",
      "        [0.0613],\n",
      "        [0.0617],\n",
      "        [0.0664],\n",
      "        [0.0668],\n",
      "        [0.0675],\n",
      "        [0.1588],\n",
      "        [0.0693],\n",
      "        [0.1607],\n",
      "        [0.0695],\n",
      "        [0.0723],\n",
      "        [0.1646],\n",
      "        [0.0742],\n",
      "        [0.0751],\n",
      "        [0.0790],\n",
      "        [0.0793],\n",
      "        [0.1715],\n",
      "        [0.0812],\n",
      "        [0.0815],\n",
      "        [0.0841],\n",
      "        [0.0858],\n",
      "        [0.0863],\n",
      "        [0.0885],\n",
      "        [0.1805],\n",
      "        [0.1806],\n",
      "        [0.0921],\n",
      "        [0.0923],\n",
      "        [0.1864],\n",
      "        [0.0956],\n",
      "        [0.0959],\n",
      "        [0.1897],\n",
      "        [0.1950],\n",
      "        [0.1990],\n",
      "        [0.2031],\n",
      "        [0.1122],\n",
      "        [0.1140],\n",
      "        [0.2068],\n",
      "        [0.1177],\n",
      "        [0.1221],\n",
      "        [0.2171],\n",
      "        [0.2175],\n",
      "        [0.1359],\n",
      "        [0.1391],\n",
      "        [0.2326],\n",
      "        [0.1418],\n",
      "        [0.1438],\n",
      "        [0.1470],\n",
      "        [0.1514],\n",
      "        [0.1549],\n",
      "        [0.2493],\n",
      "        [0.2502],\n",
      "        [0.2524],\n",
      "        [0.1623],\n",
      "        [0.2566],\n",
      "        [0.1662],\n",
      "        [0.1688],\n",
      "        [0.1699],\n",
      "        [0.2641],\n",
      "        [0.2653],\n",
      "        [0.2656],\n",
      "        [0.1763],\n",
      "        [0.1764],\n",
      "        [0.1798],\n",
      "        [0.1915],\n",
      "        [0.2972],\n",
      "        [0.2075],\n",
      "        [0.2156],\n",
      "        [0.2289],\n",
      "        [0.2309],\n",
      "        [0.2319],\n",
      "        [0.2383],\n",
      "        [0.2417],\n",
      "        [0.2460],\n",
      "        [0.2507],\n",
      "        [0.2518]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 35.31846261024475\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 134\n",
      "剩餘X 資料 torch.Size([26, 18])\n",
      "剩餘Y 資料 torch.Size([26, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06885766983032227, 2)\n",
      "The second_loss value of k: (0.07092870026826859, 19)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.4404])\n",
      "目前模型的Data狀態 torch.Size([134, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028],\n",
      "        [0.7028]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0435],\n",
      "        [0.0433],\n",
      "        [0.0427],\n",
      "        [0.0558],\n",
      "        [0.0578],\n",
      "        [0.0327],\n",
      "        [0.0314],\n",
      "        [0.0189],\n",
      "        [0.0139],\n",
      "        [0.0133],\n",
      "        [0.0089],\n",
      "        [0.0051],\n",
      "        [0.0049],\n",
      "        [0.0028],\n",
      "        [0.0022],\n",
      "        [0.0010],\n",
      "        [0.0005],\n",
      "        [0.0908],\n",
      "        [0.0062],\n",
      "        [0.0078],\n",
      "        [0.1008],\n",
      "        [0.1014],\n",
      "        [0.0114],\n",
      "        [0.0121],\n",
      "        [0.0136],\n",
      "        [0.1066],\n",
      "        [0.0193],\n",
      "        [0.1116],\n",
      "        [0.1138],\n",
      "        [0.0234],\n",
      "        [0.0257],\n",
      "        [0.1171],\n",
      "        [0.0261],\n",
      "        [0.0308],\n",
      "        [0.0311],\n",
      "        [0.1267],\n",
      "        [0.1268],\n",
      "        [0.1287],\n",
      "        [0.0398],\n",
      "        [0.1327],\n",
      "        [0.1328],\n",
      "        [0.0418],\n",
      "        [0.0431],\n",
      "        [0.0437],\n",
      "        [0.0438],\n",
      "        [0.0444],\n",
      "        [0.1372],\n",
      "        [0.0475],\n",
      "        [0.1397],\n",
      "        [0.1405],\n",
      "        [0.1434],\n",
      "        [0.0536],\n",
      "        [0.1464],\n",
      "        [0.0554],\n",
      "        [0.1468],\n",
      "        [0.0574],\n",
      "        [0.1496],\n",
      "        [0.0589],\n",
      "        [0.0599],\n",
      "        [0.0613],\n",
      "        [0.0617],\n",
      "        [0.0664],\n",
      "        [0.0668],\n",
      "        [0.0675],\n",
      "        [0.1588],\n",
      "        [0.0693],\n",
      "        [0.1607],\n",
      "        [0.0695],\n",
      "        [0.0723],\n",
      "        [0.1646],\n",
      "        [0.0742],\n",
      "        [0.0751],\n",
      "        [0.0790],\n",
      "        [0.0793],\n",
      "        [0.1715],\n",
      "        [0.0812],\n",
      "        [0.0815],\n",
      "        [0.0841],\n",
      "        [0.0858],\n",
      "        [0.0863],\n",
      "        [0.0885],\n",
      "        [0.1805],\n",
      "        [0.1806],\n",
      "        [0.0921],\n",
      "        [0.0923],\n",
      "        [0.1864],\n",
      "        [0.0956],\n",
      "        [0.0959],\n",
      "        [0.1897],\n",
      "        [0.1950],\n",
      "        [0.1990],\n",
      "        [0.2031],\n",
      "        [0.1122],\n",
      "        [0.1140],\n",
      "        [0.2068],\n",
      "        [0.1177],\n",
      "        [0.1221],\n",
      "        [0.2171],\n",
      "        [0.2175],\n",
      "        [0.1359],\n",
      "        [0.1391],\n",
      "        [0.2326],\n",
      "        [0.1418],\n",
      "        [0.1438],\n",
      "        [0.1470],\n",
      "        [0.1514],\n",
      "        [0.1549],\n",
      "        [0.2493],\n",
      "        [0.2502],\n",
      "        [0.2524],\n",
      "        [0.1623],\n",
      "        [0.2566],\n",
      "        [0.1662],\n",
      "        [0.1688],\n",
      "        [0.1699],\n",
      "        [0.2641],\n",
      "        [0.2653],\n",
      "        [0.2656],\n",
      "        [0.1763],\n",
      "        [0.1764],\n",
      "        [0.1798],\n",
      "        [0.1915],\n",
      "        [0.2972],\n",
      "        [0.2075],\n",
      "        [0.2156],\n",
      "        [0.2289],\n",
      "        [0.2309],\n",
      "        [0.2319],\n",
      "        [0.2383],\n",
      "        [0.2417],\n",
      "        [0.2460],\n",
      "        [0.2507],\n",
      "        [0.2518],\n",
      "        [0.2624]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0454],\n",
      "        [0.0453],\n",
      "        [0.0446],\n",
      "        [0.0577],\n",
      "        [0.0597],\n",
      "        [0.0346],\n",
      "        [0.0334],\n",
      "        [0.0209],\n",
      "        [0.0159],\n",
      "        [0.0152],\n",
      "        [0.0109],\n",
      "        [0.0070],\n",
      "        [0.0069],\n",
      "        [0.0048],\n",
      "        [0.0041],\n",
      "        [0.0030],\n",
      "        [0.0025],\n",
      "        [0.0927],\n",
      "        [0.0043],\n",
      "        [0.0059],\n",
      "        [0.1028],\n",
      "        [0.1034],\n",
      "        [0.0095],\n",
      "        [0.0102],\n",
      "        [0.0116],\n",
      "        [0.1086],\n",
      "        [0.0173],\n",
      "        [0.1136],\n",
      "        [0.1158],\n",
      "        [0.0214],\n",
      "        [0.0237],\n",
      "        [0.1191],\n",
      "        [0.0241],\n",
      "        [0.0288],\n",
      "        [0.0292],\n",
      "        [0.1287],\n",
      "        [0.1288],\n",
      "        [0.1307],\n",
      "        [0.0379],\n",
      "        [0.1346],\n",
      "        [0.1348],\n",
      "        [0.0398],\n",
      "        [0.0411],\n",
      "        [0.0417],\n",
      "        [0.0419],\n",
      "        [0.0424],\n",
      "        [0.1392],\n",
      "        [0.0456],\n",
      "        [0.1417],\n",
      "        [0.1424],\n",
      "        [0.1454],\n",
      "        [0.0516],\n",
      "        [0.1483],\n",
      "        [0.0534],\n",
      "        [0.1487],\n",
      "        [0.0555],\n",
      "        [0.1515],\n",
      "        [0.0569],\n",
      "        [0.0579],\n",
      "        [0.0593],\n",
      "        [0.0597],\n",
      "        [0.0645],\n",
      "        [0.0649],\n",
      "        [0.0655],\n",
      "        [0.1608],\n",
      "        [0.0674],\n",
      "        [0.1627],\n",
      "        [0.0675],\n",
      "        [0.0703],\n",
      "        [0.1665],\n",
      "        [0.0722],\n",
      "        [0.0731],\n",
      "        [0.0770],\n",
      "        [0.0773],\n",
      "        [0.1735],\n",
      "        [0.0793],\n",
      "        [0.0795],\n",
      "        [0.0822],\n",
      "        [0.0838],\n",
      "        [0.0843],\n",
      "        [0.0865],\n",
      "        [0.1824],\n",
      "        [0.1826],\n",
      "        [0.0901],\n",
      "        [0.0904],\n",
      "        [0.1884],\n",
      "        [0.0936],\n",
      "        [0.0939],\n",
      "        [0.1917],\n",
      "        [0.1970],\n",
      "        [0.2010],\n",
      "        [0.2050],\n",
      "        [0.1102],\n",
      "        [0.1120],\n",
      "        [0.2087],\n",
      "        [0.1158],\n",
      "        [0.1202],\n",
      "        [0.2191],\n",
      "        [0.2194],\n",
      "        [0.1340],\n",
      "        [0.1371],\n",
      "        [0.2346],\n",
      "        [0.1398],\n",
      "        [0.1419],\n",
      "        [0.1450],\n",
      "        [0.1494],\n",
      "        [0.1529],\n",
      "        [0.2513],\n",
      "        [0.2521],\n",
      "        [0.2544],\n",
      "        [0.1603],\n",
      "        [0.2585],\n",
      "        [0.1643],\n",
      "        [0.1669],\n",
      "        [0.1679],\n",
      "        [0.2661],\n",
      "        [0.2673],\n",
      "        [0.2676],\n",
      "        [0.1744],\n",
      "        [0.1744],\n",
      "        [0.1778],\n",
      "        [0.1895],\n",
      "        [0.2991],\n",
      "        [0.2055],\n",
      "        [0.2136],\n",
      "        [0.2269],\n",
      "        [0.2290],\n",
      "        [0.2299],\n",
      "        [0.2363],\n",
      "        [0.2397],\n",
      "        [0.2440],\n",
      "        [0.2487],\n",
      "        [0.2498],\n",
      "        [0.2605]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 35.55610275268555\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 135\n",
      "剩餘X 資料 torch.Size([25, 18])\n",
      "剩餘Y 資料 torch.Size([25, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06989199668169022, 18)\n",
      "The second_loss value of k: (0.07682841271162033, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引18，y= tensor([0.4365])\n",
      "目前模型的Data狀態 torch.Size([135, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009],\n",
      "        [0.7009]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0454],\n",
      "        [0.0453],\n",
      "        [0.0446],\n",
      "        [0.0577],\n",
      "        [0.0597],\n",
      "        [0.0346],\n",
      "        [0.0334],\n",
      "        [0.0209],\n",
      "        [0.0159],\n",
      "        [0.0152],\n",
      "        [0.0109],\n",
      "        [0.0070],\n",
      "        [0.0069],\n",
      "        [0.0048],\n",
      "        [0.0041],\n",
      "        [0.0030],\n",
      "        [0.0025],\n",
      "        [0.0927],\n",
      "        [0.0043],\n",
      "        [0.0059],\n",
      "        [0.1028],\n",
      "        [0.1034],\n",
      "        [0.0095],\n",
      "        [0.0102],\n",
      "        [0.0116],\n",
      "        [0.1086],\n",
      "        [0.0173],\n",
      "        [0.1136],\n",
      "        [0.1158],\n",
      "        [0.0214],\n",
      "        [0.0237],\n",
      "        [0.1191],\n",
      "        [0.0241],\n",
      "        [0.0288],\n",
      "        [0.0292],\n",
      "        [0.1287],\n",
      "        [0.1288],\n",
      "        [0.1307],\n",
      "        [0.0379],\n",
      "        [0.1346],\n",
      "        [0.1348],\n",
      "        [0.0398],\n",
      "        [0.0411],\n",
      "        [0.0417],\n",
      "        [0.0419],\n",
      "        [0.0424],\n",
      "        [0.1392],\n",
      "        [0.0456],\n",
      "        [0.1417],\n",
      "        [0.1424],\n",
      "        [0.1454],\n",
      "        [0.0516],\n",
      "        [0.1483],\n",
      "        [0.0534],\n",
      "        [0.1487],\n",
      "        [0.0555],\n",
      "        [0.1515],\n",
      "        [0.0569],\n",
      "        [0.0579],\n",
      "        [0.0593],\n",
      "        [0.0597],\n",
      "        [0.0645],\n",
      "        [0.0649],\n",
      "        [0.0655],\n",
      "        [0.1608],\n",
      "        [0.0674],\n",
      "        [0.1627],\n",
      "        [0.0675],\n",
      "        [0.0703],\n",
      "        [0.1665],\n",
      "        [0.0722],\n",
      "        [0.0731],\n",
      "        [0.0770],\n",
      "        [0.0773],\n",
      "        [0.1735],\n",
      "        [0.0793],\n",
      "        [0.0795],\n",
      "        [0.0822],\n",
      "        [0.0838],\n",
      "        [0.0843],\n",
      "        [0.0865],\n",
      "        [0.1824],\n",
      "        [0.1826],\n",
      "        [0.0901],\n",
      "        [0.0904],\n",
      "        [0.1884],\n",
      "        [0.0936],\n",
      "        [0.0939],\n",
      "        [0.1917],\n",
      "        [0.1970],\n",
      "        [0.2010],\n",
      "        [0.2050],\n",
      "        [0.1102],\n",
      "        [0.1120],\n",
      "        [0.2087],\n",
      "        [0.1158],\n",
      "        [0.1202],\n",
      "        [0.2191],\n",
      "        [0.2194],\n",
      "        [0.1340],\n",
      "        [0.1371],\n",
      "        [0.2346],\n",
      "        [0.1398],\n",
      "        [0.1419],\n",
      "        [0.1450],\n",
      "        [0.1494],\n",
      "        [0.1529],\n",
      "        [0.2513],\n",
      "        [0.2521],\n",
      "        [0.2544],\n",
      "        [0.1603],\n",
      "        [0.2585],\n",
      "        [0.1643],\n",
      "        [0.1669],\n",
      "        [0.1679],\n",
      "        [0.2661],\n",
      "        [0.2673],\n",
      "        [0.2676],\n",
      "        [0.1744],\n",
      "        [0.1744],\n",
      "        [0.1778],\n",
      "        [0.1895],\n",
      "        [0.2991],\n",
      "        [0.2055],\n",
      "        [0.2136],\n",
      "        [0.2269],\n",
      "        [0.2290],\n",
      "        [0.2299],\n",
      "        [0.2363],\n",
      "        [0.2397],\n",
      "        [0.2440],\n",
      "        [0.2487],\n",
      "        [0.2498],\n",
      "        [0.2605],\n",
      "        [0.2644]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0474],\n",
      "        [0.0472],\n",
      "        [0.0466],\n",
      "        [0.0597],\n",
      "        [0.0617],\n",
      "        [0.0366],\n",
      "        [0.0353],\n",
      "        [0.0229],\n",
      "        [0.0179],\n",
      "        [0.0172],\n",
      "        [0.0128],\n",
      "        [0.0090],\n",
      "        [0.0089],\n",
      "        [0.0067],\n",
      "        [0.0061],\n",
      "        [0.0049],\n",
      "        [0.0045],\n",
      "        [0.0947],\n",
      "        [0.0023],\n",
      "        [0.0039],\n",
      "        [0.1047],\n",
      "        [0.1054],\n",
      "        [0.0075],\n",
      "        [0.0082],\n",
      "        [0.0096],\n",
      "        [0.1105],\n",
      "        [0.0154],\n",
      "        [0.1156],\n",
      "        [0.1177],\n",
      "        [0.0195],\n",
      "        [0.0217],\n",
      "        [0.1211],\n",
      "        [0.0222],\n",
      "        [0.0269],\n",
      "        [0.0272],\n",
      "        [0.1306],\n",
      "        [0.1307],\n",
      "        [0.1326],\n",
      "        [0.0359],\n",
      "        [0.1366],\n",
      "        [0.1367],\n",
      "        [0.0378],\n",
      "        [0.0392],\n",
      "        [0.0398],\n",
      "        [0.0399],\n",
      "        [0.0404],\n",
      "        [0.1411],\n",
      "        [0.0436],\n",
      "        [0.1436],\n",
      "        [0.1444],\n",
      "        [0.1474],\n",
      "        [0.0497],\n",
      "        [0.1503],\n",
      "        [0.0515],\n",
      "        [0.1507],\n",
      "        [0.0535],\n",
      "        [0.1535],\n",
      "        [0.0550],\n",
      "        [0.0559],\n",
      "        [0.0574],\n",
      "        [0.0577],\n",
      "        [0.0625],\n",
      "        [0.0629],\n",
      "        [0.0636],\n",
      "        [0.1627],\n",
      "        [0.0654],\n",
      "        [0.1646],\n",
      "        [0.0656],\n",
      "        [0.0684],\n",
      "        [0.1685],\n",
      "        [0.0703],\n",
      "        [0.0711],\n",
      "        [0.0750],\n",
      "        [0.0753],\n",
      "        [0.1754],\n",
      "        [0.0773],\n",
      "        [0.0776],\n",
      "        [0.0802],\n",
      "        [0.0818],\n",
      "        [0.0823],\n",
      "        [0.0846],\n",
      "        [0.1844],\n",
      "        [0.1846],\n",
      "        [0.0882],\n",
      "        [0.0884],\n",
      "        [0.1903],\n",
      "        [0.0917],\n",
      "        [0.0919],\n",
      "        [0.1937],\n",
      "        [0.1989],\n",
      "        [0.2029],\n",
      "        [0.2070],\n",
      "        [0.1082],\n",
      "        [0.1101],\n",
      "        [0.2107],\n",
      "        [0.1138],\n",
      "        [0.1182],\n",
      "        [0.2211],\n",
      "        [0.2214],\n",
      "        [0.1320],\n",
      "        [0.1351],\n",
      "        [0.2365],\n",
      "        [0.1378],\n",
      "        [0.1399],\n",
      "        [0.1431],\n",
      "        [0.1475],\n",
      "        [0.1510],\n",
      "        [0.2532],\n",
      "        [0.2541],\n",
      "        [0.2563],\n",
      "        [0.1583],\n",
      "        [0.2605],\n",
      "        [0.1623],\n",
      "        [0.1649],\n",
      "        [0.1660],\n",
      "        [0.2680],\n",
      "        [0.2693],\n",
      "        [0.2696],\n",
      "        [0.1724],\n",
      "        [0.1725],\n",
      "        [0.1759],\n",
      "        [0.1875],\n",
      "        [0.3011],\n",
      "        [0.2036],\n",
      "        [0.2117],\n",
      "        [0.2250],\n",
      "        [0.2270],\n",
      "        [0.2280],\n",
      "        [0.2344],\n",
      "        [0.2378],\n",
      "        [0.2420],\n",
      "        [0.2467],\n",
      "        [0.2479],\n",
      "        [0.2585],\n",
      "        [0.2624]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 35.793503284454346\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 136\n",
      "剩餘X 資料 torch.Size([24, 18])\n",
      "剩餘Y 資料 torch.Size([24, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.07573801279067993, 1)\n",
      "The second_loss value of k: (0.07751620560884476, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.4237])\n",
      "目前模型的Data狀態 torch.Size([136, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989],\n",
      "        [0.6989]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0474],\n",
      "        [0.0472],\n",
      "        [0.0466],\n",
      "        [0.0597],\n",
      "        [0.0617],\n",
      "        [0.0366],\n",
      "        [0.0353],\n",
      "        [0.0229],\n",
      "        [0.0179],\n",
      "        [0.0172],\n",
      "        [0.0128],\n",
      "        [0.0090],\n",
      "        [0.0089],\n",
      "        [0.0067],\n",
      "        [0.0061],\n",
      "        [0.0049],\n",
      "        [0.0045],\n",
      "        [0.0947],\n",
      "        [0.0023],\n",
      "        [0.0039],\n",
      "        [0.1047],\n",
      "        [0.1054],\n",
      "        [0.0075],\n",
      "        [0.0082],\n",
      "        [0.0096],\n",
      "        [0.1105],\n",
      "        [0.0154],\n",
      "        [0.1156],\n",
      "        [0.1177],\n",
      "        [0.0195],\n",
      "        [0.0217],\n",
      "        [0.1211],\n",
      "        [0.0222],\n",
      "        [0.0269],\n",
      "        [0.0272],\n",
      "        [0.1306],\n",
      "        [0.1307],\n",
      "        [0.1326],\n",
      "        [0.0359],\n",
      "        [0.1366],\n",
      "        [0.1367],\n",
      "        [0.0378],\n",
      "        [0.0392],\n",
      "        [0.0398],\n",
      "        [0.0399],\n",
      "        [0.0404],\n",
      "        [0.1411],\n",
      "        [0.0436],\n",
      "        [0.1436],\n",
      "        [0.1444],\n",
      "        [0.1474],\n",
      "        [0.0497],\n",
      "        [0.1503],\n",
      "        [0.0515],\n",
      "        [0.1507],\n",
      "        [0.0535],\n",
      "        [0.1535],\n",
      "        [0.0550],\n",
      "        [0.0559],\n",
      "        [0.0574],\n",
      "        [0.0577],\n",
      "        [0.0625],\n",
      "        [0.0629],\n",
      "        [0.0636],\n",
      "        [0.1627],\n",
      "        [0.0654],\n",
      "        [0.1646],\n",
      "        [0.0656],\n",
      "        [0.0684],\n",
      "        [0.1685],\n",
      "        [0.0703],\n",
      "        [0.0711],\n",
      "        [0.0750],\n",
      "        [0.0753],\n",
      "        [0.1754],\n",
      "        [0.0773],\n",
      "        [0.0776],\n",
      "        [0.0802],\n",
      "        [0.0818],\n",
      "        [0.0823],\n",
      "        [0.0846],\n",
      "        [0.1844],\n",
      "        [0.1846],\n",
      "        [0.0882],\n",
      "        [0.0884],\n",
      "        [0.1903],\n",
      "        [0.0917],\n",
      "        [0.0919],\n",
      "        [0.1937],\n",
      "        [0.1989],\n",
      "        [0.2029],\n",
      "        [0.2070],\n",
      "        [0.1082],\n",
      "        [0.1101],\n",
      "        [0.2107],\n",
      "        [0.1138],\n",
      "        [0.1182],\n",
      "        [0.2211],\n",
      "        [0.2214],\n",
      "        [0.1320],\n",
      "        [0.1351],\n",
      "        [0.2365],\n",
      "        [0.1378],\n",
      "        [0.1399],\n",
      "        [0.1431],\n",
      "        [0.1475],\n",
      "        [0.1510],\n",
      "        [0.2532],\n",
      "        [0.2541],\n",
      "        [0.2563],\n",
      "        [0.1583],\n",
      "        [0.2605],\n",
      "        [0.1623],\n",
      "        [0.1649],\n",
      "        [0.1660],\n",
      "        [0.2680],\n",
      "        [0.2693],\n",
      "        [0.2696],\n",
      "        [0.1724],\n",
      "        [0.1725],\n",
      "        [0.1759],\n",
      "        [0.1875],\n",
      "        [0.3011],\n",
      "        [0.2036],\n",
      "        [0.2117],\n",
      "        [0.2250],\n",
      "        [0.2270],\n",
      "        [0.2280],\n",
      "        [0.2344],\n",
      "        [0.2378],\n",
      "        [0.2420],\n",
      "        [0.2467],\n",
      "        [0.2479],\n",
      "        [0.2585],\n",
      "        [0.2624],\n",
      "        [0.2752]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0494],\n",
      "        [    0.0493],\n",
      "        [    0.0486],\n",
      "        [    0.0617],\n",
      "        [    0.0637],\n",
      "        [    0.0386],\n",
      "        [    0.0374],\n",
      "        [    0.0249],\n",
      "        [    0.0199],\n",
      "        [    0.0192],\n",
      "        [    0.0149],\n",
      "        [    0.0110],\n",
      "        [    0.0109],\n",
      "        [    0.0088],\n",
      "        [    0.0081],\n",
      "        [    0.0070],\n",
      "        [    0.0065],\n",
      "        [    0.0967],\n",
      "        [    0.0003],\n",
      "        [    0.0019],\n",
      "        [    0.1068],\n",
      "        [    0.1074],\n",
      "        [    0.0055],\n",
      "        [    0.0062],\n",
      "        [    0.0076],\n",
      "        [    0.1126],\n",
      "        [    0.0133],\n",
      "        [    0.1176],\n",
      "        [    0.1198],\n",
      "        [    0.0174],\n",
      "        [    0.0197],\n",
      "        [    0.1231],\n",
      "        [    0.0202],\n",
      "        [    0.0249],\n",
      "        [    0.0252],\n",
      "        [    0.1327],\n",
      "        [    0.1328],\n",
      "        [    0.1346],\n",
      "        [    0.0339],\n",
      "        [    0.1386],\n",
      "        [    0.1388],\n",
      "        [    0.0358],\n",
      "        [    0.0371],\n",
      "        [    0.0377],\n",
      "        [    0.0379],\n",
      "        [    0.0384],\n",
      "        [    0.1431],\n",
      "        [    0.0416],\n",
      "        [    0.1457],\n",
      "        [    0.1464],\n",
      "        [    0.1494],\n",
      "        [    0.0476],\n",
      "        [    0.1523],\n",
      "        [    0.0494],\n",
      "        [    0.1527],\n",
      "        [    0.0515],\n",
      "        [    0.1555],\n",
      "        [    0.0529],\n",
      "        [    0.0539],\n",
      "        [    0.0553],\n",
      "        [    0.0557],\n",
      "        [    0.0605],\n",
      "        [    0.0609],\n",
      "        [    0.0616],\n",
      "        [    0.1648],\n",
      "        [    0.0634],\n",
      "        [    0.1666],\n",
      "        [    0.0635],\n",
      "        [    0.0663],\n",
      "        [    0.1705],\n",
      "        [    0.0683],\n",
      "        [    0.0691],\n",
      "        [    0.0730],\n",
      "        [    0.0733],\n",
      "        [    0.1775],\n",
      "        [    0.0753],\n",
      "        [    0.0755],\n",
      "        [    0.0782],\n",
      "        [    0.0798],\n",
      "        [    0.0803],\n",
      "        [    0.0825],\n",
      "        [    0.1864],\n",
      "        [    0.1866],\n",
      "        [    0.0861],\n",
      "        [    0.0864],\n",
      "        [    0.1923],\n",
      "        [    0.0896],\n",
      "        [    0.0899],\n",
      "        [    0.1957],\n",
      "        [    0.2010],\n",
      "        [    0.2050],\n",
      "        [    0.2090],\n",
      "        [    0.1062],\n",
      "        [    0.1080],\n",
      "        [    0.2127],\n",
      "        [    0.1118],\n",
      "        [    0.1162],\n",
      "        [    0.2231],\n",
      "        [    0.2234],\n",
      "        [    0.1300],\n",
      "        [    0.1331],\n",
      "        [    0.2386],\n",
      "        [    0.1358],\n",
      "        [    0.1379],\n",
      "        [    0.1410],\n",
      "        [    0.1455],\n",
      "        [    0.1489],\n",
      "        [    0.2553],\n",
      "        [    0.2561],\n",
      "        [    0.2584],\n",
      "        [    0.1563],\n",
      "        [    0.2625],\n",
      "        [    0.1603],\n",
      "        [    0.1629],\n",
      "        [    0.1639],\n",
      "        [    0.2701],\n",
      "        [    0.2713],\n",
      "        [    0.2716],\n",
      "        [    0.1704],\n",
      "        [    0.1704],\n",
      "        [    0.1739],\n",
      "        [    0.1855],\n",
      "        [    0.3031],\n",
      "        [    0.2015],\n",
      "        [    0.2097],\n",
      "        [    0.2229],\n",
      "        [    0.2250],\n",
      "        [    0.2259],\n",
      "        [    0.2323],\n",
      "        [    0.2357],\n",
      "        [    0.2400],\n",
      "        [    0.2447],\n",
      "        [    0.2458],\n",
      "        [    0.2565],\n",
      "        [    0.2604],\n",
      "        [    0.2732]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 36.03475594520569\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 137\n",
      "剩餘X 資料 torch.Size([23, 18])\n",
      "剩餘Y 資料 torch.Size([23, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.07639563828706741, 0)\n",
      "The second_loss value of k: (0.08876878768205643, 17)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.4205])\n",
      "目前模型的Data狀態 torch.Size([137, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969],\n",
      "        [0.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0494],\n",
      "        [    0.0493],\n",
      "        [    0.0486],\n",
      "        [    0.0617],\n",
      "        [    0.0637],\n",
      "        [    0.0386],\n",
      "        [    0.0374],\n",
      "        [    0.0249],\n",
      "        [    0.0199],\n",
      "        [    0.0192],\n",
      "        [    0.0149],\n",
      "        [    0.0110],\n",
      "        [    0.0109],\n",
      "        [    0.0088],\n",
      "        [    0.0081],\n",
      "        [    0.0070],\n",
      "        [    0.0065],\n",
      "        [    0.0967],\n",
      "        [    0.0003],\n",
      "        [    0.0019],\n",
      "        [    0.1068],\n",
      "        [    0.1074],\n",
      "        [    0.0055],\n",
      "        [    0.0062],\n",
      "        [    0.0076],\n",
      "        [    0.1126],\n",
      "        [    0.0133],\n",
      "        [    0.1176],\n",
      "        [    0.1198],\n",
      "        [    0.0174],\n",
      "        [    0.0197],\n",
      "        [    0.1231],\n",
      "        [    0.0202],\n",
      "        [    0.0249],\n",
      "        [    0.0252],\n",
      "        [    0.1327],\n",
      "        [    0.1328],\n",
      "        [    0.1346],\n",
      "        [    0.0339],\n",
      "        [    0.1386],\n",
      "        [    0.1388],\n",
      "        [    0.0358],\n",
      "        [    0.0371],\n",
      "        [    0.0377],\n",
      "        [    0.0379],\n",
      "        [    0.0384],\n",
      "        [    0.1431],\n",
      "        [    0.0416],\n",
      "        [    0.1457],\n",
      "        [    0.1464],\n",
      "        [    0.1494],\n",
      "        [    0.0476],\n",
      "        [    0.1523],\n",
      "        [    0.0494],\n",
      "        [    0.1527],\n",
      "        [    0.0515],\n",
      "        [    0.1555],\n",
      "        [    0.0529],\n",
      "        [    0.0539],\n",
      "        [    0.0553],\n",
      "        [    0.0557],\n",
      "        [    0.0605],\n",
      "        [    0.0609],\n",
      "        [    0.0616],\n",
      "        [    0.1648],\n",
      "        [    0.0634],\n",
      "        [    0.1666],\n",
      "        [    0.0635],\n",
      "        [    0.0663],\n",
      "        [    0.1705],\n",
      "        [    0.0683],\n",
      "        [    0.0691],\n",
      "        [    0.0730],\n",
      "        [    0.0733],\n",
      "        [    0.1775],\n",
      "        [    0.0753],\n",
      "        [    0.0755],\n",
      "        [    0.0782],\n",
      "        [    0.0798],\n",
      "        [    0.0803],\n",
      "        [    0.0825],\n",
      "        [    0.1864],\n",
      "        [    0.1866],\n",
      "        [    0.0861],\n",
      "        [    0.0864],\n",
      "        [    0.1923],\n",
      "        [    0.0896],\n",
      "        [    0.0899],\n",
      "        [    0.1957],\n",
      "        [    0.2010],\n",
      "        [    0.2050],\n",
      "        [    0.2090],\n",
      "        [    0.1062],\n",
      "        [    0.1080],\n",
      "        [    0.2127],\n",
      "        [    0.1118],\n",
      "        [    0.1162],\n",
      "        [    0.2231],\n",
      "        [    0.2234],\n",
      "        [    0.1300],\n",
      "        [    0.1331],\n",
      "        [    0.2386],\n",
      "        [    0.1358],\n",
      "        [    0.1379],\n",
      "        [    0.1410],\n",
      "        [    0.1455],\n",
      "        [    0.1489],\n",
      "        [    0.2553],\n",
      "        [    0.2561],\n",
      "        [    0.2584],\n",
      "        [    0.1563],\n",
      "        [    0.2625],\n",
      "        [    0.1603],\n",
      "        [    0.1629],\n",
      "        [    0.1639],\n",
      "        [    0.2701],\n",
      "        [    0.2713],\n",
      "        [    0.2716],\n",
      "        [    0.1704],\n",
      "        [    0.1704],\n",
      "        [    0.1739],\n",
      "        [    0.1855],\n",
      "        [    0.3031],\n",
      "        [    0.2015],\n",
      "        [    0.2097],\n",
      "        [    0.2229],\n",
      "        [    0.2250],\n",
      "        [    0.2259],\n",
      "        [    0.2323],\n",
      "        [    0.2357],\n",
      "        [    0.2400],\n",
      "        [    0.2447],\n",
      "        [    0.2458],\n",
      "        [    0.2565],\n",
      "        [    0.2604],\n",
      "        [    0.2732],\n",
      "        [    0.2764]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 85\n",
      "Number of shrink: 15\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0514],\n",
      "        [    0.0513],\n",
      "        [    0.0506],\n",
      "        [    0.0637],\n",
      "        [    0.0657],\n",
      "        [    0.0406],\n",
      "        [    0.0394],\n",
      "        [    0.0269],\n",
      "        [    0.0219],\n",
      "        [    0.0212],\n",
      "        [    0.0169],\n",
      "        [    0.0130],\n",
      "        [    0.0129],\n",
      "        [    0.0108],\n",
      "        [    0.0101],\n",
      "        [    0.0090],\n",
      "        [    0.0085],\n",
      "        [    0.0987],\n",
      "        [    0.0017],\n",
      "        [    0.0002],\n",
      "        [    0.1088],\n",
      "        [    0.1094],\n",
      "        [    0.0035],\n",
      "        [    0.0042],\n",
      "        [    0.0056],\n",
      "        [    0.1146],\n",
      "        [    0.0113],\n",
      "        [    0.1196],\n",
      "        [    0.1218],\n",
      "        [    0.0154],\n",
      "        [    0.0177],\n",
      "        [    0.1251],\n",
      "        [    0.0181],\n",
      "        [    0.0228],\n",
      "        [    0.0232],\n",
      "        [    0.1347],\n",
      "        [    0.1348],\n",
      "        [    0.1367],\n",
      "        [    0.0319],\n",
      "        [    0.1406],\n",
      "        [    0.1408],\n",
      "        [    0.0338],\n",
      "        [    0.0351],\n",
      "        [    0.0357],\n",
      "        [    0.0359],\n",
      "        [    0.0364],\n",
      "        [    0.1452],\n",
      "        [    0.0396],\n",
      "        [    0.1477],\n",
      "        [    0.1484],\n",
      "        [    0.1514],\n",
      "        [    0.0456],\n",
      "        [    0.1543],\n",
      "        [    0.0474],\n",
      "        [    0.1547],\n",
      "        [    0.0495],\n",
      "        [    0.1575],\n",
      "        [    0.0509],\n",
      "        [    0.0519],\n",
      "        [    0.0533],\n",
      "        [    0.0537],\n",
      "        [    0.0585],\n",
      "        [    0.0589],\n",
      "        [    0.0595],\n",
      "        [    0.1668],\n",
      "        [    0.0614],\n",
      "        [    0.1687],\n",
      "        [    0.0615],\n",
      "        [    0.0643],\n",
      "        [    0.1725],\n",
      "        [    0.0662],\n",
      "        [    0.0671],\n",
      "        [    0.0710],\n",
      "        [    0.0713],\n",
      "        [    0.1795],\n",
      "        [    0.0733],\n",
      "        [    0.0735],\n",
      "        [    0.0762],\n",
      "        [    0.0778],\n",
      "        [    0.0783],\n",
      "        [    0.0805],\n",
      "        [    0.1884],\n",
      "        [    0.1886],\n",
      "        [    0.0841],\n",
      "        [    0.0844],\n",
      "        [    0.1944],\n",
      "        [    0.0876],\n",
      "        [    0.0879],\n",
      "        [    0.1977],\n",
      "        [    0.2030],\n",
      "        [    0.2070],\n",
      "        [    0.2110],\n",
      "        [    0.1042],\n",
      "        [    0.1060],\n",
      "        [    0.2147],\n",
      "        [    0.1098],\n",
      "        [    0.1142],\n",
      "        [    0.2251],\n",
      "        [    0.2254],\n",
      "        [    0.1280],\n",
      "        [    0.1311],\n",
      "        [    0.2406],\n",
      "        [    0.1338],\n",
      "        [    0.1359],\n",
      "        [    0.1390],\n",
      "        [    0.1434],\n",
      "        [    0.1469],\n",
      "        [    0.2573],\n",
      "        [    0.2581],\n",
      "        [    0.2604],\n",
      "        [    0.1543],\n",
      "        [    0.2645],\n",
      "        [    0.1583],\n",
      "        [    0.1609],\n",
      "        [    0.1619],\n",
      "        [    0.2721],\n",
      "        [    0.2733],\n",
      "        [    0.2736],\n",
      "        [    0.1684],\n",
      "        [    0.1684],\n",
      "        [    0.1718],\n",
      "        [    0.1835],\n",
      "        [    0.3051],\n",
      "        [    0.1995],\n",
      "        [    0.2076],\n",
      "        [    0.2209],\n",
      "        [    0.2230],\n",
      "        [    0.2239],\n",
      "        [    0.2303],\n",
      "        [    0.2337],\n",
      "        [    0.2380],\n",
      "        [    0.2427],\n",
      "        [    0.2438],\n",
      "        [    0.2545],\n",
      "        [    0.2584],\n",
      "        [    0.2712],\n",
      "        [    0.2744]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 36.273109912872314\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 138\n",
      "剩餘X 資料 torch.Size([22, 18])\n",
      "剩餘Y 資料 torch.Size([22, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.08757630735635757, 16)\n",
      "The second_loss value of k: (0.09227430075407028, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.3989])\n",
      "目前模型的Data狀態 torch.Size([138, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949],\n",
      "        [0.6949]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0514],\n",
      "        [    0.0513],\n",
      "        [    0.0506],\n",
      "        [    0.0637],\n",
      "        [    0.0657],\n",
      "        [    0.0406],\n",
      "        [    0.0394],\n",
      "        [    0.0269],\n",
      "        [    0.0219],\n",
      "        [    0.0212],\n",
      "        [    0.0169],\n",
      "        [    0.0130],\n",
      "        [    0.0129],\n",
      "        [    0.0108],\n",
      "        [    0.0101],\n",
      "        [    0.0090],\n",
      "        [    0.0085],\n",
      "        [    0.0987],\n",
      "        [    0.0017],\n",
      "        [    0.0002],\n",
      "        [    0.1088],\n",
      "        [    0.1094],\n",
      "        [    0.0035],\n",
      "        [    0.0042],\n",
      "        [    0.0056],\n",
      "        [    0.1146],\n",
      "        [    0.0113],\n",
      "        [    0.1196],\n",
      "        [    0.1218],\n",
      "        [    0.0154],\n",
      "        [    0.0177],\n",
      "        [    0.1251],\n",
      "        [    0.0181],\n",
      "        [    0.0228],\n",
      "        [    0.0232],\n",
      "        [    0.1347],\n",
      "        [    0.1348],\n",
      "        [    0.1367],\n",
      "        [    0.0319],\n",
      "        [    0.1406],\n",
      "        [    0.1408],\n",
      "        [    0.0338],\n",
      "        [    0.0351],\n",
      "        [    0.0357],\n",
      "        [    0.0359],\n",
      "        [    0.0364],\n",
      "        [    0.1452],\n",
      "        [    0.0396],\n",
      "        [    0.1477],\n",
      "        [    0.1484],\n",
      "        [    0.1514],\n",
      "        [    0.0456],\n",
      "        [    0.1543],\n",
      "        [    0.0474],\n",
      "        [    0.1547],\n",
      "        [    0.0495],\n",
      "        [    0.1575],\n",
      "        [    0.0509],\n",
      "        [    0.0519],\n",
      "        [    0.0533],\n",
      "        [    0.0537],\n",
      "        [    0.0585],\n",
      "        [    0.0589],\n",
      "        [    0.0595],\n",
      "        [    0.1668],\n",
      "        [    0.0614],\n",
      "        [    0.1687],\n",
      "        [    0.0615],\n",
      "        [    0.0643],\n",
      "        [    0.1725],\n",
      "        [    0.0662],\n",
      "        [    0.0671],\n",
      "        [    0.0710],\n",
      "        [    0.0713],\n",
      "        [    0.1795],\n",
      "        [    0.0733],\n",
      "        [    0.0735],\n",
      "        [    0.0762],\n",
      "        [    0.0778],\n",
      "        [    0.0783],\n",
      "        [    0.0805],\n",
      "        [    0.1884],\n",
      "        [    0.1886],\n",
      "        [    0.0841],\n",
      "        [    0.0844],\n",
      "        [    0.1944],\n",
      "        [    0.0876],\n",
      "        [    0.0879],\n",
      "        [    0.1977],\n",
      "        [    0.2030],\n",
      "        [    0.2070],\n",
      "        [    0.2110],\n",
      "        [    0.1042],\n",
      "        [    0.1060],\n",
      "        [    0.2147],\n",
      "        [    0.1098],\n",
      "        [    0.1142],\n",
      "        [    0.2251],\n",
      "        [    0.2254],\n",
      "        [    0.1280],\n",
      "        [    0.1311],\n",
      "        [    0.2406],\n",
      "        [    0.1338],\n",
      "        [    0.1359],\n",
      "        [    0.1390],\n",
      "        [    0.1434],\n",
      "        [    0.1469],\n",
      "        [    0.2573],\n",
      "        [    0.2581],\n",
      "        [    0.2604],\n",
      "        [    0.1543],\n",
      "        [    0.2645],\n",
      "        [    0.1583],\n",
      "        [    0.1609],\n",
      "        [    0.1619],\n",
      "        [    0.2721],\n",
      "        [    0.2733],\n",
      "        [    0.2736],\n",
      "        [    0.1684],\n",
      "        [    0.1684],\n",
      "        [    0.1718],\n",
      "        [    0.1835],\n",
      "        [    0.3051],\n",
      "        [    0.1995],\n",
      "        [    0.2076],\n",
      "        [    0.2209],\n",
      "        [    0.2230],\n",
      "        [    0.2239],\n",
      "        [    0.2303],\n",
      "        [    0.2337],\n",
      "        [    0.2380],\n",
      "        [    0.2427],\n",
      "        [    0.2438],\n",
      "        [    0.2545],\n",
      "        [    0.2584],\n",
      "        [    0.2712],\n",
      "        [    0.2744],\n",
      "        [    0.2959]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0536],\n",
      "        [0.0534],\n",
      "        [0.0528],\n",
      "        [0.0659],\n",
      "        [0.0679],\n",
      "        [0.0428],\n",
      "        [0.0415],\n",
      "        [0.0290],\n",
      "        [0.0240],\n",
      "        [0.0234],\n",
      "        [0.0190],\n",
      "        [0.0152],\n",
      "        [0.0150],\n",
      "        [0.0129],\n",
      "        [0.0123],\n",
      "        [0.0111],\n",
      "        [0.0106],\n",
      "        [0.1009],\n",
      "        [0.0039],\n",
      "        [0.0023],\n",
      "        [0.1109],\n",
      "        [0.1115],\n",
      "        [0.0013],\n",
      "        [0.0020],\n",
      "        [0.0035],\n",
      "        [0.1167],\n",
      "        [0.0092],\n",
      "        [0.1217],\n",
      "        [0.1239],\n",
      "        [0.0133],\n",
      "        [0.0156],\n",
      "        [0.1272],\n",
      "        [0.0160],\n",
      "        [0.0207],\n",
      "        [0.0210],\n",
      "        [0.1368],\n",
      "        [0.1369],\n",
      "        [0.1388],\n",
      "        [0.0297],\n",
      "        [0.1428],\n",
      "        [0.1429],\n",
      "        [0.0317],\n",
      "        [0.0330],\n",
      "        [0.0336],\n",
      "        [0.0337],\n",
      "        [0.0343],\n",
      "        [0.1473],\n",
      "        [0.0374],\n",
      "        [0.1498],\n",
      "        [0.1506],\n",
      "        [0.1535],\n",
      "        [0.0435],\n",
      "        [0.1565],\n",
      "        [0.0453],\n",
      "        [0.1569],\n",
      "        [0.0473],\n",
      "        [0.1597],\n",
      "        [0.0488],\n",
      "        [0.0498],\n",
      "        [0.0512],\n",
      "        [0.0516],\n",
      "        [0.0563],\n",
      "        [0.0567],\n",
      "        [0.0574],\n",
      "        [0.1689],\n",
      "        [0.0592],\n",
      "        [0.1708],\n",
      "        [0.0594],\n",
      "        [0.0622],\n",
      "        [0.1747],\n",
      "        [0.0641],\n",
      "        [0.0650],\n",
      "        [0.0689],\n",
      "        [0.0692],\n",
      "        [0.1816],\n",
      "        [0.0711],\n",
      "        [0.0714],\n",
      "        [0.0740],\n",
      "        [0.0757],\n",
      "        [0.0762],\n",
      "        [0.0784],\n",
      "        [0.1906],\n",
      "        [0.1907],\n",
      "        [0.0820],\n",
      "        [0.0822],\n",
      "        [0.1965],\n",
      "        [0.0855],\n",
      "        [0.0858],\n",
      "        [0.1998],\n",
      "        [0.2051],\n",
      "        [0.2091],\n",
      "        [0.2132],\n",
      "        [0.1021],\n",
      "        [0.1039],\n",
      "        [0.2169],\n",
      "        [0.1076],\n",
      "        [0.1120],\n",
      "        [0.2272],\n",
      "        [0.2276],\n",
      "        [0.1258],\n",
      "        [0.1290],\n",
      "        [0.2427],\n",
      "        [0.1317],\n",
      "        [0.1337],\n",
      "        [0.1369],\n",
      "        [0.1413],\n",
      "        [0.1448],\n",
      "        [0.2594],\n",
      "        [0.2603],\n",
      "        [0.2625],\n",
      "        [0.1522],\n",
      "        [0.2667],\n",
      "        [0.1561],\n",
      "        [0.1587],\n",
      "        [0.1598],\n",
      "        [0.2742],\n",
      "        [0.2754],\n",
      "        [0.2757],\n",
      "        [0.1662],\n",
      "        [0.1663],\n",
      "        [0.1697],\n",
      "        [0.1814],\n",
      "        [0.3073],\n",
      "        [0.1974],\n",
      "        [0.2055],\n",
      "        [0.2188],\n",
      "        [0.2208],\n",
      "        [0.2218],\n",
      "        [0.2282],\n",
      "        [0.2316],\n",
      "        [0.2359],\n",
      "        [0.2406],\n",
      "        [0.2417],\n",
      "        [0.2523],\n",
      "        [0.2562],\n",
      "        [0.2690],\n",
      "        [0.2722],\n",
      "        [0.2938]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 36.51270031929016\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 139\n",
      "剩餘X 資料 torch.Size([21, 18])\n",
      "剩餘Y 資料 torch.Size([21, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.09097614139318466, 15)\n",
      "The second_loss value of k: (0.107753224670887, 16)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.3911])\n",
      "目前模型的Data狀態 torch.Size([139, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927],\n",
      "        [0.6927]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0536],\n",
      "        [0.0534],\n",
      "        [0.0528],\n",
      "        [0.0659],\n",
      "        [0.0679],\n",
      "        [0.0428],\n",
      "        [0.0415],\n",
      "        [0.0290],\n",
      "        [0.0240],\n",
      "        [0.0234],\n",
      "        [0.0190],\n",
      "        [0.0152],\n",
      "        [0.0150],\n",
      "        [0.0129],\n",
      "        [0.0123],\n",
      "        [0.0111],\n",
      "        [0.0106],\n",
      "        [0.1009],\n",
      "        [0.0039],\n",
      "        [0.0023],\n",
      "        [0.1109],\n",
      "        [0.1115],\n",
      "        [0.0013],\n",
      "        [0.0020],\n",
      "        [0.0035],\n",
      "        [0.1167],\n",
      "        [0.0092],\n",
      "        [0.1217],\n",
      "        [0.1239],\n",
      "        [0.0133],\n",
      "        [0.0156],\n",
      "        [0.1272],\n",
      "        [0.0160],\n",
      "        [0.0207],\n",
      "        [0.0210],\n",
      "        [0.1368],\n",
      "        [0.1369],\n",
      "        [0.1388],\n",
      "        [0.0297],\n",
      "        [0.1428],\n",
      "        [0.1429],\n",
      "        [0.0317],\n",
      "        [0.0330],\n",
      "        [0.0336],\n",
      "        [0.0337],\n",
      "        [0.0343],\n",
      "        [0.1473],\n",
      "        [0.0374],\n",
      "        [0.1498],\n",
      "        [0.1506],\n",
      "        [0.1535],\n",
      "        [0.0435],\n",
      "        [0.1565],\n",
      "        [0.0453],\n",
      "        [0.1569],\n",
      "        [0.0473],\n",
      "        [0.1597],\n",
      "        [0.0488],\n",
      "        [0.0498],\n",
      "        [0.0512],\n",
      "        [0.0516],\n",
      "        [0.0563],\n",
      "        [0.0567],\n",
      "        [0.0574],\n",
      "        [0.1689],\n",
      "        [0.0592],\n",
      "        [0.1708],\n",
      "        [0.0594],\n",
      "        [0.0622],\n",
      "        [0.1747],\n",
      "        [0.0641],\n",
      "        [0.0650],\n",
      "        [0.0689],\n",
      "        [0.0692],\n",
      "        [0.1816],\n",
      "        [0.0711],\n",
      "        [0.0714],\n",
      "        [0.0740],\n",
      "        [0.0757],\n",
      "        [0.0762],\n",
      "        [0.0784],\n",
      "        [0.1906],\n",
      "        [0.1907],\n",
      "        [0.0820],\n",
      "        [0.0822],\n",
      "        [0.1965],\n",
      "        [0.0855],\n",
      "        [0.0858],\n",
      "        [0.1998],\n",
      "        [0.2051],\n",
      "        [0.2091],\n",
      "        [0.2132],\n",
      "        [0.1021],\n",
      "        [0.1039],\n",
      "        [0.2169],\n",
      "        [0.1076],\n",
      "        [0.1120],\n",
      "        [0.2272],\n",
      "        [0.2276],\n",
      "        [0.1258],\n",
      "        [0.1290],\n",
      "        [0.2427],\n",
      "        [0.1317],\n",
      "        [0.1337],\n",
      "        [0.1369],\n",
      "        [0.1413],\n",
      "        [0.1448],\n",
      "        [0.2594],\n",
      "        [0.2603],\n",
      "        [0.2625],\n",
      "        [0.1522],\n",
      "        [0.2667],\n",
      "        [0.1561],\n",
      "        [0.1587],\n",
      "        [0.1598],\n",
      "        [0.2742],\n",
      "        [0.2754],\n",
      "        [0.2757],\n",
      "        [0.1662],\n",
      "        [0.1663],\n",
      "        [0.1697],\n",
      "        [0.1814],\n",
      "        [0.3073],\n",
      "        [0.1974],\n",
      "        [0.2055],\n",
      "        [0.2188],\n",
      "        [0.2208],\n",
      "        [0.2218],\n",
      "        [0.2282],\n",
      "        [0.2316],\n",
      "        [0.2359],\n",
      "        [0.2406],\n",
      "        [0.2417],\n",
      "        [0.2523],\n",
      "        [0.2562],\n",
      "        [0.2690],\n",
      "        [0.2722],\n",
      "        [0.2938],\n",
      "        [0.3016]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0557],\n",
      "        [    0.0556],\n",
      "        [    0.0550],\n",
      "        [    0.0680],\n",
      "        [    0.0700],\n",
      "        [    0.0449],\n",
      "        [    0.0437],\n",
      "        [    0.0312],\n",
      "        [    0.0262],\n",
      "        [    0.0255],\n",
      "        [    0.0212],\n",
      "        [    0.0174],\n",
      "        [    0.0172],\n",
      "        [    0.0151],\n",
      "        [    0.0145],\n",
      "        [    0.0133],\n",
      "        [    0.0128],\n",
      "        [    0.1031],\n",
      "        [    0.0060],\n",
      "        [    0.0045],\n",
      "        [    0.1131],\n",
      "        [    0.1137],\n",
      "        [    0.0009],\n",
      "        [    0.0002],\n",
      "        [    0.0013],\n",
      "        [    0.1189],\n",
      "        [    0.0070],\n",
      "        [    0.1239],\n",
      "        [    0.1261],\n",
      "        [    0.0111],\n",
      "        [    0.0134],\n",
      "        [    0.1294],\n",
      "        [    0.0138],\n",
      "        [    0.0185],\n",
      "        [    0.0188],\n",
      "        [    0.1390],\n",
      "        [    0.1391],\n",
      "        [    0.1410],\n",
      "        [    0.0275],\n",
      "        [    0.1449],\n",
      "        [    0.1451],\n",
      "        [    0.0295],\n",
      "        [    0.0308],\n",
      "        [    0.0314],\n",
      "        [    0.0316],\n",
      "        [    0.0321],\n",
      "        [    0.1495],\n",
      "        [    0.0353],\n",
      "        [    0.1520],\n",
      "        [    0.1527],\n",
      "        [    0.1557],\n",
      "        [    0.0413],\n",
      "        [    0.1586],\n",
      "        [    0.0431],\n",
      "        [    0.1590],\n",
      "        [    0.0452],\n",
      "        [    0.1618],\n",
      "        [    0.0466],\n",
      "        [    0.0476],\n",
      "        [    0.0490],\n",
      "        [    0.0494],\n",
      "        [    0.0542],\n",
      "        [    0.0546],\n",
      "        [    0.0552],\n",
      "        [    0.1711],\n",
      "        [    0.0571],\n",
      "        [    0.1730],\n",
      "        [    0.0572],\n",
      "        [    0.0600],\n",
      "        [    0.1768],\n",
      "        [    0.0619],\n",
      "        [    0.0628],\n",
      "        [    0.0667],\n",
      "        [    0.0670],\n",
      "        [    0.1838],\n",
      "        [    0.0690],\n",
      "        [    0.0692],\n",
      "        [    0.0718],\n",
      "        [    0.0735],\n",
      "        [    0.0740],\n",
      "        [    0.0762],\n",
      "        [    0.1928],\n",
      "        [    0.1929],\n",
      "        [    0.0798],\n",
      "        [    0.0801],\n",
      "        [    0.1987],\n",
      "        [    0.0833],\n",
      "        [    0.0836],\n",
      "        [    0.2020],\n",
      "        [    0.2073],\n",
      "        [    0.2113],\n",
      "        [    0.2154],\n",
      "        [    0.0999],\n",
      "        [    0.1017],\n",
      "        [    0.2190],\n",
      "        [    0.1054],\n",
      "        [    0.1099],\n",
      "        [    0.2294],\n",
      "        [    0.2297],\n",
      "        [    0.1237],\n",
      "        [    0.1268],\n",
      "        [    0.2449],\n",
      "        [    0.1295],\n",
      "        [    0.1316],\n",
      "        [    0.1347],\n",
      "        [    0.1391],\n",
      "        [    0.1426],\n",
      "        [    0.2616],\n",
      "        [    0.2624],\n",
      "        [    0.2647],\n",
      "        [    0.1500],\n",
      "        [    0.2688],\n",
      "        [    0.1539],\n",
      "        [    0.1566],\n",
      "        [    0.1576],\n",
      "        [    0.2764],\n",
      "        [    0.2776],\n",
      "        [    0.2779],\n",
      "        [    0.1640],\n",
      "        [    0.1641],\n",
      "        [    0.1675],\n",
      "        [    0.1792],\n",
      "        [    0.3094],\n",
      "        [    0.1952],\n",
      "        [    0.2033],\n",
      "        [    0.2166],\n",
      "        [    0.2186],\n",
      "        [    0.2196],\n",
      "        [    0.2260],\n",
      "        [    0.2294],\n",
      "        [    0.2337],\n",
      "        [    0.2384],\n",
      "        [    0.2395],\n",
      "        [    0.2501],\n",
      "        [    0.2541],\n",
      "        [    0.2669],\n",
      "        [    0.2701],\n",
      "        [    0.2916],\n",
      "        [    0.2995]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 36.751243352890015\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 140\n",
      "剩餘X 資料 torch.Size([20, 18])\n",
      "剩餘Y 資料 torch.Size([20, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10633331537246704, 15)\n",
      "The second_loss value of k: (0.10699853301048279, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.3645])\n",
      "目前模型的Data狀態 torch.Size([140, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906],\n",
      "        [0.6906]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0557],\n",
      "        [    0.0556],\n",
      "        [    0.0550],\n",
      "        [    0.0680],\n",
      "        [    0.0700],\n",
      "        [    0.0449],\n",
      "        [    0.0437],\n",
      "        [    0.0312],\n",
      "        [    0.0262],\n",
      "        [    0.0255],\n",
      "        [    0.0212],\n",
      "        [    0.0174],\n",
      "        [    0.0172],\n",
      "        [    0.0151],\n",
      "        [    0.0145],\n",
      "        [    0.0133],\n",
      "        [    0.0128],\n",
      "        [    0.1031],\n",
      "        [    0.0060],\n",
      "        [    0.0045],\n",
      "        [    0.1131],\n",
      "        [    0.1137],\n",
      "        [    0.0009],\n",
      "        [    0.0002],\n",
      "        [    0.0013],\n",
      "        [    0.1189],\n",
      "        [    0.0070],\n",
      "        [    0.1239],\n",
      "        [    0.1261],\n",
      "        [    0.0111],\n",
      "        [    0.0134],\n",
      "        [    0.1294],\n",
      "        [    0.0138],\n",
      "        [    0.0185],\n",
      "        [    0.0188],\n",
      "        [    0.1390],\n",
      "        [    0.1391],\n",
      "        [    0.1410],\n",
      "        [    0.0275],\n",
      "        [    0.1449],\n",
      "        [    0.1451],\n",
      "        [    0.0295],\n",
      "        [    0.0308],\n",
      "        [    0.0314],\n",
      "        [    0.0316],\n",
      "        [    0.0321],\n",
      "        [    0.1495],\n",
      "        [    0.0353],\n",
      "        [    0.1520],\n",
      "        [    0.1527],\n",
      "        [    0.1557],\n",
      "        [    0.0413],\n",
      "        [    0.1586],\n",
      "        [    0.0431],\n",
      "        [    0.1590],\n",
      "        [    0.0452],\n",
      "        [    0.1618],\n",
      "        [    0.0466],\n",
      "        [    0.0476],\n",
      "        [    0.0490],\n",
      "        [    0.0494],\n",
      "        [    0.0542],\n",
      "        [    0.0546],\n",
      "        [    0.0552],\n",
      "        [    0.1711],\n",
      "        [    0.0571],\n",
      "        [    0.1730],\n",
      "        [    0.0572],\n",
      "        [    0.0600],\n",
      "        [    0.1768],\n",
      "        [    0.0619],\n",
      "        [    0.0628],\n",
      "        [    0.0667],\n",
      "        [    0.0670],\n",
      "        [    0.1838],\n",
      "        [    0.0690],\n",
      "        [    0.0692],\n",
      "        [    0.0718],\n",
      "        [    0.0735],\n",
      "        [    0.0740],\n",
      "        [    0.0762],\n",
      "        [    0.1928],\n",
      "        [    0.1929],\n",
      "        [    0.0798],\n",
      "        [    0.0801],\n",
      "        [    0.1987],\n",
      "        [    0.0833],\n",
      "        [    0.0836],\n",
      "        [    0.2020],\n",
      "        [    0.2073],\n",
      "        [    0.2113],\n",
      "        [    0.2154],\n",
      "        [    0.0999],\n",
      "        [    0.1017],\n",
      "        [    0.2190],\n",
      "        [    0.1054],\n",
      "        [    0.1099],\n",
      "        [    0.2294],\n",
      "        [    0.2297],\n",
      "        [    0.1237],\n",
      "        [    0.1268],\n",
      "        [    0.2449],\n",
      "        [    0.1295],\n",
      "        [    0.1316],\n",
      "        [    0.1347],\n",
      "        [    0.1391],\n",
      "        [    0.1426],\n",
      "        [    0.2616],\n",
      "        [    0.2624],\n",
      "        [    0.2647],\n",
      "        [    0.1500],\n",
      "        [    0.2688],\n",
      "        [    0.1539],\n",
      "        [    0.1566],\n",
      "        [    0.1576],\n",
      "        [    0.2764],\n",
      "        [    0.2776],\n",
      "        [    0.2779],\n",
      "        [    0.1640],\n",
      "        [    0.1641],\n",
      "        [    0.1675],\n",
      "        [    0.1792],\n",
      "        [    0.3094],\n",
      "        [    0.1952],\n",
      "        [    0.2033],\n",
      "        [    0.2166],\n",
      "        [    0.2186],\n",
      "        [    0.2196],\n",
      "        [    0.2260],\n",
      "        [    0.2294],\n",
      "        [    0.2337],\n",
      "        [    0.2384],\n",
      "        [    0.2395],\n",
      "        [    0.2501],\n",
      "        [    0.2541],\n",
      "        [    0.2669],\n",
      "        [    0.2701],\n",
      "        [    0.2916],\n",
      "        [    0.2995],\n",
      "        [    0.3261]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 81\n",
      "Number of shrink: 19\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0581],\n",
      "        [0.0579],\n",
      "        [0.0573],\n",
      "        [0.0704],\n",
      "        [0.0724],\n",
      "        [0.0473],\n",
      "        [0.0460],\n",
      "        [0.0335],\n",
      "        [0.0285],\n",
      "        [0.0279],\n",
      "        [0.0235],\n",
      "        [0.0197],\n",
      "        [0.0195],\n",
      "        [0.0174],\n",
      "        [0.0168],\n",
      "        [0.0156],\n",
      "        [0.0151],\n",
      "        [0.1054],\n",
      "        [0.0084],\n",
      "        [0.0068],\n",
      "        [0.1154],\n",
      "        [0.1160],\n",
      "        [0.0032],\n",
      "        [0.0025],\n",
      "        [0.0010],\n",
      "        [0.1212],\n",
      "        [0.0047],\n",
      "        [0.1262],\n",
      "        [0.1284],\n",
      "        [0.0088],\n",
      "        [0.0111],\n",
      "        [0.1317],\n",
      "        [0.0115],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.1413],\n",
      "        [0.1414],\n",
      "        [0.1433],\n",
      "        [0.0252],\n",
      "        [0.1473],\n",
      "        [0.1474],\n",
      "        [0.0272],\n",
      "        [0.0285],\n",
      "        [0.0291],\n",
      "        [0.0292],\n",
      "        [0.0298],\n",
      "        [0.1518],\n",
      "        [0.0329],\n",
      "        [0.1543],\n",
      "        [0.1551],\n",
      "        [0.1580],\n",
      "        [0.0390],\n",
      "        [0.1610],\n",
      "        [0.0408],\n",
      "        [0.1614],\n",
      "        [0.0428],\n",
      "        [0.1642],\n",
      "        [0.0443],\n",
      "        [0.0453],\n",
      "        [0.0467],\n",
      "        [0.0471],\n",
      "        [0.0518],\n",
      "        [0.0522],\n",
      "        [0.0529],\n",
      "        [0.1734],\n",
      "        [0.0547],\n",
      "        [0.1753],\n",
      "        [0.0549],\n",
      "        [0.0577],\n",
      "        [0.1792],\n",
      "        [0.0596],\n",
      "        [0.0605],\n",
      "        [0.0644],\n",
      "        [0.0647],\n",
      "        [0.1861],\n",
      "        [0.0666],\n",
      "        [0.0669],\n",
      "        [0.0695],\n",
      "        [0.0712],\n",
      "        [0.0717],\n",
      "        [0.0739],\n",
      "        [0.1951],\n",
      "        [0.1952],\n",
      "        [0.0775],\n",
      "        [0.0777],\n",
      "        [0.2010],\n",
      "        [0.0810],\n",
      "        [0.0813],\n",
      "        [0.2043],\n",
      "        [0.2096],\n",
      "        [0.2136],\n",
      "        [0.2177],\n",
      "        [0.0976],\n",
      "        [0.0994],\n",
      "        [0.2214],\n",
      "        [0.1031],\n",
      "        [0.1075],\n",
      "        [0.2317],\n",
      "        [0.2321],\n",
      "        [0.1213],\n",
      "        [0.1245],\n",
      "        [0.2472],\n",
      "        [0.1272],\n",
      "        [0.1292],\n",
      "        [0.1324],\n",
      "        [0.1368],\n",
      "        [0.1403],\n",
      "        [0.2639],\n",
      "        [0.2648],\n",
      "        [0.2670],\n",
      "        [0.1477],\n",
      "        [0.2711],\n",
      "        [0.1516],\n",
      "        [0.1542],\n",
      "        [0.1553],\n",
      "        [0.2787],\n",
      "        [0.2799],\n",
      "        [0.2802],\n",
      "        [0.1617],\n",
      "        [0.1618],\n",
      "        [0.1652],\n",
      "        [0.1769],\n",
      "        [0.3118],\n",
      "        [0.1929],\n",
      "        [0.2010],\n",
      "        [0.2143],\n",
      "        [0.2163],\n",
      "        [0.2173],\n",
      "        [0.2237],\n",
      "        [0.2271],\n",
      "        [0.2314],\n",
      "        [0.2361],\n",
      "        [0.2372],\n",
      "        [0.2478],\n",
      "        [0.2517],\n",
      "        [0.2645],\n",
      "        [0.2677],\n",
      "        [0.2893],\n",
      "        [0.2971],\n",
      "        [0.3238]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 36.98780608177185\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 141\n",
      "剩餘X 資料 torch.Size([19, 18])\n",
      "剩餘Y 資料 torch.Size([19, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10548010468482971, 14)\n",
      "The second_loss value of k: (0.10858114063739777, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.3635])\n",
      "目前模型的Data狀態 torch.Size([141, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882],\n",
      "        [0.6882]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0581],\n",
      "        [0.0579],\n",
      "        [0.0573],\n",
      "        [0.0704],\n",
      "        [0.0724],\n",
      "        [0.0473],\n",
      "        [0.0460],\n",
      "        [0.0335],\n",
      "        [0.0285],\n",
      "        [0.0279],\n",
      "        [0.0235],\n",
      "        [0.0197],\n",
      "        [0.0195],\n",
      "        [0.0174],\n",
      "        [0.0168],\n",
      "        [0.0156],\n",
      "        [0.0151],\n",
      "        [0.1054],\n",
      "        [0.0084],\n",
      "        [0.0068],\n",
      "        [0.1154],\n",
      "        [0.1160],\n",
      "        [0.0032],\n",
      "        [0.0025],\n",
      "        [0.0010],\n",
      "        [0.1212],\n",
      "        [0.0047],\n",
      "        [0.1262],\n",
      "        [0.1284],\n",
      "        [0.0088],\n",
      "        [0.0111],\n",
      "        [0.1317],\n",
      "        [0.0115],\n",
      "        [0.0162],\n",
      "        [0.0165],\n",
      "        [0.1413],\n",
      "        [0.1414],\n",
      "        [0.1433],\n",
      "        [0.0252],\n",
      "        [0.1473],\n",
      "        [0.1474],\n",
      "        [0.0272],\n",
      "        [0.0285],\n",
      "        [0.0291],\n",
      "        [0.0292],\n",
      "        [0.0298],\n",
      "        [0.1518],\n",
      "        [0.0329],\n",
      "        [0.1543],\n",
      "        [0.1551],\n",
      "        [0.1580],\n",
      "        [0.0390],\n",
      "        [0.1610],\n",
      "        [0.0408],\n",
      "        [0.1614],\n",
      "        [0.0428],\n",
      "        [0.1642],\n",
      "        [0.0443],\n",
      "        [0.0453],\n",
      "        [0.0467],\n",
      "        [0.0471],\n",
      "        [0.0518],\n",
      "        [0.0522],\n",
      "        [0.0529],\n",
      "        [0.1734],\n",
      "        [0.0547],\n",
      "        [0.1753],\n",
      "        [0.0549],\n",
      "        [0.0577],\n",
      "        [0.1792],\n",
      "        [0.0596],\n",
      "        [0.0605],\n",
      "        [0.0644],\n",
      "        [0.0647],\n",
      "        [0.1861],\n",
      "        [0.0666],\n",
      "        [0.0669],\n",
      "        [0.0695],\n",
      "        [0.0712],\n",
      "        [0.0717],\n",
      "        [0.0739],\n",
      "        [0.1951],\n",
      "        [0.1952],\n",
      "        [0.0775],\n",
      "        [0.0777],\n",
      "        [0.2010],\n",
      "        [0.0810],\n",
      "        [0.0813],\n",
      "        [0.2043],\n",
      "        [0.2096],\n",
      "        [0.2136],\n",
      "        [0.2177],\n",
      "        [0.0976],\n",
      "        [0.0994],\n",
      "        [0.2214],\n",
      "        [0.1031],\n",
      "        [0.1075],\n",
      "        [0.2317],\n",
      "        [0.2321],\n",
      "        [0.1213],\n",
      "        [0.1245],\n",
      "        [0.2472],\n",
      "        [0.1272],\n",
      "        [0.1292],\n",
      "        [0.1324],\n",
      "        [0.1368],\n",
      "        [0.1403],\n",
      "        [0.2639],\n",
      "        [0.2648],\n",
      "        [0.2670],\n",
      "        [0.1477],\n",
      "        [0.2711],\n",
      "        [0.1516],\n",
      "        [0.1542],\n",
      "        [0.1553],\n",
      "        [0.2787],\n",
      "        [0.2799],\n",
      "        [0.2802],\n",
      "        [0.1617],\n",
      "        [0.1618],\n",
      "        [0.1652],\n",
      "        [0.1769],\n",
      "        [0.3118],\n",
      "        [0.1929],\n",
      "        [0.2010],\n",
      "        [0.2143],\n",
      "        [0.2163],\n",
      "        [0.2173],\n",
      "        [0.2237],\n",
      "        [0.2271],\n",
      "        [0.2314],\n",
      "        [0.2361],\n",
      "        [0.2372],\n",
      "        [0.2478],\n",
      "        [0.2517],\n",
      "        [0.2645],\n",
      "        [0.2677],\n",
      "        [0.2893],\n",
      "        [0.2971],\n",
      "        [0.3238],\n",
      "        [0.3248]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0604],\n",
      "        [0.0602],\n",
      "        [0.0596],\n",
      "        [0.0727],\n",
      "        [0.0747],\n",
      "        [0.0496],\n",
      "        [0.0483],\n",
      "        [0.0359],\n",
      "        [0.0309],\n",
      "        [0.0302],\n",
      "        [0.0258],\n",
      "        [0.0220],\n",
      "        [0.0219],\n",
      "        [0.0197],\n",
      "        [0.0191],\n",
      "        [0.0179],\n",
      "        [0.0175],\n",
      "        [0.1077],\n",
      "        [0.0107],\n",
      "        [0.0091],\n",
      "        [0.1177],\n",
      "        [0.1184],\n",
      "        [0.0055],\n",
      "        [0.0048],\n",
      "        [0.0034],\n",
      "        [0.1235],\n",
      "        [0.0024],\n",
      "        [0.1285],\n",
      "        [0.1307],\n",
      "        [0.0065],\n",
      "        [0.0088],\n",
      "        [0.1341],\n",
      "        [0.0092],\n",
      "        [0.0139],\n",
      "        [0.0142],\n",
      "        [0.1436],\n",
      "        [0.1437],\n",
      "        [0.1456],\n",
      "        [0.0229],\n",
      "        [0.1496],\n",
      "        [0.1497],\n",
      "        [0.0248],\n",
      "        [0.0262],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0274],\n",
      "        [0.1541],\n",
      "        [0.0306],\n",
      "        [0.1566],\n",
      "        [0.1574],\n",
      "        [0.1603],\n",
      "        [0.0367],\n",
      "        [0.1633],\n",
      "        [0.0385],\n",
      "        [0.1637],\n",
      "        [0.0405],\n",
      "        [0.1665],\n",
      "        [0.0420],\n",
      "        [0.0429],\n",
      "        [0.0444],\n",
      "        [0.0447],\n",
      "        [0.0495],\n",
      "        [0.0499],\n",
      "        [0.0506],\n",
      "        [0.1757],\n",
      "        [0.0524],\n",
      "        [0.1776],\n",
      "        [0.0526],\n",
      "        [0.0554],\n",
      "        [0.1815],\n",
      "        [0.0573],\n",
      "        [0.0581],\n",
      "        [0.0621],\n",
      "        [0.0623],\n",
      "        [0.1884],\n",
      "        [0.0643],\n",
      "        [0.0646],\n",
      "        [0.0672],\n",
      "        [0.0688],\n",
      "        [0.0693],\n",
      "        [0.0716],\n",
      "        [0.1974],\n",
      "        [0.1976],\n",
      "        [0.0752],\n",
      "        [0.0754],\n",
      "        [0.2033],\n",
      "        [0.0787],\n",
      "        [0.0789],\n",
      "        [0.2067],\n",
      "        [0.2119],\n",
      "        [0.2159],\n",
      "        [0.2200],\n",
      "        [0.0952],\n",
      "        [0.0971],\n",
      "        [0.2237],\n",
      "        [0.1008],\n",
      "        [0.1052],\n",
      "        [0.2341],\n",
      "        [0.2344],\n",
      "        [0.1190],\n",
      "        [0.1221],\n",
      "        [0.2495],\n",
      "        [0.1248],\n",
      "        [0.1269],\n",
      "        [0.1301],\n",
      "        [0.1345],\n",
      "        [0.1380],\n",
      "        [0.2662],\n",
      "        [0.2671],\n",
      "        [0.2693],\n",
      "        [0.1453],\n",
      "        [0.2735],\n",
      "        [0.1493],\n",
      "        [0.1519],\n",
      "        [0.1530],\n",
      "        [0.2810],\n",
      "        [0.2823],\n",
      "        [0.2826],\n",
      "        [0.1594],\n",
      "        [0.1595],\n",
      "        [0.1629],\n",
      "        [0.1746],\n",
      "        [0.3141],\n",
      "        [0.1906],\n",
      "        [0.1987],\n",
      "        [0.2120],\n",
      "        [0.2140],\n",
      "        [0.2150],\n",
      "        [0.2214],\n",
      "        [0.2248],\n",
      "        [0.2290],\n",
      "        [0.2337],\n",
      "        [0.2349],\n",
      "        [0.2455],\n",
      "        [0.2494],\n",
      "        [0.2622],\n",
      "        [0.2654],\n",
      "        [0.2870],\n",
      "        [0.2948],\n",
      "        [0.3214],\n",
      "        [0.3225]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 37.2290985584259\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 142\n",
      "剩餘X 資料 torch.Size([18, 18])\n",
      "剩餘Y 資料 torch.Size([18, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10705936700105667, 12)\n",
      "The second_loss value of k: (0.10716191679239273, 13)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引12，y= tensor([0.3587])\n",
      "目前模型的Data狀態 torch.Size([142, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859],\n",
      "        [0.6859]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0604],\n",
      "        [0.0602],\n",
      "        [0.0596],\n",
      "        [0.0727],\n",
      "        [0.0747],\n",
      "        [0.0496],\n",
      "        [0.0483],\n",
      "        [0.0359],\n",
      "        [0.0309],\n",
      "        [0.0302],\n",
      "        [0.0258],\n",
      "        [0.0220],\n",
      "        [0.0219],\n",
      "        [0.0197],\n",
      "        [0.0191],\n",
      "        [0.0179],\n",
      "        [0.0175],\n",
      "        [0.1077],\n",
      "        [0.0107],\n",
      "        [0.0091],\n",
      "        [0.1177],\n",
      "        [0.1184],\n",
      "        [0.0055],\n",
      "        [0.0048],\n",
      "        [0.0034],\n",
      "        [0.1235],\n",
      "        [0.0024],\n",
      "        [0.1285],\n",
      "        [0.1307],\n",
      "        [0.0065],\n",
      "        [0.0088],\n",
      "        [0.1341],\n",
      "        [0.0092],\n",
      "        [0.0139],\n",
      "        [0.0142],\n",
      "        [0.1436],\n",
      "        [0.1437],\n",
      "        [0.1456],\n",
      "        [0.0229],\n",
      "        [0.1496],\n",
      "        [0.1497],\n",
      "        [0.0248],\n",
      "        [0.0262],\n",
      "        [0.0268],\n",
      "        [0.0269],\n",
      "        [0.0274],\n",
      "        [0.1541],\n",
      "        [0.0306],\n",
      "        [0.1566],\n",
      "        [0.1574],\n",
      "        [0.1603],\n",
      "        [0.0367],\n",
      "        [0.1633],\n",
      "        [0.0385],\n",
      "        [0.1637],\n",
      "        [0.0405],\n",
      "        [0.1665],\n",
      "        [0.0420],\n",
      "        [0.0429],\n",
      "        [0.0444],\n",
      "        [0.0447],\n",
      "        [0.0495],\n",
      "        [0.0499],\n",
      "        [0.0506],\n",
      "        [0.1757],\n",
      "        [0.0524],\n",
      "        [0.1776],\n",
      "        [0.0526],\n",
      "        [0.0554],\n",
      "        [0.1815],\n",
      "        [0.0573],\n",
      "        [0.0581],\n",
      "        [0.0621],\n",
      "        [0.0623],\n",
      "        [0.1884],\n",
      "        [0.0643],\n",
      "        [0.0646],\n",
      "        [0.0672],\n",
      "        [0.0688],\n",
      "        [0.0693],\n",
      "        [0.0716],\n",
      "        [0.1974],\n",
      "        [0.1976],\n",
      "        [0.0752],\n",
      "        [0.0754],\n",
      "        [0.2033],\n",
      "        [0.0787],\n",
      "        [0.0789],\n",
      "        [0.2067],\n",
      "        [0.2119],\n",
      "        [0.2159],\n",
      "        [0.2200],\n",
      "        [0.0952],\n",
      "        [0.0971],\n",
      "        [0.2237],\n",
      "        [0.1008],\n",
      "        [0.1052],\n",
      "        [0.2341],\n",
      "        [0.2344],\n",
      "        [0.1190],\n",
      "        [0.1221],\n",
      "        [0.2495],\n",
      "        [0.1248],\n",
      "        [0.1269],\n",
      "        [0.1301],\n",
      "        [0.1345],\n",
      "        [0.1380],\n",
      "        [0.2662],\n",
      "        [0.2671],\n",
      "        [0.2693],\n",
      "        [0.1453],\n",
      "        [0.2735],\n",
      "        [0.1493],\n",
      "        [0.1519],\n",
      "        [0.1530],\n",
      "        [0.2810],\n",
      "        [0.2823],\n",
      "        [0.2826],\n",
      "        [0.1594],\n",
      "        [0.1595],\n",
      "        [0.1629],\n",
      "        [0.1746],\n",
      "        [0.3141],\n",
      "        [0.1906],\n",
      "        [0.1987],\n",
      "        [0.2120],\n",
      "        [0.2140],\n",
      "        [0.2150],\n",
      "        [0.2214],\n",
      "        [0.2248],\n",
      "        [0.2290],\n",
      "        [0.2337],\n",
      "        [0.2349],\n",
      "        [0.2455],\n",
      "        [0.2494],\n",
      "        [0.2622],\n",
      "        [0.2654],\n",
      "        [0.2870],\n",
      "        [0.2948],\n",
      "        [0.3214],\n",
      "        [0.3225],\n",
      "        [0.3272]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0627],\n",
      "        [    0.0625],\n",
      "        [    0.0619],\n",
      "        [    0.0750],\n",
      "        [    0.0770],\n",
      "        [    0.0519],\n",
      "        [    0.0506],\n",
      "        [    0.0382],\n",
      "        [    0.0331],\n",
      "        [    0.0325],\n",
      "        [    0.0281],\n",
      "        [    0.0243],\n",
      "        [    0.0241],\n",
      "        [    0.0220],\n",
      "        [    0.0214],\n",
      "        [    0.0202],\n",
      "        [    0.0197],\n",
      "        [    0.1100],\n",
      "        [    0.0130],\n",
      "        [    0.0114],\n",
      "        [    0.1200],\n",
      "        [    0.1206],\n",
      "        [    0.0078],\n",
      "        [    0.0071],\n",
      "        [    0.0056],\n",
      "        [    0.1258],\n",
      "        [    0.0001],\n",
      "        [    0.1308],\n",
      "        [    0.1330],\n",
      "        [    0.0042],\n",
      "        [    0.0065],\n",
      "        [    0.1364],\n",
      "        [    0.0069],\n",
      "        [    0.0116],\n",
      "        [    0.0119],\n",
      "        [    0.1459],\n",
      "        [    0.1460],\n",
      "        [    0.1479],\n",
      "        [    0.0206],\n",
      "        [    0.1519],\n",
      "        [    0.1520],\n",
      "        [    0.0226],\n",
      "        [    0.0239],\n",
      "        [    0.0245],\n",
      "        [    0.0246],\n",
      "        [    0.0251],\n",
      "        [    0.1564],\n",
      "        [    0.0283],\n",
      "        [    0.1589],\n",
      "        [    0.1597],\n",
      "        [    0.1626],\n",
      "        [    0.0344],\n",
      "        [    0.1656],\n",
      "        [    0.0362],\n",
      "        [    0.1660],\n",
      "        [    0.0382],\n",
      "        [    0.1688],\n",
      "        [    0.0397],\n",
      "        [    0.0407],\n",
      "        [    0.0421],\n",
      "        [    0.0425],\n",
      "        [    0.0472],\n",
      "        [    0.0476],\n",
      "        [    0.0483],\n",
      "        [    0.1780],\n",
      "        [    0.0501],\n",
      "        [    0.1799],\n",
      "        [    0.0503],\n",
      "        [    0.0531],\n",
      "        [    0.1838],\n",
      "        [    0.0550],\n",
      "        [    0.0559],\n",
      "        [    0.0598],\n",
      "        [    0.0600],\n",
      "        [    0.1907],\n",
      "        [    0.0620],\n",
      "        [    0.0623],\n",
      "        [    0.0649],\n",
      "        [    0.0665],\n",
      "        [    0.0671],\n",
      "        [    0.0693],\n",
      "        [    0.1997],\n",
      "        [    0.1998],\n",
      "        [    0.0729],\n",
      "        [    0.0731],\n",
      "        [    0.2056],\n",
      "        [    0.0764],\n",
      "        [    0.0767],\n",
      "        [    0.2090],\n",
      "        [    0.2142],\n",
      "        [    0.2182],\n",
      "        [    0.2223],\n",
      "        [    0.0929],\n",
      "        [    0.0948],\n",
      "        [    0.2260],\n",
      "        [    0.0985],\n",
      "        [    0.1029],\n",
      "        [    0.2364],\n",
      "        [    0.2367],\n",
      "        [    0.1167],\n",
      "        [    0.1199],\n",
      "        [    0.2518],\n",
      "        [    0.1226],\n",
      "        [    0.1246],\n",
      "        [    0.1278],\n",
      "        [    0.1322],\n",
      "        [    0.1357],\n",
      "        [    0.2685],\n",
      "        [    0.2694],\n",
      "        [    0.2716],\n",
      "        [    0.1430],\n",
      "        [    0.2758],\n",
      "        [    0.1470],\n",
      "        [    0.1496],\n",
      "        [    0.1507],\n",
      "        [    0.2833],\n",
      "        [    0.2846],\n",
      "        [    0.2848],\n",
      "        [    0.1571],\n",
      "        [    0.1572],\n",
      "        [    0.1606],\n",
      "        [    0.1723],\n",
      "        [    0.3164],\n",
      "        [    0.1883],\n",
      "        [    0.1964],\n",
      "        [    0.2097],\n",
      "        [    0.2117],\n",
      "        [    0.2127],\n",
      "        [    0.2191],\n",
      "        [    0.2225],\n",
      "        [    0.2267],\n",
      "        [    0.2314],\n",
      "        [    0.2326],\n",
      "        [    0.2432],\n",
      "        [    0.2471],\n",
      "        [    0.2599],\n",
      "        [    0.2631],\n",
      "        [    0.2847],\n",
      "        [    0.2925],\n",
      "        [    0.3192],\n",
      "        [    0.3202],\n",
      "        [    0.3249]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 37.467329263687134\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 143\n",
      "剩餘X 資料 torch.Size([17, 18])\n",
      "剩餘Y 資料 torch.Size([17, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10566728562116623, 12)\n",
      "The second_loss value of k: (0.1060241088271141, 11)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引12，y= tensor([0.3586])\n",
      "目前模型的Data狀態 torch.Size([143, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836],\n",
      "        [0.6836]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0627],\n",
      "        [    0.0625],\n",
      "        [    0.0619],\n",
      "        [    0.0750],\n",
      "        [    0.0770],\n",
      "        [    0.0519],\n",
      "        [    0.0506],\n",
      "        [    0.0382],\n",
      "        [    0.0331],\n",
      "        [    0.0325],\n",
      "        [    0.0281],\n",
      "        [    0.0243],\n",
      "        [    0.0241],\n",
      "        [    0.0220],\n",
      "        [    0.0214],\n",
      "        [    0.0202],\n",
      "        [    0.0197],\n",
      "        [    0.1100],\n",
      "        [    0.0130],\n",
      "        [    0.0114],\n",
      "        [    0.1200],\n",
      "        [    0.1206],\n",
      "        [    0.0078],\n",
      "        [    0.0071],\n",
      "        [    0.0056],\n",
      "        [    0.1258],\n",
      "        [    0.0001],\n",
      "        [    0.1308],\n",
      "        [    0.1330],\n",
      "        [    0.0042],\n",
      "        [    0.0065],\n",
      "        [    0.1364],\n",
      "        [    0.0069],\n",
      "        [    0.0116],\n",
      "        [    0.0119],\n",
      "        [    0.1459],\n",
      "        [    0.1460],\n",
      "        [    0.1479],\n",
      "        [    0.0206],\n",
      "        [    0.1519],\n",
      "        [    0.1520],\n",
      "        [    0.0226],\n",
      "        [    0.0239],\n",
      "        [    0.0245],\n",
      "        [    0.0246],\n",
      "        [    0.0251],\n",
      "        [    0.1564],\n",
      "        [    0.0283],\n",
      "        [    0.1589],\n",
      "        [    0.1597],\n",
      "        [    0.1626],\n",
      "        [    0.0344],\n",
      "        [    0.1656],\n",
      "        [    0.0362],\n",
      "        [    0.1660],\n",
      "        [    0.0382],\n",
      "        [    0.1688],\n",
      "        [    0.0397],\n",
      "        [    0.0407],\n",
      "        [    0.0421],\n",
      "        [    0.0425],\n",
      "        [    0.0472],\n",
      "        [    0.0476],\n",
      "        [    0.0483],\n",
      "        [    0.1780],\n",
      "        [    0.0501],\n",
      "        [    0.1799],\n",
      "        [    0.0503],\n",
      "        [    0.0531],\n",
      "        [    0.1838],\n",
      "        [    0.0550],\n",
      "        [    0.0559],\n",
      "        [    0.0598],\n",
      "        [    0.0600],\n",
      "        [    0.1907],\n",
      "        [    0.0620],\n",
      "        [    0.0623],\n",
      "        [    0.0649],\n",
      "        [    0.0665],\n",
      "        [    0.0671],\n",
      "        [    0.0693],\n",
      "        [    0.1997],\n",
      "        [    0.1998],\n",
      "        [    0.0729],\n",
      "        [    0.0731],\n",
      "        [    0.2056],\n",
      "        [    0.0764],\n",
      "        [    0.0767],\n",
      "        [    0.2090],\n",
      "        [    0.2142],\n",
      "        [    0.2182],\n",
      "        [    0.2223],\n",
      "        [    0.0929],\n",
      "        [    0.0948],\n",
      "        [    0.2260],\n",
      "        [    0.0985],\n",
      "        [    0.1029],\n",
      "        [    0.2364],\n",
      "        [    0.2367],\n",
      "        [    0.1167],\n",
      "        [    0.1199],\n",
      "        [    0.2518],\n",
      "        [    0.1226],\n",
      "        [    0.1246],\n",
      "        [    0.1278],\n",
      "        [    0.1322],\n",
      "        [    0.1357],\n",
      "        [    0.2685],\n",
      "        [    0.2694],\n",
      "        [    0.2716],\n",
      "        [    0.1430],\n",
      "        [    0.2758],\n",
      "        [    0.1470],\n",
      "        [    0.1496],\n",
      "        [    0.1507],\n",
      "        [    0.2833],\n",
      "        [    0.2846],\n",
      "        [    0.2848],\n",
      "        [    0.1571],\n",
      "        [    0.1572],\n",
      "        [    0.1606],\n",
      "        [    0.1723],\n",
      "        [    0.3164],\n",
      "        [    0.1883],\n",
      "        [    0.1964],\n",
      "        [    0.2097],\n",
      "        [    0.2117],\n",
      "        [    0.2127],\n",
      "        [    0.2191],\n",
      "        [    0.2225],\n",
      "        [    0.2267],\n",
      "        [    0.2314],\n",
      "        [    0.2326],\n",
      "        [    0.2432],\n",
      "        [    0.2471],\n",
      "        [    0.2599],\n",
      "        [    0.2631],\n",
      "        [    0.2847],\n",
      "        [    0.2925],\n",
      "        [    0.3192],\n",
      "        [    0.3202],\n",
      "        [    0.3249],\n",
      "        [    0.3251]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 71\n",
      "Number of shrink: 29\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0649],\n",
      "        [0.0648],\n",
      "        [0.0642],\n",
      "        [0.0772],\n",
      "        [0.0792],\n",
      "        [0.0541],\n",
      "        [0.0529],\n",
      "        [0.0404],\n",
      "        [0.0354],\n",
      "        [0.0347],\n",
      "        [0.0304],\n",
      "        [0.0266],\n",
      "        [0.0264],\n",
      "        [0.0243],\n",
      "        [0.0237],\n",
      "        [0.0225],\n",
      "        [0.0220],\n",
      "        [0.1123],\n",
      "        [0.0152],\n",
      "        [0.0137],\n",
      "        [0.1223],\n",
      "        [0.1229],\n",
      "        [0.0101],\n",
      "        [0.0094],\n",
      "        [0.0079],\n",
      "        [0.1281],\n",
      "        [0.0022],\n",
      "        [0.1331],\n",
      "        [0.1353],\n",
      "        [0.0019],\n",
      "        [0.0042],\n",
      "        [0.1386],\n",
      "        [0.0046],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.1482],\n",
      "        [0.1483],\n",
      "        [0.1502],\n",
      "        [0.0183],\n",
      "        [0.1541],\n",
      "        [0.1543],\n",
      "        [0.0203],\n",
      "        [0.0216],\n",
      "        [0.0222],\n",
      "        [0.0224],\n",
      "        [0.0229],\n",
      "        [0.1587],\n",
      "        [0.0260],\n",
      "        [0.1612],\n",
      "        [0.1619],\n",
      "        [0.1649],\n",
      "        [0.0321],\n",
      "        [0.1678],\n",
      "        [0.0339],\n",
      "        [0.1682],\n",
      "        [0.0360],\n",
      "        [0.1711],\n",
      "        [0.0374],\n",
      "        [0.0384],\n",
      "        [0.0398],\n",
      "        [0.0402],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0460],\n",
      "        [0.1803],\n",
      "        [0.0479],\n",
      "        [0.1822],\n",
      "        [0.0480],\n",
      "        [0.0508],\n",
      "        [0.1861],\n",
      "        [0.0527],\n",
      "        [0.0536],\n",
      "        [0.0575],\n",
      "        [0.0578],\n",
      "        [0.1930],\n",
      "        [0.0598],\n",
      "        [0.0600],\n",
      "        [0.0626],\n",
      "        [0.0643],\n",
      "        [0.0648],\n",
      "        [0.0670],\n",
      "        [0.2020],\n",
      "        [0.2021],\n",
      "        [0.0706],\n",
      "        [0.0709],\n",
      "        [0.2079],\n",
      "        [0.0741],\n",
      "        [0.0744],\n",
      "        [0.2112],\n",
      "        [0.2165],\n",
      "        [0.2205],\n",
      "        [0.2246],\n",
      "        [0.0907],\n",
      "        [0.0925],\n",
      "        [0.2282],\n",
      "        [0.0962],\n",
      "        [0.1007],\n",
      "        [0.2386],\n",
      "        [0.2389],\n",
      "        [0.1145],\n",
      "        [0.1176],\n",
      "        [0.2541],\n",
      "        [0.1203],\n",
      "        [0.1224],\n",
      "        [0.1255],\n",
      "        [0.1299],\n",
      "        [0.1334],\n",
      "        [0.2708],\n",
      "        [0.2716],\n",
      "        [0.2739],\n",
      "        [0.1408],\n",
      "        [0.2780],\n",
      "        [0.1447],\n",
      "        [0.1474],\n",
      "        [0.1484],\n",
      "        [0.2856],\n",
      "        [0.2868],\n",
      "        [0.2871],\n",
      "        [0.1548],\n",
      "        [0.1549],\n",
      "        [0.1583],\n",
      "        [0.1700],\n",
      "        [0.3186],\n",
      "        [0.1860],\n",
      "        [0.1941],\n",
      "        [0.2074],\n",
      "        [0.2094],\n",
      "        [0.2104],\n",
      "        [0.2168],\n",
      "        [0.2202],\n",
      "        [0.2245],\n",
      "        [0.2292],\n",
      "        [0.2303],\n",
      "        [0.2409],\n",
      "        [0.2448],\n",
      "        [0.2577],\n",
      "        [0.2609],\n",
      "        [0.2824],\n",
      "        [0.2902],\n",
      "        [0.3169],\n",
      "        [0.3179],\n",
      "        [0.3226],\n",
      "        [0.3228]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 37.70649766921997\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 144\n",
      "剩餘X 資料 torch.Size([16, 18])\n",
      "剩餘Y 資料 torch.Size([16, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10455002635717392, 11)\n",
      "The second_loss value of k: (0.10462602972984314, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引11，y= tensor([0.3580])\n",
      "目前模型的Data狀態 torch.Size([144, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814],\n",
      "        [0.6814]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0649],\n",
      "        [0.0648],\n",
      "        [0.0642],\n",
      "        [0.0772],\n",
      "        [0.0792],\n",
      "        [0.0541],\n",
      "        [0.0529],\n",
      "        [0.0404],\n",
      "        [0.0354],\n",
      "        [0.0347],\n",
      "        [0.0304],\n",
      "        [0.0266],\n",
      "        [0.0264],\n",
      "        [0.0243],\n",
      "        [0.0237],\n",
      "        [0.0225],\n",
      "        [0.0220],\n",
      "        [0.1123],\n",
      "        [0.0152],\n",
      "        [0.0137],\n",
      "        [0.1223],\n",
      "        [0.1229],\n",
      "        [0.0101],\n",
      "        [0.0094],\n",
      "        [0.0079],\n",
      "        [0.1281],\n",
      "        [0.0022],\n",
      "        [0.1331],\n",
      "        [0.1353],\n",
      "        [0.0019],\n",
      "        [0.0042],\n",
      "        [0.1386],\n",
      "        [0.0046],\n",
      "        [0.0093],\n",
      "        [0.0096],\n",
      "        [0.1482],\n",
      "        [0.1483],\n",
      "        [0.1502],\n",
      "        [0.0183],\n",
      "        [0.1541],\n",
      "        [0.1543],\n",
      "        [0.0203],\n",
      "        [0.0216],\n",
      "        [0.0222],\n",
      "        [0.0224],\n",
      "        [0.0229],\n",
      "        [0.1587],\n",
      "        [0.0260],\n",
      "        [0.1612],\n",
      "        [0.1619],\n",
      "        [0.1649],\n",
      "        [0.0321],\n",
      "        [0.1678],\n",
      "        [0.0339],\n",
      "        [0.1682],\n",
      "        [0.0360],\n",
      "        [0.1711],\n",
      "        [0.0374],\n",
      "        [0.0384],\n",
      "        [0.0398],\n",
      "        [0.0402],\n",
      "        [0.0450],\n",
      "        [0.0454],\n",
      "        [0.0460],\n",
      "        [0.1803],\n",
      "        [0.0479],\n",
      "        [0.1822],\n",
      "        [0.0480],\n",
      "        [0.0508],\n",
      "        [0.1861],\n",
      "        [0.0527],\n",
      "        [0.0536],\n",
      "        [0.0575],\n",
      "        [0.0578],\n",
      "        [0.1930],\n",
      "        [0.0598],\n",
      "        [0.0600],\n",
      "        [0.0626],\n",
      "        [0.0643],\n",
      "        [0.0648],\n",
      "        [0.0670],\n",
      "        [0.2020],\n",
      "        [0.2021],\n",
      "        [0.0706],\n",
      "        [0.0709],\n",
      "        [0.2079],\n",
      "        [0.0741],\n",
      "        [0.0744],\n",
      "        [0.2112],\n",
      "        [0.2165],\n",
      "        [0.2205],\n",
      "        [0.2246],\n",
      "        [0.0907],\n",
      "        [0.0925],\n",
      "        [0.2282],\n",
      "        [0.0962],\n",
      "        [0.1007],\n",
      "        [0.2386],\n",
      "        [0.2389],\n",
      "        [0.1145],\n",
      "        [0.1176],\n",
      "        [0.2541],\n",
      "        [0.1203],\n",
      "        [0.1224],\n",
      "        [0.1255],\n",
      "        [0.1299],\n",
      "        [0.1334],\n",
      "        [0.2708],\n",
      "        [0.2716],\n",
      "        [0.2739],\n",
      "        [0.1408],\n",
      "        [0.2780],\n",
      "        [0.1447],\n",
      "        [0.1474],\n",
      "        [0.1484],\n",
      "        [0.2856],\n",
      "        [0.2868],\n",
      "        [0.2871],\n",
      "        [0.1548],\n",
      "        [0.1549],\n",
      "        [0.1583],\n",
      "        [0.1700],\n",
      "        [0.3186],\n",
      "        [0.1860],\n",
      "        [0.1941],\n",
      "        [0.2074],\n",
      "        [0.2094],\n",
      "        [0.2104],\n",
      "        [0.2168],\n",
      "        [0.2202],\n",
      "        [0.2245],\n",
      "        [0.2292],\n",
      "        [0.2303],\n",
      "        [0.2409],\n",
      "        [0.2448],\n",
      "        [0.2577],\n",
      "        [0.2609],\n",
      "        [0.2824],\n",
      "        [0.2902],\n",
      "        [0.3169],\n",
      "        [0.3179],\n",
      "        [0.3226],\n",
      "        [0.3228],\n",
      "        [0.3233]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 76\n",
      "Number of shrink: 24\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0672],\n",
      "        [0.0670],\n",
      "        [0.0664],\n",
      "        [0.0795],\n",
      "        [0.0815],\n",
      "        [0.0564],\n",
      "        [0.0551],\n",
      "        [0.0427],\n",
      "        [0.0377],\n",
      "        [0.0370],\n",
      "        [0.0326],\n",
      "        [0.0288],\n",
      "        [0.0287],\n",
      "        [0.0265],\n",
      "        [0.0259],\n",
      "        [0.0247],\n",
      "        [0.0243],\n",
      "        [0.1145],\n",
      "        [0.0175],\n",
      "        [0.0159],\n",
      "        [0.1245],\n",
      "        [0.1252],\n",
      "        [0.0123],\n",
      "        [0.0116],\n",
      "        [0.0102],\n",
      "        [0.1303],\n",
      "        [0.0044],\n",
      "        [0.1353],\n",
      "        [0.1375],\n",
      "        [0.0003],\n",
      "        [0.0019],\n",
      "        [0.1409],\n",
      "        [0.0024],\n",
      "        [0.0071],\n",
      "        [0.0074],\n",
      "        [0.1504],\n",
      "        [0.1505],\n",
      "        [0.1524],\n",
      "        [0.0161],\n",
      "        [0.1564],\n",
      "        [0.1565],\n",
      "        [0.0180],\n",
      "        [0.0194],\n",
      "        [0.0200],\n",
      "        [0.0201],\n",
      "        [0.0206],\n",
      "        [0.1609],\n",
      "        [0.0238],\n",
      "        [0.1634],\n",
      "        [0.1642],\n",
      "        [0.1672],\n",
      "        [0.0299],\n",
      "        [0.1701],\n",
      "        [0.0317],\n",
      "        [0.1705],\n",
      "        [0.0337],\n",
      "        [0.1733],\n",
      "        [0.0352],\n",
      "        [0.0361],\n",
      "        [0.0376],\n",
      "        [0.0379],\n",
      "        [0.0427],\n",
      "        [0.0431],\n",
      "        [0.0438],\n",
      "        [0.1825],\n",
      "        [0.0456],\n",
      "        [0.1844],\n",
      "        [0.0458],\n",
      "        [0.0486],\n",
      "        [0.1883],\n",
      "        [0.0505],\n",
      "        [0.0513],\n",
      "        [0.0553],\n",
      "        [0.0555],\n",
      "        [0.1952],\n",
      "        [0.0575],\n",
      "        [0.0578],\n",
      "        [0.0604],\n",
      "        [0.0620],\n",
      "        [0.0625],\n",
      "        [0.0648],\n",
      "        [0.2042],\n",
      "        [0.2044],\n",
      "        [0.0684],\n",
      "        [0.0686],\n",
      "        [0.2101],\n",
      "        [0.0719],\n",
      "        [0.0721],\n",
      "        [0.2135],\n",
      "        [0.2187],\n",
      "        [0.2227],\n",
      "        [0.2268],\n",
      "        [0.0884],\n",
      "        [0.0903],\n",
      "        [0.2305],\n",
      "        [0.0940],\n",
      "        [0.0984],\n",
      "        [0.2409],\n",
      "        [0.2412],\n",
      "        [0.1122],\n",
      "        [0.1153],\n",
      "        [0.2563],\n",
      "        [0.1180],\n",
      "        [0.1201],\n",
      "        [0.1233],\n",
      "        [0.1277],\n",
      "        [0.1312],\n",
      "        [0.2730],\n",
      "        [0.2739],\n",
      "        [0.2761],\n",
      "        [0.1385],\n",
      "        [0.2803],\n",
      "        [0.1425],\n",
      "        [0.1451],\n",
      "        [0.1462],\n",
      "        [0.2878],\n",
      "        [0.2891],\n",
      "        [0.2894],\n",
      "        [0.1526],\n",
      "        [0.1527],\n",
      "        [0.1561],\n",
      "        [0.1677],\n",
      "        [0.3209],\n",
      "        [0.1838],\n",
      "        [0.1919],\n",
      "        [0.2052],\n",
      "        [0.2072],\n",
      "        [0.2082],\n",
      "        [0.2146],\n",
      "        [0.2180],\n",
      "        [0.2222],\n",
      "        [0.2269],\n",
      "        [0.2281],\n",
      "        [0.2387],\n",
      "        [0.2426],\n",
      "        [0.2554],\n",
      "        [0.2586],\n",
      "        [0.2802],\n",
      "        [0.2880],\n",
      "        [0.3146],\n",
      "        [0.3157],\n",
      "        [0.3204],\n",
      "        [0.3205],\n",
      "        [0.3211]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 37.944443464279175\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 145\n",
      "剩餘X 資料 torch.Size([15, 18])\n",
      "剩餘Y 資料 torch.Size([15, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10317769646644592, 10)\n",
      "The second_loss value of k: (0.10550571978092194, 11)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引10，y= tensor([0.3579])\n",
      "目前模型的Data狀態 torch.Size([145, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791],\n",
      "        [0.6791]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0672],\n",
      "        [0.0670],\n",
      "        [0.0664],\n",
      "        [0.0795],\n",
      "        [0.0815],\n",
      "        [0.0564],\n",
      "        [0.0551],\n",
      "        [0.0427],\n",
      "        [0.0377],\n",
      "        [0.0370],\n",
      "        [0.0326],\n",
      "        [0.0288],\n",
      "        [0.0287],\n",
      "        [0.0265],\n",
      "        [0.0259],\n",
      "        [0.0247],\n",
      "        [0.0243],\n",
      "        [0.1145],\n",
      "        [0.0175],\n",
      "        [0.0159],\n",
      "        [0.1245],\n",
      "        [0.1252],\n",
      "        [0.0123],\n",
      "        [0.0116],\n",
      "        [0.0102],\n",
      "        [0.1303],\n",
      "        [0.0044],\n",
      "        [0.1353],\n",
      "        [0.1375],\n",
      "        [0.0003],\n",
      "        [0.0019],\n",
      "        [0.1409],\n",
      "        [0.0024],\n",
      "        [0.0071],\n",
      "        [0.0074],\n",
      "        [0.1504],\n",
      "        [0.1505],\n",
      "        [0.1524],\n",
      "        [0.0161],\n",
      "        [0.1564],\n",
      "        [0.1565],\n",
      "        [0.0180],\n",
      "        [0.0194],\n",
      "        [0.0200],\n",
      "        [0.0201],\n",
      "        [0.0206],\n",
      "        [0.1609],\n",
      "        [0.0238],\n",
      "        [0.1634],\n",
      "        [0.1642],\n",
      "        [0.1672],\n",
      "        [0.0299],\n",
      "        [0.1701],\n",
      "        [0.0317],\n",
      "        [0.1705],\n",
      "        [0.0337],\n",
      "        [0.1733],\n",
      "        [0.0352],\n",
      "        [0.0361],\n",
      "        [0.0376],\n",
      "        [0.0379],\n",
      "        [0.0427],\n",
      "        [0.0431],\n",
      "        [0.0438],\n",
      "        [0.1825],\n",
      "        [0.0456],\n",
      "        [0.1844],\n",
      "        [0.0458],\n",
      "        [0.0486],\n",
      "        [0.1883],\n",
      "        [0.0505],\n",
      "        [0.0513],\n",
      "        [0.0553],\n",
      "        [0.0555],\n",
      "        [0.1952],\n",
      "        [0.0575],\n",
      "        [0.0578],\n",
      "        [0.0604],\n",
      "        [0.0620],\n",
      "        [0.0625],\n",
      "        [0.0648],\n",
      "        [0.2042],\n",
      "        [0.2044],\n",
      "        [0.0684],\n",
      "        [0.0686],\n",
      "        [0.2101],\n",
      "        [0.0719],\n",
      "        [0.0721],\n",
      "        [0.2135],\n",
      "        [0.2187],\n",
      "        [0.2227],\n",
      "        [0.2268],\n",
      "        [0.0884],\n",
      "        [0.0903],\n",
      "        [0.2305],\n",
      "        [0.0940],\n",
      "        [0.0984],\n",
      "        [0.2409],\n",
      "        [0.2412],\n",
      "        [0.1122],\n",
      "        [0.1153],\n",
      "        [0.2563],\n",
      "        [0.1180],\n",
      "        [0.1201],\n",
      "        [0.1233],\n",
      "        [0.1277],\n",
      "        [0.1312],\n",
      "        [0.2730],\n",
      "        [0.2739],\n",
      "        [0.2761],\n",
      "        [0.1385],\n",
      "        [0.2803],\n",
      "        [0.1425],\n",
      "        [0.1451],\n",
      "        [0.1462],\n",
      "        [0.2878],\n",
      "        [0.2891],\n",
      "        [0.2894],\n",
      "        [0.1526],\n",
      "        [0.1527],\n",
      "        [0.1561],\n",
      "        [0.1677],\n",
      "        [0.3209],\n",
      "        [0.1838],\n",
      "        [0.1919],\n",
      "        [0.2052],\n",
      "        [0.2072],\n",
      "        [0.2082],\n",
      "        [0.2146],\n",
      "        [0.2180],\n",
      "        [0.2222],\n",
      "        [0.2269],\n",
      "        [0.2281],\n",
      "        [0.2387],\n",
      "        [0.2426],\n",
      "        [0.2554],\n",
      "        [0.2586],\n",
      "        [0.2802],\n",
      "        [0.2880],\n",
      "        [0.3146],\n",
      "        [0.3157],\n",
      "        [0.3204],\n",
      "        [0.3205],\n",
      "        [0.3211],\n",
      "        [0.3212]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 74\n",
      "Number of shrink: 26\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0694],\n",
      "        [    0.0693],\n",
      "        [    0.0686],\n",
      "        [    0.0817],\n",
      "        [    0.0837],\n",
      "        [    0.0586],\n",
      "        [    0.0573],\n",
      "        [    0.0449],\n",
      "        [    0.0399],\n",
      "        [    0.0392],\n",
      "        [    0.0349],\n",
      "        [    0.0310],\n",
      "        [    0.0309],\n",
      "        [    0.0288],\n",
      "        [    0.0281],\n",
      "        [    0.0269],\n",
      "        [    0.0265],\n",
      "        [    0.1167],\n",
      "        [    0.0197],\n",
      "        [    0.0181],\n",
      "        [    0.1268],\n",
      "        [    0.1274],\n",
      "        [    0.0145],\n",
      "        [    0.0138],\n",
      "        [    0.0124],\n",
      "        [    0.1325],\n",
      "        [    0.0067],\n",
      "        [    0.1376],\n",
      "        [    0.1398],\n",
      "        [    0.0025],\n",
      "        [    0.0003],\n",
      "        [    0.1431],\n",
      "        [    0.0002],\n",
      "        [    0.0049],\n",
      "        [    0.0052],\n",
      "        [    0.1526],\n",
      "        [    0.1528],\n",
      "        [    0.1546],\n",
      "        [    0.0139],\n",
      "        [    0.1586],\n",
      "        [    0.1588],\n",
      "        [    0.0158],\n",
      "        [    0.0172],\n",
      "        [    0.0177],\n",
      "        [    0.0179],\n",
      "        [    0.0184],\n",
      "        [    0.1631],\n",
      "        [    0.0216],\n",
      "        [    0.1656],\n",
      "        [    0.1664],\n",
      "        [    0.1694],\n",
      "        [    0.0277],\n",
      "        [    0.1723],\n",
      "        [    0.0295],\n",
      "        [    0.1727],\n",
      "        [    0.0315],\n",
      "        [    0.1755],\n",
      "        [    0.0329],\n",
      "        [    0.0339],\n",
      "        [    0.0353],\n",
      "        [    0.0357],\n",
      "        [    0.0405],\n",
      "        [    0.0409],\n",
      "        [    0.0416],\n",
      "        [    0.1848],\n",
      "        [    0.0434],\n",
      "        [    0.1866],\n",
      "        [    0.0436],\n",
      "        [    0.0463],\n",
      "        [    0.1905],\n",
      "        [    0.0483],\n",
      "        [    0.0491],\n",
      "        [    0.0530],\n",
      "        [    0.0533],\n",
      "        [    0.1975],\n",
      "        [    0.0553],\n",
      "        [    0.0555],\n",
      "        [    0.0582],\n",
      "        [    0.0598],\n",
      "        [    0.0603],\n",
      "        [    0.0626],\n",
      "        [    0.2064],\n",
      "        [    0.2066],\n",
      "        [    0.0661],\n",
      "        [    0.0664],\n",
      "        [    0.2123],\n",
      "        [    0.0696],\n",
      "        [    0.0699],\n",
      "        [    0.2157],\n",
      "        [    0.2210],\n",
      "        [    0.2250],\n",
      "        [    0.2290],\n",
      "        [    0.0862],\n",
      "        [    0.0881],\n",
      "        [    0.2327],\n",
      "        [    0.0918],\n",
      "        [    0.0962],\n",
      "        [    0.2431],\n",
      "        [    0.2434],\n",
      "        [    0.1100],\n",
      "        [    0.1131],\n",
      "        [    0.2586],\n",
      "        [    0.1158],\n",
      "        [    0.1179],\n",
      "        [    0.1211],\n",
      "        [    0.1255],\n",
      "        [    0.1289],\n",
      "        [    0.2752],\n",
      "        [    0.2761],\n",
      "        [    0.2784],\n",
      "        [    0.1363],\n",
      "        [    0.2825],\n",
      "        [    0.1403],\n",
      "        [    0.1429],\n",
      "        [    0.1439],\n",
      "        [    0.2901],\n",
      "        [    0.2913],\n",
      "        [    0.2916],\n",
      "        [    0.1504],\n",
      "        [    0.1505],\n",
      "        [    0.1539],\n",
      "        [    0.1655],\n",
      "        [    0.3231],\n",
      "        [    0.1816],\n",
      "        [    0.1897],\n",
      "        [    0.2029],\n",
      "        [    0.2050],\n",
      "        [    0.2060],\n",
      "        [    0.2123],\n",
      "        [    0.2157],\n",
      "        [    0.2200],\n",
      "        [    0.2247],\n",
      "        [    0.2259],\n",
      "        [    0.2365],\n",
      "        [    0.2404],\n",
      "        [    0.2532],\n",
      "        [    0.2564],\n",
      "        [    0.2779],\n",
      "        [    0.2858],\n",
      "        [    0.3124],\n",
      "        [    0.3134],\n",
      "        [    0.3182],\n",
      "        [    0.3183],\n",
      "        [    0.3189],\n",
      "        [    0.3190]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 38.182400941848755\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 146\n",
      "剩餘X 資料 torch.Size([14, 18])\n",
      "剩餘Y 資料 torch.Size([14, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10407136380672455, 10)\n",
      "The second_loss value of k: (0.12287048250436783, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引10，y= tensor([0.3543])\n",
      "目前模型的Data狀態 torch.Size([146, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769],\n",
      "        [0.6769]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0694],\n",
      "        [    0.0693],\n",
      "        [    0.0686],\n",
      "        [    0.0817],\n",
      "        [    0.0837],\n",
      "        [    0.0586],\n",
      "        [    0.0573],\n",
      "        [    0.0449],\n",
      "        [    0.0399],\n",
      "        [    0.0392],\n",
      "        [    0.0349],\n",
      "        [    0.0310],\n",
      "        [    0.0309],\n",
      "        [    0.0288],\n",
      "        [    0.0281],\n",
      "        [    0.0269],\n",
      "        [    0.0265],\n",
      "        [    0.1167],\n",
      "        [    0.0197],\n",
      "        [    0.0181],\n",
      "        [    0.1268],\n",
      "        [    0.1274],\n",
      "        [    0.0145],\n",
      "        [    0.0138],\n",
      "        [    0.0124],\n",
      "        [    0.1325],\n",
      "        [    0.0067],\n",
      "        [    0.1376],\n",
      "        [    0.1398],\n",
      "        [    0.0025],\n",
      "        [    0.0003],\n",
      "        [    0.1431],\n",
      "        [    0.0002],\n",
      "        [    0.0049],\n",
      "        [    0.0052],\n",
      "        [    0.1526],\n",
      "        [    0.1528],\n",
      "        [    0.1546],\n",
      "        [    0.0139],\n",
      "        [    0.1586],\n",
      "        [    0.1588],\n",
      "        [    0.0158],\n",
      "        [    0.0172],\n",
      "        [    0.0177],\n",
      "        [    0.0179],\n",
      "        [    0.0184],\n",
      "        [    0.1631],\n",
      "        [    0.0216],\n",
      "        [    0.1656],\n",
      "        [    0.1664],\n",
      "        [    0.1694],\n",
      "        [    0.0277],\n",
      "        [    0.1723],\n",
      "        [    0.0295],\n",
      "        [    0.1727],\n",
      "        [    0.0315],\n",
      "        [    0.1755],\n",
      "        [    0.0329],\n",
      "        [    0.0339],\n",
      "        [    0.0353],\n",
      "        [    0.0357],\n",
      "        [    0.0405],\n",
      "        [    0.0409],\n",
      "        [    0.0416],\n",
      "        [    0.1848],\n",
      "        [    0.0434],\n",
      "        [    0.1866],\n",
      "        [    0.0436],\n",
      "        [    0.0463],\n",
      "        [    0.1905],\n",
      "        [    0.0483],\n",
      "        [    0.0491],\n",
      "        [    0.0530],\n",
      "        [    0.0533],\n",
      "        [    0.1975],\n",
      "        [    0.0553],\n",
      "        [    0.0555],\n",
      "        [    0.0582],\n",
      "        [    0.0598],\n",
      "        [    0.0603],\n",
      "        [    0.0626],\n",
      "        [    0.2064],\n",
      "        [    0.2066],\n",
      "        [    0.0661],\n",
      "        [    0.0664],\n",
      "        [    0.2123],\n",
      "        [    0.0696],\n",
      "        [    0.0699],\n",
      "        [    0.2157],\n",
      "        [    0.2210],\n",
      "        [    0.2250],\n",
      "        [    0.2290],\n",
      "        [    0.0862],\n",
      "        [    0.0881],\n",
      "        [    0.2327],\n",
      "        [    0.0918],\n",
      "        [    0.0962],\n",
      "        [    0.2431],\n",
      "        [    0.2434],\n",
      "        [    0.1100],\n",
      "        [    0.1131],\n",
      "        [    0.2586],\n",
      "        [    0.1158],\n",
      "        [    0.1179],\n",
      "        [    0.1211],\n",
      "        [    0.1255],\n",
      "        [    0.1289],\n",
      "        [    0.2752],\n",
      "        [    0.2761],\n",
      "        [    0.2784],\n",
      "        [    0.1363],\n",
      "        [    0.2825],\n",
      "        [    0.1403],\n",
      "        [    0.1429],\n",
      "        [    0.1439],\n",
      "        [    0.2901],\n",
      "        [    0.2913],\n",
      "        [    0.2916],\n",
      "        [    0.1504],\n",
      "        [    0.1505],\n",
      "        [    0.1539],\n",
      "        [    0.1655],\n",
      "        [    0.3231],\n",
      "        [    0.1816],\n",
      "        [    0.1897],\n",
      "        [    0.2029],\n",
      "        [    0.2050],\n",
      "        [    0.2060],\n",
      "        [    0.2123],\n",
      "        [    0.2157],\n",
      "        [    0.2200],\n",
      "        [    0.2247],\n",
      "        [    0.2259],\n",
      "        [    0.2365],\n",
      "        [    0.2404],\n",
      "        [    0.2532],\n",
      "        [    0.2564],\n",
      "        [    0.2779],\n",
      "        [    0.2858],\n",
      "        [    0.3124],\n",
      "        [    0.3134],\n",
      "        [    0.3182],\n",
      "        [    0.3183],\n",
      "        [    0.3189],\n",
      "        [    0.3190],\n",
      "        [    0.3226]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 77\n",
      "Number of shrink: 23\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0716],\n",
      "        [0.0715],\n",
      "        [0.0708],\n",
      "        [0.0839],\n",
      "        [0.0859],\n",
      "        [0.0608],\n",
      "        [0.0596],\n",
      "        [0.0471],\n",
      "        [0.0421],\n",
      "        [0.0414],\n",
      "        [0.0371],\n",
      "        [0.0332],\n",
      "        [0.0331],\n",
      "        [0.0310],\n",
      "        [0.0303],\n",
      "        [0.0292],\n",
      "        [0.0287],\n",
      "        [0.1189],\n",
      "        [0.0219],\n",
      "        [0.0203],\n",
      "        [0.1290],\n",
      "        [0.1296],\n",
      "        [0.0167],\n",
      "        [0.0160],\n",
      "        [0.0146],\n",
      "        [0.1348],\n",
      "        [0.0089],\n",
      "        [0.1398],\n",
      "        [0.1420],\n",
      "        [0.0048],\n",
      "        [0.0025],\n",
      "        [0.1453],\n",
      "        [0.0021],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.1549],\n",
      "        [0.1550],\n",
      "        [0.1569],\n",
      "        [0.0117],\n",
      "        [0.1608],\n",
      "        [0.1610],\n",
      "        [0.0136],\n",
      "        [0.0149],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.1654],\n",
      "        [0.0194],\n",
      "        [0.1679],\n",
      "        [0.1686],\n",
      "        [0.1716],\n",
      "        [0.0254],\n",
      "        [0.1745],\n",
      "        [0.0272],\n",
      "        [0.1749],\n",
      "        [0.0293],\n",
      "        [0.1777],\n",
      "        [0.0307],\n",
      "        [0.0317],\n",
      "        [0.0331],\n",
      "        [0.0335],\n",
      "        [0.0383],\n",
      "        [0.0387],\n",
      "        [0.0393],\n",
      "        [0.1870],\n",
      "        [0.0412],\n",
      "        [0.1889],\n",
      "        [0.0413],\n",
      "        [0.0441],\n",
      "        [0.1927],\n",
      "        [0.0460],\n",
      "        [0.0469],\n",
      "        [0.0508],\n",
      "        [0.0511],\n",
      "        [0.1997],\n",
      "        [0.0531],\n",
      "        [0.0533],\n",
      "        [0.0560],\n",
      "        [0.0576],\n",
      "        [0.0581],\n",
      "        [0.0603],\n",
      "        [0.2086],\n",
      "        [0.2088],\n",
      "        [0.0639],\n",
      "        [0.0642],\n",
      "        [0.2145],\n",
      "        [0.0674],\n",
      "        [0.0677],\n",
      "        [0.2179],\n",
      "        [0.2232],\n",
      "        [0.2272],\n",
      "        [0.2312],\n",
      "        [0.0840],\n",
      "        [0.0858],\n",
      "        [0.2349],\n",
      "        [0.0896],\n",
      "        [0.0940],\n",
      "        [0.2453],\n",
      "        [0.2456],\n",
      "        [0.1078],\n",
      "        [0.1109],\n",
      "        [0.2608],\n",
      "        [0.1136],\n",
      "        [0.1157],\n",
      "        [0.1188],\n",
      "        [0.1233],\n",
      "        [0.1267],\n",
      "        [0.2775],\n",
      "        [0.2783],\n",
      "        [0.2806],\n",
      "        [0.1341],\n",
      "        [0.2847],\n",
      "        [0.1381],\n",
      "        [0.1407],\n",
      "        [0.1417],\n",
      "        [0.2923],\n",
      "        [0.2935],\n",
      "        [0.2938],\n",
      "        [0.1482],\n",
      "        [0.1482],\n",
      "        [0.1516],\n",
      "        [0.1633],\n",
      "        [0.3253],\n",
      "        [0.1793],\n",
      "        [0.1874],\n",
      "        [0.2007],\n",
      "        [0.2028],\n",
      "        [0.2037],\n",
      "        [0.2101],\n",
      "        [0.2135],\n",
      "        [0.2178],\n",
      "        [0.2225],\n",
      "        [0.2236],\n",
      "        [0.2343],\n",
      "        [0.2382],\n",
      "        [0.2510],\n",
      "        [0.2542],\n",
      "        [0.2757],\n",
      "        [0.2836],\n",
      "        [0.3102],\n",
      "        [0.3112],\n",
      "        [0.3160],\n",
      "        [0.3161],\n",
      "        [0.3167],\n",
      "        [0.3168],\n",
      "        [0.3204]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 38.41939997673035\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 147\n",
      "剩餘X 資料 torch.Size([13, 18])\n",
      "剩餘Y 資料 torch.Size([13, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12132647633552551, 0)\n",
      "The second_loss value of k: (0.12209171801805496, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.3264])\n",
      "目前模型的Data狀態 torch.Size([147, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747],\n",
      "        [0.6747]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0716],\n",
      "        [0.0715],\n",
      "        [0.0708],\n",
      "        [0.0839],\n",
      "        [0.0859],\n",
      "        [0.0608],\n",
      "        [0.0596],\n",
      "        [0.0471],\n",
      "        [0.0421],\n",
      "        [0.0414],\n",
      "        [0.0371],\n",
      "        [0.0332],\n",
      "        [0.0331],\n",
      "        [0.0310],\n",
      "        [0.0303],\n",
      "        [0.0292],\n",
      "        [0.0287],\n",
      "        [0.1189],\n",
      "        [0.0219],\n",
      "        [0.0203],\n",
      "        [0.1290],\n",
      "        [0.1296],\n",
      "        [0.0167],\n",
      "        [0.0160],\n",
      "        [0.0146],\n",
      "        [0.1348],\n",
      "        [0.0089],\n",
      "        [0.1398],\n",
      "        [0.1420],\n",
      "        [0.0048],\n",
      "        [0.0025],\n",
      "        [0.1453],\n",
      "        [0.0021],\n",
      "        [0.0026],\n",
      "        [0.0030],\n",
      "        [0.1549],\n",
      "        [0.1550],\n",
      "        [0.1569],\n",
      "        [0.0117],\n",
      "        [0.1608],\n",
      "        [0.1610],\n",
      "        [0.0136],\n",
      "        [0.0149],\n",
      "        [0.0155],\n",
      "        [0.0157],\n",
      "        [0.0162],\n",
      "        [0.1654],\n",
      "        [0.0194],\n",
      "        [0.1679],\n",
      "        [0.1686],\n",
      "        [0.1716],\n",
      "        [0.0254],\n",
      "        [0.1745],\n",
      "        [0.0272],\n",
      "        [0.1749],\n",
      "        [0.0293],\n",
      "        [0.1777],\n",
      "        [0.0307],\n",
      "        [0.0317],\n",
      "        [0.0331],\n",
      "        [0.0335],\n",
      "        [0.0383],\n",
      "        [0.0387],\n",
      "        [0.0393],\n",
      "        [0.1870],\n",
      "        [0.0412],\n",
      "        [0.1889],\n",
      "        [0.0413],\n",
      "        [0.0441],\n",
      "        [0.1927],\n",
      "        [0.0460],\n",
      "        [0.0469],\n",
      "        [0.0508],\n",
      "        [0.0511],\n",
      "        [0.1997],\n",
      "        [0.0531],\n",
      "        [0.0533],\n",
      "        [0.0560],\n",
      "        [0.0576],\n",
      "        [0.0581],\n",
      "        [0.0603],\n",
      "        [0.2086],\n",
      "        [0.2088],\n",
      "        [0.0639],\n",
      "        [0.0642],\n",
      "        [0.2145],\n",
      "        [0.0674],\n",
      "        [0.0677],\n",
      "        [0.2179],\n",
      "        [0.2232],\n",
      "        [0.2272],\n",
      "        [0.2312],\n",
      "        [0.0840],\n",
      "        [0.0858],\n",
      "        [0.2349],\n",
      "        [0.0896],\n",
      "        [0.0940],\n",
      "        [0.2453],\n",
      "        [0.2456],\n",
      "        [0.1078],\n",
      "        [0.1109],\n",
      "        [0.2608],\n",
      "        [0.1136],\n",
      "        [0.1157],\n",
      "        [0.1188],\n",
      "        [0.1233],\n",
      "        [0.1267],\n",
      "        [0.2775],\n",
      "        [0.2783],\n",
      "        [0.2806],\n",
      "        [0.1341],\n",
      "        [0.2847],\n",
      "        [0.1381],\n",
      "        [0.1407],\n",
      "        [0.1417],\n",
      "        [0.2923],\n",
      "        [0.2935],\n",
      "        [0.2938],\n",
      "        [0.1482],\n",
      "        [0.1482],\n",
      "        [0.1516],\n",
      "        [0.1633],\n",
      "        [0.3253],\n",
      "        [0.1793],\n",
      "        [0.1874],\n",
      "        [0.2007],\n",
      "        [0.2028],\n",
      "        [0.2037],\n",
      "        [0.2101],\n",
      "        [0.2135],\n",
      "        [0.2178],\n",
      "        [0.2225],\n",
      "        [0.2236],\n",
      "        [0.2343],\n",
      "        [0.2382],\n",
      "        [0.2510],\n",
      "        [0.2542],\n",
      "        [0.2757],\n",
      "        [0.2836],\n",
      "        [0.3102],\n",
      "        [0.3112],\n",
      "        [0.3160],\n",
      "        [0.3161],\n",
      "        [0.3167],\n",
      "        [0.3168],\n",
      "        [0.3204],\n",
      "        [0.3483]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0740],\n",
      "        [    0.0738],\n",
      "        [    0.0732],\n",
      "        [    0.0863],\n",
      "        [    0.0883],\n",
      "        [    0.0632],\n",
      "        [    0.0619],\n",
      "        [    0.0495],\n",
      "        [    0.0445],\n",
      "        [    0.0438],\n",
      "        [    0.0394],\n",
      "        [    0.0356],\n",
      "        [    0.0355],\n",
      "        [    0.0333],\n",
      "        [    0.0327],\n",
      "        [    0.0315],\n",
      "        [    0.0311],\n",
      "        [    0.1213],\n",
      "        [    0.0243],\n",
      "        [    0.0227],\n",
      "        [    0.1313],\n",
      "        [    0.1320],\n",
      "        [    0.0191],\n",
      "        [    0.0184],\n",
      "        [    0.0170],\n",
      "        [    0.1371],\n",
      "        [    0.0112],\n",
      "        [    0.1421],\n",
      "        [    0.1443],\n",
      "        [    0.0071],\n",
      "        [    0.0049],\n",
      "        [    0.1477],\n",
      "        [    0.0044],\n",
      "        [    0.0003],\n",
      "        [    0.0006],\n",
      "        [    0.1572],\n",
      "        [    0.1573],\n",
      "        [    0.1592],\n",
      "        [    0.0093],\n",
      "        [    0.1632],\n",
      "        [    0.1633],\n",
      "        [    0.0112],\n",
      "        [    0.0126],\n",
      "        [    0.0132],\n",
      "        [    0.0133],\n",
      "        [    0.0138],\n",
      "        [    0.1677],\n",
      "        [    0.0170],\n",
      "        [    0.1702],\n",
      "        [    0.1710],\n",
      "        [    0.1739],\n",
      "        [    0.0231],\n",
      "        [    0.1769],\n",
      "        [    0.0249],\n",
      "        [    0.1773],\n",
      "        [    0.0269],\n",
      "        [    0.1801],\n",
      "        [    0.0284],\n",
      "        [    0.0293],\n",
      "        [    0.0308],\n",
      "        [    0.0311],\n",
      "        [    0.0359],\n",
      "        [    0.0363],\n",
      "        [    0.0370],\n",
      "        [    0.1893],\n",
      "        [    0.0388],\n",
      "        [    0.1912],\n",
      "        [    0.0390],\n",
      "        [    0.0418],\n",
      "        [    0.1951],\n",
      "        [    0.0437],\n",
      "        [    0.0445],\n",
      "        [    0.0485],\n",
      "        [    0.0487],\n",
      "        [    0.2020],\n",
      "        [    0.0507],\n",
      "        [    0.0510],\n",
      "        [    0.0536],\n",
      "        [    0.0552],\n",
      "        [    0.0557],\n",
      "        [    0.0580],\n",
      "        [    0.2110],\n",
      "        [    0.2112],\n",
      "        [    0.0616],\n",
      "        [    0.0618],\n",
      "        [    0.2169],\n",
      "        [    0.0651],\n",
      "        [    0.0653],\n",
      "        [    0.2203],\n",
      "        [    0.2255],\n",
      "        [    0.2295],\n",
      "        [    0.2336],\n",
      "        [    0.0816],\n",
      "        [    0.0835],\n",
      "        [    0.2373],\n",
      "        [    0.0872],\n",
      "        [    0.0916],\n",
      "        [    0.2477],\n",
      "        [    0.2480],\n",
      "        [    0.1054],\n",
      "        [    0.1085],\n",
      "        [    0.2631],\n",
      "        [    0.1112],\n",
      "        [    0.1133],\n",
      "        [    0.1165],\n",
      "        [    0.1209],\n",
      "        [    0.1244],\n",
      "        [    0.2798],\n",
      "        [    0.2807],\n",
      "        [    0.2829],\n",
      "        [    0.1317],\n",
      "        [    0.2871],\n",
      "        [    0.1357],\n",
      "        [    0.1383],\n",
      "        [    0.1394],\n",
      "        [    0.2946],\n",
      "        [    0.2959],\n",
      "        [    0.2962],\n",
      "        [    0.1458],\n",
      "        [    0.1459],\n",
      "        [    0.1493],\n",
      "        [    0.1610],\n",
      "        [    0.3277],\n",
      "        [    0.1770],\n",
      "        [    0.1851],\n",
      "        [    0.1984],\n",
      "        [    0.2004],\n",
      "        [    0.2014],\n",
      "        [    0.2078],\n",
      "        [    0.2112],\n",
      "        [    0.2154],\n",
      "        [    0.2201],\n",
      "        [    0.2213],\n",
      "        [    0.2319],\n",
      "        [    0.2358],\n",
      "        [    0.2486],\n",
      "        [    0.2518],\n",
      "        [    0.2734],\n",
      "        [    0.2812],\n",
      "        [    0.3078],\n",
      "        [    0.3089],\n",
      "        [    0.3136],\n",
      "        [    0.3138],\n",
      "        [    0.3143],\n",
      "        [    0.3144],\n",
      "        [    0.3180],\n",
      "        [    0.3460]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 38.65621066093445\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 148\n",
      "剩餘X 資料 torch.Size([12, 18])\n",
      "剩餘Y 資料 torch.Size([12, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12044147402048111, 11)\n",
      "The second_loss value of k: (0.12466470897197723, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引11，y= tensor([0.3253])\n",
      "目前模型的Data狀態 torch.Size([148, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723],\n",
      "        [0.6723]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0740],\n",
      "        [    0.0738],\n",
      "        [    0.0732],\n",
      "        [    0.0863],\n",
      "        [    0.0883],\n",
      "        [    0.0632],\n",
      "        [    0.0619],\n",
      "        [    0.0495],\n",
      "        [    0.0445],\n",
      "        [    0.0438],\n",
      "        [    0.0394],\n",
      "        [    0.0356],\n",
      "        [    0.0355],\n",
      "        [    0.0333],\n",
      "        [    0.0327],\n",
      "        [    0.0315],\n",
      "        [    0.0311],\n",
      "        [    0.1213],\n",
      "        [    0.0243],\n",
      "        [    0.0227],\n",
      "        [    0.1313],\n",
      "        [    0.1320],\n",
      "        [    0.0191],\n",
      "        [    0.0184],\n",
      "        [    0.0170],\n",
      "        [    0.1371],\n",
      "        [    0.0112],\n",
      "        [    0.1421],\n",
      "        [    0.1443],\n",
      "        [    0.0071],\n",
      "        [    0.0049],\n",
      "        [    0.1477],\n",
      "        [    0.0044],\n",
      "        [    0.0003],\n",
      "        [    0.0006],\n",
      "        [    0.1572],\n",
      "        [    0.1573],\n",
      "        [    0.1592],\n",
      "        [    0.0093],\n",
      "        [    0.1632],\n",
      "        [    0.1633],\n",
      "        [    0.0112],\n",
      "        [    0.0126],\n",
      "        [    0.0132],\n",
      "        [    0.0133],\n",
      "        [    0.0138],\n",
      "        [    0.1677],\n",
      "        [    0.0170],\n",
      "        [    0.1702],\n",
      "        [    0.1710],\n",
      "        [    0.1739],\n",
      "        [    0.0231],\n",
      "        [    0.1769],\n",
      "        [    0.0249],\n",
      "        [    0.1773],\n",
      "        [    0.0269],\n",
      "        [    0.1801],\n",
      "        [    0.0284],\n",
      "        [    0.0293],\n",
      "        [    0.0308],\n",
      "        [    0.0311],\n",
      "        [    0.0359],\n",
      "        [    0.0363],\n",
      "        [    0.0370],\n",
      "        [    0.1893],\n",
      "        [    0.0388],\n",
      "        [    0.1912],\n",
      "        [    0.0390],\n",
      "        [    0.0418],\n",
      "        [    0.1951],\n",
      "        [    0.0437],\n",
      "        [    0.0445],\n",
      "        [    0.0485],\n",
      "        [    0.0487],\n",
      "        [    0.2020],\n",
      "        [    0.0507],\n",
      "        [    0.0510],\n",
      "        [    0.0536],\n",
      "        [    0.0552],\n",
      "        [    0.0557],\n",
      "        [    0.0580],\n",
      "        [    0.2110],\n",
      "        [    0.2112],\n",
      "        [    0.0616],\n",
      "        [    0.0618],\n",
      "        [    0.2169],\n",
      "        [    0.0651],\n",
      "        [    0.0653],\n",
      "        [    0.2203],\n",
      "        [    0.2255],\n",
      "        [    0.2295],\n",
      "        [    0.2336],\n",
      "        [    0.0816],\n",
      "        [    0.0835],\n",
      "        [    0.2373],\n",
      "        [    0.0872],\n",
      "        [    0.0916],\n",
      "        [    0.2477],\n",
      "        [    0.2480],\n",
      "        [    0.1054],\n",
      "        [    0.1085],\n",
      "        [    0.2631],\n",
      "        [    0.1112],\n",
      "        [    0.1133],\n",
      "        [    0.1165],\n",
      "        [    0.1209],\n",
      "        [    0.1244],\n",
      "        [    0.2798],\n",
      "        [    0.2807],\n",
      "        [    0.2829],\n",
      "        [    0.1317],\n",
      "        [    0.2871],\n",
      "        [    0.1357],\n",
      "        [    0.1383],\n",
      "        [    0.1394],\n",
      "        [    0.2946],\n",
      "        [    0.2959],\n",
      "        [    0.2962],\n",
      "        [    0.1458],\n",
      "        [    0.1459],\n",
      "        [    0.1493],\n",
      "        [    0.1610],\n",
      "        [    0.3277],\n",
      "        [    0.1770],\n",
      "        [    0.1851],\n",
      "        [    0.1984],\n",
      "        [    0.2004],\n",
      "        [    0.2014],\n",
      "        [    0.2078],\n",
      "        [    0.2112],\n",
      "        [    0.2154],\n",
      "        [    0.2201],\n",
      "        [    0.2213],\n",
      "        [    0.2319],\n",
      "        [    0.2358],\n",
      "        [    0.2486],\n",
      "        [    0.2518],\n",
      "        [    0.2734],\n",
      "        [    0.2812],\n",
      "        [    0.3078],\n",
      "        [    0.3089],\n",
      "        [    0.3136],\n",
      "        [    0.3138],\n",
      "        [    0.3143],\n",
      "        [    0.3144],\n",
      "        [    0.3180],\n",
      "        [    0.3460],\n",
      "        [    0.3470]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0763],\n",
      "        [0.0762],\n",
      "        [0.0755],\n",
      "        [0.0886],\n",
      "        [0.0906],\n",
      "        [0.0655],\n",
      "        [0.0643],\n",
      "        [0.0518],\n",
      "        [0.0468],\n",
      "        [0.0461],\n",
      "        [0.0418],\n",
      "        [0.0379],\n",
      "        [0.0378],\n",
      "        [0.0357],\n",
      "        [0.0350],\n",
      "        [0.0339],\n",
      "        [0.0334],\n",
      "        [0.1236],\n",
      "        [0.0266],\n",
      "        [0.0251],\n",
      "        [0.1337],\n",
      "        [0.1343],\n",
      "        [0.0215],\n",
      "        [0.0208],\n",
      "        [0.0193],\n",
      "        [0.1395],\n",
      "        [0.0136],\n",
      "        [0.1445],\n",
      "        [0.1467],\n",
      "        [0.0095],\n",
      "        [0.0072],\n",
      "        [0.1500],\n",
      "        [0.0068],\n",
      "        [0.0021],\n",
      "        [0.0018],\n",
      "        [0.1596],\n",
      "        [0.1597],\n",
      "        [0.1616],\n",
      "        [0.0069],\n",
      "        [0.1655],\n",
      "        [0.1657],\n",
      "        [0.0089],\n",
      "        [0.0102],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0115],\n",
      "        [0.1701],\n",
      "        [0.0147],\n",
      "        [0.1726],\n",
      "        [0.1733],\n",
      "        [0.1763],\n",
      "        [0.0207],\n",
      "        [0.1792],\n",
      "        [0.0225],\n",
      "        [0.1796],\n",
      "        [0.0246],\n",
      "        [0.1824],\n",
      "        [0.0260],\n",
      "        [0.0270],\n",
      "        [0.0284],\n",
      "        [0.0288],\n",
      "        [0.0336],\n",
      "        [0.0340],\n",
      "        [0.0346],\n",
      "        [0.1917],\n",
      "        [0.0365],\n",
      "        [0.1936],\n",
      "        [0.0366],\n",
      "        [0.0394],\n",
      "        [0.1974],\n",
      "        [0.0413],\n",
      "        [0.0422],\n",
      "        [0.0461],\n",
      "        [0.0464],\n",
      "        [0.2044],\n",
      "        [0.0484],\n",
      "        [0.0486],\n",
      "        [0.0512],\n",
      "        [0.0529],\n",
      "        [0.0534],\n",
      "        [0.0556],\n",
      "        [0.2133],\n",
      "        [0.2135],\n",
      "        [0.0592],\n",
      "        [0.0595],\n",
      "        [0.2193],\n",
      "        [0.0627],\n",
      "        [0.0630],\n",
      "        [0.2226],\n",
      "        [0.2279],\n",
      "        [0.2319],\n",
      "        [0.2359],\n",
      "        [0.0793],\n",
      "        [0.0811],\n",
      "        [0.2396],\n",
      "        [0.0849],\n",
      "        [0.0893],\n",
      "        [0.2500],\n",
      "        [0.2503],\n",
      "        [0.1031],\n",
      "        [0.1062],\n",
      "        [0.2655],\n",
      "        [0.1089],\n",
      "        [0.1110],\n",
      "        [0.1141],\n",
      "        [0.1185],\n",
      "        [0.1220],\n",
      "        [0.2822],\n",
      "        [0.2830],\n",
      "        [0.2853],\n",
      "        [0.1294],\n",
      "        [0.2894],\n",
      "        [0.1333],\n",
      "        [0.1360],\n",
      "        [0.1370],\n",
      "        [0.2970],\n",
      "        [0.2982],\n",
      "        [0.2985],\n",
      "        [0.1434],\n",
      "        [0.1435],\n",
      "        [0.1469],\n",
      "        [0.1586],\n",
      "        [0.3300],\n",
      "        [0.1746],\n",
      "        [0.1827],\n",
      "        [0.1960],\n",
      "        [0.1981],\n",
      "        [0.1990],\n",
      "        [0.2054],\n",
      "        [0.2088],\n",
      "        [0.2131],\n",
      "        [0.2178],\n",
      "        [0.2189],\n",
      "        [0.2295],\n",
      "        [0.2335],\n",
      "        [0.2463],\n",
      "        [0.2495],\n",
      "        [0.2710],\n",
      "        [0.2789],\n",
      "        [0.3055],\n",
      "        [0.3065],\n",
      "        [0.3113],\n",
      "        [0.3114],\n",
      "        [0.3120],\n",
      "        [0.3121],\n",
      "        [0.3157],\n",
      "        [0.3436],\n",
      "        [0.3447]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 38.8938524723053\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 149\n",
      "剩餘X 資料 torch.Size([11, 18])\n",
      "剩餘Y 資料 torch.Size([11, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1230146735906601, 6)\n",
      "The second_loss value of k: (0.12658464908599854, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.3192])\n",
      "目前模型的Data狀態 torch.Size([149, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700],\n",
      "        [0.6700]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0763],\n",
      "        [0.0762],\n",
      "        [0.0755],\n",
      "        [0.0886],\n",
      "        [0.0906],\n",
      "        [0.0655],\n",
      "        [0.0643],\n",
      "        [0.0518],\n",
      "        [0.0468],\n",
      "        [0.0461],\n",
      "        [0.0418],\n",
      "        [0.0379],\n",
      "        [0.0378],\n",
      "        [0.0357],\n",
      "        [0.0350],\n",
      "        [0.0339],\n",
      "        [0.0334],\n",
      "        [0.1236],\n",
      "        [0.0266],\n",
      "        [0.0251],\n",
      "        [0.1337],\n",
      "        [0.1343],\n",
      "        [0.0215],\n",
      "        [0.0208],\n",
      "        [0.0193],\n",
      "        [0.1395],\n",
      "        [0.0136],\n",
      "        [0.1445],\n",
      "        [0.1467],\n",
      "        [0.0095],\n",
      "        [0.0072],\n",
      "        [0.1500],\n",
      "        [0.0068],\n",
      "        [0.0021],\n",
      "        [0.0018],\n",
      "        [0.1596],\n",
      "        [0.1597],\n",
      "        [0.1616],\n",
      "        [0.0069],\n",
      "        [0.1655],\n",
      "        [0.1657],\n",
      "        [0.0089],\n",
      "        [0.0102],\n",
      "        [0.0108],\n",
      "        [0.0110],\n",
      "        [0.0115],\n",
      "        [0.1701],\n",
      "        [0.0147],\n",
      "        [0.1726],\n",
      "        [0.1733],\n",
      "        [0.1763],\n",
      "        [0.0207],\n",
      "        [0.1792],\n",
      "        [0.0225],\n",
      "        [0.1796],\n",
      "        [0.0246],\n",
      "        [0.1824],\n",
      "        [0.0260],\n",
      "        [0.0270],\n",
      "        [0.0284],\n",
      "        [0.0288],\n",
      "        [0.0336],\n",
      "        [0.0340],\n",
      "        [0.0346],\n",
      "        [0.1917],\n",
      "        [0.0365],\n",
      "        [0.1936],\n",
      "        [0.0366],\n",
      "        [0.0394],\n",
      "        [0.1974],\n",
      "        [0.0413],\n",
      "        [0.0422],\n",
      "        [0.0461],\n",
      "        [0.0464],\n",
      "        [0.2044],\n",
      "        [0.0484],\n",
      "        [0.0486],\n",
      "        [0.0512],\n",
      "        [0.0529],\n",
      "        [0.0534],\n",
      "        [0.0556],\n",
      "        [0.2133],\n",
      "        [0.2135],\n",
      "        [0.0592],\n",
      "        [0.0595],\n",
      "        [0.2193],\n",
      "        [0.0627],\n",
      "        [0.0630],\n",
      "        [0.2226],\n",
      "        [0.2279],\n",
      "        [0.2319],\n",
      "        [0.2359],\n",
      "        [0.0793],\n",
      "        [0.0811],\n",
      "        [0.2396],\n",
      "        [0.0849],\n",
      "        [0.0893],\n",
      "        [0.2500],\n",
      "        [0.2503],\n",
      "        [0.1031],\n",
      "        [0.1062],\n",
      "        [0.2655],\n",
      "        [0.1089],\n",
      "        [0.1110],\n",
      "        [0.1141],\n",
      "        [0.1185],\n",
      "        [0.1220],\n",
      "        [0.2822],\n",
      "        [0.2830],\n",
      "        [0.2853],\n",
      "        [0.1294],\n",
      "        [0.2894],\n",
      "        [0.1333],\n",
      "        [0.1360],\n",
      "        [0.1370],\n",
      "        [0.2970],\n",
      "        [0.2982],\n",
      "        [0.2985],\n",
      "        [0.1434],\n",
      "        [0.1435],\n",
      "        [0.1469],\n",
      "        [0.1586],\n",
      "        [0.3300],\n",
      "        [0.1746],\n",
      "        [0.1827],\n",
      "        [0.1960],\n",
      "        [0.1981],\n",
      "        [0.1990],\n",
      "        [0.2054],\n",
      "        [0.2088],\n",
      "        [0.2131],\n",
      "        [0.2178],\n",
      "        [0.2189],\n",
      "        [0.2295],\n",
      "        [0.2335],\n",
      "        [0.2463],\n",
      "        [0.2495],\n",
      "        [0.2710],\n",
      "        [0.2789],\n",
      "        [0.3055],\n",
      "        [0.3065],\n",
      "        [0.3113],\n",
      "        [0.3114],\n",
      "        [0.3120],\n",
      "        [0.3121],\n",
      "        [0.3157],\n",
      "        [0.3436],\n",
      "        [0.3447],\n",
      "        [0.3507]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 81\n",
      "Number of shrink: 19\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0787],\n",
      "        [0.0785],\n",
      "        [0.0779],\n",
      "        [0.0910],\n",
      "        [0.0930],\n",
      "        [0.0679],\n",
      "        [0.0666],\n",
      "        [0.0542],\n",
      "        [0.0492],\n",
      "        [0.0485],\n",
      "        [0.0441],\n",
      "        [0.0403],\n",
      "        [0.0402],\n",
      "        [0.0380],\n",
      "        [0.0374],\n",
      "        [0.0362],\n",
      "        [0.0358],\n",
      "        [0.1260],\n",
      "        [0.0290],\n",
      "        [0.0274],\n",
      "        [0.1360],\n",
      "        [0.1367],\n",
      "        [0.0238],\n",
      "        [0.0231],\n",
      "        [0.0217],\n",
      "        [0.1418],\n",
      "        [0.0159],\n",
      "        [0.1468],\n",
      "        [0.1490],\n",
      "        [0.0118],\n",
      "        [0.0096],\n",
      "        [0.1524],\n",
      "        [0.0091],\n",
      "        [0.0044],\n",
      "        [0.0041],\n",
      "        [0.1619],\n",
      "        [0.1620],\n",
      "        [0.1639],\n",
      "        [0.0046],\n",
      "        [0.1679],\n",
      "        [0.1680],\n",
      "        [0.0065],\n",
      "        [0.0079],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0091],\n",
      "        [0.1724],\n",
      "        [0.0123],\n",
      "        [0.1749],\n",
      "        [0.1757],\n",
      "        [0.1786],\n",
      "        [0.0184],\n",
      "        [0.1816],\n",
      "        [0.0202],\n",
      "        [0.1820],\n",
      "        [0.0222],\n",
      "        [0.1848],\n",
      "        [0.0237],\n",
      "        [0.0246],\n",
      "        [0.0261],\n",
      "        [0.0264],\n",
      "        [0.0312],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.1940],\n",
      "        [0.0341],\n",
      "        [0.1959],\n",
      "        [0.0343],\n",
      "        [0.0371],\n",
      "        [0.1998],\n",
      "        [0.0390],\n",
      "        [0.0398],\n",
      "        [0.0438],\n",
      "        [0.0440],\n",
      "        [0.2067],\n",
      "        [0.0460],\n",
      "        [0.0463],\n",
      "        [0.0489],\n",
      "        [0.0505],\n",
      "        [0.0510],\n",
      "        [0.0533],\n",
      "        [0.2157],\n",
      "        [0.2159],\n",
      "        [0.0569],\n",
      "        [0.0571],\n",
      "        [0.2216],\n",
      "        [0.0604],\n",
      "        [0.0606],\n",
      "        [0.2250],\n",
      "        [0.2302],\n",
      "        [0.2342],\n",
      "        [0.2383],\n",
      "        [0.0769],\n",
      "        [0.0788],\n",
      "        [0.2420],\n",
      "        [0.0825],\n",
      "        [0.0869],\n",
      "        [0.2524],\n",
      "        [0.2527],\n",
      "        [0.1007],\n",
      "        [0.1038],\n",
      "        [0.2678],\n",
      "        [0.1065],\n",
      "        [0.1086],\n",
      "        [0.1118],\n",
      "        [0.1162],\n",
      "        [0.1197],\n",
      "        [0.2845],\n",
      "        [0.2854],\n",
      "        [0.2876],\n",
      "        [0.1270],\n",
      "        [0.2918],\n",
      "        [0.1310],\n",
      "        [0.1336],\n",
      "        [0.1347],\n",
      "        [0.2993],\n",
      "        [0.3006],\n",
      "        [0.3009],\n",
      "        [0.1411],\n",
      "        [0.1412],\n",
      "        [0.1446],\n",
      "        [0.1563],\n",
      "        [0.3324],\n",
      "        [0.1723],\n",
      "        [0.1804],\n",
      "        [0.1937],\n",
      "        [0.1957],\n",
      "        [0.1967],\n",
      "        [0.2031],\n",
      "        [0.2065],\n",
      "        [0.2107],\n",
      "        [0.2154],\n",
      "        [0.2166],\n",
      "        [0.2272],\n",
      "        [0.2311],\n",
      "        [0.2439],\n",
      "        [0.2471],\n",
      "        [0.2687],\n",
      "        [0.2765],\n",
      "        [0.3031],\n",
      "        [0.3042],\n",
      "        [0.3089],\n",
      "        [0.3091],\n",
      "        [0.3096],\n",
      "        [0.3097],\n",
      "        [0.3133],\n",
      "        [0.3413],\n",
      "        [0.3423],\n",
      "        [0.3484]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 39.13794302940369\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 150\n",
      "剩餘X 資料 torch.Size([10, 18])\n",
      "剩餘Y 資料 torch.Size([10, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12491478770971298, 8)\n",
      "The second_loss value of k: (0.12686043977737427, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.3142])\n",
      "目前模型的Data狀態 torch.Size([150, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676],\n",
      "        [0.6676]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0787],\n",
      "        [0.0785],\n",
      "        [0.0779],\n",
      "        [0.0910],\n",
      "        [0.0930],\n",
      "        [0.0679],\n",
      "        [0.0666],\n",
      "        [0.0542],\n",
      "        [0.0492],\n",
      "        [0.0485],\n",
      "        [0.0441],\n",
      "        [0.0403],\n",
      "        [0.0402],\n",
      "        [0.0380],\n",
      "        [0.0374],\n",
      "        [0.0362],\n",
      "        [0.0358],\n",
      "        [0.1260],\n",
      "        [0.0290],\n",
      "        [0.0274],\n",
      "        [0.1360],\n",
      "        [0.1367],\n",
      "        [0.0238],\n",
      "        [0.0231],\n",
      "        [0.0217],\n",
      "        [0.1418],\n",
      "        [0.0159],\n",
      "        [0.1468],\n",
      "        [0.1490],\n",
      "        [0.0118],\n",
      "        [0.0096],\n",
      "        [0.1524],\n",
      "        [0.0091],\n",
      "        [0.0044],\n",
      "        [0.0041],\n",
      "        [0.1619],\n",
      "        [0.1620],\n",
      "        [0.1639],\n",
      "        [0.0046],\n",
      "        [0.1679],\n",
      "        [0.1680],\n",
      "        [0.0065],\n",
      "        [0.0079],\n",
      "        [0.0085],\n",
      "        [0.0086],\n",
      "        [0.0091],\n",
      "        [0.1724],\n",
      "        [0.0123],\n",
      "        [0.1749],\n",
      "        [0.1757],\n",
      "        [0.1786],\n",
      "        [0.0184],\n",
      "        [0.1816],\n",
      "        [0.0202],\n",
      "        [0.1820],\n",
      "        [0.0222],\n",
      "        [0.1848],\n",
      "        [0.0237],\n",
      "        [0.0246],\n",
      "        [0.0261],\n",
      "        [0.0264],\n",
      "        [0.0312],\n",
      "        [0.0316],\n",
      "        [0.0323],\n",
      "        [0.1940],\n",
      "        [0.0341],\n",
      "        [0.1959],\n",
      "        [0.0343],\n",
      "        [0.0371],\n",
      "        [0.1998],\n",
      "        [0.0390],\n",
      "        [0.0398],\n",
      "        [0.0438],\n",
      "        [0.0440],\n",
      "        [0.2067],\n",
      "        [0.0460],\n",
      "        [0.0463],\n",
      "        [0.0489],\n",
      "        [0.0505],\n",
      "        [0.0510],\n",
      "        [0.0533],\n",
      "        [0.2157],\n",
      "        [0.2159],\n",
      "        [0.0569],\n",
      "        [0.0571],\n",
      "        [0.2216],\n",
      "        [0.0604],\n",
      "        [0.0606],\n",
      "        [0.2250],\n",
      "        [0.2302],\n",
      "        [0.2342],\n",
      "        [0.2383],\n",
      "        [0.0769],\n",
      "        [0.0788],\n",
      "        [0.2420],\n",
      "        [0.0825],\n",
      "        [0.0869],\n",
      "        [0.2524],\n",
      "        [0.2527],\n",
      "        [0.1007],\n",
      "        [0.1038],\n",
      "        [0.2678],\n",
      "        [0.1065],\n",
      "        [0.1086],\n",
      "        [0.1118],\n",
      "        [0.1162],\n",
      "        [0.1197],\n",
      "        [0.2845],\n",
      "        [0.2854],\n",
      "        [0.2876],\n",
      "        [0.1270],\n",
      "        [0.2918],\n",
      "        [0.1310],\n",
      "        [0.1336],\n",
      "        [0.1347],\n",
      "        [0.2993],\n",
      "        [0.3006],\n",
      "        [0.3009],\n",
      "        [0.1411],\n",
      "        [0.1412],\n",
      "        [0.1446],\n",
      "        [0.1563],\n",
      "        [0.3324],\n",
      "        [0.1723],\n",
      "        [0.1804],\n",
      "        [0.1937],\n",
      "        [0.1957],\n",
      "        [0.1967],\n",
      "        [0.2031],\n",
      "        [0.2065],\n",
      "        [0.2107],\n",
      "        [0.2154],\n",
      "        [0.2166],\n",
      "        [0.2272],\n",
      "        [0.2311],\n",
      "        [0.2439],\n",
      "        [0.2471],\n",
      "        [0.2687],\n",
      "        [0.2765],\n",
      "        [0.3031],\n",
      "        [0.3042],\n",
      "        [0.3089],\n",
      "        [0.3091],\n",
      "        [0.3096],\n",
      "        [0.3097],\n",
      "        [0.3133],\n",
      "        [0.3413],\n",
      "        [0.3423],\n",
      "        [0.3484],\n",
      "        [0.3534]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0810],\n",
      "        [0.0809],\n",
      "        [0.0803],\n",
      "        [0.0933],\n",
      "        [0.0953],\n",
      "        [0.0702],\n",
      "        [0.0690],\n",
      "        [0.0565],\n",
      "        [0.0515],\n",
      "        [0.0508],\n",
      "        [0.0465],\n",
      "        [0.0427],\n",
      "        [0.0425],\n",
      "        [0.0404],\n",
      "        [0.0398],\n",
      "        [0.0386],\n",
      "        [0.0381],\n",
      "        [0.1284],\n",
      "        [0.0313],\n",
      "        [0.0298],\n",
      "        [0.1384],\n",
      "        [0.1390],\n",
      "        [0.0262],\n",
      "        [0.0255],\n",
      "        [0.0240],\n",
      "        [0.1442],\n",
      "        [0.0183],\n",
      "        [0.1492],\n",
      "        [0.1514],\n",
      "        [0.0142],\n",
      "        [0.0119],\n",
      "        [0.1547],\n",
      "        [0.0115],\n",
      "        [0.0068],\n",
      "        [0.0065],\n",
      "        [0.1643],\n",
      "        [0.1644],\n",
      "        [0.1663],\n",
      "        [0.0022],\n",
      "        [0.1702],\n",
      "        [0.1704],\n",
      "        [0.0042],\n",
      "        [0.0055],\n",
      "        [0.0061],\n",
      "        [0.0063],\n",
      "        [0.0068],\n",
      "        [0.1748],\n",
      "        [0.0099],\n",
      "        [0.1773],\n",
      "        [0.1780],\n",
      "        [0.1810],\n",
      "        [0.0160],\n",
      "        [0.1839],\n",
      "        [0.0178],\n",
      "        [0.1843],\n",
      "        [0.0199],\n",
      "        [0.1872],\n",
      "        [0.0213],\n",
      "        [0.0223],\n",
      "        [0.0237],\n",
      "        [0.0241],\n",
      "        [0.0289],\n",
      "        [0.0293],\n",
      "        [0.0299],\n",
      "        [0.1964],\n",
      "        [0.0318],\n",
      "        [0.1983],\n",
      "        [0.0319],\n",
      "        [0.0347],\n",
      "        [0.2022],\n",
      "        [0.0366],\n",
      "        [0.0375],\n",
      "        [0.0414],\n",
      "        [0.0417],\n",
      "        [0.2091],\n",
      "        [0.0437],\n",
      "        [0.0439],\n",
      "        [0.0465],\n",
      "        [0.0482],\n",
      "        [0.0487],\n",
      "        [0.0509],\n",
      "        [0.2181],\n",
      "        [0.2182],\n",
      "        [0.0545],\n",
      "        [0.0548],\n",
      "        [0.2240],\n",
      "        [0.0580],\n",
      "        [0.0583],\n",
      "        [0.2273],\n",
      "        [0.2326],\n",
      "        [0.2366],\n",
      "        [0.2407],\n",
      "        [0.0746],\n",
      "        [0.0764],\n",
      "        [0.2443],\n",
      "        [0.0801],\n",
      "        [0.0846],\n",
      "        [0.2547],\n",
      "        [0.2550],\n",
      "        [0.0984],\n",
      "        [0.1015],\n",
      "        [0.2702],\n",
      "        [0.1042],\n",
      "        [0.1063],\n",
      "        [0.1094],\n",
      "        [0.1138],\n",
      "        [0.1173],\n",
      "        [0.2869],\n",
      "        [0.2877],\n",
      "        [0.2900],\n",
      "        [0.1247],\n",
      "        [0.2941],\n",
      "        [0.1286],\n",
      "        [0.1313],\n",
      "        [0.1323],\n",
      "        [0.3017],\n",
      "        [0.3029],\n",
      "        [0.3032],\n",
      "        [0.1387],\n",
      "        [0.1388],\n",
      "        [0.1422],\n",
      "        [0.1539],\n",
      "        [0.3347],\n",
      "        [0.1699],\n",
      "        [0.1780],\n",
      "        [0.1913],\n",
      "        [0.1933],\n",
      "        [0.1943],\n",
      "        [0.2007],\n",
      "        [0.2041],\n",
      "        [0.2084],\n",
      "        [0.2131],\n",
      "        [0.2142],\n",
      "        [0.2248],\n",
      "        [0.2287],\n",
      "        [0.2416],\n",
      "        [0.2448],\n",
      "        [0.2663],\n",
      "        [0.2741],\n",
      "        [0.3008],\n",
      "        [0.3018],\n",
      "        [0.3065],\n",
      "        [0.3067],\n",
      "        [0.3072],\n",
      "        [0.3074],\n",
      "        [0.3110],\n",
      "        [0.3389],\n",
      "        [0.3400],\n",
      "        [0.3460],\n",
      "        [0.3511]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 39.37089467048645\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 151\n",
      "剩餘X 資料 torch.Size([9, 18])\n",
      "剩餘Y 資料 torch.Size([9, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12518753111362457, 8)\n",
      "The second_loss value of k: (0.127778559923172, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.3114])\n",
      "目前模型的Data狀態 torch.Size([151, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653],\n",
      "        [0.6653]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0810],\n",
      "        [0.0809],\n",
      "        [0.0803],\n",
      "        [0.0933],\n",
      "        [0.0953],\n",
      "        [0.0702],\n",
      "        [0.0690],\n",
      "        [0.0565],\n",
      "        [0.0515],\n",
      "        [0.0508],\n",
      "        [0.0465],\n",
      "        [0.0427],\n",
      "        [0.0425],\n",
      "        [0.0404],\n",
      "        [0.0398],\n",
      "        [0.0386],\n",
      "        [0.0381],\n",
      "        [0.1284],\n",
      "        [0.0313],\n",
      "        [0.0298],\n",
      "        [0.1384],\n",
      "        [0.1390],\n",
      "        [0.0262],\n",
      "        [0.0255],\n",
      "        [0.0240],\n",
      "        [0.1442],\n",
      "        [0.0183],\n",
      "        [0.1492],\n",
      "        [0.1514],\n",
      "        [0.0142],\n",
      "        [0.0119],\n",
      "        [0.1547],\n",
      "        [0.0115],\n",
      "        [0.0068],\n",
      "        [0.0065],\n",
      "        [0.1643],\n",
      "        [0.1644],\n",
      "        [0.1663],\n",
      "        [0.0022],\n",
      "        [0.1702],\n",
      "        [0.1704],\n",
      "        [0.0042],\n",
      "        [0.0055],\n",
      "        [0.0061],\n",
      "        [0.0063],\n",
      "        [0.0068],\n",
      "        [0.1748],\n",
      "        [0.0099],\n",
      "        [0.1773],\n",
      "        [0.1780],\n",
      "        [0.1810],\n",
      "        [0.0160],\n",
      "        [0.1839],\n",
      "        [0.0178],\n",
      "        [0.1843],\n",
      "        [0.0199],\n",
      "        [0.1872],\n",
      "        [0.0213],\n",
      "        [0.0223],\n",
      "        [0.0237],\n",
      "        [0.0241],\n",
      "        [0.0289],\n",
      "        [0.0293],\n",
      "        [0.0299],\n",
      "        [0.1964],\n",
      "        [0.0318],\n",
      "        [0.1983],\n",
      "        [0.0319],\n",
      "        [0.0347],\n",
      "        [0.2022],\n",
      "        [0.0366],\n",
      "        [0.0375],\n",
      "        [0.0414],\n",
      "        [0.0417],\n",
      "        [0.2091],\n",
      "        [0.0437],\n",
      "        [0.0439],\n",
      "        [0.0465],\n",
      "        [0.0482],\n",
      "        [0.0487],\n",
      "        [0.0509],\n",
      "        [0.2181],\n",
      "        [0.2182],\n",
      "        [0.0545],\n",
      "        [0.0548],\n",
      "        [0.2240],\n",
      "        [0.0580],\n",
      "        [0.0583],\n",
      "        [0.2273],\n",
      "        [0.2326],\n",
      "        [0.2366],\n",
      "        [0.2407],\n",
      "        [0.0746],\n",
      "        [0.0764],\n",
      "        [0.2443],\n",
      "        [0.0801],\n",
      "        [0.0846],\n",
      "        [0.2547],\n",
      "        [0.2550],\n",
      "        [0.0984],\n",
      "        [0.1015],\n",
      "        [0.2702],\n",
      "        [0.1042],\n",
      "        [0.1063],\n",
      "        [0.1094],\n",
      "        [0.1138],\n",
      "        [0.1173],\n",
      "        [0.2869],\n",
      "        [0.2877],\n",
      "        [0.2900],\n",
      "        [0.1247],\n",
      "        [0.2941],\n",
      "        [0.1286],\n",
      "        [0.1313],\n",
      "        [0.1323],\n",
      "        [0.3017],\n",
      "        [0.3029],\n",
      "        [0.3032],\n",
      "        [0.1387],\n",
      "        [0.1388],\n",
      "        [0.1422],\n",
      "        [0.1539],\n",
      "        [0.3347],\n",
      "        [0.1699],\n",
      "        [0.1780],\n",
      "        [0.1913],\n",
      "        [0.1933],\n",
      "        [0.1943],\n",
      "        [0.2007],\n",
      "        [0.2041],\n",
      "        [0.2084],\n",
      "        [0.2131],\n",
      "        [0.2142],\n",
      "        [0.2248],\n",
      "        [0.2287],\n",
      "        [0.2416],\n",
      "        [0.2448],\n",
      "        [0.2663],\n",
      "        [0.2741],\n",
      "        [0.3008],\n",
      "        [0.3018],\n",
      "        [0.3065],\n",
      "        [0.3067],\n",
      "        [0.3072],\n",
      "        [0.3074],\n",
      "        [0.3110],\n",
      "        [0.3389],\n",
      "        [0.3400],\n",
      "        [0.3460],\n",
      "        [0.3511],\n",
      "        [0.3538]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 71\n",
      "Number of shrink: 29\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0834],\n",
      "        [    0.0832],\n",
      "        [    0.0826],\n",
      "        [    0.0957],\n",
      "        [    0.0977],\n",
      "        [    0.0726],\n",
      "        [    0.0713],\n",
      "        [    0.0589],\n",
      "        [    0.0539],\n",
      "        [    0.0532],\n",
      "        [    0.0488],\n",
      "        [    0.0450],\n",
      "        [    0.0449],\n",
      "        [    0.0427],\n",
      "        [    0.0421],\n",
      "        [    0.0409],\n",
      "        [    0.0405],\n",
      "        [    0.1307],\n",
      "        [    0.0337],\n",
      "        [    0.0321],\n",
      "        [    0.1407],\n",
      "        [    0.1414],\n",
      "        [    0.0285],\n",
      "        [    0.0278],\n",
      "        [    0.0264],\n",
      "        [    0.1465],\n",
      "        [    0.0206],\n",
      "        [    0.1515],\n",
      "        [    0.1537],\n",
      "        [    0.0165],\n",
      "        [    0.0143],\n",
      "        [    0.1571],\n",
      "        [    0.0138],\n",
      "        [    0.0091],\n",
      "        [    0.0088],\n",
      "        [    0.1666],\n",
      "        [    0.1667],\n",
      "        [    0.1686],\n",
      "        [    0.0001],\n",
      "        [    0.1726],\n",
      "        [    0.1727],\n",
      "        [    0.0018],\n",
      "        [    0.0032],\n",
      "        [    0.0038],\n",
      "        [    0.0039],\n",
      "        [    0.0044],\n",
      "        [    0.1771],\n",
      "        [    0.0076],\n",
      "        [    0.1796],\n",
      "        [    0.1804],\n",
      "        [    0.1833],\n",
      "        [    0.0137],\n",
      "        [    0.1863],\n",
      "        [    0.0155],\n",
      "        [    0.1867],\n",
      "        [    0.0175],\n",
      "        [    0.1895],\n",
      "        [    0.0190],\n",
      "        [    0.0199],\n",
      "        [    0.0214],\n",
      "        [    0.0217],\n",
      "        [    0.0265],\n",
      "        [    0.0269],\n",
      "        [    0.0276],\n",
      "        [    0.1987],\n",
      "        [    0.0294],\n",
      "        [    0.2006],\n",
      "        [    0.0296],\n",
      "        [    0.0324],\n",
      "        [    0.2045],\n",
      "        [    0.0343],\n",
      "        [    0.0351],\n",
      "        [    0.0391],\n",
      "        [    0.0393],\n",
      "        [    0.2114],\n",
      "        [    0.0413],\n",
      "        [    0.0416],\n",
      "        [    0.0442],\n",
      "        [    0.0458],\n",
      "        [    0.0463],\n",
      "        [    0.0486],\n",
      "        [    0.2204],\n",
      "        [    0.2206],\n",
      "        [    0.0522],\n",
      "        [    0.0524],\n",
      "        [    0.2263],\n",
      "        [    0.0557],\n",
      "        [    0.0559],\n",
      "        [    0.2297],\n",
      "        [    0.2349],\n",
      "        [    0.2389],\n",
      "        [    0.2430],\n",
      "        [    0.0722],\n",
      "        [    0.0741],\n",
      "        [    0.2467],\n",
      "        [    0.0778],\n",
      "        [    0.0822],\n",
      "        [    0.2571],\n",
      "        [    0.2574],\n",
      "        [    0.0960],\n",
      "        [    0.0991],\n",
      "        [    0.2725],\n",
      "        [    0.1018],\n",
      "        [    0.1039],\n",
      "        [    0.1071],\n",
      "        [    0.1115],\n",
      "        [    0.1150],\n",
      "        [    0.2892],\n",
      "        [    0.2901],\n",
      "        [    0.2923],\n",
      "        [    0.1223],\n",
      "        [    0.2965],\n",
      "        [    0.1263],\n",
      "        [    0.1289],\n",
      "        [    0.1300],\n",
      "        [    0.3040],\n",
      "        [    0.3053],\n",
      "        [    0.3056],\n",
      "        [    0.1364],\n",
      "        [    0.1365],\n",
      "        [    0.1399],\n",
      "        [    0.1516],\n",
      "        [    0.3371],\n",
      "        [    0.1676],\n",
      "        [    0.1757],\n",
      "        [    0.1890],\n",
      "        [    0.1910],\n",
      "        [    0.1920],\n",
      "        [    0.1984],\n",
      "        [    0.2018],\n",
      "        [    0.2060],\n",
      "        [    0.2107],\n",
      "        [    0.2119],\n",
      "        [    0.2225],\n",
      "        [    0.2264],\n",
      "        [    0.2392],\n",
      "        [    0.2424],\n",
      "        [    0.2640],\n",
      "        [    0.2718],\n",
      "        [    0.2984],\n",
      "        [    0.2995],\n",
      "        [    0.3042],\n",
      "        [    0.3044],\n",
      "        [    0.3049],\n",
      "        [    0.3050],\n",
      "        [    0.3086],\n",
      "        [    0.3366],\n",
      "        [    0.3376],\n",
      "        [    0.3437],\n",
      "        [    0.3487],\n",
      "        [    0.3515]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 39.606441259384155\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 152\n",
      "剩餘X 資料 torch.Size([8, 18])\n",
      "剩餘Y 資料 torch.Size([8, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1261080950498581, 6)\n",
      "The second_loss value of k: (0.12660935521125793, 7)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.3078])\n",
      "目前模型的Data狀態 torch.Size([152, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629],\n",
      "        [0.6629]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0834],\n",
      "        [    0.0832],\n",
      "        [    0.0826],\n",
      "        [    0.0957],\n",
      "        [    0.0977],\n",
      "        [    0.0726],\n",
      "        [    0.0713],\n",
      "        [    0.0589],\n",
      "        [    0.0539],\n",
      "        [    0.0532],\n",
      "        [    0.0488],\n",
      "        [    0.0450],\n",
      "        [    0.0449],\n",
      "        [    0.0427],\n",
      "        [    0.0421],\n",
      "        [    0.0409],\n",
      "        [    0.0405],\n",
      "        [    0.1307],\n",
      "        [    0.0337],\n",
      "        [    0.0321],\n",
      "        [    0.1407],\n",
      "        [    0.1414],\n",
      "        [    0.0285],\n",
      "        [    0.0278],\n",
      "        [    0.0264],\n",
      "        [    0.1465],\n",
      "        [    0.0206],\n",
      "        [    0.1515],\n",
      "        [    0.1537],\n",
      "        [    0.0165],\n",
      "        [    0.0143],\n",
      "        [    0.1571],\n",
      "        [    0.0138],\n",
      "        [    0.0091],\n",
      "        [    0.0088],\n",
      "        [    0.1666],\n",
      "        [    0.1667],\n",
      "        [    0.1686],\n",
      "        [    0.0001],\n",
      "        [    0.1726],\n",
      "        [    0.1727],\n",
      "        [    0.0018],\n",
      "        [    0.0032],\n",
      "        [    0.0038],\n",
      "        [    0.0039],\n",
      "        [    0.0044],\n",
      "        [    0.1771],\n",
      "        [    0.0076],\n",
      "        [    0.1796],\n",
      "        [    0.1804],\n",
      "        [    0.1833],\n",
      "        [    0.0137],\n",
      "        [    0.1863],\n",
      "        [    0.0155],\n",
      "        [    0.1867],\n",
      "        [    0.0175],\n",
      "        [    0.1895],\n",
      "        [    0.0190],\n",
      "        [    0.0199],\n",
      "        [    0.0214],\n",
      "        [    0.0217],\n",
      "        [    0.0265],\n",
      "        [    0.0269],\n",
      "        [    0.0276],\n",
      "        [    0.1987],\n",
      "        [    0.0294],\n",
      "        [    0.2006],\n",
      "        [    0.0296],\n",
      "        [    0.0324],\n",
      "        [    0.2045],\n",
      "        [    0.0343],\n",
      "        [    0.0351],\n",
      "        [    0.0391],\n",
      "        [    0.0393],\n",
      "        [    0.2114],\n",
      "        [    0.0413],\n",
      "        [    0.0416],\n",
      "        [    0.0442],\n",
      "        [    0.0458],\n",
      "        [    0.0463],\n",
      "        [    0.0486],\n",
      "        [    0.2204],\n",
      "        [    0.2206],\n",
      "        [    0.0522],\n",
      "        [    0.0524],\n",
      "        [    0.2263],\n",
      "        [    0.0557],\n",
      "        [    0.0559],\n",
      "        [    0.2297],\n",
      "        [    0.2349],\n",
      "        [    0.2389],\n",
      "        [    0.2430],\n",
      "        [    0.0722],\n",
      "        [    0.0741],\n",
      "        [    0.2467],\n",
      "        [    0.0778],\n",
      "        [    0.0822],\n",
      "        [    0.2571],\n",
      "        [    0.2574],\n",
      "        [    0.0960],\n",
      "        [    0.0991],\n",
      "        [    0.2725],\n",
      "        [    0.1018],\n",
      "        [    0.1039],\n",
      "        [    0.1071],\n",
      "        [    0.1115],\n",
      "        [    0.1150],\n",
      "        [    0.2892],\n",
      "        [    0.2901],\n",
      "        [    0.2923],\n",
      "        [    0.1223],\n",
      "        [    0.2965],\n",
      "        [    0.1263],\n",
      "        [    0.1289],\n",
      "        [    0.1300],\n",
      "        [    0.3040],\n",
      "        [    0.3053],\n",
      "        [    0.3056],\n",
      "        [    0.1364],\n",
      "        [    0.1365],\n",
      "        [    0.1399],\n",
      "        [    0.1516],\n",
      "        [    0.3371],\n",
      "        [    0.1676],\n",
      "        [    0.1757],\n",
      "        [    0.1890],\n",
      "        [    0.1910],\n",
      "        [    0.1920],\n",
      "        [    0.1984],\n",
      "        [    0.2018],\n",
      "        [    0.2060],\n",
      "        [    0.2107],\n",
      "        [    0.2119],\n",
      "        [    0.2225],\n",
      "        [    0.2264],\n",
      "        [    0.2392],\n",
      "        [    0.2424],\n",
      "        [    0.2640],\n",
      "        [    0.2718],\n",
      "        [    0.2984],\n",
      "        [    0.2995],\n",
      "        [    0.3042],\n",
      "        [    0.3044],\n",
      "        [    0.3049],\n",
      "        [    0.3050],\n",
      "        [    0.3086],\n",
      "        [    0.3366],\n",
      "        [    0.3376],\n",
      "        [    0.3437],\n",
      "        [    0.3487],\n",
      "        [    0.3515],\n",
      "        [    0.3551]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 64\n",
      "Number of shrink: 36\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0858],\n",
      "        [0.0856],\n",
      "        [0.0850],\n",
      "        [0.0981],\n",
      "        [0.1001],\n",
      "        [0.0750],\n",
      "        [0.0737],\n",
      "        [0.0612],\n",
      "        [0.0562],\n",
      "        [0.0556],\n",
      "        [0.0512],\n",
      "        [0.0474],\n",
      "        [0.0472],\n",
      "        [0.0451],\n",
      "        [0.0445],\n",
      "        [0.0433],\n",
      "        [0.0428],\n",
      "        [0.1331],\n",
      "        [0.0361],\n",
      "        [0.0345],\n",
      "        [0.1431],\n",
      "        [0.1437],\n",
      "        [0.0309],\n",
      "        [0.0302],\n",
      "        [0.0287],\n",
      "        [0.1489],\n",
      "        [0.0230],\n",
      "        [0.1539],\n",
      "        [0.1561],\n",
      "        [0.0189],\n",
      "        [0.0166],\n",
      "        [0.1594],\n",
      "        [0.0162],\n",
      "        [0.0115],\n",
      "        [0.0112],\n",
      "        [0.1690],\n",
      "        [0.1691],\n",
      "        [0.1710],\n",
      "        [0.0025],\n",
      "        [0.1750],\n",
      "        [0.1751],\n",
      "        [0.0005],\n",
      "        [0.0008],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0021],\n",
      "        [0.1795],\n",
      "        [0.0052],\n",
      "        [0.1820],\n",
      "        [0.1827],\n",
      "        [0.1857],\n",
      "        [0.0113],\n",
      "        [0.1887],\n",
      "        [0.0131],\n",
      "        [0.1891],\n",
      "        [0.0151],\n",
      "        [0.1919],\n",
      "        [0.0166],\n",
      "        [0.0176],\n",
      "        [0.0190],\n",
      "        [0.0194],\n",
      "        [0.0241],\n",
      "        [0.0245],\n",
      "        [0.0252],\n",
      "        [0.2011],\n",
      "        [0.0270],\n",
      "        [0.2030],\n",
      "        [0.0272],\n",
      "        [0.0300],\n",
      "        [0.2069],\n",
      "        [0.0319],\n",
      "        [0.0328],\n",
      "        [0.0367],\n",
      "        [0.0370],\n",
      "        [0.2138],\n",
      "        [0.0390],\n",
      "        [0.0392],\n",
      "        [0.0418],\n",
      "        [0.0435],\n",
      "        [0.0440],\n",
      "        [0.0462],\n",
      "        [0.2228],\n",
      "        [0.2229],\n",
      "        [0.0498],\n",
      "        [0.0500],\n",
      "        [0.2287],\n",
      "        [0.0533],\n",
      "        [0.0536],\n",
      "        [0.2320],\n",
      "        [0.2373],\n",
      "        [0.2413],\n",
      "        [0.2454],\n",
      "        [0.0699],\n",
      "        [0.0717],\n",
      "        [0.2491],\n",
      "        [0.0754],\n",
      "        [0.0798],\n",
      "        [0.2594],\n",
      "        [0.2598],\n",
      "        [0.0936],\n",
      "        [0.0968],\n",
      "        [0.2749],\n",
      "        [0.0995],\n",
      "        [0.1015],\n",
      "        [0.1047],\n",
      "        [0.1091],\n",
      "        [0.1126],\n",
      "        [0.2916],\n",
      "        [0.2925],\n",
      "        [0.2947],\n",
      "        [0.1200],\n",
      "        [0.2988],\n",
      "        [0.1239],\n",
      "        [0.1265],\n",
      "        [0.1276],\n",
      "        [0.3064],\n",
      "        [0.3076],\n",
      "        [0.3079],\n",
      "        [0.1340],\n",
      "        [0.1341],\n",
      "        [0.1375],\n",
      "        [0.1492],\n",
      "        [0.3395],\n",
      "        [0.1652],\n",
      "        [0.1733],\n",
      "        [0.1866],\n",
      "        [0.1886],\n",
      "        [0.1896],\n",
      "        [0.1960],\n",
      "        [0.1994],\n",
      "        [0.2037],\n",
      "        [0.2084],\n",
      "        [0.2095],\n",
      "        [0.2201],\n",
      "        [0.2240],\n",
      "        [0.2368],\n",
      "        [0.2400],\n",
      "        [0.2616],\n",
      "        [0.2694],\n",
      "        [0.2961],\n",
      "        [0.2971],\n",
      "        [0.3018],\n",
      "        [0.3020],\n",
      "        [0.3025],\n",
      "        [0.3026],\n",
      "        [0.3062],\n",
      "        [0.3342],\n",
      "        [0.3353],\n",
      "        [0.3413],\n",
      "        [0.3464],\n",
      "        [0.3491],\n",
      "        [0.3527]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 39.86286735534668\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 153\n",
      "剩餘X 資料 torch.Size([7, 18])\n",
      "剩餘Y 資料 torch.Size([7, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12492264807224274, 6)\n",
      "The second_loss value of k: (0.12805701792240143, 5)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.3071])\n",
      "目前模型的Data狀態 torch.Size([153, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605],\n",
      "        [0.6605]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0858],\n",
      "        [0.0856],\n",
      "        [0.0850],\n",
      "        [0.0981],\n",
      "        [0.1001],\n",
      "        [0.0750],\n",
      "        [0.0737],\n",
      "        [0.0612],\n",
      "        [0.0562],\n",
      "        [0.0556],\n",
      "        [0.0512],\n",
      "        [0.0474],\n",
      "        [0.0472],\n",
      "        [0.0451],\n",
      "        [0.0445],\n",
      "        [0.0433],\n",
      "        [0.0428],\n",
      "        [0.1331],\n",
      "        [0.0361],\n",
      "        [0.0345],\n",
      "        [0.1431],\n",
      "        [0.1437],\n",
      "        [0.0309],\n",
      "        [0.0302],\n",
      "        [0.0287],\n",
      "        [0.1489],\n",
      "        [0.0230],\n",
      "        [0.1539],\n",
      "        [0.1561],\n",
      "        [0.0189],\n",
      "        [0.0166],\n",
      "        [0.1594],\n",
      "        [0.0162],\n",
      "        [0.0115],\n",
      "        [0.0112],\n",
      "        [0.1690],\n",
      "        [0.1691],\n",
      "        [0.1710],\n",
      "        [0.0025],\n",
      "        [0.1750],\n",
      "        [0.1751],\n",
      "        [0.0005],\n",
      "        [0.0008],\n",
      "        [0.0014],\n",
      "        [0.0015],\n",
      "        [0.0021],\n",
      "        [0.1795],\n",
      "        [0.0052],\n",
      "        [0.1820],\n",
      "        [0.1827],\n",
      "        [0.1857],\n",
      "        [0.0113],\n",
      "        [0.1887],\n",
      "        [0.0131],\n",
      "        [0.1891],\n",
      "        [0.0151],\n",
      "        [0.1919],\n",
      "        [0.0166],\n",
      "        [0.0176],\n",
      "        [0.0190],\n",
      "        [0.0194],\n",
      "        [0.0241],\n",
      "        [0.0245],\n",
      "        [0.0252],\n",
      "        [0.2011],\n",
      "        [0.0270],\n",
      "        [0.2030],\n",
      "        [0.0272],\n",
      "        [0.0300],\n",
      "        [0.2069],\n",
      "        [0.0319],\n",
      "        [0.0328],\n",
      "        [0.0367],\n",
      "        [0.0370],\n",
      "        [0.2138],\n",
      "        [0.0390],\n",
      "        [0.0392],\n",
      "        [0.0418],\n",
      "        [0.0435],\n",
      "        [0.0440],\n",
      "        [0.0462],\n",
      "        [0.2228],\n",
      "        [0.2229],\n",
      "        [0.0498],\n",
      "        [0.0500],\n",
      "        [0.2287],\n",
      "        [0.0533],\n",
      "        [0.0536],\n",
      "        [0.2320],\n",
      "        [0.2373],\n",
      "        [0.2413],\n",
      "        [0.2454],\n",
      "        [0.0699],\n",
      "        [0.0717],\n",
      "        [0.2491],\n",
      "        [0.0754],\n",
      "        [0.0798],\n",
      "        [0.2594],\n",
      "        [0.2598],\n",
      "        [0.0936],\n",
      "        [0.0968],\n",
      "        [0.2749],\n",
      "        [0.0995],\n",
      "        [0.1015],\n",
      "        [0.1047],\n",
      "        [0.1091],\n",
      "        [0.1126],\n",
      "        [0.2916],\n",
      "        [0.2925],\n",
      "        [0.2947],\n",
      "        [0.1200],\n",
      "        [0.2988],\n",
      "        [0.1239],\n",
      "        [0.1265],\n",
      "        [0.1276],\n",
      "        [0.3064],\n",
      "        [0.3076],\n",
      "        [0.3079],\n",
      "        [0.1340],\n",
      "        [0.1341],\n",
      "        [0.1375],\n",
      "        [0.1492],\n",
      "        [0.3395],\n",
      "        [0.1652],\n",
      "        [0.1733],\n",
      "        [0.1866],\n",
      "        [0.1886],\n",
      "        [0.1896],\n",
      "        [0.1960],\n",
      "        [0.1994],\n",
      "        [0.2037],\n",
      "        [0.2084],\n",
      "        [0.2095],\n",
      "        [0.2201],\n",
      "        [0.2240],\n",
      "        [0.2368],\n",
      "        [0.2400],\n",
      "        [0.2616],\n",
      "        [0.2694],\n",
      "        [0.2961],\n",
      "        [0.2971],\n",
      "        [0.3018],\n",
      "        [0.3020],\n",
      "        [0.3025],\n",
      "        [0.3026],\n",
      "        [0.3062],\n",
      "        [0.3342],\n",
      "        [0.3353],\n",
      "        [0.3413],\n",
      "        [0.3464],\n",
      "        [0.3491],\n",
      "        [0.3527],\n",
      "        [0.3534]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0880],\n",
      "        [    0.0879],\n",
      "        [    0.0873],\n",
      "        [    0.1003],\n",
      "        [    0.1023],\n",
      "        [    0.0772],\n",
      "        [    0.0760],\n",
      "        [    0.0635],\n",
      "        [    0.0585],\n",
      "        [    0.0578],\n",
      "        [    0.0535],\n",
      "        [    0.0497],\n",
      "        [    0.0495],\n",
      "        [    0.0474],\n",
      "        [    0.0468],\n",
      "        [    0.0456],\n",
      "        [    0.0451],\n",
      "        [    0.1354],\n",
      "        [    0.0383],\n",
      "        [    0.0368],\n",
      "        [    0.1454],\n",
      "        [    0.1460],\n",
      "        [    0.0332],\n",
      "        [    0.0325],\n",
      "        [    0.0310],\n",
      "        [    0.1512],\n",
      "        [    0.0253],\n",
      "        [    0.1562],\n",
      "        [    0.1584],\n",
      "        [    0.0212],\n",
      "        [    0.0189],\n",
      "        [    0.1617],\n",
      "        [    0.0185],\n",
      "        [    0.0138],\n",
      "        [    0.0135],\n",
      "        [    0.1713],\n",
      "        [    0.1714],\n",
      "        [    0.1733],\n",
      "        [    0.0048],\n",
      "        [    0.1772],\n",
      "        [    0.1774],\n",
      "        [    0.0028],\n",
      "        [    0.0015],\n",
      "        [    0.0009],\n",
      "        [    0.0007],\n",
      "        [    0.0002],\n",
      "        [    0.1818],\n",
      "        [    0.0029],\n",
      "        [    0.1843],\n",
      "        [    0.1850],\n",
      "        [    0.1880],\n",
      "        [    0.0090],\n",
      "        [    0.1909],\n",
      "        [    0.0108],\n",
      "        [    0.1913],\n",
      "        [    0.0129],\n",
      "        [    0.1942],\n",
      "        [    0.0143],\n",
      "        [    0.0153],\n",
      "        [    0.0167],\n",
      "        [    0.0171],\n",
      "        [    0.0219],\n",
      "        [    0.0223],\n",
      "        [    0.0229],\n",
      "        [    0.2034],\n",
      "        [    0.0248],\n",
      "        [    0.2053],\n",
      "        [    0.0249],\n",
      "        [    0.0277],\n",
      "        [    0.2092],\n",
      "        [    0.0296],\n",
      "        [    0.0305],\n",
      "        [    0.0344],\n",
      "        [    0.0347],\n",
      "        [    0.2161],\n",
      "        [    0.0367],\n",
      "        [    0.0369],\n",
      "        [    0.0395],\n",
      "        [    0.0412],\n",
      "        [    0.0417],\n",
      "        [    0.0439],\n",
      "        [    0.2251],\n",
      "        [    0.2252],\n",
      "        [    0.0475],\n",
      "        [    0.0478],\n",
      "        [    0.2310],\n",
      "        [    0.0510],\n",
      "        [    0.0513],\n",
      "        [    0.2343],\n",
      "        [    0.2396],\n",
      "        [    0.2436],\n",
      "        [    0.2477],\n",
      "        [    0.0676],\n",
      "        [    0.0694],\n",
      "        [    0.2513],\n",
      "        [    0.0731],\n",
      "        [    0.0776],\n",
      "        [    0.2617],\n",
      "        [    0.2620],\n",
      "        [    0.0913],\n",
      "        [    0.0945],\n",
      "        [    0.2772],\n",
      "        [    0.0972],\n",
      "        [    0.0993],\n",
      "        [    0.1024],\n",
      "        [    0.1068],\n",
      "        [    0.1103],\n",
      "        [    0.2939],\n",
      "        [    0.2947],\n",
      "        [    0.2970],\n",
      "        [    0.1177],\n",
      "        [    0.3011],\n",
      "        [    0.1216],\n",
      "        [    0.1242],\n",
      "        [    0.1253],\n",
      "        [    0.3087],\n",
      "        [    0.3099],\n",
      "        [    0.3102],\n",
      "        [    0.1317],\n",
      "        [    0.1318],\n",
      "        [    0.1352],\n",
      "        [    0.1469],\n",
      "        [    0.3418],\n",
      "        [    0.1629],\n",
      "        [    0.1710],\n",
      "        [    0.1843],\n",
      "        [    0.1863],\n",
      "        [    0.1873],\n",
      "        [    0.1937],\n",
      "        [    0.1971],\n",
      "        [    0.2014],\n",
      "        [    0.2061],\n",
      "        [    0.2072],\n",
      "        [    0.2178],\n",
      "        [    0.2217],\n",
      "        [    0.2346],\n",
      "        [    0.2378],\n",
      "        [    0.2593],\n",
      "        [    0.2671],\n",
      "        [    0.2938],\n",
      "        [    0.2948],\n",
      "        [    0.2995],\n",
      "        [    0.2997],\n",
      "        [    0.3002],\n",
      "        [    0.3004],\n",
      "        [    0.3040],\n",
      "        [    0.3319],\n",
      "        [    0.3330],\n",
      "        [    0.3390],\n",
      "        [    0.3441],\n",
      "        [    0.3468],\n",
      "        [    0.3505],\n",
      "        [    0.3512]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 40.15914034843445\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 2 個區塊累積花費時間(s) 6.335416316986084\n",
      "<<The performance of 2 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 6.335416316986084\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 1\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 3557.39\n",
      "The MAPE for l = 1: 0.07%\n",
      "The RMSE for l = 1: 4452.36\n",
      "The accuracy(2000) for l = 1: 39.87%\n",
      "The accuracy(3000) for l = 1: 47.71%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in toutlier>>\n",
      "The MAE for l = 1: 9826.17\n",
      "The MAPE for l = 1: 0.24%\n",
      "The RMSE for l = 1: 9845.88\n",
      "The accuracy(2000) for l = 1: 0.00%\n",
      "The accuracy(3000) for l = 1: 0.00%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 13810.0\n",
      "The MAPE for l = 1: 0.4%\n",
      "The RMSE for l = 1: 13949.2\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 0.0%\n",
      "------------------------------------------------------------\n",
      "0.39869281045751637\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<3>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (5.364951505271165e-08, 19)\n",
      "The second_loss value of k: (5.48823379631358e-07, 52)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [19, 52, 51, 47, 33, 31, 26, 18, 49, 59, 81, 57, 88, 82, 58, 50, 80, 46, 53, 60, 61, 48, 30, 84, 45, 79, 85, 89, 56, 55, 39, 73, 32, 83, 44, 28, 54, 75, 74, 87, 29, 38, 17, 43, 40, 76, 71, 42, 90, 86, 37, 41, 34, 25, 91, 62, 72, 36, 35, 78, 21, 20, 23, 63, 77, 93, 92, 24, 22, 94, 70, 95, 97, 98, 99, 100, 69, 96, 101, 15, 68, 16, 67, 103, 14, 13, 104, 105, 102, 106, 27, 66, 11, 125, 124, 1, 12, 107, 126, 10, 0, 2, 65, 64, 4, 3, 127, 5, 9, 123, 6, 128, 122, 8, 120, 121, 119, 118, 7, 129, 108, 132, 115, 130, 131, 116, 117]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0007],\n",
      "        [    0.0009],\n",
      "        [    0.0015],\n",
      "        [    0.0028],\n",
      "        [    0.0029],\n",
      "        [    0.0048],\n",
      "        [    0.0090],\n",
      "        [    0.0108],\n",
      "        [    0.0129],\n",
      "        [    0.0135],\n",
      "        [    0.0138],\n",
      "        [    0.0143],\n",
      "        [    0.0153],\n",
      "        [    0.0167],\n",
      "        [    0.0171],\n",
      "        [    0.0185],\n",
      "        [    0.0189],\n",
      "        [    0.0212],\n",
      "        [    0.0219],\n",
      "        [    0.0223],\n",
      "        [    0.0229],\n",
      "        [    0.0248],\n",
      "        [    0.0249],\n",
      "        [    0.0253],\n",
      "        [    0.0277],\n",
      "        [    0.0296],\n",
      "        [    0.0305],\n",
      "        [    0.0310],\n",
      "        [    0.0325],\n",
      "        [    0.0332],\n",
      "        [    0.0344],\n",
      "        [    0.0347],\n",
      "        [    0.0367],\n",
      "        [    0.0368],\n",
      "        [    0.0369],\n",
      "        [    0.0383],\n",
      "        [    0.0395],\n",
      "        [    0.0412],\n",
      "        [    0.0417],\n",
      "        [    0.0439],\n",
      "        [    0.0451],\n",
      "        [    0.0456],\n",
      "        [    0.0468],\n",
      "        [    0.0474],\n",
      "        [    0.0475],\n",
      "        [    0.0478],\n",
      "        [    0.0495],\n",
      "        [    0.0510],\n",
      "        [    0.0513],\n",
      "        [    0.0535],\n",
      "        [    0.0578],\n",
      "        [    0.0585],\n",
      "        [    0.0635],\n",
      "        [    0.0676],\n",
      "        [    0.0694],\n",
      "        [    0.0731],\n",
      "        [    0.0760],\n",
      "        [    0.0772],\n",
      "        [    0.0776],\n",
      "        [    0.0873],\n",
      "        [    0.0879],\n",
      "        [    0.0880],\n",
      "        [    0.0913],\n",
      "        [    0.0945],\n",
      "        [    0.0972],\n",
      "        [    0.0993],\n",
      "        [    0.1003],\n",
      "        [    0.1023],\n",
      "        [    0.1024],\n",
      "        [    0.1068],\n",
      "        [    0.1103],\n",
      "        [    0.1177],\n",
      "        [    0.1216],\n",
      "        [    0.1242],\n",
      "        [    0.1253],\n",
      "        [    0.1317],\n",
      "        [    0.1318],\n",
      "        [    0.1352],\n",
      "        [    0.1354],\n",
      "        [    0.1469],\n",
      "        [    0.1584],\n",
      "        [    0.1629],\n",
      "        [    0.1710],\n",
      "        [    0.1772],\n",
      "        [    0.1774],\n",
      "        [    0.1843],\n",
      "        [    0.1863],\n",
      "        [    0.1873],\n",
      "        [    0.1937],\n",
      "        [    0.1971],\n",
      "        [    0.2014],\n",
      "        [    0.2034],\n",
      "        [    0.2061],\n",
      "        [    0.2072],\n",
      "        [    0.2092],\n",
      "        [    0.2161],\n",
      "        [    0.2178],\n",
      "        [    0.2217],\n",
      "        [    0.2251],\n",
      "        [    0.2252],\n",
      "        [    0.2343],\n",
      "        [    0.2346],\n",
      "        [    0.2378],\n",
      "        [    0.2477],\n",
      "        [    0.2513],\n",
      "        [    0.2593],\n",
      "        [    0.2617],\n",
      "        [    0.2620],\n",
      "        [    0.2671],\n",
      "        [    0.2772],\n",
      "        [    0.2938],\n",
      "        [    0.2948],\n",
      "        [    0.2970],\n",
      "        [    0.2995],\n",
      "        [    0.2997],\n",
      "        [    0.3002],\n",
      "        [    0.3004],\n",
      "        [    0.3011],\n",
      "        [    0.3040],\n",
      "        [    0.3319],\n",
      "        [    0.3330],\n",
      "        [    0.3390],\n",
      "        [    0.3441],\n",
      "        [    0.3468],\n",
      "        [    0.3505],\n",
      "        [    0.3512]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 128\n",
      "剩餘X 資料 torch.Size([32, 18])\n",
      "剩餘Y 資料 torch.Size([32, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1264268159866333, 5)\n",
      "The second_loss value of k: (0.13490693271160126, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.3027])\n",
      "目前模型的Data狀態 torch.Size([128, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582],\n",
      "        [0.6582]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0007],\n",
      "        [    0.0009],\n",
      "        [    0.0015],\n",
      "        [    0.0028],\n",
      "        [    0.0029],\n",
      "        [    0.0048],\n",
      "        [    0.0090],\n",
      "        [    0.0108],\n",
      "        [    0.0129],\n",
      "        [    0.0135],\n",
      "        [    0.0138],\n",
      "        [    0.0143],\n",
      "        [    0.0153],\n",
      "        [    0.0167],\n",
      "        [    0.0171],\n",
      "        [    0.0185],\n",
      "        [    0.0189],\n",
      "        [    0.0212],\n",
      "        [    0.0219],\n",
      "        [    0.0223],\n",
      "        [    0.0229],\n",
      "        [    0.0248],\n",
      "        [    0.0249],\n",
      "        [    0.0253],\n",
      "        [    0.0277],\n",
      "        [    0.0296],\n",
      "        [    0.0305],\n",
      "        [    0.0310],\n",
      "        [    0.0325],\n",
      "        [    0.0332],\n",
      "        [    0.0344],\n",
      "        [    0.0347],\n",
      "        [    0.0367],\n",
      "        [    0.0368],\n",
      "        [    0.0369],\n",
      "        [    0.0383],\n",
      "        [    0.0395],\n",
      "        [    0.0412],\n",
      "        [    0.0417],\n",
      "        [    0.0439],\n",
      "        [    0.0451],\n",
      "        [    0.0456],\n",
      "        [    0.0468],\n",
      "        [    0.0474],\n",
      "        [    0.0475],\n",
      "        [    0.0478],\n",
      "        [    0.0495],\n",
      "        [    0.0510],\n",
      "        [    0.0513],\n",
      "        [    0.0535],\n",
      "        [    0.0578],\n",
      "        [    0.0585],\n",
      "        [    0.0635],\n",
      "        [    0.0676],\n",
      "        [    0.0694],\n",
      "        [    0.0731],\n",
      "        [    0.0760],\n",
      "        [    0.0772],\n",
      "        [    0.0776],\n",
      "        [    0.0873],\n",
      "        [    0.0879],\n",
      "        [    0.0880],\n",
      "        [    0.0913],\n",
      "        [    0.0945],\n",
      "        [    0.0972],\n",
      "        [    0.0993],\n",
      "        [    0.1003],\n",
      "        [    0.1023],\n",
      "        [    0.1024],\n",
      "        [    0.1068],\n",
      "        [    0.1103],\n",
      "        [    0.1177],\n",
      "        [    0.1216],\n",
      "        [    0.1242],\n",
      "        [    0.1253],\n",
      "        [    0.1317],\n",
      "        [    0.1318],\n",
      "        [    0.1352],\n",
      "        [    0.1354],\n",
      "        [    0.1469],\n",
      "        [    0.1584],\n",
      "        [    0.1629],\n",
      "        [    0.1710],\n",
      "        [    0.1772],\n",
      "        [    0.1774],\n",
      "        [    0.1843],\n",
      "        [    0.1863],\n",
      "        [    0.1873],\n",
      "        [    0.1937],\n",
      "        [    0.1971],\n",
      "        [    0.2014],\n",
      "        [    0.2034],\n",
      "        [    0.2061],\n",
      "        [    0.2072],\n",
      "        [    0.2092],\n",
      "        [    0.2161],\n",
      "        [    0.2178],\n",
      "        [    0.2217],\n",
      "        [    0.2251],\n",
      "        [    0.2252],\n",
      "        [    0.2343],\n",
      "        [    0.2346],\n",
      "        [    0.2378],\n",
      "        [    0.2477],\n",
      "        [    0.2513],\n",
      "        [    0.2593],\n",
      "        [    0.2617],\n",
      "        [    0.2620],\n",
      "        [    0.2671],\n",
      "        [    0.2772],\n",
      "        [    0.2938],\n",
      "        [    0.2948],\n",
      "        [    0.2970],\n",
      "        [    0.2995],\n",
      "        [    0.2997],\n",
      "        [    0.3002],\n",
      "        [    0.3004],\n",
      "        [    0.3011],\n",
      "        [    0.3040],\n",
      "        [    0.3319],\n",
      "        [    0.3330],\n",
      "        [    0.3390],\n",
      "        [    0.3441],\n",
      "        [    0.3468],\n",
      "        [    0.3505],\n",
      "        [    0.3512],\n",
      "        [    0.3556]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 71\n",
      "Number of shrink: 29\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0454],\n",
      "        [0.0459],\n",
      "        [0.0460],\n",
      "        [0.0466],\n",
      "        [0.0479],\n",
      "        [0.0422],\n",
      "        [0.0499],\n",
      "        [0.0361],\n",
      "        [0.0343],\n",
      "        [0.0323],\n",
      "        [0.0586],\n",
      "        [0.0589],\n",
      "        [0.0308],\n",
      "        [0.0298],\n",
      "        [0.0284],\n",
      "        [0.0280],\n",
      "        [0.0636],\n",
      "        [0.0640],\n",
      "        [0.0663],\n",
      "        [0.0233],\n",
      "        [0.0229],\n",
      "        [0.0222],\n",
      "        [0.0204],\n",
      "        [0.0202],\n",
      "        [0.0704],\n",
      "        [0.0174],\n",
      "        [0.0155],\n",
      "        [0.0146],\n",
      "        [0.0761],\n",
      "        [0.0776],\n",
      "        [0.0783],\n",
      "        [0.0107],\n",
      "        [0.0105],\n",
      "        [0.0085],\n",
      "        [0.0819],\n",
      "        [0.0082],\n",
      "        [0.0835],\n",
      "        [0.0056],\n",
      "        [0.0040],\n",
      "        [0.0034],\n",
      "        [0.0012],\n",
      "        [0.0902],\n",
      "        [0.0907],\n",
      "        [0.0919],\n",
      "        [0.0925],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0946],\n",
      "        [0.0059],\n",
      "        [0.0062],\n",
      "        [0.0986],\n",
      "        [0.1030],\n",
      "        [0.1036],\n",
      "        [0.1087],\n",
      "        [0.0225],\n",
      "        [0.0243],\n",
      "        [0.0280],\n",
      "        [0.1211],\n",
      "        [0.1224],\n",
      "        [0.0324],\n",
      "        [0.1324],\n",
      "        [0.1330],\n",
      "        [0.1332],\n",
      "        [0.0462],\n",
      "        [0.0494],\n",
      "        [0.0521],\n",
      "        [0.0541],\n",
      "        [0.1455],\n",
      "        [0.1475],\n",
      "        [0.0573],\n",
      "        [0.0617],\n",
      "        [0.0652],\n",
      "        [0.0725],\n",
      "        [0.0765],\n",
      "        [0.0791],\n",
      "        [0.0802],\n",
      "        [0.0866],\n",
      "        [0.0867],\n",
      "        [0.0901],\n",
      "        [0.1805],\n",
      "        [0.1018],\n",
      "        [0.2035],\n",
      "        [0.1178],\n",
      "        [0.1259],\n",
      "        [0.2224],\n",
      "        [0.2225],\n",
      "        [0.1392],\n",
      "        [0.1412],\n",
      "        [0.1422],\n",
      "        [0.1486],\n",
      "        [0.1520],\n",
      "        [0.1563],\n",
      "        [0.2485],\n",
      "        [0.1610],\n",
      "        [0.1621],\n",
      "        [0.2543],\n",
      "        [0.2612],\n",
      "        [0.1727],\n",
      "        [0.1766],\n",
      "        [0.2702],\n",
      "        [0.2703],\n",
      "        [0.2794],\n",
      "        [0.1894],\n",
      "        [0.1926],\n",
      "        [0.2928],\n",
      "        [0.2965],\n",
      "        [0.2142],\n",
      "        [0.3068],\n",
      "        [0.3072],\n",
      "        [0.2220],\n",
      "        [0.3223],\n",
      "        [0.2487],\n",
      "        [0.2497],\n",
      "        [0.3421],\n",
      "        [0.2544],\n",
      "        [0.2546],\n",
      "        [0.2551],\n",
      "        [0.2552],\n",
      "        [0.3463],\n",
      "        [0.2588],\n",
      "        [0.2868],\n",
      "        [0.2879],\n",
      "        [0.2939],\n",
      "        [0.2989],\n",
      "        [0.3017],\n",
      "        [0.3053],\n",
      "        [0.3060],\n",
      "        [0.3104]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 40.63918375968933\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 129\n",
      "剩餘X 資料 torch.Size([31, 18])\n",
      "剩餘Y 資料 torch.Size([31, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10379738360643387, 4)\n",
      "The second_loss value of k: (0.10651551187038422, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引4，y= tensor([0.2910])\n",
      "目前模型的Data狀態 torch.Size([129, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131],\n",
      "        [0.6131]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0454],\n",
      "        [0.0459],\n",
      "        [0.0460],\n",
      "        [0.0466],\n",
      "        [0.0479],\n",
      "        [0.0422],\n",
      "        [0.0499],\n",
      "        [0.0361],\n",
      "        [0.0343],\n",
      "        [0.0323],\n",
      "        [0.0586],\n",
      "        [0.0589],\n",
      "        [0.0308],\n",
      "        [0.0298],\n",
      "        [0.0284],\n",
      "        [0.0280],\n",
      "        [0.0636],\n",
      "        [0.0640],\n",
      "        [0.0663],\n",
      "        [0.0233],\n",
      "        [0.0229],\n",
      "        [0.0222],\n",
      "        [0.0204],\n",
      "        [0.0202],\n",
      "        [0.0704],\n",
      "        [0.0174],\n",
      "        [0.0155],\n",
      "        [0.0146],\n",
      "        [0.0761],\n",
      "        [0.0776],\n",
      "        [0.0783],\n",
      "        [0.0107],\n",
      "        [0.0105],\n",
      "        [0.0085],\n",
      "        [0.0819],\n",
      "        [0.0082],\n",
      "        [0.0835],\n",
      "        [0.0056],\n",
      "        [0.0040],\n",
      "        [0.0034],\n",
      "        [0.0012],\n",
      "        [0.0902],\n",
      "        [0.0907],\n",
      "        [0.0919],\n",
      "        [0.0925],\n",
      "        [0.0024],\n",
      "        [0.0026],\n",
      "        [0.0946],\n",
      "        [0.0059],\n",
      "        [0.0062],\n",
      "        [0.0986],\n",
      "        [0.1030],\n",
      "        [0.1036],\n",
      "        [0.1087],\n",
      "        [0.0225],\n",
      "        [0.0243],\n",
      "        [0.0280],\n",
      "        [0.1211],\n",
      "        [0.1224],\n",
      "        [0.0324],\n",
      "        [0.1324],\n",
      "        [0.1330],\n",
      "        [0.1332],\n",
      "        [0.0462],\n",
      "        [0.0494],\n",
      "        [0.0521],\n",
      "        [0.0541],\n",
      "        [0.1455],\n",
      "        [0.1475],\n",
      "        [0.0573],\n",
      "        [0.0617],\n",
      "        [0.0652],\n",
      "        [0.0725],\n",
      "        [0.0765],\n",
      "        [0.0791],\n",
      "        [0.0802],\n",
      "        [0.0866],\n",
      "        [0.0867],\n",
      "        [0.0901],\n",
      "        [0.1805],\n",
      "        [0.1018],\n",
      "        [0.2035],\n",
      "        [0.1178],\n",
      "        [0.1259],\n",
      "        [0.2224],\n",
      "        [0.2225],\n",
      "        [0.1392],\n",
      "        [0.1412],\n",
      "        [0.1422],\n",
      "        [0.1486],\n",
      "        [0.1520],\n",
      "        [0.1563],\n",
      "        [0.2485],\n",
      "        [0.1610],\n",
      "        [0.1621],\n",
      "        [0.2543],\n",
      "        [0.2612],\n",
      "        [0.1727],\n",
      "        [0.1766],\n",
      "        [0.2702],\n",
      "        [0.2703],\n",
      "        [0.2794],\n",
      "        [0.1894],\n",
      "        [0.1926],\n",
      "        [0.2928],\n",
      "        [0.2965],\n",
      "        [0.2142],\n",
      "        [0.3068],\n",
      "        [0.3072],\n",
      "        [0.2220],\n",
      "        [0.3223],\n",
      "        [0.2487],\n",
      "        [0.2497],\n",
      "        [0.3421],\n",
      "        [0.2544],\n",
      "        [0.2546],\n",
      "        [0.2551],\n",
      "        [0.2552],\n",
      "        [0.3463],\n",
      "        [0.2588],\n",
      "        [0.2868],\n",
      "        [0.2879],\n",
      "        [0.2939],\n",
      "        [0.2989],\n",
      "        [0.3017],\n",
      "        [0.3053],\n",
      "        [0.3060],\n",
      "        [0.3104],\n",
      "        [0.3222]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0478],\n",
      "        [    0.0484],\n",
      "        [    0.0485],\n",
      "        [    0.0491],\n",
      "        [    0.0504],\n",
      "        [    0.0447],\n",
      "        [    0.0524],\n",
      "        [    0.0386],\n",
      "        [    0.0368],\n",
      "        [    0.0347],\n",
      "        [    0.0611],\n",
      "        [    0.0614],\n",
      "        [    0.0333],\n",
      "        [    0.0323],\n",
      "        [    0.0309],\n",
      "        [    0.0305],\n",
      "        [    0.0661],\n",
      "        [    0.0665],\n",
      "        [    0.0688],\n",
      "        [    0.0258],\n",
      "        [    0.0254],\n",
      "        [    0.0247],\n",
      "        [    0.0229],\n",
      "        [    0.0227],\n",
      "        [    0.0729],\n",
      "        [    0.0199],\n",
      "        [    0.0180],\n",
      "        [    0.0171],\n",
      "        [    0.0786],\n",
      "        [    0.0801],\n",
      "        [    0.0808],\n",
      "        [    0.0132],\n",
      "        [    0.0129],\n",
      "        [    0.0109],\n",
      "        [    0.0844],\n",
      "        [    0.0107],\n",
      "        [    0.0860],\n",
      "        [    0.0081],\n",
      "        [    0.0064],\n",
      "        [    0.0059],\n",
      "        [    0.0037],\n",
      "        [    0.0927],\n",
      "        [    0.0932],\n",
      "        [    0.0944],\n",
      "        [    0.0950],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0971],\n",
      "        [    0.0034],\n",
      "        [    0.0037],\n",
      "        [    0.1011],\n",
      "        [    0.1054],\n",
      "        [    0.1061],\n",
      "        [    0.1111],\n",
      "        [    0.0200],\n",
      "        [    0.0218],\n",
      "        [    0.0255],\n",
      "        [    0.1236],\n",
      "        [    0.1249],\n",
      "        [    0.0299],\n",
      "        [    0.1349],\n",
      "        [    0.1355],\n",
      "        [    0.1357],\n",
      "        [    0.0437],\n",
      "        [    0.0469],\n",
      "        [    0.0496],\n",
      "        [    0.0516],\n",
      "        [    0.1480],\n",
      "        [    0.1500],\n",
      "        [    0.0548],\n",
      "        [    0.0592],\n",
      "        [    0.0627],\n",
      "        [    0.0701],\n",
      "        [    0.0740],\n",
      "        [    0.0766],\n",
      "        [    0.0777],\n",
      "        [    0.0841],\n",
      "        [    0.0842],\n",
      "        [    0.0876],\n",
      "        [    0.1830],\n",
      "        [    0.0993],\n",
      "        [    0.2060],\n",
      "        [    0.1153],\n",
      "        [    0.1234],\n",
      "        [    0.2249],\n",
      "        [    0.2250],\n",
      "        [    0.1367],\n",
      "        [    0.1387],\n",
      "        [    0.1397],\n",
      "        [    0.1461],\n",
      "        [    0.1495],\n",
      "        [    0.1538],\n",
      "        [    0.2510],\n",
      "        [    0.1585],\n",
      "        [    0.1596],\n",
      "        [    0.2568],\n",
      "        [    0.2637],\n",
      "        [    0.1702],\n",
      "        [    0.1741],\n",
      "        [    0.2727],\n",
      "        [    0.2728],\n",
      "        [    0.2819],\n",
      "        [    0.1869],\n",
      "        [    0.1902],\n",
      "        [    0.2953],\n",
      "        [    0.2990],\n",
      "        [    0.2117],\n",
      "        [    0.3093],\n",
      "        [    0.3097],\n",
      "        [    0.2195],\n",
      "        [    0.3248],\n",
      "        [    0.2462],\n",
      "        [    0.2472],\n",
      "        [    0.3446],\n",
      "        [    0.2519],\n",
      "        [    0.2521],\n",
      "        [    0.2526],\n",
      "        [    0.2527],\n",
      "        [    0.3487],\n",
      "        [    0.2563],\n",
      "        [    0.2843],\n",
      "        [    0.2854],\n",
      "        [    0.2914],\n",
      "        [    0.2965],\n",
      "        [    0.2992],\n",
      "        [    0.3028],\n",
      "        [    0.3035],\n",
      "        [    0.3080],\n",
      "        [    0.3197]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 40.88114833831787\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 130\n",
      "剩餘X 資料 torch.Size([30, 18])\n",
      "剩餘Y 資料 torch.Size([30, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10489610582590103, 0)\n",
      "The second_loss value of k: (0.11537030339241028, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.2868])\n",
      "目前模型的Data狀態 torch.Size([130, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106],\n",
      "        [0.6106]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0478],\n",
      "        [    0.0484],\n",
      "        [    0.0485],\n",
      "        [    0.0491],\n",
      "        [    0.0504],\n",
      "        [    0.0447],\n",
      "        [    0.0524],\n",
      "        [    0.0386],\n",
      "        [    0.0368],\n",
      "        [    0.0347],\n",
      "        [    0.0611],\n",
      "        [    0.0614],\n",
      "        [    0.0333],\n",
      "        [    0.0323],\n",
      "        [    0.0309],\n",
      "        [    0.0305],\n",
      "        [    0.0661],\n",
      "        [    0.0665],\n",
      "        [    0.0688],\n",
      "        [    0.0258],\n",
      "        [    0.0254],\n",
      "        [    0.0247],\n",
      "        [    0.0229],\n",
      "        [    0.0227],\n",
      "        [    0.0729],\n",
      "        [    0.0199],\n",
      "        [    0.0180],\n",
      "        [    0.0171],\n",
      "        [    0.0786],\n",
      "        [    0.0801],\n",
      "        [    0.0808],\n",
      "        [    0.0132],\n",
      "        [    0.0129],\n",
      "        [    0.0109],\n",
      "        [    0.0844],\n",
      "        [    0.0107],\n",
      "        [    0.0860],\n",
      "        [    0.0081],\n",
      "        [    0.0064],\n",
      "        [    0.0059],\n",
      "        [    0.0037],\n",
      "        [    0.0927],\n",
      "        [    0.0932],\n",
      "        [    0.0944],\n",
      "        [    0.0950],\n",
      "        [    0.0001],\n",
      "        [    0.0001],\n",
      "        [    0.0971],\n",
      "        [    0.0034],\n",
      "        [    0.0037],\n",
      "        [    0.1011],\n",
      "        [    0.1054],\n",
      "        [    0.1061],\n",
      "        [    0.1111],\n",
      "        [    0.0200],\n",
      "        [    0.0218],\n",
      "        [    0.0255],\n",
      "        [    0.1236],\n",
      "        [    0.1249],\n",
      "        [    0.0299],\n",
      "        [    0.1349],\n",
      "        [    0.1355],\n",
      "        [    0.1357],\n",
      "        [    0.0437],\n",
      "        [    0.0469],\n",
      "        [    0.0496],\n",
      "        [    0.0516],\n",
      "        [    0.1480],\n",
      "        [    0.1500],\n",
      "        [    0.0548],\n",
      "        [    0.0592],\n",
      "        [    0.0627],\n",
      "        [    0.0701],\n",
      "        [    0.0740],\n",
      "        [    0.0766],\n",
      "        [    0.0777],\n",
      "        [    0.0841],\n",
      "        [    0.0842],\n",
      "        [    0.0876],\n",
      "        [    0.1830],\n",
      "        [    0.0993],\n",
      "        [    0.2060],\n",
      "        [    0.1153],\n",
      "        [    0.1234],\n",
      "        [    0.2249],\n",
      "        [    0.2250],\n",
      "        [    0.1367],\n",
      "        [    0.1387],\n",
      "        [    0.1397],\n",
      "        [    0.1461],\n",
      "        [    0.1495],\n",
      "        [    0.1538],\n",
      "        [    0.2510],\n",
      "        [    0.1585],\n",
      "        [    0.1596],\n",
      "        [    0.2568],\n",
      "        [    0.2637],\n",
      "        [    0.1702],\n",
      "        [    0.1741],\n",
      "        [    0.2727],\n",
      "        [    0.2728],\n",
      "        [    0.2819],\n",
      "        [    0.1869],\n",
      "        [    0.1902],\n",
      "        [    0.2953],\n",
      "        [    0.2990],\n",
      "        [    0.2117],\n",
      "        [    0.3093],\n",
      "        [    0.3097],\n",
      "        [    0.2195],\n",
      "        [    0.3248],\n",
      "        [    0.2462],\n",
      "        [    0.2472],\n",
      "        [    0.3446],\n",
      "        [    0.2519],\n",
      "        [    0.2521],\n",
      "        [    0.2526],\n",
      "        [    0.2527],\n",
      "        [    0.3487],\n",
      "        [    0.2563],\n",
      "        [    0.2843],\n",
      "        [    0.2854],\n",
      "        [    0.2914],\n",
      "        [    0.2965],\n",
      "        [    0.2992],\n",
      "        [    0.3028],\n",
      "        [    0.3035],\n",
      "        [    0.3080],\n",
      "        [    0.3197],\n",
      "        [    0.3239]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0503],\n",
      "        [0.0508],\n",
      "        [0.0510],\n",
      "        [0.0516],\n",
      "        [0.0529],\n",
      "        [0.0472],\n",
      "        [0.0549],\n",
      "        [0.0411],\n",
      "        [0.0393],\n",
      "        [0.0372],\n",
      "        [0.0636],\n",
      "        [0.0639],\n",
      "        [0.0358],\n",
      "        [0.0348],\n",
      "        [0.0334],\n",
      "        [0.0330],\n",
      "        [0.0686],\n",
      "        [0.0690],\n",
      "        [0.0713],\n",
      "        [0.0282],\n",
      "        [0.0278],\n",
      "        [0.0272],\n",
      "        [0.0253],\n",
      "        [0.0252],\n",
      "        [0.0754],\n",
      "        [0.0224],\n",
      "        [0.0205],\n",
      "        [0.0196],\n",
      "        [0.0811],\n",
      "        [0.0826],\n",
      "        [0.0833],\n",
      "        [0.0157],\n",
      "        [0.0154],\n",
      "        [0.0134],\n",
      "        [0.0869],\n",
      "        [0.0132],\n",
      "        [0.0884],\n",
      "        [0.0106],\n",
      "        [0.0089],\n",
      "        [0.0084],\n",
      "        [0.0062],\n",
      "        [0.0952],\n",
      "        [0.0957],\n",
      "        [0.0969],\n",
      "        [0.0975],\n",
      "        [0.0026],\n",
      "        [0.0023],\n",
      "        [0.0996],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.1036],\n",
      "        [0.1079],\n",
      "        [0.1086],\n",
      "        [0.1136],\n",
      "        [0.0175],\n",
      "        [0.0193],\n",
      "        [0.0230],\n",
      "        [0.1261],\n",
      "        [0.1273],\n",
      "        [0.0275],\n",
      "        [0.1374],\n",
      "        [0.1380],\n",
      "        [0.1381],\n",
      "        [0.0413],\n",
      "        [0.0444],\n",
      "        [0.0471],\n",
      "        [0.0492],\n",
      "        [0.1504],\n",
      "        [0.1524],\n",
      "        [0.0523],\n",
      "        [0.0567],\n",
      "        [0.0602],\n",
      "        [0.0676],\n",
      "        [0.0715],\n",
      "        [0.0742],\n",
      "        [0.0752],\n",
      "        [0.0816],\n",
      "        [0.0817],\n",
      "        [0.0851],\n",
      "        [0.1855],\n",
      "        [0.0968],\n",
      "        [0.2085],\n",
      "        [0.1128],\n",
      "        [0.1209],\n",
      "        [0.2273],\n",
      "        [0.2275],\n",
      "        [0.1342],\n",
      "        [0.1362],\n",
      "        [0.1372],\n",
      "        [0.1436],\n",
      "        [0.1470],\n",
      "        [0.1513],\n",
      "        [0.2535],\n",
      "        [0.1560],\n",
      "        [0.1571],\n",
      "        [0.2593],\n",
      "        [0.2662],\n",
      "        [0.1677],\n",
      "        [0.1716],\n",
      "        [0.2752],\n",
      "        [0.2753],\n",
      "        [0.2844],\n",
      "        [0.1845],\n",
      "        [0.1877],\n",
      "        [0.2978],\n",
      "        [0.3014],\n",
      "        [0.2092],\n",
      "        [0.3118],\n",
      "        [0.3121],\n",
      "        [0.2170],\n",
      "        [0.3273],\n",
      "        [0.2437],\n",
      "        [0.2447],\n",
      "        [0.3471],\n",
      "        [0.2494],\n",
      "        [0.2496],\n",
      "        [0.2501],\n",
      "        [0.2503],\n",
      "        [0.3512],\n",
      "        [0.2539],\n",
      "        [0.2818],\n",
      "        [0.2829],\n",
      "        [0.2889],\n",
      "        [0.2940],\n",
      "        [0.2967],\n",
      "        [0.3004],\n",
      "        [0.3011],\n",
      "        [0.3055],\n",
      "        [0.3172],\n",
      "        [0.3214]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 41.12194466590881\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 131\n",
      "剩餘X 資料 torch.Size([29, 18])\n",
      "剩餘Y 資料 torch.Size([29, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11369054764509201, 3)\n",
      "The second_loss value of k: (0.11608026921749115, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.2710])\n",
      "目前模型的Data狀態 torch.Size([131, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082],\n",
      "        [0.6082]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0503],\n",
      "        [0.0508],\n",
      "        [0.0510],\n",
      "        [0.0516],\n",
      "        [0.0529],\n",
      "        [0.0472],\n",
      "        [0.0549],\n",
      "        [0.0411],\n",
      "        [0.0393],\n",
      "        [0.0372],\n",
      "        [0.0636],\n",
      "        [0.0639],\n",
      "        [0.0358],\n",
      "        [0.0348],\n",
      "        [0.0334],\n",
      "        [0.0330],\n",
      "        [0.0686],\n",
      "        [0.0690],\n",
      "        [0.0713],\n",
      "        [0.0282],\n",
      "        [0.0278],\n",
      "        [0.0272],\n",
      "        [0.0253],\n",
      "        [0.0252],\n",
      "        [0.0754],\n",
      "        [0.0224],\n",
      "        [0.0205],\n",
      "        [0.0196],\n",
      "        [0.0811],\n",
      "        [0.0826],\n",
      "        [0.0833],\n",
      "        [0.0157],\n",
      "        [0.0154],\n",
      "        [0.0134],\n",
      "        [0.0869],\n",
      "        [0.0132],\n",
      "        [0.0884],\n",
      "        [0.0106],\n",
      "        [0.0089],\n",
      "        [0.0084],\n",
      "        [0.0062],\n",
      "        [0.0952],\n",
      "        [0.0957],\n",
      "        [0.0969],\n",
      "        [0.0975],\n",
      "        [0.0026],\n",
      "        [0.0023],\n",
      "        [0.0996],\n",
      "        [0.0009],\n",
      "        [0.0012],\n",
      "        [0.1036],\n",
      "        [0.1079],\n",
      "        [0.1086],\n",
      "        [0.1136],\n",
      "        [0.0175],\n",
      "        [0.0193],\n",
      "        [0.0230],\n",
      "        [0.1261],\n",
      "        [0.1273],\n",
      "        [0.0275],\n",
      "        [0.1374],\n",
      "        [0.1380],\n",
      "        [0.1381],\n",
      "        [0.0413],\n",
      "        [0.0444],\n",
      "        [0.0471],\n",
      "        [0.0492],\n",
      "        [0.1504],\n",
      "        [0.1524],\n",
      "        [0.0523],\n",
      "        [0.0567],\n",
      "        [0.0602],\n",
      "        [0.0676],\n",
      "        [0.0715],\n",
      "        [0.0742],\n",
      "        [0.0752],\n",
      "        [0.0816],\n",
      "        [0.0817],\n",
      "        [0.0851],\n",
      "        [0.1855],\n",
      "        [0.0968],\n",
      "        [0.2085],\n",
      "        [0.1128],\n",
      "        [0.1209],\n",
      "        [0.2273],\n",
      "        [0.2275],\n",
      "        [0.1342],\n",
      "        [0.1362],\n",
      "        [0.1372],\n",
      "        [0.1436],\n",
      "        [0.1470],\n",
      "        [0.1513],\n",
      "        [0.2535],\n",
      "        [0.1560],\n",
      "        [0.1571],\n",
      "        [0.2593],\n",
      "        [0.2662],\n",
      "        [0.1677],\n",
      "        [0.1716],\n",
      "        [0.2752],\n",
      "        [0.2753],\n",
      "        [0.2844],\n",
      "        [0.1845],\n",
      "        [0.1877],\n",
      "        [0.2978],\n",
      "        [0.3014],\n",
      "        [0.2092],\n",
      "        [0.3118],\n",
      "        [0.3121],\n",
      "        [0.2170],\n",
      "        [0.3273],\n",
      "        [0.2437],\n",
      "        [0.2447],\n",
      "        [0.3471],\n",
      "        [0.2494],\n",
      "        [0.2496],\n",
      "        [0.2501],\n",
      "        [0.2503],\n",
      "        [0.3512],\n",
      "        [0.2539],\n",
      "        [0.2818],\n",
      "        [0.2829],\n",
      "        [0.2889],\n",
      "        [0.2940],\n",
      "        [0.2967],\n",
      "        [0.3004],\n",
      "        [0.3011],\n",
      "        [0.3055],\n",
      "        [0.3172],\n",
      "        [0.3214],\n",
      "        [0.3372]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 100\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0529],\n",
      "        [0.0534],\n",
      "        [0.0536],\n",
      "        [0.0542],\n",
      "        [0.0555],\n",
      "        [0.0497],\n",
      "        [0.0574],\n",
      "        [0.0437],\n",
      "        [0.0419],\n",
      "        [0.0398],\n",
      "        [0.0661],\n",
      "        [0.0665],\n",
      "        [0.0384],\n",
      "        [0.0374],\n",
      "        [0.0360],\n",
      "        [0.0356],\n",
      "        [0.0712],\n",
      "        [0.0716],\n",
      "        [0.0739],\n",
      "        [0.0308],\n",
      "        [0.0304],\n",
      "        [0.0297],\n",
      "        [0.0279],\n",
      "        [0.0278],\n",
      "        [0.0780],\n",
      "        [0.0250],\n",
      "        [0.0231],\n",
      "        [0.0222],\n",
      "        [0.0837],\n",
      "        [0.0851],\n",
      "        [0.0858],\n",
      "        [0.0183],\n",
      "        [0.0180],\n",
      "        [0.0160],\n",
      "        [0.0894],\n",
      "        [0.0158],\n",
      "        [0.0910],\n",
      "        [0.0131],\n",
      "        [0.0115],\n",
      "        [0.0110],\n",
      "        [0.0088],\n",
      "        [0.0978],\n",
      "        [0.0983],\n",
      "        [0.0994],\n",
      "        [0.1001],\n",
      "        [0.0052],\n",
      "        [0.0049],\n",
      "        [0.1022],\n",
      "        [0.0017],\n",
      "        [0.0014],\n",
      "        [0.1062],\n",
      "        [0.1105],\n",
      "        [0.1112],\n",
      "        [0.1162],\n",
      "        [0.0149],\n",
      "        [0.0167],\n",
      "        [0.0205],\n",
      "        [0.1287],\n",
      "        [0.1299],\n",
      "        [0.0249],\n",
      "        [0.1399],\n",
      "        [0.1406],\n",
      "        [0.1407],\n",
      "        [0.0387],\n",
      "        [0.0418],\n",
      "        [0.0445],\n",
      "        [0.0466],\n",
      "        [0.1530],\n",
      "        [0.1550],\n",
      "        [0.0497],\n",
      "        [0.0542],\n",
      "        [0.0576],\n",
      "        [0.0650],\n",
      "        [0.0690],\n",
      "        [0.0716],\n",
      "        [0.0726],\n",
      "        [0.0791],\n",
      "        [0.0791],\n",
      "        [0.0826],\n",
      "        [0.1880],\n",
      "        [0.0942],\n",
      "        [0.2111],\n",
      "        [0.1102],\n",
      "        [0.1184],\n",
      "        [0.2299],\n",
      "        [0.2301],\n",
      "        [0.1316],\n",
      "        [0.1337],\n",
      "        [0.1346],\n",
      "        [0.1410],\n",
      "        [0.1444],\n",
      "        [0.1487],\n",
      "        [0.2561],\n",
      "        [0.1534],\n",
      "        [0.1545],\n",
      "        [0.2618],\n",
      "        [0.2688],\n",
      "        [0.1652],\n",
      "        [0.1691],\n",
      "        [0.2777],\n",
      "        [0.2779],\n",
      "        [0.2870],\n",
      "        [0.1819],\n",
      "        [0.1851],\n",
      "        [0.3003],\n",
      "        [0.3040],\n",
      "        [0.2066],\n",
      "        [0.3144],\n",
      "        [0.3147],\n",
      "        [0.2145],\n",
      "        [0.3299],\n",
      "        [0.2411],\n",
      "        [0.2421],\n",
      "        [0.3497],\n",
      "        [0.2469],\n",
      "        [0.2470],\n",
      "        [0.2476],\n",
      "        [0.2477],\n",
      "        [0.3538],\n",
      "        [0.2513],\n",
      "        [0.2792],\n",
      "        [0.2803],\n",
      "        [0.2863],\n",
      "        [0.2914],\n",
      "        [0.2941],\n",
      "        [0.2978],\n",
      "        [0.2985],\n",
      "        [0.3029],\n",
      "        [0.3146],\n",
      "        [0.3188],\n",
      "        [0.3346]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 41.35410666465759\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 132\n",
      "剩餘X 資料 torch.Size([28, 18])\n",
      "剩餘Y 資料 torch.Size([28, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11433308571577072, 2)\n",
      "The second_loss value of k: (0.11603466421365738, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.2675])\n",
      "目前模型的Data狀態 torch.Size([132, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056],\n",
      "        [0.6056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0529],\n",
      "        [0.0534],\n",
      "        [0.0536],\n",
      "        [0.0542],\n",
      "        [0.0555],\n",
      "        [0.0497],\n",
      "        [0.0574],\n",
      "        [0.0437],\n",
      "        [0.0419],\n",
      "        [0.0398],\n",
      "        [0.0661],\n",
      "        [0.0665],\n",
      "        [0.0384],\n",
      "        [0.0374],\n",
      "        [0.0360],\n",
      "        [0.0356],\n",
      "        [0.0712],\n",
      "        [0.0716],\n",
      "        [0.0739],\n",
      "        [0.0308],\n",
      "        [0.0304],\n",
      "        [0.0297],\n",
      "        [0.0279],\n",
      "        [0.0278],\n",
      "        [0.0780],\n",
      "        [0.0250],\n",
      "        [0.0231],\n",
      "        [0.0222],\n",
      "        [0.0837],\n",
      "        [0.0851],\n",
      "        [0.0858],\n",
      "        [0.0183],\n",
      "        [0.0180],\n",
      "        [0.0160],\n",
      "        [0.0894],\n",
      "        [0.0158],\n",
      "        [0.0910],\n",
      "        [0.0131],\n",
      "        [0.0115],\n",
      "        [0.0110],\n",
      "        [0.0088],\n",
      "        [0.0978],\n",
      "        [0.0983],\n",
      "        [0.0994],\n",
      "        [0.1001],\n",
      "        [0.0052],\n",
      "        [0.0049],\n",
      "        [0.1022],\n",
      "        [0.0017],\n",
      "        [0.0014],\n",
      "        [0.1062],\n",
      "        [0.1105],\n",
      "        [0.1112],\n",
      "        [0.1162],\n",
      "        [0.0149],\n",
      "        [0.0167],\n",
      "        [0.0205],\n",
      "        [0.1287],\n",
      "        [0.1299],\n",
      "        [0.0249],\n",
      "        [0.1399],\n",
      "        [0.1406],\n",
      "        [0.1407],\n",
      "        [0.0387],\n",
      "        [0.0418],\n",
      "        [0.0445],\n",
      "        [0.0466],\n",
      "        [0.1530],\n",
      "        [0.1550],\n",
      "        [0.0497],\n",
      "        [0.0542],\n",
      "        [0.0576],\n",
      "        [0.0650],\n",
      "        [0.0690],\n",
      "        [0.0716],\n",
      "        [0.0726],\n",
      "        [0.0791],\n",
      "        [0.0791],\n",
      "        [0.0826],\n",
      "        [0.1880],\n",
      "        [0.0942],\n",
      "        [0.2111],\n",
      "        [0.1102],\n",
      "        [0.1184],\n",
      "        [0.2299],\n",
      "        [0.2301],\n",
      "        [0.1316],\n",
      "        [0.1337],\n",
      "        [0.1346],\n",
      "        [0.1410],\n",
      "        [0.1444],\n",
      "        [0.1487],\n",
      "        [0.2561],\n",
      "        [0.1534],\n",
      "        [0.1545],\n",
      "        [0.2618],\n",
      "        [0.2688],\n",
      "        [0.1652],\n",
      "        [0.1691],\n",
      "        [0.2777],\n",
      "        [0.2779],\n",
      "        [0.2870],\n",
      "        [0.1819],\n",
      "        [0.1851],\n",
      "        [0.3003],\n",
      "        [0.3040],\n",
      "        [0.2066],\n",
      "        [0.3144],\n",
      "        [0.3147],\n",
      "        [0.2145],\n",
      "        [0.3299],\n",
      "        [0.2411],\n",
      "        [0.2421],\n",
      "        [0.3497],\n",
      "        [0.2469],\n",
      "        [0.2470],\n",
      "        [0.2476],\n",
      "        [0.2477],\n",
      "        [0.3538],\n",
      "        [0.2513],\n",
      "        [0.2792],\n",
      "        [0.2803],\n",
      "        [0.2863],\n",
      "        [0.2914],\n",
      "        [0.2941],\n",
      "        [0.2978],\n",
      "        [0.2985],\n",
      "        [0.3029],\n",
      "        [0.3146],\n",
      "        [0.3188],\n",
      "        [0.3346],\n",
      "        [0.3381]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 80\n",
      "Number of shrink: 20\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0555],\n",
      "        [0.0560],\n",
      "        [0.0561],\n",
      "        [0.0567],\n",
      "        [0.0580],\n",
      "        [0.0523],\n",
      "        [0.0600],\n",
      "        [0.0462],\n",
      "        [0.0444],\n",
      "        [0.0424],\n",
      "        [0.0687],\n",
      "        [0.0690],\n",
      "        [0.0409],\n",
      "        [0.0399],\n",
      "        [0.0385],\n",
      "        [0.0381],\n",
      "        [0.0737],\n",
      "        [0.0741],\n",
      "        [0.0764],\n",
      "        [0.0334],\n",
      "        [0.0330],\n",
      "        [0.0323],\n",
      "        [0.0305],\n",
      "        [0.0303],\n",
      "        [0.0805],\n",
      "        [0.0275],\n",
      "        [0.0256],\n",
      "        [0.0248],\n",
      "        [0.0862],\n",
      "        [0.0877],\n",
      "        [0.0884],\n",
      "        [0.0208],\n",
      "        [0.0206],\n",
      "        [0.0186],\n",
      "        [0.0920],\n",
      "        [0.0183],\n",
      "        [0.0936],\n",
      "        [0.0157],\n",
      "        [0.0141],\n",
      "        [0.0135],\n",
      "        [0.0113],\n",
      "        [0.1003],\n",
      "        [0.1008],\n",
      "        [0.1020],\n",
      "        [0.1026],\n",
      "        [0.0077],\n",
      "        [0.0075],\n",
      "        [0.1047],\n",
      "        [0.0042],\n",
      "        [0.0040],\n",
      "        [0.1087],\n",
      "        [0.1131],\n",
      "        [0.1137],\n",
      "        [0.1188],\n",
      "        [0.0123],\n",
      "        [0.0142],\n",
      "        [0.0179],\n",
      "        [0.1312],\n",
      "        [0.1325],\n",
      "        [0.0223],\n",
      "        [0.1425],\n",
      "        [0.1431],\n",
      "        [0.1433],\n",
      "        [0.0361],\n",
      "        [0.0393],\n",
      "        [0.0420],\n",
      "        [0.0440],\n",
      "        [0.1556],\n",
      "        [0.1576],\n",
      "        [0.0472],\n",
      "        [0.0516],\n",
      "        [0.0551],\n",
      "        [0.0624],\n",
      "        [0.0664],\n",
      "        [0.0690],\n",
      "        [0.0701],\n",
      "        [0.0765],\n",
      "        [0.0766],\n",
      "        [0.0800],\n",
      "        [0.1906],\n",
      "        [0.0917],\n",
      "        [0.2136],\n",
      "        [0.1077],\n",
      "        [0.1158],\n",
      "        [0.2325],\n",
      "        [0.2326],\n",
      "        [0.1291],\n",
      "        [0.1311],\n",
      "        [0.1321],\n",
      "        [0.1385],\n",
      "        [0.1419],\n",
      "        [0.1461],\n",
      "        [0.2586],\n",
      "        [0.1508],\n",
      "        [0.1520],\n",
      "        [0.2644],\n",
      "        [0.2713],\n",
      "        [0.1626],\n",
      "        [0.1665],\n",
      "        [0.2803],\n",
      "        [0.2805],\n",
      "        [0.2896],\n",
      "        [0.1793],\n",
      "        [0.1825],\n",
      "        [0.3029],\n",
      "        [0.3066],\n",
      "        [0.2041],\n",
      "        [0.3170],\n",
      "        [0.3173],\n",
      "        [0.2119],\n",
      "        [0.3324],\n",
      "        [0.2385],\n",
      "        [0.2396],\n",
      "        [0.3522],\n",
      "        [0.2443],\n",
      "        [0.2445],\n",
      "        [0.2450],\n",
      "        [0.2451],\n",
      "        [0.3564],\n",
      "        [0.2487],\n",
      "        [0.2767],\n",
      "        [0.2778],\n",
      "        [0.2838],\n",
      "        [0.2888],\n",
      "        [0.2916],\n",
      "        [0.2952],\n",
      "        [0.2959],\n",
      "        [0.3003],\n",
      "        [0.3121],\n",
      "        [0.3163],\n",
      "        [0.3320],\n",
      "        [0.3356]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 41.59872579574585\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 133\n",
      "剩餘X 資料 torch.Size([27, 18])\n",
      "剩餘Y 資料 torch.Size([27, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11429540067911148, 1)\n",
      "The second_loss value of k: (0.11615683883428574, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.2649])\n",
      "目前模型的Data狀態 torch.Size([133, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030],\n",
      "        [0.6030]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0555],\n",
      "        [0.0560],\n",
      "        [0.0561],\n",
      "        [0.0567],\n",
      "        [0.0580],\n",
      "        [0.0523],\n",
      "        [0.0600],\n",
      "        [0.0462],\n",
      "        [0.0444],\n",
      "        [0.0424],\n",
      "        [0.0687],\n",
      "        [0.0690],\n",
      "        [0.0409],\n",
      "        [0.0399],\n",
      "        [0.0385],\n",
      "        [0.0381],\n",
      "        [0.0737],\n",
      "        [0.0741],\n",
      "        [0.0764],\n",
      "        [0.0334],\n",
      "        [0.0330],\n",
      "        [0.0323],\n",
      "        [0.0305],\n",
      "        [0.0303],\n",
      "        [0.0805],\n",
      "        [0.0275],\n",
      "        [0.0256],\n",
      "        [0.0248],\n",
      "        [0.0862],\n",
      "        [0.0877],\n",
      "        [0.0884],\n",
      "        [0.0208],\n",
      "        [0.0206],\n",
      "        [0.0186],\n",
      "        [0.0920],\n",
      "        [0.0183],\n",
      "        [0.0936],\n",
      "        [0.0157],\n",
      "        [0.0141],\n",
      "        [0.0135],\n",
      "        [0.0113],\n",
      "        [0.1003],\n",
      "        [0.1008],\n",
      "        [0.1020],\n",
      "        [0.1026],\n",
      "        [0.0077],\n",
      "        [0.0075],\n",
      "        [0.1047],\n",
      "        [0.0042],\n",
      "        [0.0040],\n",
      "        [0.1087],\n",
      "        [0.1131],\n",
      "        [0.1137],\n",
      "        [0.1188],\n",
      "        [0.0123],\n",
      "        [0.0142],\n",
      "        [0.0179],\n",
      "        [0.1312],\n",
      "        [0.1325],\n",
      "        [0.0223],\n",
      "        [0.1425],\n",
      "        [0.1431],\n",
      "        [0.1433],\n",
      "        [0.0361],\n",
      "        [0.0393],\n",
      "        [0.0420],\n",
      "        [0.0440],\n",
      "        [0.1556],\n",
      "        [0.1576],\n",
      "        [0.0472],\n",
      "        [0.0516],\n",
      "        [0.0551],\n",
      "        [0.0624],\n",
      "        [0.0664],\n",
      "        [0.0690],\n",
      "        [0.0701],\n",
      "        [0.0765],\n",
      "        [0.0766],\n",
      "        [0.0800],\n",
      "        [0.1906],\n",
      "        [0.0917],\n",
      "        [0.2136],\n",
      "        [0.1077],\n",
      "        [0.1158],\n",
      "        [0.2325],\n",
      "        [0.2326],\n",
      "        [0.1291],\n",
      "        [0.1311],\n",
      "        [0.1321],\n",
      "        [0.1385],\n",
      "        [0.1419],\n",
      "        [0.1461],\n",
      "        [0.2586],\n",
      "        [0.1508],\n",
      "        [0.1520],\n",
      "        [0.2644],\n",
      "        [0.2713],\n",
      "        [0.1626],\n",
      "        [0.1665],\n",
      "        [0.2803],\n",
      "        [0.2805],\n",
      "        [0.2896],\n",
      "        [0.1793],\n",
      "        [0.1825],\n",
      "        [0.3029],\n",
      "        [0.3066],\n",
      "        [0.2041],\n",
      "        [0.3170],\n",
      "        [0.3173],\n",
      "        [0.2119],\n",
      "        [0.3324],\n",
      "        [0.2385],\n",
      "        [0.2396],\n",
      "        [0.3522],\n",
      "        [0.2443],\n",
      "        [0.2445],\n",
      "        [0.2450],\n",
      "        [0.2451],\n",
      "        [0.3564],\n",
      "        [0.2487],\n",
      "        [0.2767],\n",
      "        [0.2778],\n",
      "        [0.2838],\n",
      "        [0.2888],\n",
      "        [0.2916],\n",
      "        [0.2952],\n",
      "        [0.2959],\n",
      "        [0.3003],\n",
      "        [0.3121],\n",
      "        [0.3163],\n",
      "        [0.3320],\n",
      "        [0.3356],\n",
      "        [0.3381]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0580],\n",
      "        [0.0585],\n",
      "        [0.0587],\n",
      "        [0.0593],\n",
      "        [0.0606],\n",
      "        [0.0548],\n",
      "        [0.0626],\n",
      "        [0.0488],\n",
      "        [0.0470],\n",
      "        [0.0449],\n",
      "        [0.0713],\n",
      "        [0.0716],\n",
      "        [0.0435],\n",
      "        [0.0425],\n",
      "        [0.0411],\n",
      "        [0.0407],\n",
      "        [0.0763],\n",
      "        [0.0767],\n",
      "        [0.0790],\n",
      "        [0.0359],\n",
      "        [0.0355],\n",
      "        [0.0349],\n",
      "        [0.0330],\n",
      "        [0.0329],\n",
      "        [0.0831],\n",
      "        [0.0301],\n",
      "        [0.0282],\n",
      "        [0.0273],\n",
      "        [0.0888],\n",
      "        [0.0903],\n",
      "        [0.0910],\n",
      "        [0.0234],\n",
      "        [0.0231],\n",
      "        [0.0211],\n",
      "        [0.0946],\n",
      "        [0.0209],\n",
      "        [0.0961],\n",
      "        [0.0183],\n",
      "        [0.0166],\n",
      "        [0.0161],\n",
      "        [0.0139],\n",
      "        [0.1029],\n",
      "        [0.1034],\n",
      "        [0.1046],\n",
      "        [0.1052],\n",
      "        [0.0103],\n",
      "        [0.0100],\n",
      "        [0.1073],\n",
      "        [0.0068],\n",
      "        [0.0065],\n",
      "        [0.1113],\n",
      "        [0.1156],\n",
      "        [0.1163],\n",
      "        [0.1213],\n",
      "        [0.0098],\n",
      "        [0.0116],\n",
      "        [0.0153],\n",
      "        [0.1338],\n",
      "        [0.1350],\n",
      "        [0.0198],\n",
      "        [0.1451],\n",
      "        [0.1457],\n",
      "        [0.1458],\n",
      "        [0.0336],\n",
      "        [0.0367],\n",
      "        [0.0394],\n",
      "        [0.0415],\n",
      "        [0.1581],\n",
      "        [0.1601],\n",
      "        [0.0446],\n",
      "        [0.0490],\n",
      "        [0.0525],\n",
      "        [0.0599],\n",
      "        [0.0638],\n",
      "        [0.0665],\n",
      "        [0.0675],\n",
      "        [0.0739],\n",
      "        [0.0740],\n",
      "        [0.0774],\n",
      "        [0.1932],\n",
      "        [0.0891],\n",
      "        [0.2162],\n",
      "        [0.1051],\n",
      "        [0.1132],\n",
      "        [0.2350],\n",
      "        [0.2352],\n",
      "        [0.1265],\n",
      "        [0.1285],\n",
      "        [0.1295],\n",
      "        [0.1359],\n",
      "        [0.1393],\n",
      "        [0.1436],\n",
      "        [0.2612],\n",
      "        [0.1483],\n",
      "        [0.1494],\n",
      "        [0.2669],\n",
      "        [0.2739],\n",
      "        [0.1600],\n",
      "        [0.1640],\n",
      "        [0.2829],\n",
      "        [0.2830],\n",
      "        [0.2921],\n",
      "        [0.1768],\n",
      "        [0.1800],\n",
      "        [0.3055],\n",
      "        [0.3091],\n",
      "        [0.2015],\n",
      "        [0.3195],\n",
      "        [0.3198],\n",
      "        [0.2094],\n",
      "        [0.3350],\n",
      "        [0.2360],\n",
      "        [0.2370],\n",
      "        [0.3548],\n",
      "        [0.2417],\n",
      "        [0.2419],\n",
      "        [0.2425],\n",
      "        [0.2426],\n",
      "        [0.3589],\n",
      "        [0.2462],\n",
      "        [0.2741],\n",
      "        [0.2752],\n",
      "        [0.2812],\n",
      "        [0.2863],\n",
      "        [0.2890],\n",
      "        [0.2927],\n",
      "        [0.2934],\n",
      "        [0.2978],\n",
      "        [0.3095],\n",
      "        [0.3137],\n",
      "        [0.3295],\n",
      "        [0.3330],\n",
      "        [0.3355]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 41.83918070793152\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 134\n",
      "剩餘X 資料 torch.Size([26, 18])\n",
      "剩餘Y 資料 torch.Size([26, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11442133784294128, 1)\n",
      "The second_loss value of k: (0.139191672205925, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.2622])\n",
      "目前模型的Data狀態 torch.Size([134, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005],\n",
      "        [0.6005]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0580],\n",
      "        [0.0585],\n",
      "        [0.0587],\n",
      "        [0.0593],\n",
      "        [0.0606],\n",
      "        [0.0548],\n",
      "        [0.0626],\n",
      "        [0.0488],\n",
      "        [0.0470],\n",
      "        [0.0449],\n",
      "        [0.0713],\n",
      "        [0.0716],\n",
      "        [0.0435],\n",
      "        [0.0425],\n",
      "        [0.0411],\n",
      "        [0.0407],\n",
      "        [0.0763],\n",
      "        [0.0767],\n",
      "        [0.0790],\n",
      "        [0.0359],\n",
      "        [0.0355],\n",
      "        [0.0349],\n",
      "        [0.0330],\n",
      "        [0.0329],\n",
      "        [0.0831],\n",
      "        [0.0301],\n",
      "        [0.0282],\n",
      "        [0.0273],\n",
      "        [0.0888],\n",
      "        [0.0903],\n",
      "        [0.0910],\n",
      "        [0.0234],\n",
      "        [0.0231],\n",
      "        [0.0211],\n",
      "        [0.0946],\n",
      "        [0.0209],\n",
      "        [0.0961],\n",
      "        [0.0183],\n",
      "        [0.0166],\n",
      "        [0.0161],\n",
      "        [0.0139],\n",
      "        [0.1029],\n",
      "        [0.1034],\n",
      "        [0.1046],\n",
      "        [0.1052],\n",
      "        [0.0103],\n",
      "        [0.0100],\n",
      "        [0.1073],\n",
      "        [0.0068],\n",
      "        [0.0065],\n",
      "        [0.1113],\n",
      "        [0.1156],\n",
      "        [0.1163],\n",
      "        [0.1213],\n",
      "        [0.0098],\n",
      "        [0.0116],\n",
      "        [0.0153],\n",
      "        [0.1338],\n",
      "        [0.1350],\n",
      "        [0.0198],\n",
      "        [0.1451],\n",
      "        [0.1457],\n",
      "        [0.1458],\n",
      "        [0.0336],\n",
      "        [0.0367],\n",
      "        [0.0394],\n",
      "        [0.0415],\n",
      "        [0.1581],\n",
      "        [0.1601],\n",
      "        [0.0446],\n",
      "        [0.0490],\n",
      "        [0.0525],\n",
      "        [0.0599],\n",
      "        [0.0638],\n",
      "        [0.0665],\n",
      "        [0.0675],\n",
      "        [0.0739],\n",
      "        [0.0740],\n",
      "        [0.0774],\n",
      "        [0.1932],\n",
      "        [0.0891],\n",
      "        [0.2162],\n",
      "        [0.1051],\n",
      "        [0.1132],\n",
      "        [0.2350],\n",
      "        [0.2352],\n",
      "        [0.1265],\n",
      "        [0.1285],\n",
      "        [0.1295],\n",
      "        [0.1359],\n",
      "        [0.1393],\n",
      "        [0.1436],\n",
      "        [0.2612],\n",
      "        [0.1483],\n",
      "        [0.1494],\n",
      "        [0.2669],\n",
      "        [0.2739],\n",
      "        [0.1600],\n",
      "        [0.1640],\n",
      "        [0.2829],\n",
      "        [0.2830],\n",
      "        [0.2921],\n",
      "        [0.1768],\n",
      "        [0.1800],\n",
      "        [0.3055],\n",
      "        [0.3091],\n",
      "        [0.2015],\n",
      "        [0.3195],\n",
      "        [0.3198],\n",
      "        [0.2094],\n",
      "        [0.3350],\n",
      "        [0.2360],\n",
      "        [0.2370],\n",
      "        [0.3548],\n",
      "        [0.2417],\n",
      "        [0.2419],\n",
      "        [0.2425],\n",
      "        [0.2426],\n",
      "        [0.3589],\n",
      "        [0.2462],\n",
      "        [0.2741],\n",
      "        [0.2752],\n",
      "        [0.2812],\n",
      "        [0.2863],\n",
      "        [0.2890],\n",
      "        [0.2927],\n",
      "        [0.2934],\n",
      "        [0.2978],\n",
      "        [0.3095],\n",
      "        [0.3137],\n",
      "        [0.3295],\n",
      "        [0.3330],\n",
      "        [0.3355],\n",
      "        [0.3383]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0605],\n",
      "        [0.0610],\n",
      "        [0.0612],\n",
      "        [0.0618],\n",
      "        [0.0631],\n",
      "        [0.0574],\n",
      "        [0.0651],\n",
      "        [0.0513],\n",
      "        [0.0495],\n",
      "        [0.0474],\n",
      "        [0.0738],\n",
      "        [0.0741],\n",
      "        [0.0460],\n",
      "        [0.0450],\n",
      "        [0.0436],\n",
      "        [0.0432],\n",
      "        [0.0788],\n",
      "        [0.0792],\n",
      "        [0.0815],\n",
      "        [0.0384],\n",
      "        [0.0380],\n",
      "        [0.0374],\n",
      "        [0.0355],\n",
      "        [0.0354],\n",
      "        [0.0856],\n",
      "        [0.0326],\n",
      "        [0.0307],\n",
      "        [0.0298],\n",
      "        [0.0913],\n",
      "        [0.0928],\n",
      "        [0.0935],\n",
      "        [0.0259],\n",
      "        [0.0256],\n",
      "        [0.0236],\n",
      "        [0.0971],\n",
      "        [0.0234],\n",
      "        [0.0986],\n",
      "        [0.0208],\n",
      "        [0.0191],\n",
      "        [0.0186],\n",
      "        [0.0164],\n",
      "        [0.1054],\n",
      "        [0.1059],\n",
      "        [0.1071],\n",
      "        [0.1077],\n",
      "        [0.0128],\n",
      "        [0.0125],\n",
      "        [0.1098],\n",
      "        [0.0093],\n",
      "        [0.0090],\n",
      "        [0.1138],\n",
      "        [0.1181],\n",
      "        [0.1188],\n",
      "        [0.1238],\n",
      "        [0.0073],\n",
      "        [0.0091],\n",
      "        [0.0128],\n",
      "        [0.1363],\n",
      "        [0.1375],\n",
      "        [0.0173],\n",
      "        [0.1476],\n",
      "        [0.1482],\n",
      "        [0.1483],\n",
      "        [0.0311],\n",
      "        [0.0342],\n",
      "        [0.0369],\n",
      "        [0.0390],\n",
      "        [0.1606],\n",
      "        [0.1626],\n",
      "        [0.0421],\n",
      "        [0.0465],\n",
      "        [0.0500],\n",
      "        [0.0574],\n",
      "        [0.0613],\n",
      "        [0.0640],\n",
      "        [0.0650],\n",
      "        [0.0714],\n",
      "        [0.0715],\n",
      "        [0.0749],\n",
      "        [0.1957],\n",
      "        [0.0866],\n",
      "        [0.2187],\n",
      "        [0.1026],\n",
      "        [0.1107],\n",
      "        [0.2375],\n",
      "        [0.2377],\n",
      "        [0.1240],\n",
      "        [0.1260],\n",
      "        [0.1270],\n",
      "        [0.1334],\n",
      "        [0.1368],\n",
      "        [0.1411],\n",
      "        [0.2637],\n",
      "        [0.1458],\n",
      "        [0.1469],\n",
      "        [0.2695],\n",
      "        [0.2764],\n",
      "        [0.1575],\n",
      "        [0.1614],\n",
      "        [0.2854],\n",
      "        [0.2855],\n",
      "        [0.2946],\n",
      "        [0.1743],\n",
      "        [0.1775],\n",
      "        [0.3080],\n",
      "        [0.3116],\n",
      "        [0.1990],\n",
      "        [0.3220],\n",
      "        [0.3223],\n",
      "        [0.2068],\n",
      "        [0.3375],\n",
      "        [0.2335],\n",
      "        [0.2345],\n",
      "        [0.3573],\n",
      "        [0.2392],\n",
      "        [0.2394],\n",
      "        [0.2399],\n",
      "        [0.2401],\n",
      "        [0.3614],\n",
      "        [0.2437],\n",
      "        [0.2716],\n",
      "        [0.2727],\n",
      "        [0.2787],\n",
      "        [0.2838],\n",
      "        [0.2865],\n",
      "        [0.2902],\n",
      "        [0.2909],\n",
      "        [0.2953],\n",
      "        [0.3070],\n",
      "        [0.3112],\n",
      "        [0.3270],\n",
      "        [0.3305],\n",
      "        [0.3330],\n",
      "        [0.3358]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 42.07920980453491\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 135\n",
      "剩餘X 資料 torch.Size([25, 18])\n",
      "剩餘Y 資料 torch.Size([25, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1373239904642105, 0)\n",
      "The second_loss value of k: (0.14274710416793823, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.2274])\n",
      "目前模型的Data狀態 torch.Size([135, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980],\n",
      "        [0.5980]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0605],\n",
      "        [0.0610],\n",
      "        [0.0612],\n",
      "        [0.0618],\n",
      "        [0.0631],\n",
      "        [0.0574],\n",
      "        [0.0651],\n",
      "        [0.0513],\n",
      "        [0.0495],\n",
      "        [0.0474],\n",
      "        [0.0738],\n",
      "        [0.0741],\n",
      "        [0.0460],\n",
      "        [0.0450],\n",
      "        [0.0436],\n",
      "        [0.0432],\n",
      "        [0.0788],\n",
      "        [0.0792],\n",
      "        [0.0815],\n",
      "        [0.0384],\n",
      "        [0.0380],\n",
      "        [0.0374],\n",
      "        [0.0355],\n",
      "        [0.0354],\n",
      "        [0.0856],\n",
      "        [0.0326],\n",
      "        [0.0307],\n",
      "        [0.0298],\n",
      "        [0.0913],\n",
      "        [0.0928],\n",
      "        [0.0935],\n",
      "        [0.0259],\n",
      "        [0.0256],\n",
      "        [0.0236],\n",
      "        [0.0971],\n",
      "        [0.0234],\n",
      "        [0.0986],\n",
      "        [0.0208],\n",
      "        [0.0191],\n",
      "        [0.0186],\n",
      "        [0.0164],\n",
      "        [0.1054],\n",
      "        [0.1059],\n",
      "        [0.1071],\n",
      "        [0.1077],\n",
      "        [0.0128],\n",
      "        [0.0125],\n",
      "        [0.1098],\n",
      "        [0.0093],\n",
      "        [0.0090],\n",
      "        [0.1138],\n",
      "        [0.1181],\n",
      "        [0.1188],\n",
      "        [0.1238],\n",
      "        [0.0073],\n",
      "        [0.0091],\n",
      "        [0.0128],\n",
      "        [0.1363],\n",
      "        [0.1375],\n",
      "        [0.0173],\n",
      "        [0.1476],\n",
      "        [0.1482],\n",
      "        [0.1483],\n",
      "        [0.0311],\n",
      "        [0.0342],\n",
      "        [0.0369],\n",
      "        [0.0390],\n",
      "        [0.1606],\n",
      "        [0.1626],\n",
      "        [0.0421],\n",
      "        [0.0465],\n",
      "        [0.0500],\n",
      "        [0.0574],\n",
      "        [0.0613],\n",
      "        [0.0640],\n",
      "        [0.0650],\n",
      "        [0.0714],\n",
      "        [0.0715],\n",
      "        [0.0749],\n",
      "        [0.1957],\n",
      "        [0.0866],\n",
      "        [0.2187],\n",
      "        [0.1026],\n",
      "        [0.1107],\n",
      "        [0.2375],\n",
      "        [0.2377],\n",
      "        [0.1240],\n",
      "        [0.1260],\n",
      "        [0.1270],\n",
      "        [0.1334],\n",
      "        [0.1368],\n",
      "        [0.1411],\n",
      "        [0.2637],\n",
      "        [0.1458],\n",
      "        [0.1469],\n",
      "        [0.2695],\n",
      "        [0.2764],\n",
      "        [0.1575],\n",
      "        [0.1614],\n",
      "        [0.2854],\n",
      "        [0.2855],\n",
      "        [0.2946],\n",
      "        [0.1743],\n",
      "        [0.1775],\n",
      "        [0.3080],\n",
      "        [0.3116],\n",
      "        [0.1990],\n",
      "        [0.3220],\n",
      "        [0.3223],\n",
      "        [0.2068],\n",
      "        [0.3375],\n",
      "        [0.2335],\n",
      "        [0.2345],\n",
      "        [0.3573],\n",
      "        [0.2392],\n",
      "        [0.2394],\n",
      "        [0.2399],\n",
      "        [0.2401],\n",
      "        [0.3614],\n",
      "        [0.2437],\n",
      "        [0.2716],\n",
      "        [0.2727],\n",
      "        [0.2787],\n",
      "        [0.2838],\n",
      "        [0.2865],\n",
      "        [0.2902],\n",
      "        [0.2909],\n",
      "        [0.2953],\n",
      "        [0.3070],\n",
      "        [0.3112],\n",
      "        [0.3270],\n",
      "        [0.3305],\n",
      "        [0.3330],\n",
      "        [0.3358],\n",
      "        [0.3706]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 65\n",
      "Number of shrink: 35\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0633],\n",
      "        [0.0638],\n",
      "        [0.0640],\n",
      "        [0.0646],\n",
      "        [0.0659],\n",
      "        [0.0601],\n",
      "        [0.0678],\n",
      "        [0.0541],\n",
      "        [0.0523],\n",
      "        [0.0502],\n",
      "        [0.0765],\n",
      "        [0.0769],\n",
      "        [0.0488],\n",
      "        [0.0478],\n",
      "        [0.0464],\n",
      "        [0.0460],\n",
      "        [0.0816],\n",
      "        [0.0820],\n",
      "        [0.0843],\n",
      "        [0.0412],\n",
      "        [0.0408],\n",
      "        [0.0402],\n",
      "        [0.0383],\n",
      "        [0.0382],\n",
      "        [0.0884],\n",
      "        [0.0354],\n",
      "        [0.0335],\n",
      "        [0.0326],\n",
      "        [0.0941],\n",
      "        [0.0955],\n",
      "        [0.0962],\n",
      "        [0.0287],\n",
      "        [0.0284],\n",
      "        [0.0264],\n",
      "        [0.0998],\n",
      "        [0.0262],\n",
      "        [0.1014],\n",
      "        [0.0235],\n",
      "        [0.0219],\n",
      "        [0.0214],\n",
      "        [0.0192],\n",
      "        [0.1082],\n",
      "        [0.1087],\n",
      "        [0.1098],\n",
      "        [0.1105],\n",
      "        [0.0156],\n",
      "        [0.0153],\n",
      "        [0.1126],\n",
      "        [0.0121],\n",
      "        [0.0118],\n",
      "        [0.1166],\n",
      "        [0.1209],\n",
      "        [0.1216],\n",
      "        [0.1266],\n",
      "        [0.0045],\n",
      "        [0.0063],\n",
      "        [0.0101],\n",
      "        [0.1391],\n",
      "        [0.1403],\n",
      "        [0.0145],\n",
      "        [0.1503],\n",
      "        [0.1510],\n",
      "        [0.1511],\n",
      "        [0.0283],\n",
      "        [0.0314],\n",
      "        [0.0341],\n",
      "        [0.0362],\n",
      "        [0.1634],\n",
      "        [0.1654],\n",
      "        [0.0393],\n",
      "        [0.0438],\n",
      "        [0.0472],\n",
      "        [0.0546],\n",
      "        [0.0586],\n",
      "        [0.0612],\n",
      "        [0.0622],\n",
      "        [0.0687],\n",
      "        [0.0687],\n",
      "        [0.0721],\n",
      "        [0.1984],\n",
      "        [0.0838],\n",
      "        [0.2215],\n",
      "        [0.0998],\n",
      "        [0.1079],\n",
      "        [0.2403],\n",
      "        [0.2405],\n",
      "        [0.1212],\n",
      "        [0.1233],\n",
      "        [0.1242],\n",
      "        [0.1306],\n",
      "        [0.1340],\n",
      "        [0.1383],\n",
      "        [0.2665],\n",
      "        [0.1430],\n",
      "        [0.1441],\n",
      "        [0.2722],\n",
      "        [0.2792],\n",
      "        [0.1548],\n",
      "        [0.1587],\n",
      "        [0.2881],\n",
      "        [0.2883],\n",
      "        [0.2974],\n",
      "        [0.1715],\n",
      "        [0.1747],\n",
      "        [0.3107],\n",
      "        [0.3144],\n",
      "        [0.1962],\n",
      "        [0.3248],\n",
      "        [0.3251],\n",
      "        [0.2041],\n",
      "        [0.3403],\n",
      "        [0.2307],\n",
      "        [0.2317],\n",
      "        [0.3601],\n",
      "        [0.2365],\n",
      "        [0.2366],\n",
      "        [0.2372],\n",
      "        [0.2373],\n",
      "        [0.3642],\n",
      "        [0.2409],\n",
      "        [0.2688],\n",
      "        [0.2699],\n",
      "        [0.2759],\n",
      "        [0.2810],\n",
      "        [0.2837],\n",
      "        [0.2874],\n",
      "        [0.2881],\n",
      "        [0.2925],\n",
      "        [0.3042],\n",
      "        [0.3084],\n",
      "        [0.3242],\n",
      "        [0.3277],\n",
      "        [0.3302],\n",
      "        [0.3330],\n",
      "        [0.3678]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 42.31909441947937\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 136\n",
      "剩餘X 資料 torch.Size([24, 18])\n",
      "剩餘Y 資料 torch.Size([24, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14066018164157867, 0)\n",
      "The second_loss value of k: (0.14239901304244995, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.2201])\n",
      "目前模型的Data狀態 torch.Size([136, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952],\n",
      "        [0.5952]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0633],\n",
      "        [0.0638],\n",
      "        [0.0640],\n",
      "        [0.0646],\n",
      "        [0.0659],\n",
      "        [0.0601],\n",
      "        [0.0678],\n",
      "        [0.0541],\n",
      "        [0.0523],\n",
      "        [0.0502],\n",
      "        [0.0765],\n",
      "        [0.0769],\n",
      "        [0.0488],\n",
      "        [0.0478],\n",
      "        [0.0464],\n",
      "        [0.0460],\n",
      "        [0.0816],\n",
      "        [0.0820],\n",
      "        [0.0843],\n",
      "        [0.0412],\n",
      "        [0.0408],\n",
      "        [0.0402],\n",
      "        [0.0383],\n",
      "        [0.0382],\n",
      "        [0.0884],\n",
      "        [0.0354],\n",
      "        [0.0335],\n",
      "        [0.0326],\n",
      "        [0.0941],\n",
      "        [0.0955],\n",
      "        [0.0962],\n",
      "        [0.0287],\n",
      "        [0.0284],\n",
      "        [0.0264],\n",
      "        [0.0998],\n",
      "        [0.0262],\n",
      "        [0.1014],\n",
      "        [0.0235],\n",
      "        [0.0219],\n",
      "        [0.0214],\n",
      "        [0.0192],\n",
      "        [0.1082],\n",
      "        [0.1087],\n",
      "        [0.1098],\n",
      "        [0.1105],\n",
      "        [0.0156],\n",
      "        [0.0153],\n",
      "        [0.1126],\n",
      "        [0.0121],\n",
      "        [0.0118],\n",
      "        [0.1166],\n",
      "        [0.1209],\n",
      "        [0.1216],\n",
      "        [0.1266],\n",
      "        [0.0045],\n",
      "        [0.0063],\n",
      "        [0.0101],\n",
      "        [0.1391],\n",
      "        [0.1403],\n",
      "        [0.0145],\n",
      "        [0.1503],\n",
      "        [0.1510],\n",
      "        [0.1511],\n",
      "        [0.0283],\n",
      "        [0.0314],\n",
      "        [0.0341],\n",
      "        [0.0362],\n",
      "        [0.1634],\n",
      "        [0.1654],\n",
      "        [0.0393],\n",
      "        [0.0438],\n",
      "        [0.0472],\n",
      "        [0.0546],\n",
      "        [0.0586],\n",
      "        [0.0612],\n",
      "        [0.0622],\n",
      "        [0.0687],\n",
      "        [0.0687],\n",
      "        [0.0721],\n",
      "        [0.1984],\n",
      "        [0.0838],\n",
      "        [0.2215],\n",
      "        [0.0998],\n",
      "        [0.1079],\n",
      "        [0.2403],\n",
      "        [0.2405],\n",
      "        [0.1212],\n",
      "        [0.1233],\n",
      "        [0.1242],\n",
      "        [0.1306],\n",
      "        [0.1340],\n",
      "        [0.1383],\n",
      "        [0.2665],\n",
      "        [0.1430],\n",
      "        [0.1441],\n",
      "        [0.2722],\n",
      "        [0.2792],\n",
      "        [0.1548],\n",
      "        [0.1587],\n",
      "        [0.2881],\n",
      "        [0.2883],\n",
      "        [0.2974],\n",
      "        [0.1715],\n",
      "        [0.1747],\n",
      "        [0.3107],\n",
      "        [0.3144],\n",
      "        [0.1962],\n",
      "        [0.3248],\n",
      "        [0.3251],\n",
      "        [0.2041],\n",
      "        [0.3403],\n",
      "        [0.2307],\n",
      "        [0.2317],\n",
      "        [0.3601],\n",
      "        [0.2365],\n",
      "        [0.2366],\n",
      "        [0.2372],\n",
      "        [0.2373],\n",
      "        [0.3642],\n",
      "        [0.2409],\n",
      "        [0.2688],\n",
      "        [0.2699],\n",
      "        [0.2759],\n",
      "        [0.2810],\n",
      "        [0.2837],\n",
      "        [0.2874],\n",
      "        [0.2881],\n",
      "        [0.2925],\n",
      "        [0.3042],\n",
      "        [0.3084],\n",
      "        [0.3242],\n",
      "        [0.3277],\n",
      "        [0.3302],\n",
      "        [0.3330],\n",
      "        [0.3678],\n",
      "        [0.3750]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 64\n",
      "Number of shrink: 36\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0661],\n",
      "        [0.0666],\n",
      "        [0.0667],\n",
      "        [0.0673],\n",
      "        [0.0687],\n",
      "        [0.0629],\n",
      "        [0.0706],\n",
      "        [0.0568],\n",
      "        [0.0550],\n",
      "        [0.0530],\n",
      "        [0.0793],\n",
      "        [0.0796],\n",
      "        [0.0515],\n",
      "        [0.0506],\n",
      "        [0.0491],\n",
      "        [0.0488],\n",
      "        [0.0843],\n",
      "        [0.0847],\n",
      "        [0.0870],\n",
      "        [0.0440],\n",
      "        [0.0436],\n",
      "        [0.0429],\n",
      "        [0.0411],\n",
      "        [0.0409],\n",
      "        [0.0911],\n",
      "        [0.0381],\n",
      "        [0.0362],\n",
      "        [0.0354],\n",
      "        [0.0969],\n",
      "        [0.0983],\n",
      "        [0.0990],\n",
      "        [0.0314],\n",
      "        [0.0312],\n",
      "        [0.0292],\n",
      "        [0.1026],\n",
      "        [0.0289],\n",
      "        [0.1042],\n",
      "        [0.0263],\n",
      "        [0.0247],\n",
      "        [0.0242],\n",
      "        [0.0219],\n",
      "        [0.1110],\n",
      "        [0.1114],\n",
      "        [0.1126],\n",
      "        [0.1132],\n",
      "        [0.0183],\n",
      "        [0.0181],\n",
      "        [0.1154],\n",
      "        [0.0148],\n",
      "        [0.0146],\n",
      "        [0.1193],\n",
      "        [0.1237],\n",
      "        [0.1244],\n",
      "        [0.1294],\n",
      "        [0.0017],\n",
      "        [0.0036],\n",
      "        [0.0073],\n",
      "        [0.1418],\n",
      "        [0.1431],\n",
      "        [0.0117],\n",
      "        [0.1531],\n",
      "        [0.1537],\n",
      "        [0.1539],\n",
      "        [0.0255],\n",
      "        [0.0286],\n",
      "        [0.0313],\n",
      "        [0.0334],\n",
      "        [0.1662],\n",
      "        [0.1682],\n",
      "        [0.0366],\n",
      "        [0.0410],\n",
      "        [0.0445],\n",
      "        [0.0518],\n",
      "        [0.0558],\n",
      "        [0.0584],\n",
      "        [0.0595],\n",
      "        [0.0659],\n",
      "        [0.0660],\n",
      "        [0.0694],\n",
      "        [0.2012],\n",
      "        [0.0811],\n",
      "        [0.2242],\n",
      "        [0.0971],\n",
      "        [0.1052],\n",
      "        [0.2431],\n",
      "        [0.2432],\n",
      "        [0.1185],\n",
      "        [0.1205],\n",
      "        [0.1215],\n",
      "        [0.1279],\n",
      "        [0.1313],\n",
      "        [0.1355],\n",
      "        [0.2692],\n",
      "        [0.1402],\n",
      "        [0.1414],\n",
      "        [0.2750],\n",
      "        [0.2819],\n",
      "        [0.1520],\n",
      "        [0.1559],\n",
      "        [0.2909],\n",
      "        [0.2911],\n",
      "        [0.3002],\n",
      "        [0.1687],\n",
      "        [0.1719],\n",
      "        [0.3135],\n",
      "        [0.3172],\n",
      "        [0.1935],\n",
      "        [0.3276],\n",
      "        [0.3279],\n",
      "        [0.2013],\n",
      "        [0.3430],\n",
      "        [0.2279],\n",
      "        [0.2290],\n",
      "        [0.3628],\n",
      "        [0.2337],\n",
      "        [0.2339],\n",
      "        [0.2344],\n",
      "        [0.2345],\n",
      "        [0.3670],\n",
      "        [0.2381],\n",
      "        [0.2661],\n",
      "        [0.2672],\n",
      "        [0.2732],\n",
      "        [0.2782],\n",
      "        [0.2810],\n",
      "        [0.2846],\n",
      "        [0.2853],\n",
      "        [0.2897],\n",
      "        [0.3015],\n",
      "        [0.3057],\n",
      "        [0.3214],\n",
      "        [0.3250],\n",
      "        [0.3275],\n",
      "        [0.3302],\n",
      "        [0.3650],\n",
      "        [0.3723]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 42.559545040130615\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 137\n",
      "剩餘X 資料 torch.Size([23, 18])\n",
      "剩餘Y 資料 torch.Size([23, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14031937718391418, 11)\n",
      "The second_loss value of k: (0.16123512387275696, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引11，y= tensor([0.2178])\n",
      "目前模型的Data狀態 torch.Size([137, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924],\n",
      "        [0.5924]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0661],\n",
      "        [0.0666],\n",
      "        [0.0667],\n",
      "        [0.0673],\n",
      "        [0.0687],\n",
      "        [0.0629],\n",
      "        [0.0706],\n",
      "        [0.0568],\n",
      "        [0.0550],\n",
      "        [0.0530],\n",
      "        [0.0793],\n",
      "        [0.0796],\n",
      "        [0.0515],\n",
      "        [0.0506],\n",
      "        [0.0491],\n",
      "        [0.0488],\n",
      "        [0.0843],\n",
      "        [0.0847],\n",
      "        [0.0870],\n",
      "        [0.0440],\n",
      "        [0.0436],\n",
      "        [0.0429],\n",
      "        [0.0411],\n",
      "        [0.0409],\n",
      "        [0.0911],\n",
      "        [0.0381],\n",
      "        [0.0362],\n",
      "        [0.0354],\n",
      "        [0.0969],\n",
      "        [0.0983],\n",
      "        [0.0990],\n",
      "        [0.0314],\n",
      "        [0.0312],\n",
      "        [0.0292],\n",
      "        [0.1026],\n",
      "        [0.0289],\n",
      "        [0.1042],\n",
      "        [0.0263],\n",
      "        [0.0247],\n",
      "        [0.0242],\n",
      "        [0.0219],\n",
      "        [0.1110],\n",
      "        [0.1114],\n",
      "        [0.1126],\n",
      "        [0.1132],\n",
      "        [0.0183],\n",
      "        [0.0181],\n",
      "        [0.1154],\n",
      "        [0.0148],\n",
      "        [0.0146],\n",
      "        [0.1193],\n",
      "        [0.1237],\n",
      "        [0.1244],\n",
      "        [0.1294],\n",
      "        [0.0017],\n",
      "        [0.0036],\n",
      "        [0.0073],\n",
      "        [0.1418],\n",
      "        [0.1431],\n",
      "        [0.0117],\n",
      "        [0.1531],\n",
      "        [0.1537],\n",
      "        [0.1539],\n",
      "        [0.0255],\n",
      "        [0.0286],\n",
      "        [0.0313],\n",
      "        [0.0334],\n",
      "        [0.1662],\n",
      "        [0.1682],\n",
      "        [0.0366],\n",
      "        [0.0410],\n",
      "        [0.0445],\n",
      "        [0.0518],\n",
      "        [0.0558],\n",
      "        [0.0584],\n",
      "        [0.0595],\n",
      "        [0.0659],\n",
      "        [0.0660],\n",
      "        [0.0694],\n",
      "        [0.2012],\n",
      "        [0.0811],\n",
      "        [0.2242],\n",
      "        [0.0971],\n",
      "        [0.1052],\n",
      "        [0.2431],\n",
      "        [0.2432],\n",
      "        [0.1185],\n",
      "        [0.1205],\n",
      "        [0.1215],\n",
      "        [0.1279],\n",
      "        [0.1313],\n",
      "        [0.1355],\n",
      "        [0.2692],\n",
      "        [0.1402],\n",
      "        [0.1414],\n",
      "        [0.2750],\n",
      "        [0.2819],\n",
      "        [0.1520],\n",
      "        [0.1559],\n",
      "        [0.2909],\n",
      "        [0.2911],\n",
      "        [0.3002],\n",
      "        [0.1687],\n",
      "        [0.1719],\n",
      "        [0.3135],\n",
      "        [0.3172],\n",
      "        [0.1935],\n",
      "        [0.3276],\n",
      "        [0.3279],\n",
      "        [0.2013],\n",
      "        [0.3430],\n",
      "        [0.2279],\n",
      "        [0.2290],\n",
      "        [0.3628],\n",
      "        [0.2337],\n",
      "        [0.2339],\n",
      "        [0.2344],\n",
      "        [0.2345],\n",
      "        [0.3670],\n",
      "        [0.2381],\n",
      "        [0.2661],\n",
      "        [0.2672],\n",
      "        [0.2732],\n",
      "        [0.2782],\n",
      "        [0.2810],\n",
      "        [0.2846],\n",
      "        [0.2853],\n",
      "        [0.2897],\n",
      "        [0.3015],\n",
      "        [0.3057],\n",
      "        [0.3214],\n",
      "        [0.3250],\n",
      "        [0.3275],\n",
      "        [0.3302],\n",
      "        [0.3650],\n",
      "        [0.3723],\n",
      "        [0.3746]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0688],\n",
      "        [0.0693],\n",
      "        [0.0694],\n",
      "        [0.0700],\n",
      "        [0.0713],\n",
      "        [0.0656],\n",
      "        [0.0733],\n",
      "        [0.0595],\n",
      "        [0.0577],\n",
      "        [0.0557],\n",
      "        [0.0820],\n",
      "        [0.0823],\n",
      "        [0.0542],\n",
      "        [0.0532],\n",
      "        [0.0518],\n",
      "        [0.0514],\n",
      "        [0.0870],\n",
      "        [0.0874],\n",
      "        [0.0897],\n",
      "        [0.0467],\n",
      "        [0.0463],\n",
      "        [0.0456],\n",
      "        [0.0438],\n",
      "        [0.0436],\n",
      "        [0.0938],\n",
      "        [0.0408],\n",
      "        [0.0389],\n",
      "        [0.0381],\n",
      "        [0.0995],\n",
      "        [0.1010],\n",
      "        [0.1017],\n",
      "        [0.0341],\n",
      "        [0.0339],\n",
      "        [0.0319],\n",
      "        [0.1053],\n",
      "        [0.0316],\n",
      "        [0.1069],\n",
      "        [0.0290],\n",
      "        [0.0274],\n",
      "        [0.0268],\n",
      "        [0.0246],\n",
      "        [0.1136],\n",
      "        [0.1141],\n",
      "        [0.1153],\n",
      "        [0.1159],\n",
      "        [0.0210],\n",
      "        [0.0208],\n",
      "        [0.1180],\n",
      "        [0.0175],\n",
      "        [0.0173],\n",
      "        [0.1220],\n",
      "        [0.1264],\n",
      "        [0.1270],\n",
      "        [0.1321],\n",
      "        [0.0010],\n",
      "        [0.0009],\n",
      "        [0.0046],\n",
      "        [0.1445],\n",
      "        [0.1458],\n",
      "        [0.0090],\n",
      "        [0.1558],\n",
      "        [0.1564],\n",
      "        [0.1566],\n",
      "        [0.0228],\n",
      "        [0.0260],\n",
      "        [0.0287],\n",
      "        [0.0307],\n",
      "        [0.1689],\n",
      "        [0.1709],\n",
      "        [0.0339],\n",
      "        [0.0383],\n",
      "        [0.0418],\n",
      "        [0.0491],\n",
      "        [0.0531],\n",
      "        [0.0557],\n",
      "        [0.0568],\n",
      "        [0.0632],\n",
      "        [0.0633],\n",
      "        [0.0667],\n",
      "        [0.2039],\n",
      "        [0.0784],\n",
      "        [0.2269],\n",
      "        [0.0944],\n",
      "        [0.1025],\n",
      "        [0.2458],\n",
      "        [0.2459],\n",
      "        [0.1158],\n",
      "        [0.1178],\n",
      "        [0.1188],\n",
      "        [0.1252],\n",
      "        [0.1286],\n",
      "        [0.1328],\n",
      "        [0.2719],\n",
      "        [0.1375],\n",
      "        [0.1387],\n",
      "        [0.2777],\n",
      "        [0.2846],\n",
      "        [0.1493],\n",
      "        [0.1532],\n",
      "        [0.2936],\n",
      "        [0.2938],\n",
      "        [0.3029],\n",
      "        [0.1660],\n",
      "        [0.1692],\n",
      "        [0.3162],\n",
      "        [0.3199],\n",
      "        [0.1908],\n",
      "        [0.3303],\n",
      "        [0.3306],\n",
      "        [0.1986],\n",
      "        [0.3457],\n",
      "        [0.2252],\n",
      "        [0.2263],\n",
      "        [0.3655],\n",
      "        [0.2310],\n",
      "        [0.2312],\n",
      "        [0.2317],\n",
      "        [0.2318],\n",
      "        [0.3697],\n",
      "        [0.2354],\n",
      "        [0.2634],\n",
      "        [0.2645],\n",
      "        [0.2705],\n",
      "        [0.2755],\n",
      "        [0.2783],\n",
      "        [0.2819],\n",
      "        [0.2826],\n",
      "        [0.2870],\n",
      "        [0.2988],\n",
      "        [0.3030],\n",
      "        [0.3187],\n",
      "        [0.3223],\n",
      "        [0.3248],\n",
      "        [0.3275],\n",
      "        [0.3623],\n",
      "        [0.3696],\n",
      "        [0.3719]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 42.797626972198486\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 138\n",
      "剩餘X 資料 torch.Size([22, 18])\n",
      "剩餘Y 資料 torch.Size([22, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15907849371433258, 0)\n",
      "The second_loss value of k: (0.16168244183063507, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.1909])\n",
      "目前模型的Data狀態 torch.Size([138, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897],\n",
      "        [0.5897]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0688],\n",
      "        [0.0693],\n",
      "        [0.0694],\n",
      "        [0.0700],\n",
      "        [0.0713],\n",
      "        [0.0656],\n",
      "        [0.0733],\n",
      "        [0.0595],\n",
      "        [0.0577],\n",
      "        [0.0557],\n",
      "        [0.0820],\n",
      "        [0.0823],\n",
      "        [0.0542],\n",
      "        [0.0532],\n",
      "        [0.0518],\n",
      "        [0.0514],\n",
      "        [0.0870],\n",
      "        [0.0874],\n",
      "        [0.0897],\n",
      "        [0.0467],\n",
      "        [0.0463],\n",
      "        [0.0456],\n",
      "        [0.0438],\n",
      "        [0.0436],\n",
      "        [0.0938],\n",
      "        [0.0408],\n",
      "        [0.0389],\n",
      "        [0.0381],\n",
      "        [0.0995],\n",
      "        [0.1010],\n",
      "        [0.1017],\n",
      "        [0.0341],\n",
      "        [0.0339],\n",
      "        [0.0319],\n",
      "        [0.1053],\n",
      "        [0.0316],\n",
      "        [0.1069],\n",
      "        [0.0290],\n",
      "        [0.0274],\n",
      "        [0.0268],\n",
      "        [0.0246],\n",
      "        [0.1136],\n",
      "        [0.1141],\n",
      "        [0.1153],\n",
      "        [0.1159],\n",
      "        [0.0210],\n",
      "        [0.0208],\n",
      "        [0.1180],\n",
      "        [0.0175],\n",
      "        [0.0173],\n",
      "        [0.1220],\n",
      "        [0.1264],\n",
      "        [0.1270],\n",
      "        [0.1321],\n",
      "        [0.0010],\n",
      "        [0.0009],\n",
      "        [0.0046],\n",
      "        [0.1445],\n",
      "        [0.1458],\n",
      "        [0.0090],\n",
      "        [0.1558],\n",
      "        [0.1564],\n",
      "        [0.1566],\n",
      "        [0.0228],\n",
      "        [0.0260],\n",
      "        [0.0287],\n",
      "        [0.0307],\n",
      "        [0.1689],\n",
      "        [0.1709],\n",
      "        [0.0339],\n",
      "        [0.0383],\n",
      "        [0.0418],\n",
      "        [0.0491],\n",
      "        [0.0531],\n",
      "        [0.0557],\n",
      "        [0.0568],\n",
      "        [0.0632],\n",
      "        [0.0633],\n",
      "        [0.0667],\n",
      "        [0.2039],\n",
      "        [0.0784],\n",
      "        [0.2269],\n",
      "        [0.0944],\n",
      "        [0.1025],\n",
      "        [0.2458],\n",
      "        [0.2459],\n",
      "        [0.1158],\n",
      "        [0.1178],\n",
      "        [0.1188],\n",
      "        [0.1252],\n",
      "        [0.1286],\n",
      "        [0.1328],\n",
      "        [0.2719],\n",
      "        [0.1375],\n",
      "        [0.1387],\n",
      "        [0.2777],\n",
      "        [0.2846],\n",
      "        [0.1493],\n",
      "        [0.1532],\n",
      "        [0.2936],\n",
      "        [0.2938],\n",
      "        [0.3029],\n",
      "        [0.1660],\n",
      "        [0.1692],\n",
      "        [0.3162],\n",
      "        [0.3199],\n",
      "        [0.1908],\n",
      "        [0.3303],\n",
      "        [0.3306],\n",
      "        [0.1986],\n",
      "        [0.3457],\n",
      "        [0.2252],\n",
      "        [0.2263],\n",
      "        [0.3655],\n",
      "        [0.2310],\n",
      "        [0.2312],\n",
      "        [0.2317],\n",
      "        [0.2318],\n",
      "        [0.3697],\n",
      "        [0.2354],\n",
      "        [0.2634],\n",
      "        [0.2645],\n",
      "        [0.2705],\n",
      "        [0.2755],\n",
      "        [0.2783],\n",
      "        [0.2819],\n",
      "        [0.2826],\n",
      "        [0.2870],\n",
      "        [0.2988],\n",
      "        [0.3030],\n",
      "        [0.3187],\n",
      "        [0.3223],\n",
      "        [0.3248],\n",
      "        [0.3275],\n",
      "        [0.3623],\n",
      "        [0.3696],\n",
      "        [0.3719],\n",
      "        [0.3988]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 80\n",
      "Number of shrink: 20\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0717],\n",
      "        [0.0722],\n",
      "        [0.0723],\n",
      "        [0.0729],\n",
      "        [0.0742],\n",
      "        [0.0685],\n",
      "        [0.0762],\n",
      "        [0.0624],\n",
      "        [0.0606],\n",
      "        [0.0586],\n",
      "        [0.0849],\n",
      "        [0.0852],\n",
      "        [0.0571],\n",
      "        [0.0561],\n",
      "        [0.0547],\n",
      "        [0.0543],\n",
      "        [0.0899],\n",
      "        [0.0903],\n",
      "        [0.0926],\n",
      "        [0.0496],\n",
      "        [0.0492],\n",
      "        [0.0485],\n",
      "        [0.0467],\n",
      "        [0.0465],\n",
      "        [0.0967],\n",
      "        [0.0437],\n",
      "        [0.0418],\n",
      "        [0.0409],\n",
      "        [0.1024],\n",
      "        [0.1039],\n",
      "        [0.1046],\n",
      "        [0.0370],\n",
      "        [0.0368],\n",
      "        [0.0348],\n",
      "        [0.1082],\n",
      "        [0.0345],\n",
      "        [0.1098],\n",
      "        [0.0319],\n",
      "        [0.0303],\n",
      "        [0.0297],\n",
      "        [0.0275],\n",
      "        [0.1165],\n",
      "        [0.1170],\n",
      "        [0.1182],\n",
      "        [0.1188],\n",
      "        [0.0239],\n",
      "        [0.0237],\n",
      "        [0.1209],\n",
      "        [0.0204],\n",
      "        [0.0201],\n",
      "        [0.1249],\n",
      "        [0.1293],\n",
      "        [0.1299],\n",
      "        [0.1350],\n",
      "        [0.0039],\n",
      "        [0.0020],\n",
      "        [0.0017],\n",
      "        [0.1474],\n",
      "        [0.1487],\n",
      "        [0.0061],\n",
      "        [0.1587],\n",
      "        [0.1593],\n",
      "        [0.1595],\n",
      "        [0.0199],\n",
      "        [0.0231],\n",
      "        [0.0258],\n",
      "        [0.0278],\n",
      "        [0.1718],\n",
      "        [0.1738],\n",
      "        [0.0310],\n",
      "        [0.0354],\n",
      "        [0.0389],\n",
      "        [0.0462],\n",
      "        [0.0502],\n",
      "        [0.0528],\n",
      "        [0.0539],\n",
      "        [0.0603],\n",
      "        [0.0604],\n",
      "        [0.0638],\n",
      "        [0.2068],\n",
      "        [0.0755],\n",
      "        [0.2298],\n",
      "        [0.0915],\n",
      "        [0.0996],\n",
      "        [0.2487],\n",
      "        [0.2488],\n",
      "        [0.1129],\n",
      "        [0.1149],\n",
      "        [0.1159],\n",
      "        [0.1223],\n",
      "        [0.1257],\n",
      "        [0.1299],\n",
      "        [0.2748],\n",
      "        [0.1346],\n",
      "        [0.1358],\n",
      "        [0.2806],\n",
      "        [0.2875],\n",
      "        [0.1464],\n",
      "        [0.1503],\n",
      "        [0.2965],\n",
      "        [0.2966],\n",
      "        [0.3058],\n",
      "        [0.1631],\n",
      "        [0.1663],\n",
      "        [0.3191],\n",
      "        [0.3228],\n",
      "        [0.1879],\n",
      "        [0.3332],\n",
      "        [0.3335],\n",
      "        [0.1957],\n",
      "        [0.3486],\n",
      "        [0.2224],\n",
      "        [0.2234],\n",
      "        [0.3684],\n",
      "        [0.2281],\n",
      "        [0.2283],\n",
      "        [0.2288],\n",
      "        [0.2289],\n",
      "        [0.3726],\n",
      "        [0.2325],\n",
      "        [0.2605],\n",
      "        [0.2616],\n",
      "        [0.2676],\n",
      "        [0.2726],\n",
      "        [0.2754],\n",
      "        [0.2790],\n",
      "        [0.2797],\n",
      "        [0.2841],\n",
      "        [0.2959],\n",
      "        [0.3001],\n",
      "        [0.3158],\n",
      "        [0.3194],\n",
      "        [0.3219],\n",
      "        [0.3246],\n",
      "        [0.3594],\n",
      "        [0.3667],\n",
      "        [0.3690],\n",
      "        [0.3960]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 43.03415393829346\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 139\n",
      "剩餘X 資料 torch.Size([21, 18])\n",
      "剩餘Y 資料 torch.Size([21, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1593618243932724, 1)\n",
      "The second_loss value of k: (0.16222038865089417, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.1876])\n",
      "目前模型的Data狀態 torch.Size([139, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868],\n",
      "        [0.5868]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0717],\n",
      "        [0.0722],\n",
      "        [0.0723],\n",
      "        [0.0729],\n",
      "        [0.0742],\n",
      "        [0.0685],\n",
      "        [0.0762],\n",
      "        [0.0624],\n",
      "        [0.0606],\n",
      "        [0.0586],\n",
      "        [0.0849],\n",
      "        [0.0852],\n",
      "        [0.0571],\n",
      "        [0.0561],\n",
      "        [0.0547],\n",
      "        [0.0543],\n",
      "        [0.0899],\n",
      "        [0.0903],\n",
      "        [0.0926],\n",
      "        [0.0496],\n",
      "        [0.0492],\n",
      "        [0.0485],\n",
      "        [0.0467],\n",
      "        [0.0465],\n",
      "        [0.0967],\n",
      "        [0.0437],\n",
      "        [0.0418],\n",
      "        [0.0409],\n",
      "        [0.1024],\n",
      "        [0.1039],\n",
      "        [0.1046],\n",
      "        [0.0370],\n",
      "        [0.0368],\n",
      "        [0.0348],\n",
      "        [0.1082],\n",
      "        [0.0345],\n",
      "        [0.1098],\n",
      "        [0.0319],\n",
      "        [0.0303],\n",
      "        [0.0297],\n",
      "        [0.0275],\n",
      "        [0.1165],\n",
      "        [0.1170],\n",
      "        [0.1182],\n",
      "        [0.1188],\n",
      "        [0.0239],\n",
      "        [0.0237],\n",
      "        [0.1209],\n",
      "        [0.0204],\n",
      "        [0.0201],\n",
      "        [0.1249],\n",
      "        [0.1293],\n",
      "        [0.1299],\n",
      "        [0.1350],\n",
      "        [0.0039],\n",
      "        [0.0020],\n",
      "        [0.0017],\n",
      "        [0.1474],\n",
      "        [0.1487],\n",
      "        [0.0061],\n",
      "        [0.1587],\n",
      "        [0.1593],\n",
      "        [0.1595],\n",
      "        [0.0199],\n",
      "        [0.0231],\n",
      "        [0.0258],\n",
      "        [0.0278],\n",
      "        [0.1718],\n",
      "        [0.1738],\n",
      "        [0.0310],\n",
      "        [0.0354],\n",
      "        [0.0389],\n",
      "        [0.0462],\n",
      "        [0.0502],\n",
      "        [0.0528],\n",
      "        [0.0539],\n",
      "        [0.0603],\n",
      "        [0.0604],\n",
      "        [0.0638],\n",
      "        [0.2068],\n",
      "        [0.0755],\n",
      "        [0.2298],\n",
      "        [0.0915],\n",
      "        [0.0996],\n",
      "        [0.2487],\n",
      "        [0.2488],\n",
      "        [0.1129],\n",
      "        [0.1149],\n",
      "        [0.1159],\n",
      "        [0.1223],\n",
      "        [0.1257],\n",
      "        [0.1299],\n",
      "        [0.2748],\n",
      "        [0.1346],\n",
      "        [0.1358],\n",
      "        [0.2806],\n",
      "        [0.2875],\n",
      "        [0.1464],\n",
      "        [0.1503],\n",
      "        [0.2965],\n",
      "        [0.2966],\n",
      "        [0.3058],\n",
      "        [0.1631],\n",
      "        [0.1663],\n",
      "        [0.3191],\n",
      "        [0.3228],\n",
      "        [0.1879],\n",
      "        [0.3332],\n",
      "        [0.3335],\n",
      "        [0.1957],\n",
      "        [0.3486],\n",
      "        [0.2224],\n",
      "        [0.2234],\n",
      "        [0.3684],\n",
      "        [0.2281],\n",
      "        [0.2283],\n",
      "        [0.2288],\n",
      "        [0.2289],\n",
      "        [0.3726],\n",
      "        [0.2325],\n",
      "        [0.2605],\n",
      "        [0.2616],\n",
      "        [0.2676],\n",
      "        [0.2726],\n",
      "        [0.2754],\n",
      "        [0.2790],\n",
      "        [0.2797],\n",
      "        [0.2841],\n",
      "        [0.2959],\n",
      "        [0.3001],\n",
      "        [0.3158],\n",
      "        [0.3194],\n",
      "        [0.3219],\n",
      "        [0.3246],\n",
      "        [0.3594],\n",
      "        [0.3667],\n",
      "        [0.3690],\n",
      "        [0.3960],\n",
      "        [0.3992]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 75\n",
      "Number of shrink: 25\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0745],\n",
      "        [0.0750],\n",
      "        [0.0752],\n",
      "        [0.0758],\n",
      "        [0.0771],\n",
      "        [0.0714],\n",
      "        [0.0791],\n",
      "        [0.0653],\n",
      "        [0.0635],\n",
      "        [0.0614],\n",
      "        [0.0878],\n",
      "        [0.0881],\n",
      "        [0.0600],\n",
      "        [0.0590],\n",
      "        [0.0576],\n",
      "        [0.0572],\n",
      "        [0.0928],\n",
      "        [0.0932],\n",
      "        [0.0955],\n",
      "        [0.0524],\n",
      "        [0.0520],\n",
      "        [0.0514],\n",
      "        [0.0495],\n",
      "        [0.0494],\n",
      "        [0.0996],\n",
      "        [0.0466],\n",
      "        [0.0447],\n",
      "        [0.0438],\n",
      "        [0.1053],\n",
      "        [0.1068],\n",
      "        [0.1075],\n",
      "        [0.0399],\n",
      "        [0.0396],\n",
      "        [0.0376],\n",
      "        [0.1111],\n",
      "        [0.0374],\n",
      "        [0.1126],\n",
      "        [0.0348],\n",
      "        [0.0331],\n",
      "        [0.0326],\n",
      "        [0.0304],\n",
      "        [0.1194],\n",
      "        [0.1199],\n",
      "        [0.1211],\n",
      "        [0.1217],\n",
      "        [0.0268],\n",
      "        [0.0265],\n",
      "        [0.1238],\n",
      "        [0.0233],\n",
      "        [0.0230],\n",
      "        [0.1278],\n",
      "        [0.1321],\n",
      "        [0.1328],\n",
      "        [0.1378],\n",
      "        [0.0067],\n",
      "        [0.0049],\n",
      "        [0.0012],\n",
      "        [0.1503],\n",
      "        [0.1515],\n",
      "        [0.0033],\n",
      "        [0.1616],\n",
      "        [0.1622],\n",
      "        [0.1623],\n",
      "        [0.0171],\n",
      "        [0.0202],\n",
      "        [0.0229],\n",
      "        [0.0250],\n",
      "        [0.1746],\n",
      "        [0.1766],\n",
      "        [0.0281],\n",
      "        [0.0325],\n",
      "        [0.0360],\n",
      "        [0.0434],\n",
      "        [0.0473],\n",
      "        [0.0500],\n",
      "        [0.0510],\n",
      "        [0.0574],\n",
      "        [0.0575],\n",
      "        [0.0609],\n",
      "        [0.2097],\n",
      "        [0.0726],\n",
      "        [0.2327],\n",
      "        [0.0886],\n",
      "        [0.0967],\n",
      "        [0.2515],\n",
      "        [0.2517],\n",
      "        [0.1100],\n",
      "        [0.1120],\n",
      "        [0.1130],\n",
      "        [0.1194],\n",
      "        [0.1228],\n",
      "        [0.1271],\n",
      "        [0.2777],\n",
      "        [0.1318],\n",
      "        [0.1329],\n",
      "        [0.2835],\n",
      "        [0.2904],\n",
      "        [0.1435],\n",
      "        [0.1474],\n",
      "        [0.2994],\n",
      "        [0.2995],\n",
      "        [0.3086],\n",
      "        [0.1603],\n",
      "        [0.1635],\n",
      "        [0.3220],\n",
      "        [0.3256],\n",
      "        [0.1850],\n",
      "        [0.3360],\n",
      "        [0.3363],\n",
      "        [0.1928],\n",
      "        [0.3515],\n",
      "        [0.2195],\n",
      "        [0.2205],\n",
      "        [0.3713],\n",
      "        [0.2252],\n",
      "        [0.2254],\n",
      "        [0.2259],\n",
      "        [0.2261],\n",
      "        [0.3754],\n",
      "        [0.2297],\n",
      "        [0.2576],\n",
      "        [0.2587],\n",
      "        [0.2647],\n",
      "        [0.2698],\n",
      "        [0.2725],\n",
      "        [0.2762],\n",
      "        [0.2769],\n",
      "        [0.2813],\n",
      "        [0.2930],\n",
      "        [0.2972],\n",
      "        [0.3130],\n",
      "        [0.3165],\n",
      "        [0.3190],\n",
      "        [0.3218],\n",
      "        [0.3566],\n",
      "        [0.3638],\n",
      "        [0.3661],\n",
      "        [0.3931],\n",
      "        [0.3963]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 43.27055835723877\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 140\n",
      "剩餘X 資料 torch.Size([20, 18])\n",
      "剩餘Y 資料 torch.Size([20, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1599169671535492, 2)\n",
      "The second_loss value of k: (0.16252772510051727, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.1841])\n",
      "目前模型的Data狀態 torch.Size([140, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840],\n",
      "        [0.5840]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0745],\n",
      "        [0.0750],\n",
      "        [0.0752],\n",
      "        [0.0758],\n",
      "        [0.0771],\n",
      "        [0.0714],\n",
      "        [0.0791],\n",
      "        [0.0653],\n",
      "        [0.0635],\n",
      "        [0.0614],\n",
      "        [0.0878],\n",
      "        [0.0881],\n",
      "        [0.0600],\n",
      "        [0.0590],\n",
      "        [0.0576],\n",
      "        [0.0572],\n",
      "        [0.0928],\n",
      "        [0.0932],\n",
      "        [0.0955],\n",
      "        [0.0524],\n",
      "        [0.0520],\n",
      "        [0.0514],\n",
      "        [0.0495],\n",
      "        [0.0494],\n",
      "        [0.0996],\n",
      "        [0.0466],\n",
      "        [0.0447],\n",
      "        [0.0438],\n",
      "        [0.1053],\n",
      "        [0.1068],\n",
      "        [0.1075],\n",
      "        [0.0399],\n",
      "        [0.0396],\n",
      "        [0.0376],\n",
      "        [0.1111],\n",
      "        [0.0374],\n",
      "        [0.1126],\n",
      "        [0.0348],\n",
      "        [0.0331],\n",
      "        [0.0326],\n",
      "        [0.0304],\n",
      "        [0.1194],\n",
      "        [0.1199],\n",
      "        [0.1211],\n",
      "        [0.1217],\n",
      "        [0.0268],\n",
      "        [0.0265],\n",
      "        [0.1238],\n",
      "        [0.0233],\n",
      "        [0.0230],\n",
      "        [0.1278],\n",
      "        [0.1321],\n",
      "        [0.1328],\n",
      "        [0.1378],\n",
      "        [0.0067],\n",
      "        [0.0049],\n",
      "        [0.0012],\n",
      "        [0.1503],\n",
      "        [0.1515],\n",
      "        [0.0033],\n",
      "        [0.1616],\n",
      "        [0.1622],\n",
      "        [0.1623],\n",
      "        [0.0171],\n",
      "        [0.0202],\n",
      "        [0.0229],\n",
      "        [0.0250],\n",
      "        [0.1746],\n",
      "        [0.1766],\n",
      "        [0.0281],\n",
      "        [0.0325],\n",
      "        [0.0360],\n",
      "        [0.0434],\n",
      "        [0.0473],\n",
      "        [0.0500],\n",
      "        [0.0510],\n",
      "        [0.0574],\n",
      "        [0.0575],\n",
      "        [0.0609],\n",
      "        [0.2097],\n",
      "        [0.0726],\n",
      "        [0.2327],\n",
      "        [0.0886],\n",
      "        [0.0967],\n",
      "        [0.2515],\n",
      "        [0.2517],\n",
      "        [0.1100],\n",
      "        [0.1120],\n",
      "        [0.1130],\n",
      "        [0.1194],\n",
      "        [0.1228],\n",
      "        [0.1271],\n",
      "        [0.2777],\n",
      "        [0.1318],\n",
      "        [0.1329],\n",
      "        [0.2835],\n",
      "        [0.2904],\n",
      "        [0.1435],\n",
      "        [0.1474],\n",
      "        [0.2994],\n",
      "        [0.2995],\n",
      "        [0.3086],\n",
      "        [0.1603],\n",
      "        [0.1635],\n",
      "        [0.3220],\n",
      "        [0.3256],\n",
      "        [0.1850],\n",
      "        [0.3360],\n",
      "        [0.3363],\n",
      "        [0.1928],\n",
      "        [0.3515],\n",
      "        [0.2195],\n",
      "        [0.2205],\n",
      "        [0.3713],\n",
      "        [0.2252],\n",
      "        [0.2254],\n",
      "        [0.2259],\n",
      "        [0.2261],\n",
      "        [0.3754],\n",
      "        [0.2297],\n",
      "        [0.2576],\n",
      "        [0.2587],\n",
      "        [0.2647],\n",
      "        [0.2698],\n",
      "        [0.2725],\n",
      "        [0.2762],\n",
      "        [0.2769],\n",
      "        [0.2813],\n",
      "        [0.2930],\n",
      "        [0.2972],\n",
      "        [0.3130],\n",
      "        [0.3165],\n",
      "        [0.3190],\n",
      "        [0.3218],\n",
      "        [0.3566],\n",
      "        [0.3638],\n",
      "        [0.3661],\n",
      "        [0.3931],\n",
      "        [0.3963],\n",
      "        [0.3999]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0774],\n",
      "        [    0.0779],\n",
      "        [    0.0781],\n",
      "        [    0.0787],\n",
      "        [    0.0800],\n",
      "        [    0.0742],\n",
      "        [    0.0819],\n",
      "        [    0.0682],\n",
      "        [    0.0664],\n",
      "        [    0.0643],\n",
      "        [    0.0906],\n",
      "        [    0.0910],\n",
      "        [    0.0629],\n",
      "        [    0.0619],\n",
      "        [    0.0605],\n",
      "        [    0.0601],\n",
      "        [    0.0957],\n",
      "        [    0.0961],\n",
      "        [    0.0984],\n",
      "        [    0.0553],\n",
      "        [    0.0549],\n",
      "        [    0.0543],\n",
      "        [    0.0524],\n",
      "        [    0.0523],\n",
      "        [    0.1025],\n",
      "        [    0.0495],\n",
      "        [    0.0476],\n",
      "        [    0.0467],\n",
      "        [    0.1082],\n",
      "        [    0.1096],\n",
      "        [    0.1103],\n",
      "        [    0.0428],\n",
      "        [    0.0425],\n",
      "        [    0.0405],\n",
      "        [    0.1139],\n",
      "        [    0.0403],\n",
      "        [    0.1155],\n",
      "        [    0.0376],\n",
      "        [    0.0360],\n",
      "        [    0.0355],\n",
      "        [    0.0333],\n",
      "        [    0.1223],\n",
      "        [    0.1228],\n",
      "        [    0.1239],\n",
      "        [    0.1246],\n",
      "        [    0.0297],\n",
      "        [    0.0294],\n",
      "        [    0.1267],\n",
      "        [    0.0262],\n",
      "        [    0.0259],\n",
      "        [    0.1307],\n",
      "        [    0.1350],\n",
      "        [    0.1357],\n",
      "        [    0.1407],\n",
      "        [    0.0096],\n",
      "        [    0.0078],\n",
      "        [    0.0040],\n",
      "        [    0.1532],\n",
      "        [    0.1544],\n",
      "        [    0.0004],\n",
      "        [    0.1644],\n",
      "        [    0.1651],\n",
      "        [    0.1652],\n",
      "        [    0.0142],\n",
      "        [    0.0173],\n",
      "        [    0.0200],\n",
      "        [    0.0221],\n",
      "        [    0.1775],\n",
      "        [    0.1795],\n",
      "        [    0.0252],\n",
      "        [    0.0296],\n",
      "        [    0.0331],\n",
      "        [    0.0405],\n",
      "        [    0.0445],\n",
      "        [    0.0471],\n",
      "        [    0.0481],\n",
      "        [    0.0546],\n",
      "        [    0.0546],\n",
      "        [    0.0580],\n",
      "        [    0.2125],\n",
      "        [    0.0697],\n",
      "        [    0.2356],\n",
      "        [    0.0857],\n",
      "        [    0.0938],\n",
      "        [    0.2544],\n",
      "        [    0.2546],\n",
      "        [    0.1071],\n",
      "        [    0.1092],\n",
      "        [    0.1101],\n",
      "        [    0.1165],\n",
      "        [    0.1199],\n",
      "        [    0.1242],\n",
      "        [    0.2806],\n",
      "        [    0.1289],\n",
      "        [    0.1300],\n",
      "        [    0.2863],\n",
      "        [    0.2933],\n",
      "        [    0.1407],\n",
      "        [    0.1446],\n",
      "        [    0.3022],\n",
      "        [    0.3024],\n",
      "        [    0.3115],\n",
      "        [    0.1574],\n",
      "        [    0.1606],\n",
      "        [    0.3248],\n",
      "        [    0.3285],\n",
      "        [    0.1821],\n",
      "        [    0.3389],\n",
      "        [    0.3392],\n",
      "        [    0.1900],\n",
      "        [    0.3544],\n",
      "        [    0.2166],\n",
      "        [    0.2176],\n",
      "        [    0.3742],\n",
      "        [    0.2224],\n",
      "        [    0.2225],\n",
      "        [    0.2231],\n",
      "        [    0.2232],\n",
      "        [    0.3783],\n",
      "        [    0.2268],\n",
      "        [    0.2547],\n",
      "        [    0.2558],\n",
      "        [    0.2618],\n",
      "        [    0.2669],\n",
      "        [    0.2696],\n",
      "        [    0.2733],\n",
      "        [    0.2740],\n",
      "        [    0.2784],\n",
      "        [    0.2901],\n",
      "        [    0.2943],\n",
      "        [    0.3101],\n",
      "        [    0.3136],\n",
      "        [    0.3161],\n",
      "        [    0.3189],\n",
      "        [    0.3537],\n",
      "        [    0.3609],\n",
      "        [    0.3633],\n",
      "        [    0.3902],\n",
      "        [    0.3935],\n",
      "        [    0.3970]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 43.50992774963379\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 141\n",
      "剩餘X 資料 torch.Size([19, 18])\n",
      "剩餘Y 資料 torch.Size([19, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16021792590618134, 1)\n",
      "The second_loss value of k: (0.17499954998493195, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.1808])\n",
      "目前模型的Data狀態 torch.Size([141, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811],\n",
      "        [0.5811]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0774],\n",
      "        [    0.0779],\n",
      "        [    0.0781],\n",
      "        [    0.0787],\n",
      "        [    0.0800],\n",
      "        [    0.0742],\n",
      "        [    0.0819],\n",
      "        [    0.0682],\n",
      "        [    0.0664],\n",
      "        [    0.0643],\n",
      "        [    0.0906],\n",
      "        [    0.0910],\n",
      "        [    0.0629],\n",
      "        [    0.0619],\n",
      "        [    0.0605],\n",
      "        [    0.0601],\n",
      "        [    0.0957],\n",
      "        [    0.0961],\n",
      "        [    0.0984],\n",
      "        [    0.0553],\n",
      "        [    0.0549],\n",
      "        [    0.0543],\n",
      "        [    0.0524],\n",
      "        [    0.0523],\n",
      "        [    0.1025],\n",
      "        [    0.0495],\n",
      "        [    0.0476],\n",
      "        [    0.0467],\n",
      "        [    0.1082],\n",
      "        [    0.1096],\n",
      "        [    0.1103],\n",
      "        [    0.0428],\n",
      "        [    0.0425],\n",
      "        [    0.0405],\n",
      "        [    0.1139],\n",
      "        [    0.0403],\n",
      "        [    0.1155],\n",
      "        [    0.0376],\n",
      "        [    0.0360],\n",
      "        [    0.0355],\n",
      "        [    0.0333],\n",
      "        [    0.1223],\n",
      "        [    0.1228],\n",
      "        [    0.1239],\n",
      "        [    0.1246],\n",
      "        [    0.0297],\n",
      "        [    0.0294],\n",
      "        [    0.1267],\n",
      "        [    0.0262],\n",
      "        [    0.0259],\n",
      "        [    0.1307],\n",
      "        [    0.1350],\n",
      "        [    0.1357],\n",
      "        [    0.1407],\n",
      "        [    0.0096],\n",
      "        [    0.0078],\n",
      "        [    0.0040],\n",
      "        [    0.1532],\n",
      "        [    0.1544],\n",
      "        [    0.0004],\n",
      "        [    0.1644],\n",
      "        [    0.1651],\n",
      "        [    0.1652],\n",
      "        [    0.0142],\n",
      "        [    0.0173],\n",
      "        [    0.0200],\n",
      "        [    0.0221],\n",
      "        [    0.1775],\n",
      "        [    0.1795],\n",
      "        [    0.0252],\n",
      "        [    0.0296],\n",
      "        [    0.0331],\n",
      "        [    0.0405],\n",
      "        [    0.0445],\n",
      "        [    0.0471],\n",
      "        [    0.0481],\n",
      "        [    0.0546],\n",
      "        [    0.0546],\n",
      "        [    0.0580],\n",
      "        [    0.2125],\n",
      "        [    0.0697],\n",
      "        [    0.2356],\n",
      "        [    0.0857],\n",
      "        [    0.0938],\n",
      "        [    0.2544],\n",
      "        [    0.2546],\n",
      "        [    0.1071],\n",
      "        [    0.1092],\n",
      "        [    0.1101],\n",
      "        [    0.1165],\n",
      "        [    0.1199],\n",
      "        [    0.1242],\n",
      "        [    0.2806],\n",
      "        [    0.1289],\n",
      "        [    0.1300],\n",
      "        [    0.2863],\n",
      "        [    0.2933],\n",
      "        [    0.1407],\n",
      "        [    0.1446],\n",
      "        [    0.3022],\n",
      "        [    0.3024],\n",
      "        [    0.3115],\n",
      "        [    0.1574],\n",
      "        [    0.1606],\n",
      "        [    0.3248],\n",
      "        [    0.3285],\n",
      "        [    0.1821],\n",
      "        [    0.3389],\n",
      "        [    0.3392],\n",
      "        [    0.1900],\n",
      "        [    0.3544],\n",
      "        [    0.2166],\n",
      "        [    0.2176],\n",
      "        [    0.3742],\n",
      "        [    0.2224],\n",
      "        [    0.2225],\n",
      "        [    0.2231],\n",
      "        [    0.2232],\n",
      "        [    0.3783],\n",
      "        [    0.2268],\n",
      "        [    0.2547],\n",
      "        [    0.2558],\n",
      "        [    0.2618],\n",
      "        [    0.2669],\n",
      "        [    0.2696],\n",
      "        [    0.2733],\n",
      "        [    0.2740],\n",
      "        [    0.2784],\n",
      "        [    0.2901],\n",
      "        [    0.2943],\n",
      "        [    0.3101],\n",
      "        [    0.3136],\n",
      "        [    0.3161],\n",
      "        [    0.3189],\n",
      "        [    0.3537],\n",
      "        [    0.3609],\n",
      "        [    0.3633],\n",
      "        [    0.3902],\n",
      "        [    0.3935],\n",
      "        [    0.3970],\n",
      "        [    0.4003]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0802],\n",
      "        [0.0807],\n",
      "        [0.0809],\n",
      "        [0.0815],\n",
      "        [0.0828],\n",
      "        [0.0771],\n",
      "        [0.0848],\n",
      "        [0.0710],\n",
      "        [0.0692],\n",
      "        [0.0671],\n",
      "        [0.0935],\n",
      "        [0.0938],\n",
      "        [0.0657],\n",
      "        [0.0647],\n",
      "        [0.0633],\n",
      "        [0.0629],\n",
      "        [0.0985],\n",
      "        [0.0989],\n",
      "        [0.1012],\n",
      "        [0.0581],\n",
      "        [0.0577],\n",
      "        [0.0571],\n",
      "        [0.0552],\n",
      "        [0.0551],\n",
      "        [0.1053],\n",
      "        [0.0523],\n",
      "        [0.0504],\n",
      "        [0.0495],\n",
      "        [0.1110],\n",
      "        [0.1125],\n",
      "        [0.1132],\n",
      "        [0.0456],\n",
      "        [0.0453],\n",
      "        [0.0433],\n",
      "        [0.1168],\n",
      "        [0.0431],\n",
      "        [0.1183],\n",
      "        [0.0405],\n",
      "        [0.0388],\n",
      "        [0.0383],\n",
      "        [0.0361],\n",
      "        [0.1251],\n",
      "        [0.1256],\n",
      "        [0.1268],\n",
      "        [0.1274],\n",
      "        [0.0325],\n",
      "        [0.0322],\n",
      "        [0.1295],\n",
      "        [0.0290],\n",
      "        [0.0287],\n",
      "        [0.1335],\n",
      "        [0.1378],\n",
      "        [0.1385],\n",
      "        [0.1435],\n",
      "        [0.0124],\n",
      "        [0.0106],\n",
      "        [0.0069],\n",
      "        [0.1560],\n",
      "        [0.1572],\n",
      "        [0.0024],\n",
      "        [0.1673],\n",
      "        [0.1679],\n",
      "        [0.1680],\n",
      "        [0.0114],\n",
      "        [0.0145],\n",
      "        [0.0172],\n",
      "        [0.0193],\n",
      "        [0.1803],\n",
      "        [0.1823],\n",
      "        [0.0224],\n",
      "        [0.0268],\n",
      "        [0.0303],\n",
      "        [0.0377],\n",
      "        [0.0416],\n",
      "        [0.0443],\n",
      "        [0.0453],\n",
      "        [0.0517],\n",
      "        [0.0518],\n",
      "        [0.0552],\n",
      "        [0.2154],\n",
      "        [0.0669],\n",
      "        [0.2384],\n",
      "        [0.0829],\n",
      "        [0.0910],\n",
      "        [0.2572],\n",
      "        [0.2574],\n",
      "        [0.1043],\n",
      "        [0.1063],\n",
      "        [0.1073],\n",
      "        [0.1137],\n",
      "        [0.1171],\n",
      "        [0.1214],\n",
      "        [0.2834],\n",
      "        [0.1261],\n",
      "        [0.1272],\n",
      "        [0.2892],\n",
      "        [0.2961],\n",
      "        [0.1378],\n",
      "        [0.1418],\n",
      "        [0.3051],\n",
      "        [0.3052],\n",
      "        [0.3143],\n",
      "        [0.1546],\n",
      "        [0.1578],\n",
      "        [0.3277],\n",
      "        [0.3313],\n",
      "        [0.1793],\n",
      "        [0.3417],\n",
      "        [0.3420],\n",
      "        [0.1871],\n",
      "        [0.3572],\n",
      "        [0.2138],\n",
      "        [0.2148],\n",
      "        [0.3770],\n",
      "        [0.2195],\n",
      "        [0.2197],\n",
      "        [0.2202],\n",
      "        [0.2204],\n",
      "        [0.3811],\n",
      "        [0.2240],\n",
      "        [0.2519],\n",
      "        [0.2530],\n",
      "        [0.2590],\n",
      "        [0.2641],\n",
      "        [0.2668],\n",
      "        [0.2705],\n",
      "        [0.2712],\n",
      "        [0.2756],\n",
      "        [0.2873],\n",
      "        [0.2915],\n",
      "        [0.3073],\n",
      "        [0.3108],\n",
      "        [0.3133],\n",
      "        [0.3161],\n",
      "        [0.3509],\n",
      "        [0.3581],\n",
      "        [0.3604],\n",
      "        [0.3874],\n",
      "        [0.3906],\n",
      "        [0.3942],\n",
      "        [0.3974]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 43.74787402153015\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 142\n",
      "剩餘X 資料 torch.Size([18, 18])\n",
      "剩餘Y 資料 torch.Size([18, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.17264579236507416, 0)\n",
      "The second_loss value of k: (0.19029776751995087, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.1627])\n",
      "目前模型的Data狀態 torch.Size([142, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.5783]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0802],\n",
      "        [0.0807],\n",
      "        [0.0809],\n",
      "        [0.0815],\n",
      "        [0.0828],\n",
      "        [0.0771],\n",
      "        [0.0848],\n",
      "        [0.0710],\n",
      "        [0.0692],\n",
      "        [0.0671],\n",
      "        [0.0935],\n",
      "        [0.0938],\n",
      "        [0.0657],\n",
      "        [0.0647],\n",
      "        [0.0633],\n",
      "        [0.0629],\n",
      "        [0.0985],\n",
      "        [0.0989],\n",
      "        [0.1012],\n",
      "        [0.0581],\n",
      "        [0.0577],\n",
      "        [0.0571],\n",
      "        [0.0552],\n",
      "        [0.0551],\n",
      "        [0.1053],\n",
      "        [0.0523],\n",
      "        [0.0504],\n",
      "        [0.0495],\n",
      "        [0.1110],\n",
      "        [0.1125],\n",
      "        [0.1132],\n",
      "        [0.0456],\n",
      "        [0.0453],\n",
      "        [0.0433],\n",
      "        [0.1168],\n",
      "        [0.0431],\n",
      "        [0.1183],\n",
      "        [0.0405],\n",
      "        [0.0388],\n",
      "        [0.0383],\n",
      "        [0.0361],\n",
      "        [0.1251],\n",
      "        [0.1256],\n",
      "        [0.1268],\n",
      "        [0.1274],\n",
      "        [0.0325],\n",
      "        [0.0322],\n",
      "        [0.1295],\n",
      "        [0.0290],\n",
      "        [0.0287],\n",
      "        [0.1335],\n",
      "        [0.1378],\n",
      "        [0.1385],\n",
      "        [0.1435],\n",
      "        [0.0124],\n",
      "        [0.0106],\n",
      "        [0.0069],\n",
      "        [0.1560],\n",
      "        [0.1572],\n",
      "        [0.0024],\n",
      "        [0.1673],\n",
      "        [0.1679],\n",
      "        [0.1680],\n",
      "        [0.0114],\n",
      "        [0.0145],\n",
      "        [0.0172],\n",
      "        [0.0193],\n",
      "        [0.1803],\n",
      "        [0.1823],\n",
      "        [0.0224],\n",
      "        [0.0268],\n",
      "        [0.0303],\n",
      "        [0.0377],\n",
      "        [0.0416],\n",
      "        [0.0443],\n",
      "        [0.0453],\n",
      "        [0.0517],\n",
      "        [0.0518],\n",
      "        [0.0552],\n",
      "        [0.2154],\n",
      "        [0.0669],\n",
      "        [0.2384],\n",
      "        [0.0829],\n",
      "        [0.0910],\n",
      "        [0.2572],\n",
      "        [0.2574],\n",
      "        [0.1043],\n",
      "        [0.1063],\n",
      "        [0.1073],\n",
      "        [0.1137],\n",
      "        [0.1171],\n",
      "        [0.1214],\n",
      "        [0.2834],\n",
      "        [0.1261],\n",
      "        [0.1272],\n",
      "        [0.2892],\n",
      "        [0.2961],\n",
      "        [0.1378],\n",
      "        [0.1418],\n",
      "        [0.3051],\n",
      "        [0.3052],\n",
      "        [0.3143],\n",
      "        [0.1546],\n",
      "        [0.1578],\n",
      "        [0.3277],\n",
      "        [0.3313],\n",
      "        [0.1793],\n",
      "        [0.3417],\n",
      "        [0.3420],\n",
      "        [0.1871],\n",
      "        [0.3572],\n",
      "        [0.2138],\n",
      "        [0.2148],\n",
      "        [0.3770],\n",
      "        [0.2195],\n",
      "        [0.2197],\n",
      "        [0.2202],\n",
      "        [0.2204],\n",
      "        [0.3811],\n",
      "        [0.2240],\n",
      "        [0.2519],\n",
      "        [0.2530],\n",
      "        [0.2590],\n",
      "        [0.2641],\n",
      "        [0.2668],\n",
      "        [0.2705],\n",
      "        [0.2712],\n",
      "        [0.2756],\n",
      "        [0.2873],\n",
      "        [0.2915],\n",
      "        [0.3073],\n",
      "        [0.3108],\n",
      "        [0.3133],\n",
      "        [0.3161],\n",
      "        [0.3509],\n",
      "        [0.3581],\n",
      "        [0.3604],\n",
      "        [0.3874],\n",
      "        [0.3906],\n",
      "        [0.3942],\n",
      "        [0.3974],\n",
      "        [0.4155]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 74\n",
      "Number of shrink: 26\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0831],\n",
      "        [0.0837],\n",
      "        [0.0838],\n",
      "        [0.0844],\n",
      "        [0.0857],\n",
      "        [0.0800],\n",
      "        [0.0877],\n",
      "        [0.0739],\n",
      "        [0.0721],\n",
      "        [0.0701],\n",
      "        [0.0964],\n",
      "        [0.0967],\n",
      "        [0.0686],\n",
      "        [0.0676],\n",
      "        [0.0662],\n",
      "        [0.0658],\n",
      "        [0.1014],\n",
      "        [0.1018],\n",
      "        [0.1041],\n",
      "        [0.0611],\n",
      "        [0.0607],\n",
      "        [0.0600],\n",
      "        [0.0582],\n",
      "        [0.0580],\n",
      "        [0.1082],\n",
      "        [0.0552],\n",
      "        [0.0533],\n",
      "        [0.0524],\n",
      "        [0.1139],\n",
      "        [0.1154],\n",
      "        [0.1161],\n",
      "        [0.0485],\n",
      "        [0.0482],\n",
      "        [0.0462],\n",
      "        [0.1197],\n",
      "        [0.0460],\n",
      "        [0.1213],\n",
      "        [0.0434],\n",
      "        [0.0417],\n",
      "        [0.0412],\n",
      "        [0.0390],\n",
      "        [0.1280],\n",
      "        [0.1285],\n",
      "        [0.1297],\n",
      "        [0.1303],\n",
      "        [0.0354],\n",
      "        [0.0352],\n",
      "        [0.1324],\n",
      "        [0.0319],\n",
      "        [0.0316],\n",
      "        [0.1364],\n",
      "        [0.1408],\n",
      "        [0.1414],\n",
      "        [0.1464],\n",
      "        [0.0153],\n",
      "        [0.0135],\n",
      "        [0.0098],\n",
      "        [0.1589],\n",
      "        [0.1602],\n",
      "        [0.0054],\n",
      "        [0.1702],\n",
      "        [0.1708],\n",
      "        [0.1710],\n",
      "        [0.0084],\n",
      "        [0.0116],\n",
      "        [0.0143],\n",
      "        [0.0163],\n",
      "        [0.1833],\n",
      "        [0.1853],\n",
      "        [0.0195],\n",
      "        [0.0239],\n",
      "        [0.0274],\n",
      "        [0.0348],\n",
      "        [0.0387],\n",
      "        [0.0413],\n",
      "        [0.0424],\n",
      "        [0.0488],\n",
      "        [0.0489],\n",
      "        [0.0523],\n",
      "        [0.2183],\n",
      "        [0.0640],\n",
      "        [0.2413],\n",
      "        [0.0800],\n",
      "        [0.0881],\n",
      "        [0.2602],\n",
      "        [0.2603],\n",
      "        [0.1014],\n",
      "        [0.1034],\n",
      "        [0.1044],\n",
      "        [0.1108],\n",
      "        [0.1142],\n",
      "        [0.1185],\n",
      "        [0.2863],\n",
      "        [0.1232],\n",
      "        [0.1243],\n",
      "        [0.2921],\n",
      "        [0.2990],\n",
      "        [0.1349],\n",
      "        [0.1388],\n",
      "        [0.3080],\n",
      "        [0.3081],\n",
      "        [0.3172],\n",
      "        [0.1516],\n",
      "        [0.1548],\n",
      "        [0.3306],\n",
      "        [0.3343],\n",
      "        [0.1764],\n",
      "        [0.3446],\n",
      "        [0.3450],\n",
      "        [0.1842],\n",
      "        [0.3601],\n",
      "        [0.2109],\n",
      "        [0.2119],\n",
      "        [0.3799],\n",
      "        [0.2166],\n",
      "        [0.2168],\n",
      "        [0.2173],\n",
      "        [0.2174],\n",
      "        [0.3840],\n",
      "        [0.2210],\n",
      "        [0.2490],\n",
      "        [0.2501],\n",
      "        [0.2561],\n",
      "        [0.2612],\n",
      "        [0.2639],\n",
      "        [0.2675],\n",
      "        [0.2682],\n",
      "        [0.2726],\n",
      "        [0.2844],\n",
      "        [0.2886],\n",
      "        [0.3044],\n",
      "        [0.3079],\n",
      "        [0.3104],\n",
      "        [0.3131],\n",
      "        [0.3480],\n",
      "        [0.3552],\n",
      "        [0.3575],\n",
      "        [0.3845],\n",
      "        [0.3877],\n",
      "        [0.3913],\n",
      "        [0.3945],\n",
      "        [0.4126]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 43.98458909988403\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 143\n",
      "剩餘X 資料 torch.Size([17, 18])\n",
      "剩餘Y 資料 torch.Size([17, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18775546550750732, 5)\n",
      "The second_loss value of k: (0.19977661967277527, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.1892])\n",
      "目前模型的Data狀態 torch.Size([143, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.5753],\n",
      "        [0.6225]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0831],\n",
      "        [0.0837],\n",
      "        [0.0838],\n",
      "        [0.0844],\n",
      "        [0.0857],\n",
      "        [0.0800],\n",
      "        [0.0877],\n",
      "        [0.0739],\n",
      "        [0.0721],\n",
      "        [0.0701],\n",
      "        [0.0964],\n",
      "        [0.0967],\n",
      "        [0.0686],\n",
      "        [0.0676],\n",
      "        [0.0662],\n",
      "        [0.0658],\n",
      "        [0.1014],\n",
      "        [0.1018],\n",
      "        [0.1041],\n",
      "        [0.0611],\n",
      "        [0.0607],\n",
      "        [0.0600],\n",
      "        [0.0582],\n",
      "        [0.0580],\n",
      "        [0.1082],\n",
      "        [0.0552],\n",
      "        [0.0533],\n",
      "        [0.0524],\n",
      "        [0.1139],\n",
      "        [0.1154],\n",
      "        [0.1161],\n",
      "        [0.0485],\n",
      "        [0.0482],\n",
      "        [0.0462],\n",
      "        [0.1197],\n",
      "        [0.0460],\n",
      "        [0.1213],\n",
      "        [0.0434],\n",
      "        [0.0417],\n",
      "        [0.0412],\n",
      "        [0.0390],\n",
      "        [0.1280],\n",
      "        [0.1285],\n",
      "        [0.1297],\n",
      "        [0.1303],\n",
      "        [0.0354],\n",
      "        [0.0352],\n",
      "        [0.1324],\n",
      "        [0.0319],\n",
      "        [0.0316],\n",
      "        [0.1364],\n",
      "        [0.1408],\n",
      "        [0.1414],\n",
      "        [0.1464],\n",
      "        [0.0153],\n",
      "        [0.0135],\n",
      "        [0.0098],\n",
      "        [0.1589],\n",
      "        [0.1602],\n",
      "        [0.0054],\n",
      "        [0.1702],\n",
      "        [0.1708],\n",
      "        [0.1710],\n",
      "        [0.0084],\n",
      "        [0.0116],\n",
      "        [0.0143],\n",
      "        [0.0163],\n",
      "        [0.1833],\n",
      "        [0.1853],\n",
      "        [0.0195],\n",
      "        [0.0239],\n",
      "        [0.0274],\n",
      "        [0.0348],\n",
      "        [0.0387],\n",
      "        [0.0413],\n",
      "        [0.0424],\n",
      "        [0.0488],\n",
      "        [0.0489],\n",
      "        [0.0523],\n",
      "        [0.2183],\n",
      "        [0.0640],\n",
      "        [0.2413],\n",
      "        [0.0800],\n",
      "        [0.0881],\n",
      "        [0.2602],\n",
      "        [0.2603],\n",
      "        [0.1014],\n",
      "        [0.1034],\n",
      "        [0.1044],\n",
      "        [0.1108],\n",
      "        [0.1142],\n",
      "        [0.1185],\n",
      "        [0.2863],\n",
      "        [0.1232],\n",
      "        [0.1243],\n",
      "        [0.2921],\n",
      "        [0.2990],\n",
      "        [0.1349],\n",
      "        [0.1388],\n",
      "        [0.3080],\n",
      "        [0.3081],\n",
      "        [0.3172],\n",
      "        [0.1516],\n",
      "        [0.1548],\n",
      "        [0.3306],\n",
      "        [0.3343],\n",
      "        [0.1764],\n",
      "        [0.3446],\n",
      "        [0.3450],\n",
      "        [0.1842],\n",
      "        [0.3601],\n",
      "        [0.2109],\n",
      "        [0.2119],\n",
      "        [0.3799],\n",
      "        [0.2166],\n",
      "        [0.2168],\n",
      "        [0.2173],\n",
      "        [0.2174],\n",
      "        [0.3840],\n",
      "        [0.2210],\n",
      "        [0.2490],\n",
      "        [0.2501],\n",
      "        [0.2561],\n",
      "        [0.2612],\n",
      "        [0.2639],\n",
      "        [0.2675],\n",
      "        [0.2682],\n",
      "        [0.2726],\n",
      "        [0.2844],\n",
      "        [0.2886],\n",
      "        [0.3044],\n",
      "        [0.3079],\n",
      "        [0.3104],\n",
      "        [0.3131],\n",
      "        [0.3480],\n",
      "        [0.3552],\n",
      "        [0.3575],\n",
      "        [0.3845],\n",
      "        [0.3877],\n",
      "        [0.3913],\n",
      "        [0.3945],\n",
      "        [0.4126],\n",
      "        [0.4333]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 25\n",
      "Number of shrink: 0\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0846],\n",
      "        [0.0851],\n",
      "        [0.0853],\n",
      "        [0.0859],\n",
      "        [0.0872],\n",
      "        [0.0814],\n",
      "        [0.0892],\n",
      "        [0.0754],\n",
      "        [0.0736],\n",
      "        [0.0715],\n",
      "        [0.0979],\n",
      "        [0.0982],\n",
      "        [0.0701],\n",
      "        [0.0691],\n",
      "        [0.0677],\n",
      "        [0.0673],\n",
      "        [0.1029],\n",
      "        [0.1033],\n",
      "        [0.1056],\n",
      "        [0.0625],\n",
      "        [0.0621],\n",
      "        [0.0615],\n",
      "        [0.0596],\n",
      "        [0.0595],\n",
      "        [0.1097],\n",
      "        [0.0567],\n",
      "        [0.0548],\n",
      "        [0.0539],\n",
      "        [0.1154],\n",
      "        [0.1169],\n",
      "        [0.1176],\n",
      "        [0.0500],\n",
      "        [0.0497],\n",
      "        [0.0477],\n",
      "        [0.1212],\n",
      "        [0.0475],\n",
      "        [0.1227],\n",
      "        [0.0449],\n",
      "        [0.0432],\n",
      "        [0.0427],\n",
      "        [0.0405],\n",
      "        [0.1295],\n",
      "        [0.1300],\n",
      "        [0.1312],\n",
      "        [0.1318],\n",
      "        [0.0369],\n",
      "        [0.0366],\n",
      "        [0.1339],\n",
      "        [0.0334],\n",
      "        [0.0331],\n",
      "        [0.1379],\n",
      "        [0.1422],\n",
      "        [0.1429],\n",
      "        [0.1479],\n",
      "        [0.0168],\n",
      "        [0.0150],\n",
      "        [0.0113],\n",
      "        [0.1604],\n",
      "        [0.1616],\n",
      "        [0.0068],\n",
      "        [0.1717],\n",
      "        [0.1723],\n",
      "        [0.1724],\n",
      "        [0.0070],\n",
      "        [0.0101],\n",
      "        [0.0128],\n",
      "        [0.0149],\n",
      "        [0.1847],\n",
      "        [0.1867],\n",
      "        [0.0180],\n",
      "        [0.0224],\n",
      "        [0.0259],\n",
      "        [0.0333],\n",
      "        [0.0372],\n",
      "        [0.0399],\n",
      "        [0.0409],\n",
      "        [0.0473],\n",
      "        [0.0474],\n",
      "        [0.0508],\n",
      "        [0.2198],\n",
      "        [0.0625],\n",
      "        [0.2428],\n",
      "        [0.0785],\n",
      "        [0.0866],\n",
      "        [0.2616],\n",
      "        [0.2618],\n",
      "        [0.0999],\n",
      "        [0.1019],\n",
      "        [0.1029],\n",
      "        [0.1093],\n",
      "        [0.1127],\n",
      "        [0.1170],\n",
      "        [0.2878],\n",
      "        [0.1217],\n",
      "        [0.1228],\n",
      "        [0.2935],\n",
      "        [0.3005],\n",
      "        [0.1334],\n",
      "        [0.1374],\n",
      "        [0.3095],\n",
      "        [0.3096],\n",
      "        [0.3187],\n",
      "        [0.1502],\n",
      "        [0.1534],\n",
      "        [0.3321],\n",
      "        [0.3357],\n",
      "        [0.1749],\n",
      "        [0.3461],\n",
      "        [0.3464],\n",
      "        [0.1828],\n",
      "        [0.3616],\n",
      "        [0.2094],\n",
      "        [0.2104],\n",
      "        [0.3814],\n",
      "        [0.2151],\n",
      "        [0.2153],\n",
      "        [0.2159],\n",
      "        [0.2160],\n",
      "        [0.3855],\n",
      "        [0.2196],\n",
      "        [0.2475],\n",
      "        [0.2486],\n",
      "        [0.2546],\n",
      "        [0.2597],\n",
      "        [0.2624],\n",
      "        [0.2661],\n",
      "        [0.2668],\n",
      "        [0.2712],\n",
      "        [0.2829],\n",
      "        [0.2871],\n",
      "        [0.3029],\n",
      "        [0.3064],\n",
      "        [0.3089],\n",
      "        [0.3117],\n",
      "        [0.3465],\n",
      "        [0.3537],\n",
      "        [0.3560],\n",
      "        [0.3830],\n",
      "        [0.3862],\n",
      "        [0.3898],\n",
      "        [0.3931],\n",
      "        [0.4111],\n",
      "        [0.4296]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0859],\n",
      "        [0.0864],\n",
      "        [0.0865],\n",
      "        [0.0871],\n",
      "        [0.0884],\n",
      "        [0.0827],\n",
      "        [0.0904],\n",
      "        [0.0766],\n",
      "        [0.0748],\n",
      "        [0.0728],\n",
      "        [0.0991],\n",
      "        [0.0994],\n",
      "        [0.0713],\n",
      "        [0.0704],\n",
      "        [0.0689],\n",
      "        [0.0686],\n",
      "        [0.1041],\n",
      "        [0.1045],\n",
      "        [0.1068],\n",
      "        [0.0638],\n",
      "        [0.0634],\n",
      "        [0.0627],\n",
      "        [0.0609],\n",
      "        [0.0607],\n",
      "        [0.1109],\n",
      "        [0.0579],\n",
      "        [0.0560],\n",
      "        [0.0552],\n",
      "        [0.1167],\n",
      "        [0.1181],\n",
      "        [0.1188],\n",
      "        [0.0512],\n",
      "        [0.0510],\n",
      "        [0.0490],\n",
      "        [0.1224],\n",
      "        [0.0487],\n",
      "        [0.1240],\n",
      "        [0.0461],\n",
      "        [0.0445],\n",
      "        [0.0440],\n",
      "        [0.0417],\n",
      "        [0.1308],\n",
      "        [0.1312],\n",
      "        [0.1324],\n",
      "        [0.1330],\n",
      "        [0.0381],\n",
      "        [0.0379],\n",
      "        [0.1352],\n",
      "        [0.0346],\n",
      "        [0.0344],\n",
      "        [0.1391],\n",
      "        [0.1435],\n",
      "        [0.1441],\n",
      "        [0.1492],\n",
      "        [0.0181],\n",
      "        [0.0162],\n",
      "        [0.0125],\n",
      "        [0.1616],\n",
      "        [0.1629],\n",
      "        [0.0081],\n",
      "        [0.1729],\n",
      "        [0.1735],\n",
      "        [0.1737],\n",
      "        [0.0057],\n",
      "        [0.0088],\n",
      "        [0.0116],\n",
      "        [0.0136],\n",
      "        [0.1860],\n",
      "        [0.1880],\n",
      "        [0.0168],\n",
      "        [0.0212],\n",
      "        [0.0247],\n",
      "        [0.0320],\n",
      "        [0.0360],\n",
      "        [0.0386],\n",
      "        [0.0397],\n",
      "        [0.0461],\n",
      "        [0.0462],\n",
      "        [0.0496],\n",
      "        [0.2210],\n",
      "        [0.0613],\n",
      "        [0.2440],\n",
      "        [0.0773],\n",
      "        [0.0854],\n",
      "        [0.2629],\n",
      "        [0.2630],\n",
      "        [0.0987],\n",
      "        [0.1007],\n",
      "        [0.1017],\n",
      "        [0.1081],\n",
      "        [0.1115],\n",
      "        [0.1157],\n",
      "        [0.2890],\n",
      "        [0.1204],\n",
      "        [0.1216],\n",
      "        [0.2948],\n",
      "        [0.3017],\n",
      "        [0.1322],\n",
      "        [0.1361],\n",
      "        [0.3107],\n",
      "        [0.3109],\n",
      "        [0.3200],\n",
      "        [0.1489],\n",
      "        [0.1521],\n",
      "        [0.3333],\n",
      "        [0.3370],\n",
      "        [0.1737],\n",
      "        [0.3474],\n",
      "        [0.3477],\n",
      "        [0.1815],\n",
      "        [0.3628],\n",
      "        [0.2081],\n",
      "        [0.2092],\n",
      "        [0.3826],\n",
      "        [0.2139],\n",
      "        [0.2141],\n",
      "        [0.2146],\n",
      "        [0.2147],\n",
      "        [0.3868],\n",
      "        [0.2183],\n",
      "        [0.2463],\n",
      "        [0.2474],\n",
      "        [0.2534],\n",
      "        [0.2584],\n",
      "        [0.2612],\n",
      "        [0.2648],\n",
      "        [0.2655],\n",
      "        [0.2699],\n",
      "        [0.2817],\n",
      "        [0.2859],\n",
      "        [0.3016],\n",
      "        [0.3052],\n",
      "        [0.3077],\n",
      "        [0.3104],\n",
      "        [0.3452],\n",
      "        [0.3525],\n",
      "        [0.3548],\n",
      "        [0.3817],\n",
      "        [0.3850],\n",
      "        [0.3886],\n",
      "        [0.3918],\n",
      "        [0.4099],\n",
      "        [0.3835]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 44.28202271461487\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 144\n",
      "剩餘X 資料 torch.Size([16, 18])\n",
      "剩餘Y 資料 torch.Size([16, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15739431977272034, 0)\n",
      "The second_loss value of k: (0.17746642231941223, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.1917])\n",
      "目前模型的Data狀態 torch.Size([144, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5726],\n",
      "        [0.5884]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0859],\n",
      "        [0.0864],\n",
      "        [0.0865],\n",
      "        [0.0871],\n",
      "        [0.0884],\n",
      "        [0.0827],\n",
      "        [0.0904],\n",
      "        [0.0766],\n",
      "        [0.0748],\n",
      "        [0.0728],\n",
      "        [0.0991],\n",
      "        [0.0994],\n",
      "        [0.0713],\n",
      "        [0.0704],\n",
      "        [0.0689],\n",
      "        [0.0686],\n",
      "        [0.1041],\n",
      "        [0.1045],\n",
      "        [0.1068],\n",
      "        [0.0638],\n",
      "        [0.0634],\n",
      "        [0.0627],\n",
      "        [0.0609],\n",
      "        [0.0607],\n",
      "        [0.1109],\n",
      "        [0.0579],\n",
      "        [0.0560],\n",
      "        [0.0552],\n",
      "        [0.1167],\n",
      "        [0.1181],\n",
      "        [0.1188],\n",
      "        [0.0512],\n",
      "        [0.0510],\n",
      "        [0.0490],\n",
      "        [0.1224],\n",
      "        [0.0487],\n",
      "        [0.1240],\n",
      "        [0.0461],\n",
      "        [0.0445],\n",
      "        [0.0440],\n",
      "        [0.0417],\n",
      "        [0.1308],\n",
      "        [0.1312],\n",
      "        [0.1324],\n",
      "        [0.1330],\n",
      "        [0.0381],\n",
      "        [0.0379],\n",
      "        [0.1352],\n",
      "        [0.0346],\n",
      "        [0.0344],\n",
      "        [0.1391],\n",
      "        [0.1435],\n",
      "        [0.1441],\n",
      "        [0.1492],\n",
      "        [0.0181],\n",
      "        [0.0162],\n",
      "        [0.0125],\n",
      "        [0.1616],\n",
      "        [0.1629],\n",
      "        [0.0081],\n",
      "        [0.1729],\n",
      "        [0.1735],\n",
      "        [0.1737],\n",
      "        [0.0057],\n",
      "        [0.0088],\n",
      "        [0.0116],\n",
      "        [0.0136],\n",
      "        [0.1860],\n",
      "        [0.1880],\n",
      "        [0.0168],\n",
      "        [0.0212],\n",
      "        [0.0247],\n",
      "        [0.0320],\n",
      "        [0.0360],\n",
      "        [0.0386],\n",
      "        [0.0397],\n",
      "        [0.0461],\n",
      "        [0.0462],\n",
      "        [0.0496],\n",
      "        [0.2210],\n",
      "        [0.0613],\n",
      "        [0.2440],\n",
      "        [0.0773],\n",
      "        [0.0854],\n",
      "        [0.2629],\n",
      "        [0.2630],\n",
      "        [0.0987],\n",
      "        [0.1007],\n",
      "        [0.1017],\n",
      "        [0.1081],\n",
      "        [0.1115],\n",
      "        [0.1157],\n",
      "        [0.2890],\n",
      "        [0.1204],\n",
      "        [0.1216],\n",
      "        [0.2948],\n",
      "        [0.3017],\n",
      "        [0.1322],\n",
      "        [0.1361],\n",
      "        [0.3107],\n",
      "        [0.3109],\n",
      "        [0.3200],\n",
      "        [0.1489],\n",
      "        [0.1521],\n",
      "        [0.3333],\n",
      "        [0.3370],\n",
      "        [0.1737],\n",
      "        [0.3474],\n",
      "        [0.3477],\n",
      "        [0.1815],\n",
      "        [0.3628],\n",
      "        [0.2081],\n",
      "        [0.2092],\n",
      "        [0.3826],\n",
      "        [0.2139],\n",
      "        [0.2141],\n",
      "        [0.2146],\n",
      "        [0.2147],\n",
      "        [0.3868],\n",
      "        [0.2183],\n",
      "        [0.2463],\n",
      "        [0.2474],\n",
      "        [0.2534],\n",
      "        [0.2584],\n",
      "        [0.2612],\n",
      "        [0.2648],\n",
      "        [0.2655],\n",
      "        [0.2699],\n",
      "        [0.2817],\n",
      "        [0.2859],\n",
      "        [0.3016],\n",
      "        [0.3052],\n",
      "        [0.3077],\n",
      "        [0.3104],\n",
      "        [0.3452],\n",
      "        [0.3525],\n",
      "        [0.3548],\n",
      "        [0.3817],\n",
      "        [0.3850],\n",
      "        [0.3886],\n",
      "        [0.3918],\n",
      "        [0.4099],\n",
      "        [0.3835],\n",
      "        [0.3967]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 79\n",
      "Number of shrink: 21\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0885],\n",
      "        [0.0890],\n",
      "        [0.0892],\n",
      "        [0.0897],\n",
      "        [0.0911],\n",
      "        [0.0853],\n",
      "        [0.0930],\n",
      "        [0.0792],\n",
      "        [0.0774],\n",
      "        [0.0754],\n",
      "        [0.1017],\n",
      "        [0.1020],\n",
      "        [0.0740],\n",
      "        [0.0730],\n",
      "        [0.0716],\n",
      "        [0.0712],\n",
      "        [0.1067],\n",
      "        [0.1072],\n",
      "        [0.1094],\n",
      "        [0.0664],\n",
      "        [0.0660],\n",
      "        [0.0653],\n",
      "        [0.0635],\n",
      "        [0.0633],\n",
      "        [0.1136],\n",
      "        [0.0606],\n",
      "        [0.0586],\n",
      "        [0.0578],\n",
      "        [0.1193],\n",
      "        [0.1207],\n",
      "        [0.1214],\n",
      "        [0.0539],\n",
      "        [0.0536],\n",
      "        [0.0516],\n",
      "        [0.1250],\n",
      "        [0.0514],\n",
      "        [0.1266],\n",
      "        [0.0487],\n",
      "        [0.0471],\n",
      "        [0.0466],\n",
      "        [0.0443],\n",
      "        [0.1334],\n",
      "        [0.1339],\n",
      "        [0.1350],\n",
      "        [0.1357],\n",
      "        [0.0408],\n",
      "        [0.0405],\n",
      "        [0.1378],\n",
      "        [0.0373],\n",
      "        [0.0370],\n",
      "        [0.1418],\n",
      "        [0.1461],\n",
      "        [0.1468],\n",
      "        [0.1518],\n",
      "        [0.0207],\n",
      "        [0.0188],\n",
      "        [0.0151],\n",
      "        [0.1642],\n",
      "        [0.1655],\n",
      "        [0.0107],\n",
      "        [0.1755],\n",
      "        [0.1762],\n",
      "        [0.1763],\n",
      "        [0.0031],\n",
      "        [0.0062],\n",
      "        [0.0089],\n",
      "        [0.0110],\n",
      "        [0.1886],\n",
      "        [0.1906],\n",
      "        [0.0142],\n",
      "        [0.0186],\n",
      "        [0.0220],\n",
      "        [0.0294],\n",
      "        [0.0334],\n",
      "        [0.0360],\n",
      "        [0.0370],\n",
      "        [0.0435],\n",
      "        [0.0435],\n",
      "        [0.0470],\n",
      "        [0.2236],\n",
      "        [0.0586],\n",
      "        [0.2467],\n",
      "        [0.0746],\n",
      "        [0.0828],\n",
      "        [0.2655],\n",
      "        [0.2657],\n",
      "        [0.0960],\n",
      "        [0.0981],\n",
      "        [0.0991],\n",
      "        [0.1054],\n",
      "        [0.1088],\n",
      "        [0.1131],\n",
      "        [0.2917],\n",
      "        [0.1178],\n",
      "        [0.1189],\n",
      "        [0.2974],\n",
      "        [0.3044],\n",
      "        [0.1296],\n",
      "        [0.1335],\n",
      "        [0.3133],\n",
      "        [0.3135],\n",
      "        [0.3226],\n",
      "        [0.1463],\n",
      "        [0.1495],\n",
      "        [0.3359],\n",
      "        [0.3396],\n",
      "        [0.1710],\n",
      "        [0.3500],\n",
      "        [0.3503],\n",
      "        [0.1789],\n",
      "        [0.3655],\n",
      "        [0.2055],\n",
      "        [0.2065],\n",
      "        [0.3853],\n",
      "        [0.2113],\n",
      "        [0.2114],\n",
      "        [0.2120],\n",
      "        [0.2121],\n",
      "        [0.3894],\n",
      "        [0.2157],\n",
      "        [0.2436],\n",
      "        [0.2447],\n",
      "        [0.2508],\n",
      "        [0.2558],\n",
      "        [0.2585],\n",
      "        [0.2622],\n",
      "        [0.2629],\n",
      "        [0.2673],\n",
      "        [0.2790],\n",
      "        [0.2832],\n",
      "        [0.2990],\n",
      "        [0.3025],\n",
      "        [0.3050],\n",
      "        [0.3078],\n",
      "        [0.3426],\n",
      "        [0.3499],\n",
      "        [0.3522],\n",
      "        [0.3791],\n",
      "        [0.3824],\n",
      "        [0.3859],\n",
      "        [0.3892],\n",
      "        [0.4072],\n",
      "        [0.3808],\n",
      "        [0.3783]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 44.51300835609436\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 145\n",
      "剩餘X 資料 torch.Size([15, 18])\n",
      "剩餘Y 資料 torch.Size([15, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16129079461097717, 2)\n",
      "The second_loss value of k: (0.16150951385498047, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.1975])\n",
      "目前模型的Data狀態 torch.Size([145, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5700],\n",
      "        [0.5991]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0885],\n",
      "        [0.0890],\n",
      "        [0.0892],\n",
      "        [0.0897],\n",
      "        [0.0911],\n",
      "        [0.0853],\n",
      "        [0.0930],\n",
      "        [0.0792],\n",
      "        [0.0774],\n",
      "        [0.0754],\n",
      "        [0.1017],\n",
      "        [0.1020],\n",
      "        [0.0740],\n",
      "        [0.0730],\n",
      "        [0.0716],\n",
      "        [0.0712],\n",
      "        [0.1067],\n",
      "        [0.1072],\n",
      "        [0.1094],\n",
      "        [0.0664],\n",
      "        [0.0660],\n",
      "        [0.0653],\n",
      "        [0.0635],\n",
      "        [0.0633],\n",
      "        [0.1136],\n",
      "        [0.0606],\n",
      "        [0.0586],\n",
      "        [0.0578],\n",
      "        [0.1193],\n",
      "        [0.1207],\n",
      "        [0.1214],\n",
      "        [0.0539],\n",
      "        [0.0536],\n",
      "        [0.0516],\n",
      "        [0.1250],\n",
      "        [0.0514],\n",
      "        [0.1266],\n",
      "        [0.0487],\n",
      "        [0.0471],\n",
      "        [0.0466],\n",
      "        [0.0443],\n",
      "        [0.1334],\n",
      "        [0.1339],\n",
      "        [0.1350],\n",
      "        [0.1357],\n",
      "        [0.0408],\n",
      "        [0.0405],\n",
      "        [0.1378],\n",
      "        [0.0373],\n",
      "        [0.0370],\n",
      "        [0.1418],\n",
      "        [0.1461],\n",
      "        [0.1468],\n",
      "        [0.1518],\n",
      "        [0.0207],\n",
      "        [0.0188],\n",
      "        [0.0151],\n",
      "        [0.1642],\n",
      "        [0.1655],\n",
      "        [0.0107],\n",
      "        [0.1755],\n",
      "        [0.1762],\n",
      "        [0.1763],\n",
      "        [0.0031],\n",
      "        [0.0062],\n",
      "        [0.0089],\n",
      "        [0.0110],\n",
      "        [0.1886],\n",
      "        [0.1906],\n",
      "        [0.0142],\n",
      "        [0.0186],\n",
      "        [0.0220],\n",
      "        [0.0294],\n",
      "        [0.0334],\n",
      "        [0.0360],\n",
      "        [0.0370],\n",
      "        [0.0435],\n",
      "        [0.0435],\n",
      "        [0.0470],\n",
      "        [0.2236],\n",
      "        [0.0586],\n",
      "        [0.2467],\n",
      "        [0.0746],\n",
      "        [0.0828],\n",
      "        [0.2655],\n",
      "        [0.2657],\n",
      "        [0.0960],\n",
      "        [0.0981],\n",
      "        [0.0991],\n",
      "        [0.1054],\n",
      "        [0.1088],\n",
      "        [0.1131],\n",
      "        [0.2917],\n",
      "        [0.1178],\n",
      "        [0.1189],\n",
      "        [0.2974],\n",
      "        [0.3044],\n",
      "        [0.1296],\n",
      "        [0.1335],\n",
      "        [0.3133],\n",
      "        [0.3135],\n",
      "        [0.3226],\n",
      "        [0.1463],\n",
      "        [0.1495],\n",
      "        [0.3359],\n",
      "        [0.3396],\n",
      "        [0.1710],\n",
      "        [0.3500],\n",
      "        [0.3503],\n",
      "        [0.1789],\n",
      "        [0.3655],\n",
      "        [0.2055],\n",
      "        [0.2065],\n",
      "        [0.3853],\n",
      "        [0.2113],\n",
      "        [0.2114],\n",
      "        [0.2120],\n",
      "        [0.2121],\n",
      "        [0.3894],\n",
      "        [0.2157],\n",
      "        [0.2436],\n",
      "        [0.2447],\n",
      "        [0.2508],\n",
      "        [0.2558],\n",
      "        [0.2585],\n",
      "        [0.2622],\n",
      "        [0.2629],\n",
      "        [0.2673],\n",
      "        [0.2790],\n",
      "        [0.2832],\n",
      "        [0.2990],\n",
      "        [0.3025],\n",
      "        [0.3050],\n",
      "        [0.3078],\n",
      "        [0.3426],\n",
      "        [0.3499],\n",
      "        [0.3522],\n",
      "        [0.3791],\n",
      "        [0.3824],\n",
      "        [0.3859],\n",
      "        [0.3892],\n",
      "        [0.4072],\n",
      "        [0.3808],\n",
      "        [0.3783],\n",
      "        [0.4016]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 65\n",
      "Number of shrink: 35\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0911],\n",
      "        [0.0916],\n",
      "        [0.0918],\n",
      "        [0.0923],\n",
      "        [0.0937],\n",
      "        [0.0879],\n",
      "        [0.0956],\n",
      "        [0.0818],\n",
      "        [0.0800],\n",
      "        [0.0780],\n",
      "        [0.1043],\n",
      "        [0.1046],\n",
      "        [0.0766],\n",
      "        [0.0756],\n",
      "        [0.0742],\n",
      "        [0.0738],\n",
      "        [0.1093],\n",
      "        [0.1098],\n",
      "        [0.1120],\n",
      "        [0.0690],\n",
      "        [0.0686],\n",
      "        [0.0679],\n",
      "        [0.0661],\n",
      "        [0.0659],\n",
      "        [0.1162],\n",
      "        [0.0632],\n",
      "        [0.0612],\n",
      "        [0.0604],\n",
      "        [0.1219],\n",
      "        [0.1233],\n",
      "        [0.1240],\n",
      "        [0.0565],\n",
      "        [0.0562],\n",
      "        [0.0542],\n",
      "        [0.1276],\n",
      "        [0.0540],\n",
      "        [0.1292],\n",
      "        [0.0513],\n",
      "        [0.0497],\n",
      "        [0.0492],\n",
      "        [0.0469],\n",
      "        [0.1360],\n",
      "        [0.1364],\n",
      "        [0.1376],\n",
      "        [0.1383],\n",
      "        [0.0434],\n",
      "        [0.0431],\n",
      "        [0.1404],\n",
      "        [0.0399],\n",
      "        [0.0396],\n",
      "        [0.1444],\n",
      "        [0.1487],\n",
      "        [0.1494],\n",
      "        [0.1544],\n",
      "        [0.0233],\n",
      "        [0.0214],\n",
      "        [0.0177],\n",
      "        [0.1668],\n",
      "        [0.1681],\n",
      "        [0.0133],\n",
      "        [0.1781],\n",
      "        [0.1788],\n",
      "        [0.1789],\n",
      "        [0.0005],\n",
      "        [0.0036],\n",
      "        [0.0063],\n",
      "        [0.0084],\n",
      "        [0.1912],\n",
      "        [0.1932],\n",
      "        [0.0116],\n",
      "        [0.0160],\n",
      "        [0.0194],\n",
      "        [0.0268],\n",
      "        [0.0308],\n",
      "        [0.0334],\n",
      "        [0.0344],\n",
      "        [0.0409],\n",
      "        [0.0409],\n",
      "        [0.0444],\n",
      "        [0.2262],\n",
      "        [0.0560],\n",
      "        [0.2493],\n",
      "        [0.0721],\n",
      "        [0.0802],\n",
      "        [0.2681],\n",
      "        [0.2683],\n",
      "        [0.0934],\n",
      "        [0.0955],\n",
      "        [0.0965],\n",
      "        [0.1028],\n",
      "        [0.1062],\n",
      "        [0.1105],\n",
      "        [0.2943],\n",
      "        [0.1152],\n",
      "        [0.1164],\n",
      "        [0.3000],\n",
      "        [0.3070],\n",
      "        [0.1270],\n",
      "        [0.1309],\n",
      "        [0.3159],\n",
      "        [0.3161],\n",
      "        [0.3252],\n",
      "        [0.1437],\n",
      "        [0.1469],\n",
      "        [0.3385],\n",
      "        [0.3422],\n",
      "        [0.1684],\n",
      "        [0.3526],\n",
      "        [0.3529],\n",
      "        [0.1763],\n",
      "        [0.3681],\n",
      "        [0.2029],\n",
      "        [0.2039],\n",
      "        [0.3879],\n",
      "        [0.2087],\n",
      "        [0.2088],\n",
      "        [0.2094],\n",
      "        [0.2095],\n",
      "        [0.3920],\n",
      "        [0.2131],\n",
      "        [0.2410],\n",
      "        [0.2421],\n",
      "        [0.2482],\n",
      "        [0.2532],\n",
      "        [0.2560],\n",
      "        [0.2596],\n",
      "        [0.2603],\n",
      "        [0.2647],\n",
      "        [0.2764],\n",
      "        [0.2806],\n",
      "        [0.2964],\n",
      "        [0.2999],\n",
      "        [0.3024],\n",
      "        [0.3052],\n",
      "        [0.3400],\n",
      "        [0.3473],\n",
      "        [0.3496],\n",
      "        [0.3765],\n",
      "        [0.3798],\n",
      "        [0.3833],\n",
      "        [0.3866],\n",
      "        [0.4046],\n",
      "        [0.3782],\n",
      "        [0.3757],\n",
      "        [0.3699]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 44.746724367141724\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 146\n",
      "剩餘X 資料 torch.Size([14, 18])\n",
      "剩餘Y 資料 torch.Size([14, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.13307245075702667, 0)\n",
      "The second_loss value of k: (0.14085131883621216, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.2336])\n",
      "目前模型的Data狀態 torch.Size([146, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5674],\n",
      "        [0.5984]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0911],\n",
      "        [0.0916],\n",
      "        [0.0918],\n",
      "        [0.0923],\n",
      "        [0.0937],\n",
      "        [0.0879],\n",
      "        [0.0956],\n",
      "        [0.0818],\n",
      "        [0.0800],\n",
      "        [0.0780],\n",
      "        [0.1043],\n",
      "        [0.1046],\n",
      "        [0.0766],\n",
      "        [0.0756],\n",
      "        [0.0742],\n",
      "        [0.0738],\n",
      "        [0.1093],\n",
      "        [0.1098],\n",
      "        [0.1120],\n",
      "        [0.0690],\n",
      "        [0.0686],\n",
      "        [0.0679],\n",
      "        [0.0661],\n",
      "        [0.0659],\n",
      "        [0.1162],\n",
      "        [0.0632],\n",
      "        [0.0612],\n",
      "        [0.0604],\n",
      "        [0.1219],\n",
      "        [0.1233],\n",
      "        [0.1240],\n",
      "        [0.0565],\n",
      "        [0.0562],\n",
      "        [0.0542],\n",
      "        [0.1276],\n",
      "        [0.0540],\n",
      "        [0.1292],\n",
      "        [0.0513],\n",
      "        [0.0497],\n",
      "        [0.0492],\n",
      "        [0.0469],\n",
      "        [0.1360],\n",
      "        [0.1364],\n",
      "        [0.1376],\n",
      "        [0.1383],\n",
      "        [0.0434],\n",
      "        [0.0431],\n",
      "        [0.1404],\n",
      "        [0.0399],\n",
      "        [0.0396],\n",
      "        [0.1444],\n",
      "        [0.1487],\n",
      "        [0.1494],\n",
      "        [0.1544],\n",
      "        [0.0233],\n",
      "        [0.0214],\n",
      "        [0.0177],\n",
      "        [0.1668],\n",
      "        [0.1681],\n",
      "        [0.0133],\n",
      "        [0.1781],\n",
      "        [0.1788],\n",
      "        [0.1789],\n",
      "        [0.0005],\n",
      "        [0.0036],\n",
      "        [0.0063],\n",
      "        [0.0084],\n",
      "        [0.1912],\n",
      "        [0.1932],\n",
      "        [0.0116],\n",
      "        [0.0160],\n",
      "        [0.0194],\n",
      "        [0.0268],\n",
      "        [0.0308],\n",
      "        [0.0334],\n",
      "        [0.0344],\n",
      "        [0.0409],\n",
      "        [0.0409],\n",
      "        [0.0444],\n",
      "        [0.2262],\n",
      "        [0.0560],\n",
      "        [0.2493],\n",
      "        [0.0721],\n",
      "        [0.0802],\n",
      "        [0.2681],\n",
      "        [0.2683],\n",
      "        [0.0934],\n",
      "        [0.0955],\n",
      "        [0.0965],\n",
      "        [0.1028],\n",
      "        [0.1062],\n",
      "        [0.1105],\n",
      "        [0.2943],\n",
      "        [0.1152],\n",
      "        [0.1164],\n",
      "        [0.3000],\n",
      "        [0.3070],\n",
      "        [0.1270],\n",
      "        [0.1309],\n",
      "        [0.3159],\n",
      "        [0.3161],\n",
      "        [0.3252],\n",
      "        [0.1437],\n",
      "        [0.1469],\n",
      "        [0.3385],\n",
      "        [0.3422],\n",
      "        [0.1684],\n",
      "        [0.3526],\n",
      "        [0.3529],\n",
      "        [0.1763],\n",
      "        [0.3681],\n",
      "        [0.2029],\n",
      "        [0.2039],\n",
      "        [0.3879],\n",
      "        [0.2087],\n",
      "        [0.2088],\n",
      "        [0.2094],\n",
      "        [0.2095],\n",
      "        [0.3920],\n",
      "        [0.2131],\n",
      "        [0.2410],\n",
      "        [0.2421],\n",
      "        [0.2482],\n",
      "        [0.2532],\n",
      "        [0.2560],\n",
      "        [0.2596],\n",
      "        [0.2603],\n",
      "        [0.2647],\n",
      "        [0.2764],\n",
      "        [0.2806],\n",
      "        [0.2964],\n",
      "        [0.2999],\n",
      "        [0.3024],\n",
      "        [0.3052],\n",
      "        [0.3400],\n",
      "        [0.3473],\n",
      "        [0.3496],\n",
      "        [0.3765],\n",
      "        [0.3798],\n",
      "        [0.3833],\n",
      "        [0.3866],\n",
      "        [0.4046],\n",
      "        [0.3782],\n",
      "        [0.3757],\n",
      "        [0.3699],\n",
      "        [0.3648]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0934],\n",
      "        [0.0939],\n",
      "        [0.0941],\n",
      "        [0.0946],\n",
      "        [0.0960],\n",
      "        [0.0902],\n",
      "        [0.0979],\n",
      "        [0.0841],\n",
      "        [0.0823],\n",
      "        [0.0803],\n",
      "        [0.1066],\n",
      "        [0.1069],\n",
      "        [0.0789],\n",
      "        [0.0779],\n",
      "        [0.0765],\n",
      "        [0.0761],\n",
      "        [0.1116],\n",
      "        [0.1121],\n",
      "        [0.1143],\n",
      "        [0.0713],\n",
      "        [0.0709],\n",
      "        [0.0702],\n",
      "        [0.0684],\n",
      "        [0.0682],\n",
      "        [0.1185],\n",
      "        [0.0655],\n",
      "        [0.0635],\n",
      "        [0.0627],\n",
      "        [0.1242],\n",
      "        [0.1256],\n",
      "        [0.1263],\n",
      "        [0.0588],\n",
      "        [0.0585],\n",
      "        [0.0565],\n",
      "        [0.1299],\n",
      "        [0.0563],\n",
      "        [0.1315],\n",
      "        [0.0536],\n",
      "        [0.0520],\n",
      "        [0.0515],\n",
      "        [0.0492],\n",
      "        [0.1383],\n",
      "        [0.1387],\n",
      "        [0.1399],\n",
      "        [0.1405],\n",
      "        [0.0457],\n",
      "        [0.0454],\n",
      "        [0.1427],\n",
      "        [0.0422],\n",
      "        [0.0419],\n",
      "        [0.1467],\n",
      "        [0.1510],\n",
      "        [0.1517],\n",
      "        [0.1567],\n",
      "        [0.0256],\n",
      "        [0.0237],\n",
      "        [0.0200],\n",
      "        [0.1691],\n",
      "        [0.1704],\n",
      "        [0.0156],\n",
      "        [0.1804],\n",
      "        [0.1810],\n",
      "        [0.1812],\n",
      "        [0.0018],\n",
      "        [0.0013],\n",
      "        [0.0040],\n",
      "        [0.0061],\n",
      "        [0.1935],\n",
      "        [0.1955],\n",
      "        [0.0093],\n",
      "        [0.0137],\n",
      "        [0.0171],\n",
      "        [0.0245],\n",
      "        [0.0285],\n",
      "        [0.0311],\n",
      "        [0.0322],\n",
      "        [0.0386],\n",
      "        [0.0387],\n",
      "        [0.0421],\n",
      "        [0.2285],\n",
      "        [0.0537],\n",
      "        [0.2516],\n",
      "        [0.0698],\n",
      "        [0.0779],\n",
      "        [0.2704],\n",
      "        [0.2706],\n",
      "        [0.0911],\n",
      "        [0.0932],\n",
      "        [0.0942],\n",
      "        [0.1005],\n",
      "        [0.1039],\n",
      "        [0.1082],\n",
      "        [0.2966],\n",
      "        [0.1129],\n",
      "        [0.1141],\n",
      "        [0.3023],\n",
      "        [0.3093],\n",
      "        [0.1247],\n",
      "        [0.1286],\n",
      "        [0.3182],\n",
      "        [0.3184],\n",
      "        [0.3275],\n",
      "        [0.1414],\n",
      "        [0.1446],\n",
      "        [0.3408],\n",
      "        [0.3445],\n",
      "        [0.1661],\n",
      "        [0.3549],\n",
      "        [0.3552],\n",
      "        [0.1740],\n",
      "        [0.3704],\n",
      "        [0.2006],\n",
      "        [0.2016],\n",
      "        [0.3902],\n",
      "        [0.2064],\n",
      "        [0.2065],\n",
      "        [0.2071],\n",
      "        [0.2072],\n",
      "        [0.3943],\n",
      "        [0.2108],\n",
      "        [0.2387],\n",
      "        [0.2398],\n",
      "        [0.2459],\n",
      "        [0.2509],\n",
      "        [0.2537],\n",
      "        [0.2573],\n",
      "        [0.2580],\n",
      "        [0.2624],\n",
      "        [0.2741],\n",
      "        [0.2783],\n",
      "        [0.2941],\n",
      "        [0.2976],\n",
      "        [0.3001],\n",
      "        [0.3029],\n",
      "        [0.3377],\n",
      "        [0.3450],\n",
      "        [0.3473],\n",
      "        [0.3742],\n",
      "        [0.3775],\n",
      "        [0.3810],\n",
      "        [0.3843],\n",
      "        [0.4023],\n",
      "        [0.3759],\n",
      "        [0.3734],\n",
      "        [0.3676],\n",
      "        [0.3315]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 44.98552322387695\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 147\n",
      "剩餘X 資料 torch.Size([13, 18])\n",
      "剩餘Y 資料 torch.Size([13, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12870343029499054, 0)\n",
      "The second_loss value of k: (0.13913224637508392, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.2546])\n",
      "目前模型的Data狀態 torch.Size([147, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.5651],\n",
      "        [0.6133]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0934],\n",
      "        [0.0939],\n",
      "        [0.0941],\n",
      "        [0.0946],\n",
      "        [0.0960],\n",
      "        [0.0902],\n",
      "        [0.0979],\n",
      "        [0.0841],\n",
      "        [0.0823],\n",
      "        [0.0803],\n",
      "        [0.1066],\n",
      "        [0.1069],\n",
      "        [0.0789],\n",
      "        [0.0779],\n",
      "        [0.0765],\n",
      "        [0.0761],\n",
      "        [0.1116],\n",
      "        [0.1121],\n",
      "        [0.1143],\n",
      "        [0.0713],\n",
      "        [0.0709],\n",
      "        [0.0702],\n",
      "        [0.0684],\n",
      "        [0.0682],\n",
      "        [0.1185],\n",
      "        [0.0655],\n",
      "        [0.0635],\n",
      "        [0.0627],\n",
      "        [0.1242],\n",
      "        [0.1256],\n",
      "        [0.1263],\n",
      "        [0.0588],\n",
      "        [0.0585],\n",
      "        [0.0565],\n",
      "        [0.1299],\n",
      "        [0.0563],\n",
      "        [0.1315],\n",
      "        [0.0536],\n",
      "        [0.0520],\n",
      "        [0.0515],\n",
      "        [0.0492],\n",
      "        [0.1383],\n",
      "        [0.1387],\n",
      "        [0.1399],\n",
      "        [0.1405],\n",
      "        [0.0457],\n",
      "        [0.0454],\n",
      "        [0.1427],\n",
      "        [0.0422],\n",
      "        [0.0419],\n",
      "        [0.1467],\n",
      "        [0.1510],\n",
      "        [0.1517],\n",
      "        [0.1567],\n",
      "        [0.0256],\n",
      "        [0.0237],\n",
      "        [0.0200],\n",
      "        [0.1691],\n",
      "        [0.1704],\n",
      "        [0.0156],\n",
      "        [0.1804],\n",
      "        [0.1810],\n",
      "        [0.1812],\n",
      "        [0.0018],\n",
      "        [0.0013],\n",
      "        [0.0040],\n",
      "        [0.0061],\n",
      "        [0.1935],\n",
      "        [0.1955],\n",
      "        [0.0093],\n",
      "        [0.0137],\n",
      "        [0.0171],\n",
      "        [0.0245],\n",
      "        [0.0285],\n",
      "        [0.0311],\n",
      "        [0.0322],\n",
      "        [0.0386],\n",
      "        [0.0387],\n",
      "        [0.0421],\n",
      "        [0.2285],\n",
      "        [0.0537],\n",
      "        [0.2516],\n",
      "        [0.0698],\n",
      "        [0.0779],\n",
      "        [0.2704],\n",
      "        [0.2706],\n",
      "        [0.0911],\n",
      "        [0.0932],\n",
      "        [0.0942],\n",
      "        [0.1005],\n",
      "        [0.1039],\n",
      "        [0.1082],\n",
      "        [0.2966],\n",
      "        [0.1129],\n",
      "        [0.1141],\n",
      "        [0.3023],\n",
      "        [0.3093],\n",
      "        [0.1247],\n",
      "        [0.1286],\n",
      "        [0.3182],\n",
      "        [0.3184],\n",
      "        [0.3275],\n",
      "        [0.1414],\n",
      "        [0.1446],\n",
      "        [0.3408],\n",
      "        [0.3445],\n",
      "        [0.1661],\n",
      "        [0.3549],\n",
      "        [0.3552],\n",
      "        [0.1740],\n",
      "        [0.3704],\n",
      "        [0.2006],\n",
      "        [0.2016],\n",
      "        [0.3902],\n",
      "        [0.2064],\n",
      "        [0.2065],\n",
      "        [0.2071],\n",
      "        [0.2072],\n",
      "        [0.3943],\n",
      "        [0.2108],\n",
      "        [0.2387],\n",
      "        [0.2398],\n",
      "        [0.2459],\n",
      "        [0.2509],\n",
      "        [0.2537],\n",
      "        [0.2573],\n",
      "        [0.2580],\n",
      "        [0.2624],\n",
      "        [0.2741],\n",
      "        [0.2783],\n",
      "        [0.2941],\n",
      "        [0.2976],\n",
      "        [0.3001],\n",
      "        [0.3029],\n",
      "        [0.3377],\n",
      "        [0.3450],\n",
      "        [0.3473],\n",
      "        [0.3742],\n",
      "        [0.3775],\n",
      "        [0.3810],\n",
      "        [0.3843],\n",
      "        [0.4023],\n",
      "        [0.3759],\n",
      "        [0.3734],\n",
      "        [0.3676],\n",
      "        [0.3315],\n",
      "        [0.3588]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 65\n",
      "Number of shrink: 35\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0955],\n",
      "        [0.0960],\n",
      "        [0.0962],\n",
      "        [0.0967],\n",
      "        [0.0981],\n",
      "        [0.0923],\n",
      "        [0.1000],\n",
      "        [0.0862],\n",
      "        [0.0844],\n",
      "        [0.0824],\n",
      "        [0.1087],\n",
      "        [0.1090],\n",
      "        [0.0810],\n",
      "        [0.0800],\n",
      "        [0.0786],\n",
      "        [0.0782],\n",
      "        [0.1137],\n",
      "        [0.1142],\n",
      "        [0.1164],\n",
      "        [0.0734],\n",
      "        [0.0730],\n",
      "        [0.0723],\n",
      "        [0.0705],\n",
      "        [0.0703],\n",
      "        [0.1206],\n",
      "        [0.0676],\n",
      "        [0.0656],\n",
      "        [0.0648],\n",
      "        [0.1263],\n",
      "        [0.1277],\n",
      "        [0.1284],\n",
      "        [0.0609],\n",
      "        [0.0606],\n",
      "        [0.0586],\n",
      "        [0.1320],\n",
      "        [0.0584],\n",
      "        [0.1336],\n",
      "        [0.0557],\n",
      "        [0.0541],\n",
      "        [0.0536],\n",
      "        [0.0513],\n",
      "        [0.1404],\n",
      "        [0.1408],\n",
      "        [0.1420],\n",
      "        [0.1427],\n",
      "        [0.0478],\n",
      "        [0.0475],\n",
      "        [0.1448],\n",
      "        [0.0443],\n",
      "        [0.0440],\n",
      "        [0.1488],\n",
      "        [0.1531],\n",
      "        [0.1538],\n",
      "        [0.1588],\n",
      "        [0.0277],\n",
      "        [0.0258],\n",
      "        [0.0221],\n",
      "        [0.1712],\n",
      "        [0.1725],\n",
      "        [0.0177],\n",
      "        [0.1825],\n",
      "        [0.1832],\n",
      "        [0.1833],\n",
      "        [0.0039],\n",
      "        [0.0008],\n",
      "        [0.0019],\n",
      "        [0.0040],\n",
      "        [0.1956],\n",
      "        [0.1976],\n",
      "        [0.0072],\n",
      "        [0.0116],\n",
      "        [0.0150],\n",
      "        [0.0224],\n",
      "        [0.0264],\n",
      "        [0.0290],\n",
      "        [0.0300],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0400],\n",
      "        [0.2306],\n",
      "        [0.0516],\n",
      "        [0.2537],\n",
      "        [0.0677],\n",
      "        [0.0758],\n",
      "        [0.2725],\n",
      "        [0.2727],\n",
      "        [0.0890],\n",
      "        [0.0911],\n",
      "        [0.0921],\n",
      "        [0.0984],\n",
      "        [0.1018],\n",
      "        [0.1061],\n",
      "        [0.2987],\n",
      "        [0.1108],\n",
      "        [0.1120],\n",
      "        [0.3044],\n",
      "        [0.3114],\n",
      "        [0.1226],\n",
      "        [0.1265],\n",
      "        [0.3203],\n",
      "        [0.3205],\n",
      "        [0.3296],\n",
      "        [0.1393],\n",
      "        [0.1425],\n",
      "        [0.3429],\n",
      "        [0.3466],\n",
      "        [0.1640],\n",
      "        [0.3570],\n",
      "        [0.3573],\n",
      "        [0.1719],\n",
      "        [0.3725],\n",
      "        [0.1985],\n",
      "        [0.1995],\n",
      "        [0.3923],\n",
      "        [0.2043],\n",
      "        [0.2044],\n",
      "        [0.2050],\n",
      "        [0.2051],\n",
      "        [0.3964],\n",
      "        [0.2087],\n",
      "        [0.2366],\n",
      "        [0.2377],\n",
      "        [0.2438],\n",
      "        [0.2488],\n",
      "        [0.2516],\n",
      "        [0.2552],\n",
      "        [0.2559],\n",
      "        [0.2603],\n",
      "        [0.2720],\n",
      "        [0.2762],\n",
      "        [0.2920],\n",
      "        [0.2955],\n",
      "        [0.2980],\n",
      "        [0.3008],\n",
      "        [0.3356],\n",
      "        [0.3429],\n",
      "        [0.3452],\n",
      "        [0.3721],\n",
      "        [0.3754],\n",
      "        [0.3789],\n",
      "        [0.3822],\n",
      "        [0.4002],\n",
      "        [0.3738],\n",
      "        [0.3713],\n",
      "        [0.3655],\n",
      "        [0.3294],\n",
      "        [0.3084]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 45.22492527961731\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 148\n",
      "剩餘X 資料 torch.Size([12, 18])\n",
      "剩餘Y 資料 torch.Size([12, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.13756880164146423, 1)\n",
      "The second_loss value of k: (0.1465137004852295, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.1921])\n",
      "目前模型的Data狀態 torch.Size([148, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630],\n",
      "        [0.5630]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0955],\n",
      "        [0.0960],\n",
      "        [0.0962],\n",
      "        [0.0967],\n",
      "        [0.0981],\n",
      "        [0.0923],\n",
      "        [0.1000],\n",
      "        [0.0862],\n",
      "        [0.0844],\n",
      "        [0.0824],\n",
      "        [0.1087],\n",
      "        [0.1090],\n",
      "        [0.0810],\n",
      "        [0.0800],\n",
      "        [0.0786],\n",
      "        [0.0782],\n",
      "        [0.1137],\n",
      "        [0.1142],\n",
      "        [0.1164],\n",
      "        [0.0734],\n",
      "        [0.0730],\n",
      "        [0.0723],\n",
      "        [0.0705],\n",
      "        [0.0703],\n",
      "        [0.1206],\n",
      "        [0.0676],\n",
      "        [0.0656],\n",
      "        [0.0648],\n",
      "        [0.1263],\n",
      "        [0.1277],\n",
      "        [0.1284],\n",
      "        [0.0609],\n",
      "        [0.0606],\n",
      "        [0.0586],\n",
      "        [0.1320],\n",
      "        [0.0584],\n",
      "        [0.1336],\n",
      "        [0.0557],\n",
      "        [0.0541],\n",
      "        [0.0536],\n",
      "        [0.0513],\n",
      "        [0.1404],\n",
      "        [0.1408],\n",
      "        [0.1420],\n",
      "        [0.1427],\n",
      "        [0.0478],\n",
      "        [0.0475],\n",
      "        [0.1448],\n",
      "        [0.0443],\n",
      "        [0.0440],\n",
      "        [0.1488],\n",
      "        [0.1531],\n",
      "        [0.1538],\n",
      "        [0.1588],\n",
      "        [0.0277],\n",
      "        [0.0258],\n",
      "        [0.0221],\n",
      "        [0.1712],\n",
      "        [0.1725],\n",
      "        [0.0177],\n",
      "        [0.1825],\n",
      "        [0.1832],\n",
      "        [0.1833],\n",
      "        [0.0039],\n",
      "        [0.0008],\n",
      "        [0.0019],\n",
      "        [0.0040],\n",
      "        [0.1956],\n",
      "        [0.1976],\n",
      "        [0.0072],\n",
      "        [0.0116],\n",
      "        [0.0150],\n",
      "        [0.0224],\n",
      "        [0.0264],\n",
      "        [0.0290],\n",
      "        [0.0300],\n",
      "        [0.0365],\n",
      "        [0.0366],\n",
      "        [0.0400],\n",
      "        [0.2306],\n",
      "        [0.0516],\n",
      "        [0.2537],\n",
      "        [0.0677],\n",
      "        [0.0758],\n",
      "        [0.2725],\n",
      "        [0.2727],\n",
      "        [0.0890],\n",
      "        [0.0911],\n",
      "        [0.0921],\n",
      "        [0.0984],\n",
      "        [0.1018],\n",
      "        [0.1061],\n",
      "        [0.2987],\n",
      "        [0.1108],\n",
      "        [0.1120],\n",
      "        [0.3044],\n",
      "        [0.3114],\n",
      "        [0.1226],\n",
      "        [0.1265],\n",
      "        [0.3203],\n",
      "        [0.3205],\n",
      "        [0.3296],\n",
      "        [0.1393],\n",
      "        [0.1425],\n",
      "        [0.3429],\n",
      "        [0.3466],\n",
      "        [0.1640],\n",
      "        [0.3570],\n",
      "        [0.3573],\n",
      "        [0.1719],\n",
      "        [0.3725],\n",
      "        [0.1985],\n",
      "        [0.1995],\n",
      "        [0.3923],\n",
      "        [0.2043],\n",
      "        [0.2044],\n",
      "        [0.2050],\n",
      "        [0.2051],\n",
      "        [0.3964],\n",
      "        [0.2087],\n",
      "        [0.2366],\n",
      "        [0.2377],\n",
      "        [0.2438],\n",
      "        [0.2488],\n",
      "        [0.2516],\n",
      "        [0.2552],\n",
      "        [0.2559],\n",
      "        [0.2603],\n",
      "        [0.2720],\n",
      "        [0.2762],\n",
      "        [0.2920],\n",
      "        [0.2955],\n",
      "        [0.2980],\n",
      "        [0.3008],\n",
      "        [0.3356],\n",
      "        [0.3429],\n",
      "        [0.3452],\n",
      "        [0.3721],\n",
      "        [0.3754],\n",
      "        [0.3789],\n",
      "        [0.3822],\n",
      "        [0.4002],\n",
      "        [0.3738],\n",
      "        [0.3713],\n",
      "        [0.3655],\n",
      "        [0.3294],\n",
      "        [0.3084],\n",
      "        [0.3709]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 34\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0980],\n",
      "        [0.0985],\n",
      "        [0.0986],\n",
      "        [0.0992],\n",
      "        [0.1005],\n",
      "        [0.0948],\n",
      "        [0.1025],\n",
      "        [0.0887],\n",
      "        [0.0869],\n",
      "        [0.0849],\n",
      "        [0.1112],\n",
      "        [0.1115],\n",
      "        [0.0834],\n",
      "        [0.0824],\n",
      "        [0.0810],\n",
      "        [0.0806],\n",
      "        [0.1162],\n",
      "        [0.1166],\n",
      "        [0.1189],\n",
      "        [0.0759],\n",
      "        [0.0755],\n",
      "        [0.0748],\n",
      "        [0.0730],\n",
      "        [0.0728],\n",
      "        [0.1230],\n",
      "        [0.0700],\n",
      "        [0.0681],\n",
      "        [0.0672],\n",
      "        [0.1287],\n",
      "        [0.1302],\n",
      "        [0.1309],\n",
      "        [0.0633],\n",
      "        [0.0631],\n",
      "        [0.0611],\n",
      "        [0.1345],\n",
      "        [0.0608],\n",
      "        [0.1361],\n",
      "        [0.0582],\n",
      "        [0.0565],\n",
      "        [0.0560],\n",
      "        [0.0538],\n",
      "        [0.1428],\n",
      "        [0.1433],\n",
      "        [0.1445],\n",
      "        [0.1451],\n",
      "        [0.0502],\n",
      "        [0.0500],\n",
      "        [0.1472],\n",
      "        [0.0467],\n",
      "        [0.0464],\n",
      "        [0.1512],\n",
      "        [0.1556],\n",
      "        [0.1562],\n",
      "        [0.1612],\n",
      "        [0.0301],\n",
      "        [0.0283],\n",
      "        [0.0246],\n",
      "        [0.1737],\n",
      "        [0.1750],\n",
      "        [0.0202],\n",
      "        [0.1850],\n",
      "        [0.1856],\n",
      "        [0.1858],\n",
      "        [0.0064],\n",
      "        [0.0032],\n",
      "        [0.0005],\n",
      "        [0.0015],\n",
      "        [0.1981],\n",
      "        [0.2001],\n",
      "        [0.0047],\n",
      "        [0.0091],\n",
      "        [0.0126],\n",
      "        [0.0199],\n",
      "        [0.0239],\n",
      "        [0.0265],\n",
      "        [0.0276],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0375],\n",
      "        [0.2331],\n",
      "        [0.0492],\n",
      "        [0.2561],\n",
      "        [0.0652],\n",
      "        [0.0733],\n",
      "        [0.2750],\n",
      "        [0.2751],\n",
      "        [0.0866],\n",
      "        [0.0886],\n",
      "        [0.0896],\n",
      "        [0.0960],\n",
      "        [0.0994],\n",
      "        [0.1037],\n",
      "        [0.3011],\n",
      "        [0.1084],\n",
      "        [0.1095],\n",
      "        [0.3069],\n",
      "        [0.3138],\n",
      "        [0.1201],\n",
      "        [0.1240],\n",
      "        [0.3228],\n",
      "        [0.3229],\n",
      "        [0.3320],\n",
      "        [0.1368],\n",
      "        [0.1400],\n",
      "        [0.3454],\n",
      "        [0.3491],\n",
      "        [0.1616],\n",
      "        [0.3594],\n",
      "        [0.3598],\n",
      "        [0.1694],\n",
      "        [0.3749],\n",
      "        [0.1961],\n",
      "        [0.1971],\n",
      "        [0.3947],\n",
      "        [0.2018],\n",
      "        [0.2020],\n",
      "        [0.2025],\n",
      "        [0.2026],\n",
      "        [0.3989],\n",
      "        [0.2062],\n",
      "        [0.2342],\n",
      "        [0.2353],\n",
      "        [0.2413],\n",
      "        [0.2463],\n",
      "        [0.2491],\n",
      "        [0.2527],\n",
      "        [0.2534],\n",
      "        [0.2578],\n",
      "        [0.2696],\n",
      "        [0.2738],\n",
      "        [0.2896],\n",
      "        [0.2931],\n",
      "        [0.2956],\n",
      "        [0.2983],\n",
      "        [0.3331],\n",
      "        [0.3404],\n",
      "        [0.3427],\n",
      "        [0.3697],\n",
      "        [0.3729],\n",
      "        [0.3765],\n",
      "        [0.3797],\n",
      "        [0.3978],\n",
      "        [0.3714],\n",
      "        [0.3688],\n",
      "        [0.3630],\n",
      "        [0.3269],\n",
      "        [0.3060],\n",
      "        [0.3684]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 45.46271562576294\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 149\n",
      "剩餘X 資料 torch.Size([11, 18])\n",
      "剩餘Y 資料 torch.Size([11, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14463551342487335, 1)\n",
      "The second_loss value of k: (0.15878987312316895, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.1802])\n",
      "目前模型的Data狀態 torch.Size([149, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605],\n",
      "        [0.5605]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0980],\n",
      "        [0.0985],\n",
      "        [0.0986],\n",
      "        [0.0992],\n",
      "        [0.1005],\n",
      "        [0.0948],\n",
      "        [0.1025],\n",
      "        [0.0887],\n",
      "        [0.0869],\n",
      "        [0.0849],\n",
      "        [0.1112],\n",
      "        [0.1115],\n",
      "        [0.0834],\n",
      "        [0.0824],\n",
      "        [0.0810],\n",
      "        [0.0806],\n",
      "        [0.1162],\n",
      "        [0.1166],\n",
      "        [0.1189],\n",
      "        [0.0759],\n",
      "        [0.0755],\n",
      "        [0.0748],\n",
      "        [0.0730],\n",
      "        [0.0728],\n",
      "        [0.1230],\n",
      "        [0.0700],\n",
      "        [0.0681],\n",
      "        [0.0672],\n",
      "        [0.1287],\n",
      "        [0.1302],\n",
      "        [0.1309],\n",
      "        [0.0633],\n",
      "        [0.0631],\n",
      "        [0.0611],\n",
      "        [0.1345],\n",
      "        [0.0608],\n",
      "        [0.1361],\n",
      "        [0.0582],\n",
      "        [0.0565],\n",
      "        [0.0560],\n",
      "        [0.0538],\n",
      "        [0.1428],\n",
      "        [0.1433],\n",
      "        [0.1445],\n",
      "        [0.1451],\n",
      "        [0.0502],\n",
      "        [0.0500],\n",
      "        [0.1472],\n",
      "        [0.0467],\n",
      "        [0.0464],\n",
      "        [0.1512],\n",
      "        [0.1556],\n",
      "        [0.1562],\n",
      "        [0.1612],\n",
      "        [0.0301],\n",
      "        [0.0283],\n",
      "        [0.0246],\n",
      "        [0.1737],\n",
      "        [0.1750],\n",
      "        [0.0202],\n",
      "        [0.1850],\n",
      "        [0.1856],\n",
      "        [0.1858],\n",
      "        [0.0064],\n",
      "        [0.0032],\n",
      "        [0.0005],\n",
      "        [0.0015],\n",
      "        [0.1981],\n",
      "        [0.2001],\n",
      "        [0.0047],\n",
      "        [0.0091],\n",
      "        [0.0126],\n",
      "        [0.0199],\n",
      "        [0.0239],\n",
      "        [0.0265],\n",
      "        [0.0276],\n",
      "        [0.0340],\n",
      "        [0.0341],\n",
      "        [0.0375],\n",
      "        [0.2331],\n",
      "        [0.0492],\n",
      "        [0.2561],\n",
      "        [0.0652],\n",
      "        [0.0733],\n",
      "        [0.2750],\n",
      "        [0.2751],\n",
      "        [0.0866],\n",
      "        [0.0886],\n",
      "        [0.0896],\n",
      "        [0.0960],\n",
      "        [0.0994],\n",
      "        [0.1037],\n",
      "        [0.3011],\n",
      "        [0.1084],\n",
      "        [0.1095],\n",
      "        [0.3069],\n",
      "        [0.3138],\n",
      "        [0.1201],\n",
      "        [0.1240],\n",
      "        [0.3228],\n",
      "        [0.3229],\n",
      "        [0.3320],\n",
      "        [0.1368],\n",
      "        [0.1400],\n",
      "        [0.3454],\n",
      "        [0.3491],\n",
      "        [0.1616],\n",
      "        [0.3594],\n",
      "        [0.3598],\n",
      "        [0.1694],\n",
      "        [0.3749],\n",
      "        [0.1961],\n",
      "        [0.1971],\n",
      "        [0.3947],\n",
      "        [0.2018],\n",
      "        [0.2020],\n",
      "        [0.2025],\n",
      "        [0.2026],\n",
      "        [0.3989],\n",
      "        [0.2062],\n",
      "        [0.2342],\n",
      "        [0.2353],\n",
      "        [0.2413],\n",
      "        [0.2463],\n",
      "        [0.2491],\n",
      "        [0.2527],\n",
      "        [0.2534],\n",
      "        [0.2578],\n",
      "        [0.2696],\n",
      "        [0.2738],\n",
      "        [0.2896],\n",
      "        [0.2931],\n",
      "        [0.2956],\n",
      "        [0.2983],\n",
      "        [0.3331],\n",
      "        [0.3404],\n",
      "        [0.3427],\n",
      "        [0.3697],\n",
      "        [0.3729],\n",
      "        [0.3765],\n",
      "        [0.3797],\n",
      "        [0.3978],\n",
      "        [0.3714],\n",
      "        [0.3688],\n",
      "        [0.3630],\n",
      "        [0.3269],\n",
      "        [0.3060],\n",
      "        [0.3684],\n",
      "        [0.3803]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 73\n",
      "Number of shrink: 27\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.1005],\n",
      "        [0.1010],\n",
      "        [0.1012],\n",
      "        [0.1018],\n",
      "        [0.1031],\n",
      "        [0.0973],\n",
      "        [0.1051],\n",
      "        [0.0913],\n",
      "        [0.0895],\n",
      "        [0.0874],\n",
      "        [0.1138],\n",
      "        [0.1141],\n",
      "        [0.0860],\n",
      "        [0.0850],\n",
      "        [0.0836],\n",
      "        [0.0832],\n",
      "        [0.1188],\n",
      "        [0.1192],\n",
      "        [0.1215],\n",
      "        [0.0784],\n",
      "        [0.0780],\n",
      "        [0.0774],\n",
      "        [0.0755],\n",
      "        [0.0754],\n",
      "        [0.1256],\n",
      "        [0.0726],\n",
      "        [0.0707],\n",
      "        [0.0698],\n",
      "        [0.1313],\n",
      "        [0.1328],\n",
      "        [0.1335],\n",
      "        [0.0659],\n",
      "        [0.0656],\n",
      "        [0.0636],\n",
      "        [0.1371],\n",
      "        [0.0634],\n",
      "        [0.1386],\n",
      "        [0.0608],\n",
      "        [0.0591],\n",
      "        [0.0586],\n",
      "        [0.0564],\n",
      "        [0.1454],\n",
      "        [0.1459],\n",
      "        [0.1471],\n",
      "        [0.1477],\n",
      "        [0.0528],\n",
      "        [0.0525],\n",
      "        [0.1498],\n",
      "        [0.0493],\n",
      "        [0.0490],\n",
      "        [0.1538],\n",
      "        [0.1581],\n",
      "        [0.1588],\n",
      "        [0.1638],\n",
      "        [0.0327],\n",
      "        [0.0309],\n",
      "        [0.0272],\n",
      "        [0.1763],\n",
      "        [0.1775],\n",
      "        [0.0227],\n",
      "        [0.1876],\n",
      "        [0.1882],\n",
      "        [0.1883],\n",
      "        [0.0089],\n",
      "        [0.0058],\n",
      "        [0.0031],\n",
      "        [0.0010],\n",
      "        [0.2006],\n",
      "        [0.2026],\n",
      "        [0.0021],\n",
      "        [0.0065],\n",
      "        [0.0100],\n",
      "        [0.0174],\n",
      "        [0.0213],\n",
      "        [0.0240],\n",
      "        [0.0250],\n",
      "        [0.0314],\n",
      "        [0.0315],\n",
      "        [0.0349],\n",
      "        [0.2357],\n",
      "        [0.0466],\n",
      "        [0.2587],\n",
      "        [0.0626],\n",
      "        [0.0707],\n",
      "        [0.2775],\n",
      "        [0.2777],\n",
      "        [0.0840],\n",
      "        [0.0860],\n",
      "        [0.0870],\n",
      "        [0.0934],\n",
      "        [0.0968],\n",
      "        [0.1011],\n",
      "        [0.3037],\n",
      "        [0.1058],\n",
      "        [0.1069],\n",
      "        [0.3095],\n",
      "        [0.3164],\n",
      "        [0.1175],\n",
      "        [0.1215],\n",
      "        [0.3254],\n",
      "        [0.3255],\n",
      "        [0.3346],\n",
      "        [0.1343],\n",
      "        [0.1375],\n",
      "        [0.3480],\n",
      "        [0.3516],\n",
      "        [0.1590],\n",
      "        [0.3620],\n",
      "        [0.3623],\n",
      "        [0.1669],\n",
      "        [0.3775],\n",
      "        [0.1935],\n",
      "        [0.1945],\n",
      "        [0.3973],\n",
      "        [0.1992],\n",
      "        [0.1994],\n",
      "        [0.1999],\n",
      "        [0.2001],\n",
      "        [0.4014],\n",
      "        [0.2037],\n",
      "        [0.2316],\n",
      "        [0.2327],\n",
      "        [0.2387],\n",
      "        [0.2438],\n",
      "        [0.2465],\n",
      "        [0.2502],\n",
      "        [0.2509],\n",
      "        [0.2553],\n",
      "        [0.2670],\n",
      "        [0.2712],\n",
      "        [0.2870],\n",
      "        [0.2905],\n",
      "        [0.2930],\n",
      "        [0.2958],\n",
      "        [0.3306],\n",
      "        [0.3378],\n",
      "        [0.3401],\n",
      "        [0.3671],\n",
      "        [0.3703],\n",
      "        [0.3739],\n",
      "        [0.3772],\n",
      "        [0.3952],\n",
      "        [0.3688],\n",
      "        [0.3663],\n",
      "        [0.3604],\n",
      "        [0.3244],\n",
      "        [0.3034],\n",
      "        [0.3659],\n",
      "        [0.3777]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 45.69870901107788\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 150\n",
      "剩餘X 資料 torch.Size([10, 18])\n",
      "剩餘Y 資料 torch.Size([10, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15674903988838196, 1)\n",
      "The second_loss value of k: (0.15909945964813232, 0)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引1，y= tensor([0.1620])\n",
      "目前模型的Data狀態 torch.Size([150, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580],\n",
      "        [0.5580]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.1005],\n",
      "        [0.1010],\n",
      "        [0.1012],\n",
      "        [0.1018],\n",
      "        [0.1031],\n",
      "        [0.0973],\n",
      "        [0.1051],\n",
      "        [0.0913],\n",
      "        [0.0895],\n",
      "        [0.0874],\n",
      "        [0.1138],\n",
      "        [0.1141],\n",
      "        [0.0860],\n",
      "        [0.0850],\n",
      "        [0.0836],\n",
      "        [0.0832],\n",
      "        [0.1188],\n",
      "        [0.1192],\n",
      "        [0.1215],\n",
      "        [0.0784],\n",
      "        [0.0780],\n",
      "        [0.0774],\n",
      "        [0.0755],\n",
      "        [0.0754],\n",
      "        [0.1256],\n",
      "        [0.0726],\n",
      "        [0.0707],\n",
      "        [0.0698],\n",
      "        [0.1313],\n",
      "        [0.1328],\n",
      "        [0.1335],\n",
      "        [0.0659],\n",
      "        [0.0656],\n",
      "        [0.0636],\n",
      "        [0.1371],\n",
      "        [0.0634],\n",
      "        [0.1386],\n",
      "        [0.0608],\n",
      "        [0.0591],\n",
      "        [0.0586],\n",
      "        [0.0564],\n",
      "        [0.1454],\n",
      "        [0.1459],\n",
      "        [0.1471],\n",
      "        [0.1477],\n",
      "        [0.0528],\n",
      "        [0.0525],\n",
      "        [0.1498],\n",
      "        [0.0493],\n",
      "        [0.0490],\n",
      "        [0.1538],\n",
      "        [0.1581],\n",
      "        [0.1588],\n",
      "        [0.1638],\n",
      "        [0.0327],\n",
      "        [0.0309],\n",
      "        [0.0272],\n",
      "        [0.1763],\n",
      "        [0.1775],\n",
      "        [0.0227],\n",
      "        [0.1876],\n",
      "        [0.1882],\n",
      "        [0.1883],\n",
      "        [0.0089],\n",
      "        [0.0058],\n",
      "        [0.0031],\n",
      "        [0.0010],\n",
      "        [0.2006],\n",
      "        [0.2026],\n",
      "        [0.0021],\n",
      "        [0.0065],\n",
      "        [0.0100],\n",
      "        [0.0174],\n",
      "        [0.0213],\n",
      "        [0.0240],\n",
      "        [0.0250],\n",
      "        [0.0314],\n",
      "        [0.0315],\n",
      "        [0.0349],\n",
      "        [0.2357],\n",
      "        [0.0466],\n",
      "        [0.2587],\n",
      "        [0.0626],\n",
      "        [0.0707],\n",
      "        [0.2775],\n",
      "        [0.2777],\n",
      "        [0.0840],\n",
      "        [0.0860],\n",
      "        [0.0870],\n",
      "        [0.0934],\n",
      "        [0.0968],\n",
      "        [0.1011],\n",
      "        [0.3037],\n",
      "        [0.1058],\n",
      "        [0.1069],\n",
      "        [0.3095],\n",
      "        [0.3164],\n",
      "        [0.1175],\n",
      "        [0.1215],\n",
      "        [0.3254],\n",
      "        [0.3255],\n",
      "        [0.3346],\n",
      "        [0.1343],\n",
      "        [0.1375],\n",
      "        [0.3480],\n",
      "        [0.3516],\n",
      "        [0.1590],\n",
      "        [0.3620],\n",
      "        [0.3623],\n",
      "        [0.1669],\n",
      "        [0.3775],\n",
      "        [0.1935],\n",
      "        [0.1945],\n",
      "        [0.3973],\n",
      "        [0.1992],\n",
      "        [0.1994],\n",
      "        [0.1999],\n",
      "        [0.2001],\n",
      "        [0.4014],\n",
      "        [0.2037],\n",
      "        [0.2316],\n",
      "        [0.2327],\n",
      "        [0.2387],\n",
      "        [0.2438],\n",
      "        [0.2465],\n",
      "        [0.2502],\n",
      "        [0.2509],\n",
      "        [0.2553],\n",
      "        [0.2670],\n",
      "        [0.2712],\n",
      "        [0.2870],\n",
      "        [0.2905],\n",
      "        [0.2930],\n",
      "        [0.2958],\n",
      "        [0.3306],\n",
      "        [0.3378],\n",
      "        [0.3401],\n",
      "        [0.3671],\n",
      "        [0.3703],\n",
      "        [0.3739],\n",
      "        [0.3772],\n",
      "        [0.3952],\n",
      "        [0.3688],\n",
      "        [0.3663],\n",
      "        [0.3604],\n",
      "        [0.3244],\n",
      "        [0.3034],\n",
      "        [0.3659],\n",
      "        [0.3777],\n",
      "        [0.3959]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 73\n",
      "Number of shrink: 27\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.1032],\n",
      "        [0.1037],\n",
      "        [0.1038],\n",
      "        [0.1044],\n",
      "        [0.1057],\n",
      "        [0.1000],\n",
      "        [0.1077],\n",
      "        [0.0939],\n",
      "        [0.0921],\n",
      "        [0.0901],\n",
      "        [0.1164],\n",
      "        [0.1167],\n",
      "        [0.0886],\n",
      "        [0.0876],\n",
      "        [0.0862],\n",
      "        [0.0858],\n",
      "        [0.1214],\n",
      "        [0.1218],\n",
      "        [0.1241],\n",
      "        [0.0811],\n",
      "        [0.0807],\n",
      "        [0.0800],\n",
      "        [0.0782],\n",
      "        [0.0780],\n",
      "        [0.1282],\n",
      "        [0.0752],\n",
      "        [0.0733],\n",
      "        [0.0724],\n",
      "        [0.1339],\n",
      "        [0.1354],\n",
      "        [0.1361],\n",
      "        [0.0685],\n",
      "        [0.0683],\n",
      "        [0.0663],\n",
      "        [0.1397],\n",
      "        [0.0660],\n",
      "        [0.1413],\n",
      "        [0.0634],\n",
      "        [0.0618],\n",
      "        [0.0612],\n",
      "        [0.0590],\n",
      "        [0.1480],\n",
      "        [0.1485],\n",
      "        [0.1497],\n",
      "        [0.1503],\n",
      "        [0.0554],\n",
      "        [0.0552],\n",
      "        [0.1524],\n",
      "        [0.0519],\n",
      "        [0.0517],\n",
      "        [0.1564],\n",
      "        [0.1608],\n",
      "        [0.1614],\n",
      "        [0.1665],\n",
      "        [0.0354],\n",
      "        [0.0335],\n",
      "        [0.0298],\n",
      "        [0.1789],\n",
      "        [0.1802],\n",
      "        [0.0254],\n",
      "        [0.1902],\n",
      "        [0.1908],\n",
      "        [0.1910],\n",
      "        [0.0116],\n",
      "        [0.0084],\n",
      "        [0.0057],\n",
      "        [0.0037],\n",
      "        [0.2033],\n",
      "        [0.2053],\n",
      "        [0.0005],\n",
      "        [0.0039],\n",
      "        [0.0074],\n",
      "        [0.0147],\n",
      "        [0.0187],\n",
      "        [0.0213],\n",
      "        [0.0224],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0323],\n",
      "        [0.2383],\n",
      "        [0.0440],\n",
      "        [0.2613],\n",
      "        [0.0600],\n",
      "        [0.0681],\n",
      "        [0.2802],\n",
      "        [0.2803],\n",
      "        [0.0814],\n",
      "        [0.0834],\n",
      "        [0.0844],\n",
      "        [0.0908],\n",
      "        [0.0942],\n",
      "        [0.0984],\n",
      "        [0.3063],\n",
      "        [0.1031],\n",
      "        [0.1043],\n",
      "        [0.3121],\n",
      "        [0.3190],\n",
      "        [0.1149],\n",
      "        [0.1188],\n",
      "        [0.3280],\n",
      "        [0.3281],\n",
      "        [0.3373],\n",
      "        [0.1316],\n",
      "        [0.1348],\n",
      "        [0.3506],\n",
      "        [0.3543],\n",
      "        [0.1564],\n",
      "        [0.3647],\n",
      "        [0.3650],\n",
      "        [0.1642],\n",
      "        [0.3801],\n",
      "        [0.1908],\n",
      "        [0.1919],\n",
      "        [0.3999],\n",
      "        [0.1966],\n",
      "        [0.1968],\n",
      "        [0.1973],\n",
      "        [0.1974],\n",
      "        [0.4041],\n",
      "        [0.2010],\n",
      "        [0.2290],\n",
      "        [0.2301],\n",
      "        [0.2361],\n",
      "        [0.2411],\n",
      "        [0.2439],\n",
      "        [0.2475],\n",
      "        [0.2482],\n",
      "        [0.2526],\n",
      "        [0.2644],\n",
      "        [0.2686],\n",
      "        [0.2843],\n",
      "        [0.2879],\n",
      "        [0.2904],\n",
      "        [0.2931],\n",
      "        [0.3279],\n",
      "        [0.3352],\n",
      "        [0.3375],\n",
      "        [0.3644],\n",
      "        [0.3677],\n",
      "        [0.3713],\n",
      "        [0.3745],\n",
      "        [0.3926],\n",
      "        [0.3662],\n",
      "        [0.3636],\n",
      "        [0.3578],\n",
      "        [0.3217],\n",
      "        [0.3008],\n",
      "        [0.3632],\n",
      "        [0.3751],\n",
      "        [0.3933]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 45.937379121780396\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 151\n",
      "剩餘X 資料 torch.Size([9, 18])\n",
      "剩餘Y 資料 torch.Size([9, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15700136125087738, 0)\n",
      "The second_loss value of k: (0.19116568565368652, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.1591])\n",
      "目前模型的Data狀態 torch.Size([151, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553],\n",
      "        [0.5553]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.1032],\n",
      "        [0.1037],\n",
      "        [0.1038],\n",
      "        [0.1044],\n",
      "        [0.1057],\n",
      "        [0.1000],\n",
      "        [0.1077],\n",
      "        [0.0939],\n",
      "        [0.0921],\n",
      "        [0.0901],\n",
      "        [0.1164],\n",
      "        [0.1167],\n",
      "        [0.0886],\n",
      "        [0.0876],\n",
      "        [0.0862],\n",
      "        [0.0858],\n",
      "        [0.1214],\n",
      "        [0.1218],\n",
      "        [0.1241],\n",
      "        [0.0811],\n",
      "        [0.0807],\n",
      "        [0.0800],\n",
      "        [0.0782],\n",
      "        [0.0780],\n",
      "        [0.1282],\n",
      "        [0.0752],\n",
      "        [0.0733],\n",
      "        [0.0724],\n",
      "        [0.1339],\n",
      "        [0.1354],\n",
      "        [0.1361],\n",
      "        [0.0685],\n",
      "        [0.0683],\n",
      "        [0.0663],\n",
      "        [0.1397],\n",
      "        [0.0660],\n",
      "        [0.1413],\n",
      "        [0.0634],\n",
      "        [0.0618],\n",
      "        [0.0612],\n",
      "        [0.0590],\n",
      "        [0.1480],\n",
      "        [0.1485],\n",
      "        [0.1497],\n",
      "        [0.1503],\n",
      "        [0.0554],\n",
      "        [0.0552],\n",
      "        [0.1524],\n",
      "        [0.0519],\n",
      "        [0.0517],\n",
      "        [0.1564],\n",
      "        [0.1608],\n",
      "        [0.1614],\n",
      "        [0.1665],\n",
      "        [0.0354],\n",
      "        [0.0335],\n",
      "        [0.0298],\n",
      "        [0.1789],\n",
      "        [0.1802],\n",
      "        [0.0254],\n",
      "        [0.1902],\n",
      "        [0.1908],\n",
      "        [0.1910],\n",
      "        [0.0116],\n",
      "        [0.0084],\n",
      "        [0.0057],\n",
      "        [0.0037],\n",
      "        [0.2033],\n",
      "        [0.2053],\n",
      "        [0.0005],\n",
      "        [0.0039],\n",
      "        [0.0074],\n",
      "        [0.0147],\n",
      "        [0.0187],\n",
      "        [0.0213],\n",
      "        [0.0224],\n",
      "        [0.0288],\n",
      "        [0.0289],\n",
      "        [0.0323],\n",
      "        [0.2383],\n",
      "        [0.0440],\n",
      "        [0.2613],\n",
      "        [0.0600],\n",
      "        [0.0681],\n",
      "        [0.2802],\n",
      "        [0.2803],\n",
      "        [0.0814],\n",
      "        [0.0834],\n",
      "        [0.0844],\n",
      "        [0.0908],\n",
      "        [0.0942],\n",
      "        [0.0984],\n",
      "        [0.3063],\n",
      "        [0.1031],\n",
      "        [0.1043],\n",
      "        [0.3121],\n",
      "        [0.3190],\n",
      "        [0.1149],\n",
      "        [0.1188],\n",
      "        [0.3280],\n",
      "        [0.3281],\n",
      "        [0.3373],\n",
      "        [0.1316],\n",
      "        [0.1348],\n",
      "        [0.3506],\n",
      "        [0.3543],\n",
      "        [0.1564],\n",
      "        [0.3647],\n",
      "        [0.3650],\n",
      "        [0.1642],\n",
      "        [0.3801],\n",
      "        [0.1908],\n",
      "        [0.1919],\n",
      "        [0.3999],\n",
      "        [0.1966],\n",
      "        [0.1968],\n",
      "        [0.1973],\n",
      "        [0.1974],\n",
      "        [0.4041],\n",
      "        [0.2010],\n",
      "        [0.2290],\n",
      "        [0.2301],\n",
      "        [0.2361],\n",
      "        [0.2411],\n",
      "        [0.2439],\n",
      "        [0.2475],\n",
      "        [0.2482],\n",
      "        [0.2526],\n",
      "        [0.2644],\n",
      "        [0.2686],\n",
      "        [0.2843],\n",
      "        [0.2879],\n",
      "        [0.2904],\n",
      "        [0.2931],\n",
      "        [0.3279],\n",
      "        [0.3352],\n",
      "        [0.3375],\n",
      "        [0.3644],\n",
      "        [0.3677],\n",
      "        [0.3713],\n",
      "        [0.3745],\n",
      "        [0.3926],\n",
      "        [0.3662],\n",
      "        [0.3636],\n",
      "        [0.3578],\n",
      "        [0.3217],\n",
      "        [0.3008],\n",
      "        [0.3632],\n",
      "        [0.3751],\n",
      "        [0.3933],\n",
      "        [0.3962]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.1058],\n",
      "        [0.1063],\n",
      "        [0.1065],\n",
      "        [0.1070],\n",
      "        [0.1084],\n",
      "        [0.1026],\n",
      "        [0.1103],\n",
      "        [0.0965],\n",
      "        [0.0947],\n",
      "        [0.0927],\n",
      "        [0.1190],\n",
      "        [0.1193],\n",
      "        [0.0913],\n",
      "        [0.0903],\n",
      "        [0.0889],\n",
      "        [0.0885],\n",
      "        [0.1240],\n",
      "        [0.1245],\n",
      "        [0.1267],\n",
      "        [0.0837],\n",
      "        [0.0833],\n",
      "        [0.0826],\n",
      "        [0.0808],\n",
      "        [0.0806],\n",
      "        [0.1309],\n",
      "        [0.0779],\n",
      "        [0.0759],\n",
      "        [0.0751],\n",
      "        [0.1366],\n",
      "        [0.1380],\n",
      "        [0.1387],\n",
      "        [0.0712],\n",
      "        [0.0709],\n",
      "        [0.0689],\n",
      "        [0.1423],\n",
      "        [0.0687],\n",
      "        [0.1439],\n",
      "        [0.0660],\n",
      "        [0.0644],\n",
      "        [0.0639],\n",
      "        [0.0616],\n",
      "        [0.1507],\n",
      "        [0.1512],\n",
      "        [0.1523],\n",
      "        [0.1530],\n",
      "        [0.0581],\n",
      "        [0.0578],\n",
      "        [0.1551],\n",
      "        [0.0546],\n",
      "        [0.0543],\n",
      "        [0.1591],\n",
      "        [0.1634],\n",
      "        [0.1641],\n",
      "        [0.1691],\n",
      "        [0.0380],\n",
      "        [0.0361],\n",
      "        [0.0324],\n",
      "        [0.1815],\n",
      "        [0.1828],\n",
      "        [0.0280],\n",
      "        [0.1928],\n",
      "        [0.1935],\n",
      "        [0.1936],\n",
      "        [0.0142],\n",
      "        [0.0111],\n",
      "        [0.0084],\n",
      "        [0.0063],\n",
      "        [0.2059],\n",
      "        [0.2079],\n",
      "        [0.0031],\n",
      "        [0.0013],\n",
      "        [0.0047],\n",
      "        [0.0121],\n",
      "        [0.0161],\n",
      "        [0.0187],\n",
      "        [0.0197],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0297],\n",
      "        [0.2409],\n",
      "        [0.0413],\n",
      "        [0.2640],\n",
      "        [0.0573],\n",
      "        [0.0655],\n",
      "        [0.2828],\n",
      "        [0.2830],\n",
      "        [0.0787],\n",
      "        [0.0808],\n",
      "        [0.0818],\n",
      "        [0.0881],\n",
      "        [0.0915],\n",
      "        [0.0958],\n",
      "        [0.3090],\n",
      "        [0.1005],\n",
      "        [0.1016],\n",
      "        [0.3147],\n",
      "        [0.3217],\n",
      "        [0.1123],\n",
      "        [0.1162],\n",
      "        [0.3306],\n",
      "        [0.3308],\n",
      "        [0.3399],\n",
      "        [0.1290],\n",
      "        [0.1322],\n",
      "        [0.3532],\n",
      "        [0.3569],\n",
      "        [0.1537],\n",
      "        [0.3673],\n",
      "        [0.3676],\n",
      "        [0.1616],\n",
      "        [0.3828],\n",
      "        [0.1882],\n",
      "        [0.1892],\n",
      "        [0.4026],\n",
      "        [0.1940],\n",
      "        [0.1941],\n",
      "        [0.1947],\n",
      "        [0.1948],\n",
      "        [0.4067],\n",
      "        [0.1984],\n",
      "        [0.2263],\n",
      "        [0.2274],\n",
      "        [0.2335],\n",
      "        [0.2385],\n",
      "        [0.2412],\n",
      "        [0.2449],\n",
      "        [0.2456],\n",
      "        [0.2500],\n",
      "        [0.2617],\n",
      "        [0.2659],\n",
      "        [0.2817],\n",
      "        [0.2852],\n",
      "        [0.2877],\n",
      "        [0.2905],\n",
      "        [0.3253],\n",
      "        [0.3326],\n",
      "        [0.3349],\n",
      "        [0.3618],\n",
      "        [0.3651],\n",
      "        [0.3686],\n",
      "        [0.3719],\n",
      "        [0.3899],\n",
      "        [0.3635],\n",
      "        [0.3610],\n",
      "        [0.3552],\n",
      "        [0.3191],\n",
      "        [0.2981],\n",
      "        [0.3606],\n",
      "        [0.3725],\n",
      "        [0.3906],\n",
      "        [0.3936]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 46.17959141731262\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 152\n",
      "剩餘X 資料 torch.Size([8, 18])\n",
      "剩餘Y 資料 torch.Size([8, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18886928260326385, 0)\n",
      "The second_loss value of k: (0.22768957912921906, 7)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.1181])\n",
      "目前模型的Data狀態 torch.Size([152, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527],\n",
      "        [0.5527]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.1058],\n",
      "        [0.1063],\n",
      "        [0.1065],\n",
      "        [0.1070],\n",
      "        [0.1084],\n",
      "        [0.1026],\n",
      "        [0.1103],\n",
      "        [0.0965],\n",
      "        [0.0947],\n",
      "        [0.0927],\n",
      "        [0.1190],\n",
      "        [0.1193],\n",
      "        [0.0913],\n",
      "        [0.0903],\n",
      "        [0.0889],\n",
      "        [0.0885],\n",
      "        [0.1240],\n",
      "        [0.1245],\n",
      "        [0.1267],\n",
      "        [0.0837],\n",
      "        [0.0833],\n",
      "        [0.0826],\n",
      "        [0.0808],\n",
      "        [0.0806],\n",
      "        [0.1309],\n",
      "        [0.0779],\n",
      "        [0.0759],\n",
      "        [0.0751],\n",
      "        [0.1366],\n",
      "        [0.1380],\n",
      "        [0.1387],\n",
      "        [0.0712],\n",
      "        [0.0709],\n",
      "        [0.0689],\n",
      "        [0.1423],\n",
      "        [0.0687],\n",
      "        [0.1439],\n",
      "        [0.0660],\n",
      "        [0.0644],\n",
      "        [0.0639],\n",
      "        [0.0616],\n",
      "        [0.1507],\n",
      "        [0.1512],\n",
      "        [0.1523],\n",
      "        [0.1530],\n",
      "        [0.0581],\n",
      "        [0.0578],\n",
      "        [0.1551],\n",
      "        [0.0546],\n",
      "        [0.0543],\n",
      "        [0.1591],\n",
      "        [0.1634],\n",
      "        [0.1641],\n",
      "        [0.1691],\n",
      "        [0.0380],\n",
      "        [0.0361],\n",
      "        [0.0324],\n",
      "        [0.1815],\n",
      "        [0.1828],\n",
      "        [0.0280],\n",
      "        [0.1928],\n",
      "        [0.1935],\n",
      "        [0.1936],\n",
      "        [0.0142],\n",
      "        [0.0111],\n",
      "        [0.0084],\n",
      "        [0.0063],\n",
      "        [0.2059],\n",
      "        [0.2079],\n",
      "        [0.0031],\n",
      "        [0.0013],\n",
      "        [0.0047],\n",
      "        [0.0121],\n",
      "        [0.0161],\n",
      "        [0.0187],\n",
      "        [0.0197],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0297],\n",
      "        [0.2409],\n",
      "        [0.0413],\n",
      "        [0.2640],\n",
      "        [0.0573],\n",
      "        [0.0655],\n",
      "        [0.2828],\n",
      "        [0.2830],\n",
      "        [0.0787],\n",
      "        [0.0808],\n",
      "        [0.0818],\n",
      "        [0.0881],\n",
      "        [0.0915],\n",
      "        [0.0958],\n",
      "        [0.3090],\n",
      "        [0.1005],\n",
      "        [0.1016],\n",
      "        [0.3147],\n",
      "        [0.3217],\n",
      "        [0.1123],\n",
      "        [0.1162],\n",
      "        [0.3306],\n",
      "        [0.3308],\n",
      "        [0.3399],\n",
      "        [0.1290],\n",
      "        [0.1322],\n",
      "        [0.3532],\n",
      "        [0.3569],\n",
      "        [0.1537],\n",
      "        [0.3673],\n",
      "        [0.3676],\n",
      "        [0.1616],\n",
      "        [0.3828],\n",
      "        [0.1882],\n",
      "        [0.1892],\n",
      "        [0.4026],\n",
      "        [0.1940],\n",
      "        [0.1941],\n",
      "        [0.1947],\n",
      "        [0.1948],\n",
      "        [0.4067],\n",
      "        [0.1984],\n",
      "        [0.2263],\n",
      "        [0.2274],\n",
      "        [0.2335],\n",
      "        [0.2385],\n",
      "        [0.2412],\n",
      "        [0.2449],\n",
      "        [0.2456],\n",
      "        [0.2500],\n",
      "        [0.2617],\n",
      "        [0.2659],\n",
      "        [0.2817],\n",
      "        [0.2852],\n",
      "        [0.2877],\n",
      "        [0.2905],\n",
      "        [0.3253],\n",
      "        [0.3326],\n",
      "        [0.3349],\n",
      "        [0.3618],\n",
      "        [0.3651],\n",
      "        [0.3686],\n",
      "        [0.3719],\n",
      "        [0.3899],\n",
      "        [0.3635],\n",
      "        [0.3610],\n",
      "        [0.3552],\n",
      "        [0.3191],\n",
      "        [0.2981],\n",
      "        [0.3606],\n",
      "        [0.3725],\n",
      "        [0.3906],\n",
      "        [0.3936],\n",
      "        [0.4346]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching的第10000回合\n",
      "Matching finished - the network is Unacceptable\n",
      "Number of enlarge: 6620\n",
      "Number of shrink: 3380\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.1058],\n",
      "        [0.1063],\n",
      "        [0.1065],\n",
      "        [0.1070],\n",
      "        [0.1084],\n",
      "        [0.1026],\n",
      "        [0.1103],\n",
      "        [0.0965],\n",
      "        [0.0947],\n",
      "        [0.0927],\n",
      "        [0.1190],\n",
      "        [0.1193],\n",
      "        [0.0913],\n",
      "        [0.0903],\n",
      "        [0.0889],\n",
      "        [0.0885],\n",
      "        [0.1240],\n",
      "        [0.1245],\n",
      "        [0.1267],\n",
      "        [0.0837],\n",
      "        [0.0833],\n",
      "        [0.0826],\n",
      "        [0.0808],\n",
      "        [0.0806],\n",
      "        [0.1309],\n",
      "        [0.0779],\n",
      "        [0.0759],\n",
      "        [0.0751],\n",
      "        [0.1366],\n",
      "        [0.1380],\n",
      "        [0.1387],\n",
      "        [0.0712],\n",
      "        [0.0709],\n",
      "        [0.0689],\n",
      "        [0.1423],\n",
      "        [0.0687],\n",
      "        [0.1439],\n",
      "        [0.0660],\n",
      "        [0.0644],\n",
      "        [0.0639],\n",
      "        [0.0616],\n",
      "        [0.1507],\n",
      "        [0.1512],\n",
      "        [0.1523],\n",
      "        [0.1530],\n",
      "        [0.0581],\n",
      "        [0.0578],\n",
      "        [0.1551],\n",
      "        [0.0546],\n",
      "        [0.0543],\n",
      "        [0.1591],\n",
      "        [0.1634],\n",
      "        [0.1641],\n",
      "        [0.1691],\n",
      "        [0.0380],\n",
      "        [0.0361],\n",
      "        [0.0324],\n",
      "        [0.1815],\n",
      "        [0.1828],\n",
      "        [0.0280],\n",
      "        [0.1928],\n",
      "        [0.1935],\n",
      "        [0.1936],\n",
      "        [0.0142],\n",
      "        [0.0111],\n",
      "        [0.0084],\n",
      "        [0.0063],\n",
      "        [0.2059],\n",
      "        [0.2079],\n",
      "        [0.0031],\n",
      "        [0.0013],\n",
      "        [0.0047],\n",
      "        [0.0121],\n",
      "        [0.0161],\n",
      "        [0.0187],\n",
      "        [0.0197],\n",
      "        [0.0262],\n",
      "        [0.0262],\n",
      "        [0.0297],\n",
      "        [0.2409],\n",
      "        [0.0413],\n",
      "        [0.2640],\n",
      "        [0.0573],\n",
      "        [0.0655],\n",
      "        [0.2828],\n",
      "        [0.2830],\n",
      "        [0.0787],\n",
      "        [0.0808],\n",
      "        [0.0818],\n",
      "        [0.0881],\n",
      "        [0.0915],\n",
      "        [0.0958],\n",
      "        [0.3090],\n",
      "        [0.1005],\n",
      "        [0.1016],\n",
      "        [0.3147],\n",
      "        [0.3217],\n",
      "        [0.1123],\n",
      "        [0.1162],\n",
      "        [0.3306],\n",
      "        [0.3308],\n",
      "        [0.3399],\n",
      "        [0.1290],\n",
      "        [0.1322],\n",
      "        [0.3532],\n",
      "        [0.3569],\n",
      "        [0.1537],\n",
      "        [0.3673],\n",
      "        [0.3676],\n",
      "        [0.1616],\n",
      "        [0.3828],\n",
      "        [0.1882],\n",
      "        [0.1892],\n",
      "        [0.4026],\n",
      "        [0.1940],\n",
      "        [0.1941],\n",
      "        [0.1947],\n",
      "        [0.1948],\n",
      "        [0.4067],\n",
      "        [0.1984],\n",
      "        [0.2263],\n",
      "        [0.2274],\n",
      "        [0.2335],\n",
      "        [0.2385],\n",
      "        [0.2412],\n",
      "        [0.2449],\n",
      "        [0.2456],\n",
      "        [0.2500],\n",
      "        [0.2617],\n",
      "        [0.2659],\n",
      "        [0.2817],\n",
      "        [0.2852],\n",
      "        [0.2877],\n",
      "        [0.2905],\n",
      "        [0.3253],\n",
      "        [0.3326],\n",
      "        [0.3349],\n",
      "        [0.3618],\n",
      "        [0.3651],\n",
      "        [0.3686],\n",
      "        [0.3719],\n",
      "        [0.3899],\n",
      "        [0.3635],\n",
      "        [0.3610],\n",
      "        [0.3552],\n",
      "        [0.3191],\n",
      "        [0.2981],\n",
      "        [0.3606],\n",
      "        [0.3725],\n",
      "        [0.3906],\n",
      "        [0.3936],\n",
      "        [0.4346]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Cramming module>>\n",
      "threshold_for_error: 0.43\n",
      "不滿足個數： 1\n",
      "The index of the undesired data: tensor([[151,   0]], device='cuda:0')\n",
      "Cramming success!\n",
      "<<Cramming後看一下差異>>\n",
      "tensor([[    0.1059],\n",
      "        [    0.1064],\n",
      "        [    0.1066],\n",
      "        [    0.1071],\n",
      "        [    0.1084],\n",
      "        [    0.1027],\n",
      "        [    0.1105],\n",
      "        [    0.0965],\n",
      "        [    0.0948],\n",
      "        [    0.0928],\n",
      "        [    0.1192],\n",
      "        [    0.1194],\n",
      "        [    0.0914],\n",
      "        [    0.0904],\n",
      "        [    0.0889],\n",
      "        [    0.0885],\n",
      "        [    0.1242],\n",
      "        [    0.1245],\n",
      "        [    0.1268],\n",
      "        [    0.0838],\n",
      "        [    0.0834],\n",
      "        [    0.0828],\n",
      "        [    0.0809],\n",
      "        [    0.0807],\n",
      "        [    0.1310],\n",
      "        [    0.0779],\n",
      "        [    0.0761],\n",
      "        [    0.0752],\n",
      "        [    0.1367],\n",
      "        [    0.1381],\n",
      "        [    0.1388],\n",
      "        [    0.0713],\n",
      "        [    0.0710],\n",
      "        [    0.0690],\n",
      "        [    0.1424],\n",
      "        [    0.0687],\n",
      "        [    0.1440],\n",
      "        [    0.0662],\n",
      "        [    0.0645],\n",
      "        [    0.0639],\n",
      "        [    0.0618],\n",
      "        [    0.1508],\n",
      "        [    0.1512],\n",
      "        [    0.1524],\n",
      "        [    0.1530],\n",
      "        [    0.0581],\n",
      "        [    0.0579],\n",
      "        [    0.1552],\n",
      "        [    0.0546],\n",
      "        [    0.0543],\n",
      "        [    0.1591],\n",
      "        [    0.1635],\n",
      "        [    0.1640],\n",
      "        [    0.1692],\n",
      "        [    0.0381],\n",
      "        [    0.0362],\n",
      "        [    0.0326],\n",
      "        [    0.1817],\n",
      "        [    0.1829],\n",
      "        [    0.0281],\n",
      "        [    0.1929],\n",
      "        [    0.1936],\n",
      "        [    0.1936],\n",
      "        [    0.0143],\n",
      "        [    0.0111],\n",
      "        [    0.0084],\n",
      "        [    0.0064],\n",
      "        [    0.2060],\n",
      "        [    0.2080],\n",
      "        [    0.0032],\n",
      "        [    0.0012],\n",
      "        [    0.0046],\n",
      "        [    0.0120],\n",
      "        [    0.0160],\n",
      "        [    0.0186],\n",
      "        [    0.0197],\n",
      "        [    0.0261],\n",
      "        [    0.0262],\n",
      "        [    0.0296],\n",
      "        [    0.2412],\n",
      "        [    0.0412],\n",
      "        [    0.2642],\n",
      "        [    0.0573],\n",
      "        [    0.0655],\n",
      "        [    0.2829],\n",
      "        [    0.2830],\n",
      "        [    0.0787],\n",
      "        [    0.0808],\n",
      "        [    0.0816],\n",
      "        [    0.0882],\n",
      "        [    0.0914],\n",
      "        [    0.0957],\n",
      "        [    0.3090],\n",
      "        [    0.1005],\n",
      "        [    0.1016],\n",
      "        [    0.3147],\n",
      "        [    0.3218],\n",
      "        [    0.1123],\n",
      "        [    0.1162],\n",
      "        [    0.3307],\n",
      "        [    0.3309],\n",
      "        [    0.3399],\n",
      "        [    0.1289],\n",
      "        [    0.1321],\n",
      "        [    0.3533],\n",
      "        [    0.3570],\n",
      "        [    0.1537],\n",
      "        [    0.3675],\n",
      "        [    0.3678],\n",
      "        [    0.1616],\n",
      "        [    0.3829],\n",
      "        [    0.1882],\n",
      "        [    0.1893],\n",
      "        [    0.4027],\n",
      "        [    0.1940],\n",
      "        [    0.1941],\n",
      "        [    0.1946],\n",
      "        [    0.1948],\n",
      "        [    0.4068],\n",
      "        [    0.1984],\n",
      "        [    0.2263],\n",
      "        [    0.2274],\n",
      "        [    0.2335],\n",
      "        [    0.2385],\n",
      "        [    0.2412],\n",
      "        [    0.2449],\n",
      "        [    0.2456],\n",
      "        [    0.2500],\n",
      "        [    0.2617],\n",
      "        [    0.2659],\n",
      "        [    0.2817],\n",
      "        [    0.2852],\n",
      "        [    0.2877],\n",
      "        [    0.2905],\n",
      "        [    0.3254],\n",
      "        [    0.3326],\n",
      "        [    0.3349],\n",
      "        [    0.3618],\n",
      "        [    0.3651],\n",
      "        [    0.3686],\n",
      "        [    0.3719],\n",
      "        [    0.3899],\n",
      "        [    0.3635],\n",
      "        [    0.3610],\n",
      "        [    0.3552],\n",
      "        [    0.3191],\n",
      "        [    0.2981],\n",
      "        [    0.3606],\n",
      "        [    0.3725],\n",
      "        [    0.3906],\n",
      "        [    0.3936],\n",
      "        [    0.0000]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 64\n",
      "Number of shrink: 36\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.1334],\n",
      "        [    0.0059],\n",
      "        [    0.0036],\n",
      "        [    0.0064],\n",
      "        [    0.0194],\n",
      "        [    0.0095],\n",
      "        [    0.0734],\n",
      "        [    0.1400],\n",
      "        [    0.0063],\n",
      "        [    0.0135],\n",
      "        [    0.0861],\n",
      "        [    0.0520],\n",
      "        [    0.0506],\n",
      "        [    0.0607],\n",
      "        [    0.0096],\n",
      "        [    0.0166],\n",
      "        [    0.0835],\n",
      "        [    0.0090],\n",
      "        [    0.0417],\n",
      "        [    0.0138],\n",
      "        [    0.0118],\n",
      "        [    0.0228],\n",
      "        [    0.0602],\n",
      "        [    0.0528],\n",
      "        [    0.0227],\n",
      "        [    0.0423],\n",
      "        [    0.0405],\n",
      "        [    0.0489],\n",
      "        [    0.0702],\n",
      "        [    0.0699],\n",
      "        [    0.0461],\n",
      "        [    0.0608],\n",
      "        [    0.0395],\n",
      "        [    0.0445],\n",
      "        [    0.0377],\n",
      "        [    0.1093],\n",
      "        [    0.0698],\n",
      "        [    0.0247],\n",
      "        [    0.0418],\n",
      "        [    0.0178],\n",
      "        [    0.0967],\n",
      "        [    0.0525],\n",
      "        [    0.0913],\n",
      "        [    0.0439],\n",
      "        [    0.0613],\n",
      "        [    0.0218],\n",
      "        [    0.0496],\n",
      "        [    0.0530],\n",
      "        [    0.0367],\n",
      "        [    0.0097],\n",
      "        [    0.0678],\n",
      "        [    0.0596],\n",
      "        [    0.0997],\n",
      "        [    0.0016],\n",
      "        [    0.0295],\n",
      "        [    0.0325],\n",
      "        [    0.0258],\n",
      "        [    0.1033],\n",
      "        [    0.1063],\n",
      "        [    0.0009],\n",
      "        [    0.0354],\n",
      "        [    0.0449],\n",
      "        [    0.0105],\n",
      "        [    0.0514],\n",
      "        [    0.0170],\n",
      "        [    0.0003],\n",
      "        [    0.0063],\n",
      "        [    0.0400],\n",
      "        [    0.0032],\n",
      "        [    0.0109],\n",
      "        [    0.0086],\n",
      "        [    0.0188],\n",
      "        [    0.0299],\n",
      "        [    0.0351],\n",
      "        [    0.0441],\n",
      "        [    0.0556],\n",
      "        [    0.0409],\n",
      "        [    0.0132],\n",
      "        [    0.0463],\n",
      "        [    0.0261],\n",
      "        [    0.0762],\n",
      "        [    0.0150],\n",
      "        [    0.0983],\n",
      "        [    0.0051],\n",
      "        [    0.0026],\n",
      "        [    0.0060],\n",
      "        [    0.0100],\n",
      "        [    0.0135],\n",
      "        [    0.0179],\n",
      "        [    0.0146],\n",
      "        [    0.2872],\n",
      "        [    0.1290],\n",
      "        [    0.0095],\n",
      "        [    0.1293],\n",
      "        [    0.1514],\n",
      "        [    0.0460],\n",
      "        [    0.0222],\n",
      "        [    0.0075],\n",
      "        [    0.1207],\n",
      "        [    0.0433],\n",
      "        [    0.0547],\n",
      "        [    0.0685],\n",
      "        [    0.1769],\n",
      "        [    0.1841],\n",
      "        [    0.0637],\n",
      "        [    0.0713],\n",
      "        [    0.0842],\n",
      "        [    0.0885],\n",
      "        [    0.0922],\n",
      "        [    0.1011],\n",
      "        [    0.1029],\n",
      "        [    0.0539],\n",
      "        [    0.0533],\n",
      "        [    0.1326],\n",
      "        [    0.0415],\n",
      "        [    0.0445],\n",
      "        [    0.0446],\n",
      "        [    0.0429],\n",
      "        [    0.1333],\n",
      "        [    0.0424],\n",
      "        [    0.1179],\n",
      "        [    0.0021],\n",
      "        [    0.0287],\n",
      "        [    0.0027],\n",
      "        [    0.0006],\n",
      "        [    0.0303],\n",
      "        [    0.0155],\n",
      "        [    0.0503],\n",
      "        [    0.0880],\n",
      "        [    0.1342],\n",
      "        [    0.0521],\n",
      "        [    0.1297],\n",
      "        [    0.1453],\n",
      "        [    0.0444],\n",
      "        [    0.1902],\n",
      "        [    0.0765],\n",
      "        [    0.0264],\n",
      "        [    0.1011],\n",
      "        [    0.0950],\n",
      "        [    0.0758],\n",
      "        [    0.0924],\n",
      "        [    0.1287],\n",
      "        [    0.0117],\n",
      "        [    0.0543],\n",
      "        [    0.3551],\n",
      "        [    0.0040],\n",
      "        [    0.0310],\n",
      "        [    0.0166],\n",
      "        [    0.0269],\n",
      "        [    0.0621],\n",
      "        [    0.0514],\n",
      "        [    0.0837]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 67.34817743301392\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 153\n",
      "剩餘X 資料 torch.Size([7, 18])\n",
      "剩餘Y 資料 torch.Size([7, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.04260881245136261, 2)\n",
      "The second_loss value of k: (0.055775731801986694, 1)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.0271])\n",
      "目前模型的Data狀態 torch.Size([153, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.7919],\n",
      "        [0.6531],\n",
      "        [0.6628],\n",
      "        [0.6661],\n",
      "        [0.6417],\n",
      "        [0.6648],\n",
      "        [0.7365],\n",
      "        [0.7892],\n",
      "        [0.6538],\n",
      "        [0.6319],\n",
      "        [0.5856],\n",
      "        [0.6200],\n",
      "        [0.5934],\n",
      "        [0.5822],\n",
      "        [0.6320],\n",
      "        [0.6578],\n",
      "        [0.5932],\n",
      "        [0.6682],\n",
      "        [0.6377],\n",
      "        [0.6226],\n",
      "        [0.6242],\n",
      "        [0.6582],\n",
      "        [0.6937],\n",
      "        [0.5806],\n",
      "        [0.6608],\n",
      "        [0.5883],\n",
      "        [0.5881],\n",
      "        [0.5789],\n",
      "        [0.6190],\n",
      "        [0.6209],\n",
      "        [0.6453],\n",
      "        [0.5631],\n",
      "        [0.6630],\n",
      "        [0.5771],\n",
      "        [0.6574],\n",
      "        [0.7306],\n",
      "        [0.6268],\n",
      "        [0.5941],\n",
      "        [0.5753],\n",
      "        [0.5988],\n",
      "        [0.7110],\n",
      "        [0.6508],\n",
      "        [0.7952],\n",
      "        [0.6611],\n",
      "        [0.6443],\n",
      "        [0.5890],\n",
      "        [0.5609],\n",
      "        [0.6548],\n",
      "        [0.5705],\n",
      "        [0.5973],\n",
      "        [0.6440],\n",
      "        [0.6565],\n",
      "        [0.6170],\n",
      "        [0.7202],\n",
      "        [0.5612],\n",
      "        [0.6213],\n",
      "        [0.5593],\n",
      "        [0.6309],\n",
      "        [0.6292],\n",
      "        [0.5798],\n",
      "        [0.7809],\n",
      "        [0.7910],\n",
      "        [0.7568],\n",
      "        [0.6183],\n",
      "        [0.5808],\n",
      "        [0.5614],\n",
      "        [0.5527],\n",
      "        [0.7186],\n",
      "        [0.7638],\n",
      "        [0.5449],\n",
      "        [0.5600],\n",
      "        [0.5292],\n",
      "        [0.5107],\n",
      "        [0.5015],\n",
      "        [0.4899],\n",
      "        [0.4773],\n",
      "        [0.5675],\n",
      "        [0.5133],\n",
      "        [0.4767],\n",
      "        [0.8197],\n",
      "        [0.5876],\n",
      "        [0.8016],\n",
      "        [0.5936],\n",
      "        [0.4923],\n",
      "        [0.8329],\n",
      "        [0.8417],\n",
      "        [0.4840],\n",
      "        [0.4584],\n",
      "        [0.4888],\n",
      "        [0.4500],\n",
      "        [0.7483],\n",
      "        [0.5858],\n",
      "        [0.8522],\n",
      "        [0.3229],\n",
      "        [0.2996],\n",
      "        [0.8214],\n",
      "        [0.8522],\n",
      "        [0.4479],\n",
      "        [0.3158],\n",
      "        [0.8400],\n",
      "        [0.8287],\n",
      "        [0.8241],\n",
      "        [0.6006],\n",
      "        [0.6046],\n",
      "        [0.8422],\n",
      "        [0.8383],\n",
      "        [0.3147],\n",
      "        [0.8314],\n",
      "        [0.8281],\n",
      "        [0.2900],\n",
      "        [0.8325],\n",
      "        [0.3106],\n",
      "        [0.3101],\n",
      "        [0.8226],\n",
      "        [0.3172],\n",
      "        [0.3140],\n",
      "        [0.3134],\n",
      "        [0.3150],\n",
      "        [0.8261],\n",
      "        [0.3119],\n",
      "        [0.4443],\n",
      "        [0.3231],\n",
      "        [0.3479],\n",
      "        [0.3169],\n",
      "        [0.3108],\n",
      "        [0.3381],\n",
      "        [0.3226],\n",
      "        [0.3530],\n",
      "        [0.3790],\n",
      "        [0.4210],\n",
      "        [0.3231],\n",
      "        [0.3972],\n",
      "        [0.4102],\n",
      "        [0.3066],\n",
      "        [0.4176],\n",
      "        [0.2967],\n",
      "        [0.1914],\n",
      "        [0.2919],\n",
      "        [0.2826],\n",
      "        [0.2599],\n",
      "        [0.2732],\n",
      "        [0.2914],\n",
      "        [0.2009],\n",
      "        [0.2460],\n",
      "        [0.5526],\n",
      "        [0.2296],\n",
      "        [0.2236],\n",
      "        [0.2087],\n",
      "        [0.2072],\n",
      "        [0.2241],\n",
      "        [0.2105],\n",
      "        [0.0343],\n",
      "        [0.2334]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.1334],\n",
      "        [    0.0059],\n",
      "        [    0.0036],\n",
      "        [    0.0064],\n",
      "        [    0.0194],\n",
      "        [    0.0095],\n",
      "        [    0.0734],\n",
      "        [    0.1400],\n",
      "        [    0.0063],\n",
      "        [    0.0135],\n",
      "        [    0.0861],\n",
      "        [    0.0520],\n",
      "        [    0.0506],\n",
      "        [    0.0607],\n",
      "        [    0.0096],\n",
      "        [    0.0166],\n",
      "        [    0.0835],\n",
      "        [    0.0090],\n",
      "        [    0.0417],\n",
      "        [    0.0138],\n",
      "        [    0.0118],\n",
      "        [    0.0228],\n",
      "        [    0.0602],\n",
      "        [    0.0528],\n",
      "        [    0.0227],\n",
      "        [    0.0423],\n",
      "        [    0.0405],\n",
      "        [    0.0489],\n",
      "        [    0.0702],\n",
      "        [    0.0699],\n",
      "        [    0.0461],\n",
      "        [    0.0608],\n",
      "        [    0.0395],\n",
      "        [    0.0445],\n",
      "        [    0.0377],\n",
      "        [    0.1093],\n",
      "        [    0.0698],\n",
      "        [    0.0247],\n",
      "        [    0.0418],\n",
      "        [    0.0178],\n",
      "        [    0.0967],\n",
      "        [    0.0525],\n",
      "        [    0.0913],\n",
      "        [    0.0439],\n",
      "        [    0.0613],\n",
      "        [    0.0218],\n",
      "        [    0.0496],\n",
      "        [    0.0530],\n",
      "        [    0.0367],\n",
      "        [    0.0097],\n",
      "        [    0.0678],\n",
      "        [    0.0596],\n",
      "        [    0.0997],\n",
      "        [    0.0016],\n",
      "        [    0.0295],\n",
      "        [    0.0325],\n",
      "        [    0.0258],\n",
      "        [    0.1033],\n",
      "        [    0.1063],\n",
      "        [    0.0009],\n",
      "        [    0.0354],\n",
      "        [    0.0449],\n",
      "        [    0.0105],\n",
      "        [    0.0514],\n",
      "        [    0.0170],\n",
      "        [    0.0003],\n",
      "        [    0.0063],\n",
      "        [    0.0400],\n",
      "        [    0.0032],\n",
      "        [    0.0109],\n",
      "        [    0.0086],\n",
      "        [    0.0188],\n",
      "        [    0.0299],\n",
      "        [    0.0351],\n",
      "        [    0.0441],\n",
      "        [    0.0556],\n",
      "        [    0.0409],\n",
      "        [    0.0132],\n",
      "        [    0.0463],\n",
      "        [    0.0261],\n",
      "        [    0.0762],\n",
      "        [    0.0150],\n",
      "        [    0.0983],\n",
      "        [    0.0051],\n",
      "        [    0.0026],\n",
      "        [    0.0060],\n",
      "        [    0.0100],\n",
      "        [    0.0135],\n",
      "        [    0.0179],\n",
      "        [    0.0146],\n",
      "        [    0.2872],\n",
      "        [    0.1290],\n",
      "        [    0.0095],\n",
      "        [    0.1293],\n",
      "        [    0.1514],\n",
      "        [    0.0460],\n",
      "        [    0.0222],\n",
      "        [    0.0075],\n",
      "        [    0.1207],\n",
      "        [    0.0433],\n",
      "        [    0.0547],\n",
      "        [    0.0685],\n",
      "        [    0.1769],\n",
      "        [    0.1841],\n",
      "        [    0.0637],\n",
      "        [    0.0713],\n",
      "        [    0.0842],\n",
      "        [    0.0885],\n",
      "        [    0.0922],\n",
      "        [    0.1011],\n",
      "        [    0.1029],\n",
      "        [    0.0539],\n",
      "        [    0.0533],\n",
      "        [    0.1326],\n",
      "        [    0.0415],\n",
      "        [    0.0445],\n",
      "        [    0.0446],\n",
      "        [    0.0429],\n",
      "        [    0.1333],\n",
      "        [    0.0424],\n",
      "        [    0.1179],\n",
      "        [    0.0021],\n",
      "        [    0.0287],\n",
      "        [    0.0027],\n",
      "        [    0.0006],\n",
      "        [    0.0303],\n",
      "        [    0.0155],\n",
      "        [    0.0503],\n",
      "        [    0.0880],\n",
      "        [    0.1342],\n",
      "        [    0.0521],\n",
      "        [    0.1297],\n",
      "        [    0.1453],\n",
      "        [    0.0444],\n",
      "        [    0.1902],\n",
      "        [    0.0765],\n",
      "        [    0.0264],\n",
      "        [    0.1011],\n",
      "        [    0.0950],\n",
      "        [    0.0758],\n",
      "        [    0.0924],\n",
      "        [    0.1287],\n",
      "        [    0.0117],\n",
      "        [    0.0543],\n",
      "        [    0.3551],\n",
      "        [    0.0040],\n",
      "        [    0.0310],\n",
      "        [    0.0166],\n",
      "        [    0.0269],\n",
      "        [    0.0621],\n",
      "        [    0.0514],\n",
      "        [    0.0837],\n",
      "        [    0.2063]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 42\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.1430],\n",
      "        [0.0060],\n",
      "        [0.0042],\n",
      "        [0.0059],\n",
      "        [0.0209],\n",
      "        [0.0097],\n",
      "        [0.0779],\n",
      "        [0.1508],\n",
      "        [0.0064],\n",
      "        [0.0119],\n",
      "        [0.0771],\n",
      "        [0.0522],\n",
      "        [0.0409],\n",
      "        [0.0509],\n",
      "        [0.0084],\n",
      "        [0.0174],\n",
      "        [0.0747],\n",
      "        [0.0091],\n",
      "        [0.0419],\n",
      "        [0.0124],\n",
      "        [0.0103],\n",
      "        [0.0227],\n",
      "        [0.0610],\n",
      "        [0.0427],\n",
      "        [0.0242],\n",
      "        [0.0350],\n",
      "        [0.0309],\n",
      "        [0.0392],\n",
      "        [0.0713],\n",
      "        [0.0703],\n",
      "        [0.0471],\n",
      "        [0.0579],\n",
      "        [0.0394],\n",
      "        [0.0350],\n",
      "        [0.0396],\n",
      "        [0.1129],\n",
      "        [0.0709],\n",
      "        [0.0184],\n",
      "        [0.0376],\n",
      "        [0.0081],\n",
      "        [0.0987],\n",
      "        [0.0527],\n",
      "        [0.1060],\n",
      "        [0.0451],\n",
      "        [0.0626],\n",
      "        [0.0149],\n",
      "        [0.0486],\n",
      "        [0.0540],\n",
      "        [0.0278],\n",
      "        [0.0004],\n",
      "        [0.0679],\n",
      "        [0.0606],\n",
      "        [0.1021],\n",
      "        [0.0026],\n",
      "        [0.0210],\n",
      "        [0.0332],\n",
      "        [0.0241],\n",
      "        [0.1045],\n",
      "        [0.1077],\n",
      "        [0.0052],\n",
      "        [0.0439],\n",
      "        [0.0540],\n",
      "        [0.0160],\n",
      "        [0.0533],\n",
      "        [0.0232],\n",
      "        [0.0070],\n",
      "        [0.0005],\n",
      "        [0.0351],\n",
      "        [0.0093],\n",
      "        [0.0046],\n",
      "        [0.0092],\n",
      "        [0.0136],\n",
      "        [0.0268],\n",
      "        [0.0327],\n",
      "        [0.0423],\n",
      "        [0.0542],\n",
      "        [0.0397],\n",
      "        [0.0097],\n",
      "        [0.0451],\n",
      "        [0.0431],\n",
      "        [0.0745],\n",
      "        [0.0013],\n",
      "        [0.0969],\n",
      "        [0.0065],\n",
      "        [0.0150],\n",
      "        [0.0230],\n",
      "        [0.0114],\n",
      "        [0.0142],\n",
      "        [0.0195],\n",
      "        [0.0163],\n",
      "        [0.2921],\n",
      "        [0.1268],\n",
      "        [0.0080],\n",
      "        [0.1436],\n",
      "        [0.1665],\n",
      "        [0.0287],\n",
      "        [0.0047],\n",
      "        [0.0060],\n",
      "        [0.1355],\n",
      "        [0.0264],\n",
      "        [0.0362],\n",
      "        [0.0508],\n",
      "        [0.1774],\n",
      "        [0.1848],\n",
      "        [0.0446],\n",
      "        [0.0530],\n",
      "        [0.0982],\n",
      "        [0.0696],\n",
      "        [0.0743],\n",
      "        [0.1155],\n",
      "        [0.0839],\n",
      "        [0.0677],\n",
      "        [0.0663],\n",
      "        [0.1149],\n",
      "        [0.0545],\n",
      "        [0.0573],\n",
      "        [0.0573],\n",
      "        [0.0545],\n",
      "        [0.1153],\n",
      "        [0.0563],\n",
      "        [0.1167],\n",
      "        [0.0160],\n",
      "        [0.0174],\n",
      "        [0.0109],\n",
      "        [0.0143],\n",
      "        [0.0186],\n",
      "        [0.0036],\n",
      "        [0.0399],\n",
      "        [0.0801],\n",
      "        [0.1305],\n",
      "        [0.0380],\n",
      "        [0.1242],\n",
      "        [0.1409],\n",
      "        [0.0290],\n",
      "        [0.1861],\n",
      "        [0.0608],\n",
      "        [0.0494],\n",
      "        [0.0854],\n",
      "        [0.0782],\n",
      "        [0.0570],\n",
      "        [0.0746],\n",
      "        [0.1121],\n",
      "        [0.0096],\n",
      "        [0.0356],\n",
      "        [0.3551],\n",
      "        [0.0237],\n",
      "        [0.0521],\n",
      "        [0.0058],\n",
      "        [0.0046],\n",
      "        [0.0408],\n",
      "        [0.0306],\n",
      "        [0.0857],\n",
      "        [0.1859]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 67.52716541290283\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 3 個區塊累積花費時間(s) 27.194713354110718\n",
      "<<The performance of 3 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 27.194713354110718\n",
      "<<The percentage of each step>>\n",
      "Step 4: 92.31%\n",
      "Step 6.1: 3.85%\n",
      "Step 6.2: 3.85%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 1\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 1411.95\n",
      "The MAPE for l = 1: 0.03%\n",
      "The RMSE for l = 1: 1961.49\n",
      "The accuracy(2000) for l = 1: 79.08%\n",
      "The accuracy(3000) for l = 1: 90.20%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in toutlier>>\n",
      "The MAE for l = 1: 11553.58\n",
      "The MAPE for l = 1: 0.32%\n",
      "The RMSE for l = 1: 11873.15\n",
      "The accuracy(2000) for l = 1: 0.00%\n",
      "The accuracy(3000) for l = 1: 0.00%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 12310.4\n",
      "The MAPE for l = 1: 0.3%\n",
      "The RMSE for l = 1: 12337.8\n",
      "The accuracy(2000) for l = 1: 0.0%\n",
      "The accuracy(3000) for l = 1: 0.0%\n",
      "------------------------------------------------------------\n",
      "0.7908496732026143\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "The <<4>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (4.896619998362439e-07, 66)\n",
      "The second_loss value of k: (6.603995643672533e-07, 60)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [66, 60, 91, 25, 68, 123, 52, 122, 81, 26, 21, 77, 23, 67, 61, 32, 20, 70, 5, 120, 44, 35, 104, 33, 78, 34, 69, 79, 105, 50, 106, 80, 89, 24, 49, 90, 76, 7, 65, 22, 51, 116, 19, 46, 71, 64, 108, 119, 59, 72, 36, 57, 53, 115, 48, 107, 63, 18, 43, 88, 6, 62, 124, 27, 73, 58, 75, 17, 13, 45, 121, 56, 117, 31, 37, 12, 16, 74, 92, 94, 103, 114, 93, 95, 47, 109, 4, 15, 14, 96, 11, 102, 29, 28, 30, 42, 54, 113, 0, 55, 112, 87, 110, 125, 41, 3, 101, 8, 10, 9, 111, 2, 97, 82, 86, 40, 83, 100, 85, 99, 98, 39, 38, 128, 84, 127, 1]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0004],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0046],\n",
      "        [0.0046],\n",
      "        [0.0052],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0060],\n",
      "        [0.0059],\n",
      "        [0.0065],\n",
      "        [0.0064],\n",
      "        [0.0070],\n",
      "        [0.0081],\n",
      "        [0.0084],\n",
      "        [0.0091],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0096],\n",
      "        [0.0092],\n",
      "        [0.0103],\n",
      "        [0.0109],\n",
      "        [0.0119],\n",
      "        [0.0114],\n",
      "        [0.0124],\n",
      "        [0.0136],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0149],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0195],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0227],\n",
      "        [0.0232],\n",
      "        [0.0237],\n",
      "        [0.0242],\n",
      "        [0.0241],\n",
      "        [0.0268],\n",
      "        [0.0278],\n",
      "        [0.0290],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0327],\n",
      "        [0.0332],\n",
      "        [0.0350],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0376],\n",
      "        [0.0380],\n",
      "        [0.0392],\n",
      "        [0.0396],\n",
      "        [0.0397],\n",
      "        [0.0399],\n",
      "        [0.0394],\n",
      "        [0.0409],\n",
      "        [0.0408],\n",
      "        [0.0419],\n",
      "        [0.0423],\n",
      "        [0.0427],\n",
      "        [0.0451],\n",
      "        [0.0451],\n",
      "        [0.0471],\n",
      "        [0.0486],\n",
      "        [0.0494],\n",
      "        [0.0509],\n",
      "        [0.0521],\n",
      "        [0.0522],\n",
      "        [0.0533],\n",
      "        [0.0527],\n",
      "        [0.0540],\n",
      "        [0.0542],\n",
      "        [0.0545],\n",
      "        [0.0545],\n",
      "        [0.0563],\n",
      "        [0.0570],\n",
      "        [0.0573],\n",
      "        [0.0573],\n",
      "        [0.0579],\n",
      "        [0.0608],\n",
      "        [0.0610],\n",
      "        [0.0606],\n",
      "        [0.0626],\n",
      "        [0.0663],\n",
      "        [0.0679],\n",
      "        [0.0677],\n",
      "        [0.0703],\n",
      "        [0.0709],\n",
      "        [0.0713],\n",
      "        [0.0745],\n",
      "        [0.0747],\n",
      "        [0.0746],\n",
      "        [0.0779],\n",
      "        [0.0771],\n",
      "        [0.0782],\n",
      "        [0.0801],\n",
      "        [0.0854],\n",
      "        [0.0857],\n",
      "        [0.0969],\n",
      "        [0.0987],\n",
      "        [0.0982],\n",
      "        [0.1021],\n",
      "        [0.1045],\n",
      "        [0.1077],\n",
      "        [0.1121],\n",
      "        [0.1129],\n",
      "        [0.1155],\n",
      "        [0.1167],\n",
      "        [0.1242],\n",
      "        [0.1268],\n",
      "        [0.1305],\n",
      "        [0.1355],\n",
      "        [0.1409],\n",
      "        [0.1436],\n",
      "        [0.1665],\n",
      "        [0.1774],\n",
      "        [0.1848],\n",
      "        [0.1859],\n",
      "        [0.1861],\n",
      "        [0.2158],\n",
      "        [0.2921]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 128\n",
      "剩餘X 資料 torch.Size([32, 18])\n",
      "剩餘Y 資料 torch.Size([32, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1260795295238495, 0)\n",
      "The second_loss value of k: (0.17819805443286896, 17)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引0，y= tensor([0.1975])\n",
      "目前模型的Data狀態 torch.Size([128, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5595],\n",
      "        [0.6074],\n",
      "        [0.3107],\n",
      "        [0.6634],\n",
      "        [0.5512],\n",
      "        [0.1848],\n",
      "        [0.5859],\n",
      "        [0.1863],\n",
      "        [0.4465],\n",
      "        [0.6530],\n",
      "        [0.6656],\n",
      "        [0.4938],\n",
      "        [0.6539],\n",
      "        [0.5681],\n",
      "        [0.6085],\n",
      "        [0.6332],\n",
      "        [0.6680],\n",
      "        [0.5168],\n",
      "        [0.6650],\n",
      "        [0.1796],\n",
      "        [0.5607],\n",
      "        [0.6257],\n",
      "        [0.3033],\n",
      "        [0.6335],\n",
      "        [0.4853],\n",
      "        [0.6240],\n",
      "        [0.5343],\n",
      "        [0.4577],\n",
      "        [0.2971],\n",
      "        [0.5958],\n",
      "        [0.3093],\n",
      "        [0.4483],\n",
      "        [0.3366],\n",
      "        [0.6586],\n",
      "        [0.6003],\n",
      "        [0.3264],\n",
      "        [0.4904],\n",
      "        [0.6401],\n",
      "        [0.5696],\n",
      "        [0.6580],\n",
      "        [0.5870],\n",
      "        [0.2099],\n",
      "        [0.6594],\n",
      "        [0.5610],\n",
      "        [0.5138],\n",
      "        [0.5795],\n",
      "        [0.2912],\n",
      "        [0.1897],\n",
      "        [0.5978],\n",
      "        [0.5040],\n",
      "        [0.6220],\n",
      "        [0.5865],\n",
      "        [0.5956],\n",
      "        [0.2273],\n",
      "        [0.5795],\n",
      "        [0.3090],\n",
      "        [0.5886],\n",
      "        [0.6555],\n",
      "        [0.5662],\n",
      "        [0.3426],\n",
      "        [0.6630],\n",
      "        [0.6030],\n",
      "        [0.2029],\n",
      "        [0.6375],\n",
      "        [0.4917],\n",
      "        [0.5906],\n",
      "        [0.4780],\n",
      "        [0.6599],\n",
      "        [0.6443],\n",
      "        [0.5619],\n",
      "        [0.1684],\n",
      "        [0.5921],\n",
      "        [0.2025],\n",
      "        [0.6199],\n",
      "        [0.6202],\n",
      "        [0.6506],\n",
      "        [0.6538],\n",
      "        [0.4787],\n",
      "        [0.3034],\n",
      "        [0.3042],\n",
      "        [0.2980],\n",
      "        [0.2411],\n",
      "        [0.3007],\n",
      "        [0.3013],\n",
      "        [0.5659],\n",
      "        [0.2810],\n",
      "        [0.6945],\n",
      "        [0.6555],\n",
      "        [0.6431],\n",
      "        [0.2972],\n",
      "        [0.6439],\n",
      "        [0.2968],\n",
      "        [0.6204],\n",
      "        [0.6257],\n",
      "        [0.6180],\n",
      "        [0.5858],\n",
      "        [0.6020],\n",
      "        [0.2554],\n",
      "        [0.7409],\n",
      "        [0.5946],\n",
      "        [0.2658],\n",
      "        [0.3711],\n",
      "        [0.2762],\n",
      "        [0.0324],\n",
      "        [0.5922],\n",
      "        [0.7130],\n",
      "        [0.3007],\n",
      "        [0.6147],\n",
      "        [0.6297],\n",
      "        [0.6278],\n",
      "        [0.2749],\n",
      "        [0.7342],\n",
      "        [0.2756],\n",
      "        [0.4431],\n",
      "        [0.3916],\n",
      "        [0.5837],\n",
      "        [0.4172],\n",
      "        [0.3010],\n",
      "        [0.4058],\n",
      "        [0.3086],\n",
      "        [0.2845],\n",
      "        [0.6011],\n",
      "        [0.6053],\n",
      "        [0.2129],\n",
      "        [0.4135],\n",
      "        [0.2158],\n",
      "        [0.7533],\n",
      "        [0.5526]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0004],\n",
      "        [0.0036],\n",
      "        [0.0042],\n",
      "        [0.0046],\n",
      "        [0.0046],\n",
      "        [0.0052],\n",
      "        [0.0058],\n",
      "        [0.0060],\n",
      "        [0.0060],\n",
      "        [0.0059],\n",
      "        [0.0065],\n",
      "        [0.0064],\n",
      "        [0.0070],\n",
      "        [0.0081],\n",
      "        [0.0084],\n",
      "        [0.0091],\n",
      "        [0.0097],\n",
      "        [0.0097],\n",
      "        [0.0096],\n",
      "        [0.0092],\n",
      "        [0.0103],\n",
      "        [0.0109],\n",
      "        [0.0119],\n",
      "        [0.0114],\n",
      "        [0.0124],\n",
      "        [0.0136],\n",
      "        [0.0142],\n",
      "        [0.0143],\n",
      "        [0.0149],\n",
      "        [0.0160],\n",
      "        [0.0163],\n",
      "        [0.0174],\n",
      "        [0.0174],\n",
      "        [0.0184],\n",
      "        [0.0186],\n",
      "        [0.0195],\n",
      "        [0.0209],\n",
      "        [0.0210],\n",
      "        [0.0227],\n",
      "        [0.0232],\n",
      "        [0.0237],\n",
      "        [0.0242],\n",
      "        [0.0241],\n",
      "        [0.0268],\n",
      "        [0.0278],\n",
      "        [0.0290],\n",
      "        [0.0306],\n",
      "        [0.0309],\n",
      "        [0.0327],\n",
      "        [0.0332],\n",
      "        [0.0350],\n",
      "        [0.0350],\n",
      "        [0.0356],\n",
      "        [0.0376],\n",
      "        [0.0380],\n",
      "        [0.0392],\n",
      "        [0.0396],\n",
      "        [0.0397],\n",
      "        [0.0399],\n",
      "        [0.0394],\n",
      "        [0.0409],\n",
      "        [0.0408],\n",
      "        [0.0419],\n",
      "        [0.0423],\n",
      "        [0.0427],\n",
      "        [0.0451],\n",
      "        [0.0451],\n",
      "        [0.0471],\n",
      "        [0.0486],\n",
      "        [0.0494],\n",
      "        [0.0509],\n",
      "        [0.0521],\n",
      "        [0.0522],\n",
      "        [0.0533],\n",
      "        [0.0527],\n",
      "        [0.0540],\n",
      "        [0.0542],\n",
      "        [0.0545],\n",
      "        [0.0545],\n",
      "        [0.0563],\n",
      "        [0.0570],\n",
      "        [0.0573],\n",
      "        [0.0573],\n",
      "        [0.0579],\n",
      "        [0.0608],\n",
      "        [0.0610],\n",
      "        [0.0606],\n",
      "        [0.0626],\n",
      "        [0.0663],\n",
      "        [0.0679],\n",
      "        [0.0677],\n",
      "        [0.0703],\n",
      "        [0.0709],\n",
      "        [0.0713],\n",
      "        [0.0745],\n",
      "        [0.0747],\n",
      "        [0.0746],\n",
      "        [0.0779],\n",
      "        [0.0771],\n",
      "        [0.0782],\n",
      "        [0.0801],\n",
      "        [0.0854],\n",
      "        [0.0857],\n",
      "        [0.0969],\n",
      "        [0.0987],\n",
      "        [0.0982],\n",
      "        [0.1021],\n",
      "        [0.1045],\n",
      "        [0.1077],\n",
      "        [0.1121],\n",
      "        [0.1129],\n",
      "        [0.1155],\n",
      "        [0.1167],\n",
      "        [0.1242],\n",
      "        [0.1268],\n",
      "        [0.1305],\n",
      "        [0.1355],\n",
      "        [0.1409],\n",
      "        [0.1436],\n",
      "        [0.1665],\n",
      "        [0.1774],\n",
      "        [0.1848],\n",
      "        [0.1859],\n",
      "        [0.1861],\n",
      "        [0.2158],\n",
      "        [0.2921],\n",
      "        [0.3551]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 62\n",
      "Number of shrink: 38\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0029],\n",
      "        [0.0075],\n",
      "        [0.0045],\n",
      "        [0.0023],\n",
      "        [0.0021],\n",
      "        [0.0057],\n",
      "        [0.0126],\n",
      "        [0.0115],\n",
      "        [0.0027],\n",
      "        [0.0118],\n",
      "        [0.0072],\n",
      "        [0.0043],\n",
      "        [0.0052],\n",
      "        [0.0095],\n",
      "        [0.0049],\n",
      "        [0.0120],\n",
      "        [0.0067],\n",
      "        [0.0074],\n",
      "        [0.0121],\n",
      "        [0.0100],\n",
      "        [0.0004],\n",
      "        [0.0148],\n",
      "        [0.0074],\n",
      "        [0.0133],\n",
      "        [0.0128],\n",
      "        [0.0142],\n",
      "        [0.0110],\n",
      "        [0.0132],\n",
      "        [0.0136],\n",
      "        [0.0053],\n",
      "        [0.0195],\n",
      "        [0.0181],\n",
      "        [0.0033],\n",
      "        [0.0161],\n",
      "        [0.0110],\n",
      "        [0.0028],\n",
      "        [0.0161],\n",
      "        [0.0050],\n",
      "        [0.0159],\n",
      "        [0.0206],\n",
      "        [0.0329],\n",
      "        [0.0202],\n",
      "        [0.0212],\n",
      "        [0.0236],\n",
      "        [0.0243],\n",
      "        [0.0201],\n",
      "        [0.0240],\n",
      "        [0.0321],\n",
      "        [0.0183],\n",
      "        [0.0310],\n",
      "        [0.0247],\n",
      "        [0.0212],\n",
      "        [0.0271],\n",
      "        [0.0358],\n",
      "        [0.0337],\n",
      "        [0.0319],\n",
      "        [0.0322],\n",
      "        [0.0381],\n",
      "        [0.0176],\n",
      "        [0.0158],\n",
      "        [0.0440],\n",
      "        [0.0374],\n",
      "        [0.0294],\n",
      "        [0.0472],\n",
      "        [0.0390],\n",
      "        [0.0269],\n",
      "        [0.0443],\n",
      "        [0.0448],\n",
      "        [0.0295],\n",
      "        [0.0520],\n",
      "        [0.0526],\n",
      "        [0.0408],\n",
      "        [0.0496],\n",
      "        [0.0528],\n",
      "        [0.0452],\n",
      "        [0.0326],\n",
      "        [0.0525],\n",
      "        [0.0527],\n",
      "        [0.0563],\n",
      "        [0.0562],\n",
      "        [0.0505],\n",
      "        [0.0535],\n",
      "        [0.0571],\n",
      "        [0.0587],\n",
      "        [0.0550],\n",
      "        [0.0578],\n",
      "        [0.0631],\n",
      "        [0.0532],\n",
      "        [0.0505],\n",
      "        [0.0633],\n",
      "        [0.0450],\n",
      "        [0.0636],\n",
      "        [0.0713],\n",
      "        [0.0744],\n",
      "        [0.0738],\n",
      "        [0.0522],\n",
      "        [0.0648],\n",
      "        [0.0705],\n",
      "        [0.0840],\n",
      "        [0.0692],\n",
      "        [0.0744],\n",
      "        [0.0609],\n",
      "        [0.0844],\n",
      "        [0.0771],\n",
      "        [0.0783],\n",
      "        [0.1041],\n",
      "        [0.0997],\n",
      "        [0.0754],\n",
      "        [0.0817],\n",
      "        [0.0841],\n",
      "        [0.1095],\n",
      "        [0.1198],\n",
      "        [0.1082],\n",
      "        [0.1167],\n",
      "        [0.1125],\n",
      "        [0.1100],\n",
      "        [0.1258],\n",
      "        [0.1376],\n",
      "        [0.1317],\n",
      "        [0.1448],\n",
      "        [0.1626],\n",
      "        [0.1686],\n",
      "        [0.1777],\n",
      "        [0.1698],\n",
      "        [0.1767],\n",
      "        [0.2030],\n",
      "        [0.2955],\n",
      "        [0.3551]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 68.01446318626404\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 129\n",
      "剩餘X 資料 torch.Size([31, 18])\n",
      "剩餘Y 資料 torch.Size([31, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.17817671597003937, 16)\n",
      "The second_loss value of k: (0.18626897037029266, 21)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.1305])\n",
      "目前模型的Data狀態 torch.Size([129, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5619],\n",
      "        [0.6145],\n",
      "        [0.3026],\n",
      "        [0.6568],\n",
      "        [0.5537],\n",
      "        [0.1745],\n",
      "        [0.5933],\n",
      "        [0.1806],\n",
      "        [0.4431],\n",
      "        [0.6472],\n",
      "        [0.6670],\n",
      "        [0.4915],\n",
      "        [0.6527],\n",
      "        [0.5705],\n",
      "        [0.6117],\n",
      "        [0.6295],\n",
      "        [0.6704],\n",
      "        [0.5190],\n",
      "        [0.6674],\n",
      "        [0.1792],\n",
      "        [0.5511],\n",
      "        [0.6212],\n",
      "        [0.3068],\n",
      "        [0.6321],\n",
      "        [0.4868],\n",
      "        [0.6222],\n",
      "        [0.5370],\n",
      "        [0.4587],\n",
      "        [0.2978],\n",
      "        [0.6055],\n",
      "        [0.3057],\n",
      "        [0.4464],\n",
      "        [0.3160],\n",
      "        [0.6573],\n",
      "        [0.6078],\n",
      "        [0.3106],\n",
      "        [0.4870],\n",
      "        [0.6560],\n",
      "        [0.5748],\n",
      "        [0.6560],\n",
      "        [0.5967],\n",
      "        [0.2134],\n",
      "        [0.6624],\n",
      "        [0.5616],\n",
      "        [0.5163],\n",
      "        [0.5871],\n",
      "        [0.2862],\n",
      "        [0.1912],\n",
      "        [0.6103],\n",
      "        [0.5056],\n",
      "        [0.6135],\n",
      "        [0.6004],\n",
      "        [0.6035],\n",
      "        [0.2275],\n",
      "        [0.5834],\n",
      "        [0.3029],\n",
      "        [0.5956],\n",
      "        [0.6569],\n",
      "        [0.5441],\n",
      "        [0.3185],\n",
      "        [0.6676],\n",
      "        [0.6065],\n",
      "        [0.1914],\n",
      "        [0.6322],\n",
      "        [0.4950],\n",
      "        [0.6065],\n",
      "        [0.4787],\n",
      "        [0.6602],\n",
      "        [0.6619],\n",
      "        [0.5585],\n",
      "        [0.1652],\n",
      "        [0.6021],\n",
      "        [0.2050],\n",
      "        [0.6192],\n",
      "        [0.6121],\n",
      "        [0.6708],\n",
      "        [0.6552],\n",
      "        [0.4802],\n",
      "        [0.3016],\n",
      "        [0.3025],\n",
      "        [0.3037],\n",
      "        [0.2376],\n",
      "        [0.3009],\n",
      "        [0.2999],\n",
      "        [0.5689],\n",
      "        [0.2779],\n",
      "        [0.6966],\n",
      "        [0.6629],\n",
      "        [0.6551],\n",
      "        [0.3002],\n",
      "        [0.6668],\n",
      "        [0.3008],\n",
      "        [0.6194],\n",
      "        [0.6222],\n",
      "        [0.6155],\n",
      "        [0.5636],\n",
      "        [0.6119],\n",
      "        [0.2513],\n",
      "        [0.7470],\n",
      "        [0.6025],\n",
      "        [0.2621],\n",
      "        [0.3518],\n",
      "        [0.2752],\n",
      "        [0.0410],\n",
      "        [0.5736],\n",
      "        [0.7185],\n",
      "        [0.2992],\n",
      "        [0.6414],\n",
      "        [0.6525],\n",
      "        [0.6513],\n",
      "        [0.2722],\n",
      "        [0.7412],\n",
      "        [0.2829],\n",
      "        [0.4430],\n",
      "        [0.3799],\n",
      "        [0.5669],\n",
      "        [0.4126],\n",
      "        [0.2990],\n",
      "        [0.3966],\n",
      "        [0.3073],\n",
      "        [0.2884],\n",
      "        [0.5923],\n",
      "        [0.5982],\n",
      "        [0.1969],\n",
      "        [0.4041],\n",
      "        [0.2030],\n",
      "        [0.7567],\n",
      "        [0.5526],\n",
      "        [0.5526]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0029],\n",
      "        [    0.0075],\n",
      "        [    0.0045],\n",
      "        [    0.0023],\n",
      "        [    0.0021],\n",
      "        [    0.0057],\n",
      "        [    0.0126],\n",
      "        [    0.0115],\n",
      "        [    0.0027],\n",
      "        [    0.0118],\n",
      "        [    0.0072],\n",
      "        [    0.0043],\n",
      "        [    0.0052],\n",
      "        [    0.0095],\n",
      "        [    0.0049],\n",
      "        [    0.0120],\n",
      "        [    0.0067],\n",
      "        [    0.0074],\n",
      "        [    0.0121],\n",
      "        [    0.0100],\n",
      "        [    0.0004],\n",
      "        [    0.0148],\n",
      "        [    0.0074],\n",
      "        [    0.0133],\n",
      "        [    0.0128],\n",
      "        [    0.0142],\n",
      "        [    0.0110],\n",
      "        [    0.0132],\n",
      "        [    0.0136],\n",
      "        [    0.0053],\n",
      "        [    0.0195],\n",
      "        [    0.0181],\n",
      "        [    0.0033],\n",
      "        [    0.0161],\n",
      "        [    0.0110],\n",
      "        [    0.0028],\n",
      "        [    0.0161],\n",
      "        [    0.0050],\n",
      "        [    0.0159],\n",
      "        [    0.0206],\n",
      "        [    0.0329],\n",
      "        [    0.0202],\n",
      "        [    0.0212],\n",
      "        [    0.0236],\n",
      "        [    0.0243],\n",
      "        [    0.0201],\n",
      "        [    0.0240],\n",
      "        [    0.0321],\n",
      "        [    0.0183],\n",
      "        [    0.0310],\n",
      "        [    0.0247],\n",
      "        [    0.0212],\n",
      "        [    0.0271],\n",
      "        [    0.0358],\n",
      "        [    0.0337],\n",
      "        [    0.0319],\n",
      "        [    0.0322],\n",
      "        [    0.0381],\n",
      "        [    0.0176],\n",
      "        [    0.0158],\n",
      "        [    0.0440],\n",
      "        [    0.0374],\n",
      "        [    0.0294],\n",
      "        [    0.0472],\n",
      "        [    0.0390],\n",
      "        [    0.0269],\n",
      "        [    0.0443],\n",
      "        [    0.0448],\n",
      "        [    0.0295],\n",
      "        [    0.0520],\n",
      "        [    0.0526],\n",
      "        [    0.0408],\n",
      "        [    0.0496],\n",
      "        [    0.0528],\n",
      "        [    0.0452],\n",
      "        [    0.0326],\n",
      "        [    0.0525],\n",
      "        [    0.0527],\n",
      "        [    0.0563],\n",
      "        [    0.0562],\n",
      "        [    0.0505],\n",
      "        [    0.0535],\n",
      "        [    0.0571],\n",
      "        [    0.0587],\n",
      "        [    0.0550],\n",
      "        [    0.0578],\n",
      "        [    0.0631],\n",
      "        [    0.0532],\n",
      "        [    0.0505],\n",
      "        [    0.0633],\n",
      "        [    0.0450],\n",
      "        [    0.0636],\n",
      "        [    0.0713],\n",
      "        [    0.0744],\n",
      "        [    0.0738],\n",
      "        [    0.0522],\n",
      "        [    0.0648],\n",
      "        [    0.0705],\n",
      "        [    0.0840],\n",
      "        [    0.0692],\n",
      "        [    0.0744],\n",
      "        [    0.0609],\n",
      "        [    0.0844],\n",
      "        [    0.0771],\n",
      "        [    0.0783],\n",
      "        [    0.1041],\n",
      "        [    0.0997],\n",
      "        [    0.0754],\n",
      "        [    0.0817],\n",
      "        [    0.0841],\n",
      "        [    0.1095],\n",
      "        [    0.1198],\n",
      "        [    0.1082],\n",
      "        [    0.1167],\n",
      "        [    0.1125],\n",
      "        [    0.1100],\n",
      "        [    0.1258],\n",
      "        [    0.1376],\n",
      "        [    0.1317],\n",
      "        [    0.1448],\n",
      "        [    0.1626],\n",
      "        [    0.1686],\n",
      "        [    0.1777],\n",
      "        [    0.1698],\n",
      "        [    0.1767],\n",
      "        [    0.2030],\n",
      "        [    0.2955],\n",
      "        [    0.3551],\n",
      "        [    0.4221]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 42\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0076],\n",
      "        [0.0086],\n",
      "        [0.0099],\n",
      "        [0.0046],\n",
      "        [0.0100],\n",
      "        [0.0122],\n",
      "        [0.0134],\n",
      "        [0.0021],\n",
      "        [0.0187],\n",
      "        [0.0026],\n",
      "        [0.0009],\n",
      "        [0.0005],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0190],\n",
      "        [0.0103],\n",
      "        [0.0101],\n",
      "        [0.0070],\n",
      "        [0.0085],\n",
      "        [0.0072],\n",
      "        [0.0218],\n",
      "        [0.0050],\n",
      "        [0.0189],\n",
      "        [0.0096],\n",
      "        [0.0196],\n",
      "        [0.0126],\n",
      "        [0.0163],\n",
      "        [0.0135],\n",
      "        [0.0047],\n",
      "        [0.0211],\n",
      "        [0.0227],\n",
      "        [0.0135],\n",
      "        [0.0106],\n",
      "        [0.0113],\n",
      "        [0.0052],\n",
      "        [0.0111],\n",
      "        [0.0032],\n",
      "        [0.0174],\n",
      "        [0.0150],\n",
      "        [0.0342],\n",
      "        [0.0162],\n",
      "        [0.0240],\n",
      "        [0.0258],\n",
      "        [0.0261],\n",
      "        [0.0198],\n",
      "        [0.0222],\n",
      "        [0.0349],\n",
      "        [0.0153],\n",
      "        [0.0336],\n",
      "        [0.0160],\n",
      "        [0.0174],\n",
      "        [0.0267],\n",
      "        [0.0380],\n",
      "        [0.0350],\n",
      "        [0.0297],\n",
      "        [0.0320],\n",
      "        [0.0426],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0410],\n",
      "        [0.0383],\n",
      "        [0.0244],\n",
      "        [0.0545],\n",
      "        [0.0401],\n",
      "        [0.0222],\n",
      "        [0.0476],\n",
      "        [0.0503],\n",
      "        [0.0263],\n",
      "        [0.0557],\n",
      "        [0.0532],\n",
      "        [0.0385],\n",
      "        [0.0464],\n",
      "        [0.0583],\n",
      "        [0.0371],\n",
      "        [0.0288],\n",
      "        [0.0574],\n",
      "        [0.0550],\n",
      "        [0.0566],\n",
      "        [0.0568],\n",
      "        [0.0471],\n",
      "        [0.0536],\n",
      "        [0.0565],\n",
      "        [0.0586],\n",
      "        [0.0563],\n",
      "        [0.0575],\n",
      "        [0.0582],\n",
      "        [0.0551],\n",
      "        [0.0500],\n",
      "        [0.0606],\n",
      "        [0.0394],\n",
      "        [0.0604],\n",
      "        [0.0755],\n",
      "        [0.0801],\n",
      "        [0.0788],\n",
      "        [0.0381],\n",
      "        [0.0634],\n",
      "        [0.0700],\n",
      "        [0.0808],\n",
      "        [0.0678],\n",
      "        [0.0742],\n",
      "        [0.0503],\n",
      "        [0.0853],\n",
      "        [0.0706],\n",
      "        [0.0662],\n",
      "        [0.1006],\n",
      "        [0.0996],\n",
      "        [0.0675],\n",
      "        [0.0760],\n",
      "        [0.0786],\n",
      "        [0.1094],\n",
      "        [0.1169],\n",
      "        [0.1031],\n",
      "        [0.1133],\n",
      "        [0.1048],\n",
      "        [0.0979],\n",
      "        [0.1208],\n",
      "        [0.1379],\n",
      "        [0.1247],\n",
      "        [0.1447],\n",
      "        [0.1596],\n",
      "        [0.1602],\n",
      "        [0.1695],\n",
      "        [0.1620],\n",
      "        [0.1694],\n",
      "        [0.1971],\n",
      "        [0.2903],\n",
      "        [0.3550],\n",
      "        [0.4221]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 68.20023822784424\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 130\n",
      "剩餘X 資料 torch.Size([30, 18])\n",
      "剩餘Y 資料 torch.Size([30, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1862538456916809, 20)\n",
      "The second_loss value of k: (0.18781231343746185, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引20，y= tensor([0.1210])\n",
      "目前模型的Data狀態 torch.Size([130, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5599],\n",
      "        [0.6146],\n",
      "        [0.2985],\n",
      "        [0.6492],\n",
      "        [0.5512],\n",
      "        [0.1702],\n",
      "        [0.5929],\n",
      "        [0.1787],\n",
      "        [0.4384],\n",
      "        [0.6403],\n",
      "        [0.6623],\n",
      "        [0.4863],\n",
      "        [0.6479],\n",
      "        [0.5677],\n",
      "        [0.6097],\n",
      "        [0.6226],\n",
      "        [0.6669],\n",
      "        [0.5164],\n",
      "        [0.6623],\n",
      "        [0.1807],\n",
      "        [0.5442],\n",
      "        [0.6142],\n",
      "        [0.3092],\n",
      "        [0.6265],\n",
      "        [0.4836],\n",
      "        [0.6168],\n",
      "        [0.5354],\n",
      "        [0.4556],\n",
      "        [0.2979],\n",
      "        [0.6060],\n",
      "        [0.3042],\n",
      "        [0.4418],\n",
      "        [0.3057],\n",
      "        [0.6518],\n",
      "        [0.6074],\n",
      "        [0.3026],\n",
      "        [0.4820],\n",
      "        [0.6579],\n",
      "        [0.5733],\n",
      "        [0.6503],\n",
      "        [0.5980],\n",
      "        [0.2174],\n",
      "        [0.6595],\n",
      "        [0.5593],\n",
      "        [0.5145],\n",
      "        [0.5875],\n",
      "        [0.2844],\n",
      "        [0.1939],\n",
      "        [0.6133],\n",
      "        [0.5031],\n",
      "        [0.6048],\n",
      "        [0.6042],\n",
      "        [0.6038],\n",
      "        [0.2297],\n",
      "        [0.5820],\n",
      "        [0.3007],\n",
      "        [0.5958],\n",
      "        [0.6524],\n",
      "        [0.5304],\n",
      "        [0.3069],\n",
      "        [0.6646],\n",
      "        [0.6057],\n",
      "        [0.1864],\n",
      "        [0.6249],\n",
      "        [0.4939],\n",
      "        [0.6111],\n",
      "        [0.4754],\n",
      "        [0.6547],\n",
      "        [0.6651],\n",
      "        [0.5548],\n",
      "        [0.1646],\n",
      "        [0.6044],\n",
      "        [0.2082],\n",
      "        [0.6137],\n",
      "        [0.6040],\n",
      "        [0.6746],\n",
      "        [0.6503],\n",
      "        [0.4779],\n",
      "        [0.3013],\n",
      "        [0.3020],\n",
      "        [0.3072],\n",
      "        [0.2377],\n",
      "        [0.3015],\n",
      "        [0.2999],\n",
      "        [0.5676],\n",
      "        [0.2776],\n",
      "        [0.6917],\n",
      "        [0.6610],\n",
      "        [0.6557],\n",
      "        [0.3028],\n",
      "        [0.6724],\n",
      "        [0.3041],\n",
      "        [0.6152],\n",
      "        [0.6165],\n",
      "        [0.6104],\n",
      "        [0.5495],\n",
      "        [0.6133],\n",
      "        [0.2508],\n",
      "        [0.7438],\n",
      "        [0.6040],\n",
      "        [0.2618],\n",
      "        [0.3413],\n",
      "        [0.2761],\n",
      "        [0.0475],\n",
      "        [0.5615],\n",
      "        [0.7150],\n",
      "        [0.2993],\n",
      "        [0.6492],\n",
      "        [0.6583],\n",
      "        [0.6569],\n",
      "        [0.2721],\n",
      "        [0.7382],\n",
      "        [0.2880],\n",
      "        [0.4397],\n",
      "        [0.3723],\n",
      "        [0.5548],\n",
      "        [0.4075],\n",
      "        [0.2986],\n",
      "        [0.3897],\n",
      "        [0.3075],\n",
      "        [0.2914],\n",
      "        [0.5839],\n",
      "        [0.5900],\n",
      "        [0.1891],\n",
      "        [0.3968],\n",
      "        [0.1971],\n",
      "        [0.7514],\n",
      "        [0.5526],\n",
      "        [0.5526],\n",
      "        [0.5526]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0076],\n",
      "        [0.0086],\n",
      "        [0.0099],\n",
      "        [0.0046],\n",
      "        [0.0100],\n",
      "        [0.0122],\n",
      "        [0.0134],\n",
      "        [0.0021],\n",
      "        [0.0187],\n",
      "        [0.0026],\n",
      "        [0.0009],\n",
      "        [0.0005],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0190],\n",
      "        [0.0103],\n",
      "        [0.0101],\n",
      "        [0.0070],\n",
      "        [0.0085],\n",
      "        [0.0072],\n",
      "        [0.0218],\n",
      "        [0.0050],\n",
      "        [0.0189],\n",
      "        [0.0096],\n",
      "        [0.0196],\n",
      "        [0.0126],\n",
      "        [0.0163],\n",
      "        [0.0135],\n",
      "        [0.0047],\n",
      "        [0.0211],\n",
      "        [0.0227],\n",
      "        [0.0135],\n",
      "        [0.0106],\n",
      "        [0.0113],\n",
      "        [0.0052],\n",
      "        [0.0111],\n",
      "        [0.0032],\n",
      "        [0.0174],\n",
      "        [0.0150],\n",
      "        [0.0342],\n",
      "        [0.0162],\n",
      "        [0.0240],\n",
      "        [0.0258],\n",
      "        [0.0261],\n",
      "        [0.0198],\n",
      "        [0.0222],\n",
      "        [0.0349],\n",
      "        [0.0153],\n",
      "        [0.0336],\n",
      "        [0.0160],\n",
      "        [0.0174],\n",
      "        [0.0267],\n",
      "        [0.0380],\n",
      "        [0.0350],\n",
      "        [0.0297],\n",
      "        [0.0320],\n",
      "        [0.0426],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0410],\n",
      "        [0.0383],\n",
      "        [0.0244],\n",
      "        [0.0545],\n",
      "        [0.0401],\n",
      "        [0.0222],\n",
      "        [0.0476],\n",
      "        [0.0503],\n",
      "        [0.0263],\n",
      "        [0.0557],\n",
      "        [0.0532],\n",
      "        [0.0385],\n",
      "        [0.0464],\n",
      "        [0.0583],\n",
      "        [0.0371],\n",
      "        [0.0288],\n",
      "        [0.0574],\n",
      "        [0.0550],\n",
      "        [0.0566],\n",
      "        [0.0568],\n",
      "        [0.0471],\n",
      "        [0.0536],\n",
      "        [0.0565],\n",
      "        [0.0586],\n",
      "        [0.0563],\n",
      "        [0.0575],\n",
      "        [0.0582],\n",
      "        [0.0551],\n",
      "        [0.0500],\n",
      "        [0.0606],\n",
      "        [0.0394],\n",
      "        [0.0604],\n",
      "        [0.0755],\n",
      "        [0.0801],\n",
      "        [0.0788],\n",
      "        [0.0381],\n",
      "        [0.0634],\n",
      "        [0.0700],\n",
      "        [0.0808],\n",
      "        [0.0678],\n",
      "        [0.0742],\n",
      "        [0.0503],\n",
      "        [0.0853],\n",
      "        [0.0706],\n",
      "        [0.0662],\n",
      "        [0.1006],\n",
      "        [0.0996],\n",
      "        [0.0675],\n",
      "        [0.0760],\n",
      "        [0.0786],\n",
      "        [0.1094],\n",
      "        [0.1169],\n",
      "        [0.1031],\n",
      "        [0.1133],\n",
      "        [0.1048],\n",
      "        [0.0979],\n",
      "        [0.1208],\n",
      "        [0.1379],\n",
      "        [0.1247],\n",
      "        [0.1447],\n",
      "        [0.1596],\n",
      "        [0.1602],\n",
      "        [0.1695],\n",
      "        [0.1620],\n",
      "        [0.1694],\n",
      "        [0.1971],\n",
      "        [0.2903],\n",
      "        [0.3550],\n",
      "        [0.4221],\n",
      "        [0.4316]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is Unacceptable\n",
      "Number of enlarge: 3\n",
      "Number of shrink: 8\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0009],\n",
      "        [0.0076],\n",
      "        [0.0086],\n",
      "        [0.0099],\n",
      "        [0.0046],\n",
      "        [0.0100],\n",
      "        [0.0122],\n",
      "        [0.0134],\n",
      "        [0.0021],\n",
      "        [0.0187],\n",
      "        [0.0026],\n",
      "        [0.0009],\n",
      "        [0.0005],\n",
      "        [0.0066],\n",
      "        [0.0069],\n",
      "        [0.0190],\n",
      "        [0.0103],\n",
      "        [0.0101],\n",
      "        [0.0070],\n",
      "        [0.0085],\n",
      "        [0.0072],\n",
      "        [0.0218],\n",
      "        [0.0050],\n",
      "        [0.0189],\n",
      "        [0.0096],\n",
      "        [0.0196],\n",
      "        [0.0126],\n",
      "        [0.0163],\n",
      "        [0.0135],\n",
      "        [0.0047],\n",
      "        [0.0211],\n",
      "        [0.0227],\n",
      "        [0.0135],\n",
      "        [0.0106],\n",
      "        [0.0113],\n",
      "        [0.0052],\n",
      "        [0.0111],\n",
      "        [0.0032],\n",
      "        [0.0174],\n",
      "        [0.0150],\n",
      "        [0.0342],\n",
      "        [0.0162],\n",
      "        [0.0240],\n",
      "        [0.0258],\n",
      "        [0.0261],\n",
      "        [0.0198],\n",
      "        [0.0222],\n",
      "        [0.0349],\n",
      "        [0.0153],\n",
      "        [0.0336],\n",
      "        [0.0160],\n",
      "        [0.0174],\n",
      "        [0.0267],\n",
      "        [0.0380],\n",
      "        [0.0350],\n",
      "        [0.0297],\n",
      "        [0.0320],\n",
      "        [0.0426],\n",
      "        [0.0039],\n",
      "        [0.0042],\n",
      "        [0.0410],\n",
      "        [0.0383],\n",
      "        [0.0244],\n",
      "        [0.0545],\n",
      "        [0.0401],\n",
      "        [0.0222],\n",
      "        [0.0476],\n",
      "        [0.0503],\n",
      "        [0.0263],\n",
      "        [0.0557],\n",
      "        [0.0532],\n",
      "        [0.0385],\n",
      "        [0.0464],\n",
      "        [0.0583],\n",
      "        [0.0371],\n",
      "        [0.0288],\n",
      "        [0.0574],\n",
      "        [0.0550],\n",
      "        [0.0566],\n",
      "        [0.0568],\n",
      "        [0.0471],\n",
      "        [0.0536],\n",
      "        [0.0565],\n",
      "        [0.0586],\n",
      "        [0.0563],\n",
      "        [0.0575],\n",
      "        [0.0582],\n",
      "        [0.0551],\n",
      "        [0.0500],\n",
      "        [0.0606],\n",
      "        [0.0394],\n",
      "        [0.0604],\n",
      "        [0.0755],\n",
      "        [0.0801],\n",
      "        [0.0788],\n",
      "        [0.0381],\n",
      "        [0.0634],\n",
      "        [0.0700],\n",
      "        [0.0808],\n",
      "        [0.0678],\n",
      "        [0.0742],\n",
      "        [0.0503],\n",
      "        [0.0853],\n",
      "        [0.0706],\n",
      "        [0.0662],\n",
      "        [0.1006],\n",
      "        [0.0996],\n",
      "        [0.0675],\n",
      "        [0.0760],\n",
      "        [0.0786],\n",
      "        [0.1094],\n",
      "        [0.1169],\n",
      "        [0.1031],\n",
      "        [0.1133],\n",
      "        [0.1048],\n",
      "        [0.0979],\n",
      "        [0.1208],\n",
      "        [0.1379],\n",
      "        [0.1247],\n",
      "        [0.1447],\n",
      "        [0.1596],\n",
      "        [0.1602],\n",
      "        [0.1695],\n",
      "        [0.1620],\n",
      "        [0.1694],\n",
      "        [0.1971],\n",
      "        [0.2903],\n",
      "        [0.3550],\n",
      "        [0.4221],\n",
      "        [0.4316]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Cramming module>>\n",
      "threshold_for_error: 0.43\n",
      "不滿足個數： 1\n",
      "The index of the undesired data: tensor([[129,   0]], device='cuda:0')\n",
      "Cramming success!\n",
      "<<Cramming後看一下差異>>\n",
      "tensor([[    0.0009],\n",
      "        [    0.0076],\n",
      "        [    0.0086],\n",
      "        [    0.0099],\n",
      "        [    0.0046],\n",
      "        [    0.0100],\n",
      "        [    0.0122],\n",
      "        [    0.0134],\n",
      "        [    0.0021],\n",
      "        [    0.0187],\n",
      "        [    0.0026],\n",
      "        [    0.0009],\n",
      "        [    0.0005],\n",
      "        [    0.0066],\n",
      "        [    0.0069],\n",
      "        [    0.0190],\n",
      "        [    0.0103],\n",
      "        [    0.0101],\n",
      "        [    0.0070],\n",
      "        [    0.0085],\n",
      "        [    0.0072],\n",
      "        [    0.0218],\n",
      "        [    0.0050],\n",
      "        [    0.0189],\n",
      "        [    0.0096],\n",
      "        [    0.0196],\n",
      "        [    0.0126],\n",
      "        [    0.0163],\n",
      "        [    0.0135],\n",
      "        [    0.0047],\n",
      "        [    0.0211],\n",
      "        [    0.0227],\n",
      "        [    0.0135],\n",
      "        [    0.0106],\n",
      "        [    0.0113],\n",
      "        [    0.0052],\n",
      "        [    0.0111],\n",
      "        [    0.0032],\n",
      "        [    0.0174],\n",
      "        [    0.0150],\n",
      "        [    0.0342],\n",
      "        [    0.0162],\n",
      "        [    0.0241],\n",
      "        [    0.0258],\n",
      "        [    0.0261],\n",
      "        [    0.0198],\n",
      "        [    0.0222],\n",
      "        [    0.0349],\n",
      "        [    0.0153],\n",
      "        [    0.0336],\n",
      "        [    0.0160],\n",
      "        [    0.0174],\n",
      "        [    0.0267],\n",
      "        [    0.0380],\n",
      "        [    0.0350],\n",
      "        [    0.0297],\n",
      "        [    0.0320],\n",
      "        [    0.0426],\n",
      "        [    0.0039],\n",
      "        [    0.0042],\n",
      "        [    0.0410],\n",
      "        [    0.0383],\n",
      "        [    0.0244],\n",
      "        [    0.0545],\n",
      "        [    0.0401],\n",
      "        [    0.0222],\n",
      "        [    0.0476],\n",
      "        [    0.0503],\n",
      "        [    0.0263],\n",
      "        [    0.0557],\n",
      "        [    0.0532],\n",
      "        [    0.0385],\n",
      "        [    0.0464],\n",
      "        [    0.0583],\n",
      "        [    0.0371],\n",
      "        [    0.0288],\n",
      "        [    0.0574],\n",
      "        [    0.0550],\n",
      "        [    0.0566],\n",
      "        [    0.0568],\n",
      "        [    0.0471],\n",
      "        [    0.0536],\n",
      "        [    0.0565],\n",
      "        [    0.0586],\n",
      "        [    0.0563],\n",
      "        [    0.0575],\n",
      "        [    0.0582],\n",
      "        [    0.0551],\n",
      "        [    0.0500],\n",
      "        [    0.0606],\n",
      "        [    0.0394],\n",
      "        [    0.0604],\n",
      "        [    0.0755],\n",
      "        [    0.0801],\n",
      "        [    0.0788],\n",
      "        [    0.0381],\n",
      "        [    0.0634],\n",
      "        [    0.0700],\n",
      "        [    0.0808],\n",
      "        [    0.0678],\n",
      "        [    0.0742],\n",
      "        [    0.0503],\n",
      "        [    0.0853],\n",
      "        [    0.0706],\n",
      "        [    0.0662],\n",
      "        [    0.1006],\n",
      "        [    0.0996],\n",
      "        [    0.0675],\n",
      "        [    0.0760],\n",
      "        [    0.0786],\n",
      "        [    0.1094],\n",
      "        [    0.1169],\n",
      "        [    0.1031],\n",
      "        [    0.1133],\n",
      "        [    0.1048],\n",
      "        [    0.0979],\n",
      "        [    0.1208],\n",
      "        [    0.1379],\n",
      "        [    0.1247],\n",
      "        [    0.1447],\n",
      "        [    0.1596],\n",
      "        [    0.1602],\n",
      "        [    0.1695],\n",
      "        [    0.1620],\n",
      "        [    0.1694],\n",
      "        [    0.1971],\n",
      "        [    0.2903],\n",
      "        [    0.3550],\n",
      "        [    0.4221],\n",
      "        [    0.0000]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 6\n",
      "Number of shrink: 10\n",
      "<<Matching module for reorganizing>>\n",
      "threshold_for_error: 0.43\n",
      "Matching(o) first finished - the network is acceptable\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "是不是可以不要這個hidden node: True\n",
      "Drop out the nero number: 1 / 7\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 3\n",
      "Number of shrink: 8\n",
      "<<Matching module for reorganizing>>\n",
      "threshold_for_error: 0.43\n",
      "Matching的第500回合\n",
      "Matching finished - the network is Unacceptable\n",
      "Number of enlarge: 340\n",
      "Number of shrink: 160\n",
      "是不是可以不要這個hidden node: False\n",
      "Cannot drop out the nero number: 1 / 6\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-Learning不能這麼小\n",
      "Number of enlarge: 26\n",
      "Number of shrink: 20\n",
      "<<Matching module for reorganizing>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished(o) - the network is acceptable\n",
      "Number of enlarge: 159\n",
      "Number of shrink: 70\n",
      "是不是可以不要這個hidden node: True\n",
      "Drop out the nero number: 2 / 6\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 6\n",
      "Number of shrink: 0\n",
      "<<Matching module for reorganizing>>\n",
      "threshold_for_error: 0.43\n",
      "Matching(o) first finished - the network is acceptable\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "是不是可以不要這個hidden node: True\n",
      "Drop out the nero number: 2 / 5\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 69.98685336112976\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 131\n",
      "剩餘X 資料 torch.Size([29, 18])\n",
      "剩餘Y 資料 torch.Size([29, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14959581196308136, 15)\n",
      "The second_loss value of k: (0.15627408027648926, 16)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.1192])\n",
      "目前模型的Data狀態 torch.Size([131, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6383],\n",
      "        [ 0.9158],\n",
      "        [ 0.2124],\n",
      "        [ 0.6738],\n",
      "        [ 0.5743],\n",
      "        [ 0.1918],\n",
      "        [ 0.9575],\n",
      "        [ 0.2576],\n",
      "        [ 0.2086],\n",
      "        [ 0.6470],\n",
      "        [ 0.6790],\n",
      "        [ 0.3425],\n",
      "        [ 0.6143],\n",
      "        [ 0.6378],\n",
      "        [ 0.9085],\n",
      "        [ 0.6254],\n",
      "        [ 0.7100],\n",
      "        [ 0.5091],\n",
      "        [ 0.6564],\n",
      "        [ 0.3726],\n",
      "        [ 0.7670],\n",
      "        [ 0.5725],\n",
      "        [ 0.0415],\n",
      "        [ 0.5955],\n",
      "        [ 0.2582],\n",
      "        [ 0.5932],\n",
      "        [ 0.5420],\n",
      "        [ 0.1343],\n",
      "        [ 0.0766],\n",
      "        [ 0.8413],\n",
      "        [ 0.1472],\n",
      "        [ 0.1527],\n",
      "        [ 0.2129],\n",
      "        [ 0.5964],\n",
      "        [ 0.8185],\n",
      "        [ 0.2258],\n",
      "        [ 0.4193],\n",
      "        [ 0.6533],\n",
      "        [ 0.6857],\n",
      "        [ 0.6459],\n",
      "        [ 0.8713],\n",
      "        [ 0.0770],\n",
      "        [ 0.7268],\n",
      "        [ 0.8796],\n",
      "        [ 0.5059],\n",
      "        [ 0.7212],\n",
      "        [ 0.1680],\n",
      "        [ 0.2812],\n",
      "        [ 0.9037],\n",
      "        [ 0.5202],\n",
      "        [ 0.6026],\n",
      "        [ 0.8362],\n",
      "        [ 0.9837],\n",
      "        [ 0.0771],\n",
      "        [ 0.8093],\n",
      "        [ 0.1930],\n",
      "        [ 0.7881],\n",
      "        [ 0.7729],\n",
      "        [ 0.7141],\n",
      "        [ 0.2186],\n",
      "        [ 0.6854],\n",
      "        [ 0.8604],\n",
      "        [-0.1779],\n",
      "        [ 0.6516],\n",
      "        [ 0.4822],\n",
      "        [ 0.8213],\n",
      "        [ 0.4365],\n",
      "        [ 0.8201],\n",
      "        [ 0.6824],\n",
      "        [ 0.8277],\n",
      "        [ 0.3859],\n",
      "        [ 0.8772],\n",
      "        [-0.1753],\n",
      "        [ 0.6260],\n",
      "        [ 0.6234],\n",
      "        [ 0.6829],\n",
      "        [ 0.7606],\n",
      "        [ 0.4307],\n",
      "        [ 0.2045],\n",
      "        [ 0.1956],\n",
      "        [ 0.0285],\n",
      "        [ 0.1485],\n",
      "        [ 0.1851],\n",
      "        [ 0.2084],\n",
      "        [ 0.8210],\n",
      "        [ 0.1878],\n",
      "        [ 0.6802],\n",
      "        [ 0.6982],\n",
      "        [ 0.6406],\n",
      "        [ 0.1899],\n",
      "        [ 0.6117],\n",
      "        [ 0.0299],\n",
      "        [ 0.5881],\n",
      "        [ 0.6233],\n",
      "        [ 0.6085],\n",
      "        [ 0.7125],\n",
      "        [ 0.9828],\n",
      "        [ 0.1670],\n",
      "        [ 0.8017],\n",
      "        [ 0.9754],\n",
      "        [ 0.2088],\n",
      "        [ 0.2264],\n",
      "        [ 0.2119],\n",
      "        [ 0.4422],\n",
      "        [ 0.6835],\n",
      "        [ 0.6975],\n",
      "        [ 0.0762],\n",
      "        [ 0.5402],\n",
      "        [ 0.5572],\n",
      "        [ 0.5523],\n",
      "        [ 0.2396],\n",
      "        [ 0.7226],\n",
      "        [ 0.1368],\n",
      "        [ 0.1962],\n",
      "        [ 0.2910],\n",
      "        [ 0.6550],\n",
      "        [ 0.2344],\n",
      "        [ 0.0865],\n",
      "        [ 0.3032],\n",
      "        [ 0.1509],\n",
      "        [ 0.1137],\n",
      "        [ 0.6644],\n",
      "        [ 0.6424],\n",
      "        [ 0.3709],\n",
      "        [ 0.3312],\n",
      "        [-0.0184],\n",
      "        [ 0.7836],\n",
      "        [ 0.3409],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.02486538887024\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 132\n",
      "剩餘X 資料 torch.Size([28, 18])\n",
      "剩餘Y 資料 torch.Size([28, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15627408027648926, 15)\n",
      "The second_loss value of k: (0.15770187973976135, 18)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.1107])\n",
      "目前模型的Data狀態 torch.Size([132, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6383],\n",
      "        [ 0.9158],\n",
      "        [ 0.2124],\n",
      "        [ 0.6738],\n",
      "        [ 0.5743],\n",
      "        [ 0.1918],\n",
      "        [ 0.9575],\n",
      "        [ 0.2576],\n",
      "        [ 0.2086],\n",
      "        [ 0.6470],\n",
      "        [ 0.6790],\n",
      "        [ 0.3425],\n",
      "        [ 0.6143],\n",
      "        [ 0.6378],\n",
      "        [ 0.9085],\n",
      "        [ 0.6254],\n",
      "        [ 0.7100],\n",
      "        [ 0.5091],\n",
      "        [ 0.6564],\n",
      "        [ 0.3726],\n",
      "        [ 0.7670],\n",
      "        [ 0.5725],\n",
      "        [ 0.0415],\n",
      "        [ 0.5955],\n",
      "        [ 0.2582],\n",
      "        [ 0.5932],\n",
      "        [ 0.5420],\n",
      "        [ 0.1343],\n",
      "        [ 0.0766],\n",
      "        [ 0.8413],\n",
      "        [ 0.1472],\n",
      "        [ 0.1527],\n",
      "        [ 0.2129],\n",
      "        [ 0.5964],\n",
      "        [ 0.8185],\n",
      "        [ 0.2258],\n",
      "        [ 0.4193],\n",
      "        [ 0.6533],\n",
      "        [ 0.6857],\n",
      "        [ 0.6459],\n",
      "        [ 0.8713],\n",
      "        [ 0.0770],\n",
      "        [ 0.7268],\n",
      "        [ 0.8796],\n",
      "        [ 0.5059],\n",
      "        [ 0.7212],\n",
      "        [ 0.1680],\n",
      "        [ 0.2812],\n",
      "        [ 0.9037],\n",
      "        [ 0.5202],\n",
      "        [ 0.6026],\n",
      "        [ 0.8362],\n",
      "        [ 0.9837],\n",
      "        [ 0.0771],\n",
      "        [ 0.8093],\n",
      "        [ 0.1930],\n",
      "        [ 0.7881],\n",
      "        [ 0.7729],\n",
      "        [ 0.7141],\n",
      "        [ 0.2186],\n",
      "        [ 0.6854],\n",
      "        [ 0.8604],\n",
      "        [-0.1779],\n",
      "        [ 0.6516],\n",
      "        [ 0.4822],\n",
      "        [ 0.8213],\n",
      "        [ 0.4365],\n",
      "        [ 0.8201],\n",
      "        [ 0.6824],\n",
      "        [ 0.8277],\n",
      "        [ 0.3859],\n",
      "        [ 0.8772],\n",
      "        [-0.1753],\n",
      "        [ 0.6260],\n",
      "        [ 0.6234],\n",
      "        [ 0.6829],\n",
      "        [ 0.7606],\n",
      "        [ 0.4307],\n",
      "        [ 0.2045],\n",
      "        [ 0.1956],\n",
      "        [ 0.0285],\n",
      "        [ 0.1485],\n",
      "        [ 0.1851],\n",
      "        [ 0.2084],\n",
      "        [ 0.8210],\n",
      "        [ 0.1878],\n",
      "        [ 0.6802],\n",
      "        [ 0.6982],\n",
      "        [ 0.6406],\n",
      "        [ 0.1899],\n",
      "        [ 0.6117],\n",
      "        [ 0.0299],\n",
      "        [ 0.5881],\n",
      "        [ 0.6233],\n",
      "        [ 0.6085],\n",
      "        [ 0.7125],\n",
      "        [ 0.9828],\n",
      "        [ 0.1670],\n",
      "        [ 0.8017],\n",
      "        [ 0.9754],\n",
      "        [ 0.2088],\n",
      "        [ 0.2264],\n",
      "        [ 0.2119],\n",
      "        [ 0.4422],\n",
      "        [ 0.6835],\n",
      "        [ 0.6975],\n",
      "        [ 0.0762],\n",
      "        [ 0.5402],\n",
      "        [ 0.5572],\n",
      "        [ 0.5523],\n",
      "        [ 0.2396],\n",
      "        [ 0.7226],\n",
      "        [ 0.1368],\n",
      "        [ 0.1962],\n",
      "        [ 0.2910],\n",
      "        [ 0.6550],\n",
      "        [ 0.2344],\n",
      "        [ 0.0865],\n",
      "        [ 0.3032],\n",
      "        [ 0.1509],\n",
      "        [ 0.1137],\n",
      "        [ 0.6644],\n",
      "        [ 0.6424],\n",
      "        [ 0.3709],\n",
      "        [ 0.3312],\n",
      "        [-0.0184],\n",
      "        [ 0.7836],\n",
      "        [ 0.3409],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.0624532699585\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 133\n",
      "剩餘X 資料 torch.Size([27, 18])\n",
      "剩餘Y 資料 torch.Size([27, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15770187973976135, 17)\n",
      "The second_loss value of k: (0.15935499966144562, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引17，y= tensor([0.1089])\n",
      "目前模型的Data狀態 torch.Size([133, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6383],\n",
      "        [ 0.9158],\n",
      "        [ 0.2124],\n",
      "        [ 0.6738],\n",
      "        [ 0.5743],\n",
      "        [ 0.1918],\n",
      "        [ 0.9575],\n",
      "        [ 0.2576],\n",
      "        [ 0.2086],\n",
      "        [ 0.6470],\n",
      "        [ 0.6790],\n",
      "        [ 0.3425],\n",
      "        [ 0.6143],\n",
      "        [ 0.6378],\n",
      "        [ 0.9085],\n",
      "        [ 0.6254],\n",
      "        [ 0.7100],\n",
      "        [ 0.5091],\n",
      "        [ 0.6564],\n",
      "        [ 0.3726],\n",
      "        [ 0.7670],\n",
      "        [ 0.5725],\n",
      "        [ 0.0415],\n",
      "        [ 0.5955],\n",
      "        [ 0.2582],\n",
      "        [ 0.5932],\n",
      "        [ 0.5420],\n",
      "        [ 0.1343],\n",
      "        [ 0.0766],\n",
      "        [ 0.8413],\n",
      "        [ 0.1472],\n",
      "        [ 0.1527],\n",
      "        [ 0.2129],\n",
      "        [ 0.5964],\n",
      "        [ 0.8185],\n",
      "        [ 0.2258],\n",
      "        [ 0.4193],\n",
      "        [ 0.6533],\n",
      "        [ 0.6857],\n",
      "        [ 0.6459],\n",
      "        [ 0.8713],\n",
      "        [ 0.0770],\n",
      "        [ 0.7268],\n",
      "        [ 0.8796],\n",
      "        [ 0.5059],\n",
      "        [ 0.7212],\n",
      "        [ 0.1680],\n",
      "        [ 0.2812],\n",
      "        [ 0.9037],\n",
      "        [ 0.5202],\n",
      "        [ 0.6026],\n",
      "        [ 0.8362],\n",
      "        [ 0.9837],\n",
      "        [ 0.0771],\n",
      "        [ 0.8093],\n",
      "        [ 0.1930],\n",
      "        [ 0.7881],\n",
      "        [ 0.7729],\n",
      "        [ 0.7141],\n",
      "        [ 0.2186],\n",
      "        [ 0.6854],\n",
      "        [ 0.8604],\n",
      "        [-0.1779],\n",
      "        [ 0.6516],\n",
      "        [ 0.4822],\n",
      "        [ 0.8213],\n",
      "        [ 0.4365],\n",
      "        [ 0.8201],\n",
      "        [ 0.6824],\n",
      "        [ 0.8277],\n",
      "        [ 0.3859],\n",
      "        [ 0.8772],\n",
      "        [-0.1753],\n",
      "        [ 0.6260],\n",
      "        [ 0.6234],\n",
      "        [ 0.6829],\n",
      "        [ 0.7606],\n",
      "        [ 0.4307],\n",
      "        [ 0.2045],\n",
      "        [ 0.1956],\n",
      "        [ 0.0285],\n",
      "        [ 0.1485],\n",
      "        [ 0.1851],\n",
      "        [ 0.2084],\n",
      "        [ 0.8210],\n",
      "        [ 0.1878],\n",
      "        [ 0.6802],\n",
      "        [ 0.6982],\n",
      "        [ 0.6406],\n",
      "        [ 0.1899],\n",
      "        [ 0.6117],\n",
      "        [ 0.0299],\n",
      "        [ 0.5881],\n",
      "        [ 0.6233],\n",
      "        [ 0.6085],\n",
      "        [ 0.7125],\n",
      "        [ 0.9828],\n",
      "        [ 0.1670],\n",
      "        [ 0.8017],\n",
      "        [ 0.9754],\n",
      "        [ 0.2088],\n",
      "        [ 0.2264],\n",
      "        [ 0.2119],\n",
      "        [ 0.4422],\n",
      "        [ 0.6835],\n",
      "        [ 0.6975],\n",
      "        [ 0.0762],\n",
      "        [ 0.5402],\n",
      "        [ 0.5572],\n",
      "        [ 0.5523],\n",
      "        [ 0.2396],\n",
      "        [ 0.7226],\n",
      "        [ 0.1368],\n",
      "        [ 0.1962],\n",
      "        [ 0.2910],\n",
      "        [ 0.6550],\n",
      "        [ 0.2344],\n",
      "        [ 0.0865],\n",
      "        [ 0.3032],\n",
      "        [ 0.1509],\n",
      "        [ 0.1137],\n",
      "        [ 0.6644],\n",
      "        [ 0.6424],\n",
      "        [ 0.3709],\n",
      "        [ 0.3312],\n",
      "        [-0.0184],\n",
      "        [ 0.7836],\n",
      "        [ 0.3409],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.11187362670898\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 134\n",
      "剩餘X 資料 torch.Size([26, 18])\n",
      "剩餘Y 資料 torch.Size([26, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15935499966144562, 14)\n",
      "The second_loss value of k: (0.16020047664642334, 25)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.1068])\n",
      "目前模型的Data狀態 torch.Size([134, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6383],\n",
      "        [ 0.9158],\n",
      "        [ 0.2124],\n",
      "        [ 0.6738],\n",
      "        [ 0.5743],\n",
      "        [ 0.1918],\n",
      "        [ 0.9575],\n",
      "        [ 0.2576],\n",
      "        [ 0.2086],\n",
      "        [ 0.6470],\n",
      "        [ 0.6790],\n",
      "        [ 0.3425],\n",
      "        [ 0.6143],\n",
      "        [ 0.6378],\n",
      "        [ 0.9085],\n",
      "        [ 0.6254],\n",
      "        [ 0.7100],\n",
      "        [ 0.5091],\n",
      "        [ 0.6564],\n",
      "        [ 0.3726],\n",
      "        [ 0.7670],\n",
      "        [ 0.5725],\n",
      "        [ 0.0415],\n",
      "        [ 0.5955],\n",
      "        [ 0.2582],\n",
      "        [ 0.5932],\n",
      "        [ 0.5420],\n",
      "        [ 0.1343],\n",
      "        [ 0.0766],\n",
      "        [ 0.8413],\n",
      "        [ 0.1472],\n",
      "        [ 0.1527],\n",
      "        [ 0.2129],\n",
      "        [ 0.5964],\n",
      "        [ 0.8185],\n",
      "        [ 0.2258],\n",
      "        [ 0.4193],\n",
      "        [ 0.6533],\n",
      "        [ 0.6857],\n",
      "        [ 0.6459],\n",
      "        [ 0.8713],\n",
      "        [ 0.0770],\n",
      "        [ 0.7268],\n",
      "        [ 0.8796],\n",
      "        [ 0.5059],\n",
      "        [ 0.7212],\n",
      "        [ 0.1680],\n",
      "        [ 0.2812],\n",
      "        [ 0.9037],\n",
      "        [ 0.5202],\n",
      "        [ 0.6026],\n",
      "        [ 0.8362],\n",
      "        [ 0.9837],\n",
      "        [ 0.0771],\n",
      "        [ 0.8093],\n",
      "        [ 0.1930],\n",
      "        [ 0.7881],\n",
      "        [ 0.7729],\n",
      "        [ 0.7141],\n",
      "        [ 0.2186],\n",
      "        [ 0.6854],\n",
      "        [ 0.8604],\n",
      "        [-0.1779],\n",
      "        [ 0.6516],\n",
      "        [ 0.4822],\n",
      "        [ 0.8213],\n",
      "        [ 0.4365],\n",
      "        [ 0.8201],\n",
      "        [ 0.6824],\n",
      "        [ 0.8277],\n",
      "        [ 0.3859],\n",
      "        [ 0.8772],\n",
      "        [-0.1753],\n",
      "        [ 0.6260],\n",
      "        [ 0.6234],\n",
      "        [ 0.6829],\n",
      "        [ 0.7606],\n",
      "        [ 0.4307],\n",
      "        [ 0.2045],\n",
      "        [ 0.1956],\n",
      "        [ 0.0285],\n",
      "        [ 0.1485],\n",
      "        [ 0.1851],\n",
      "        [ 0.2084],\n",
      "        [ 0.8210],\n",
      "        [ 0.1878],\n",
      "        [ 0.6802],\n",
      "        [ 0.6982],\n",
      "        [ 0.6406],\n",
      "        [ 0.1899],\n",
      "        [ 0.6117],\n",
      "        [ 0.0299],\n",
      "        [ 0.5881],\n",
      "        [ 0.6233],\n",
      "        [ 0.6085],\n",
      "        [ 0.7125],\n",
      "        [ 0.9828],\n",
      "        [ 0.1670],\n",
      "        [ 0.8017],\n",
      "        [ 0.9754],\n",
      "        [ 0.2088],\n",
      "        [ 0.2264],\n",
      "        [ 0.2119],\n",
      "        [ 0.4422],\n",
      "        [ 0.6835],\n",
      "        [ 0.6975],\n",
      "        [ 0.0762],\n",
      "        [ 0.5402],\n",
      "        [ 0.5572],\n",
      "        [ 0.5523],\n",
      "        [ 0.2396],\n",
      "        [ 0.7226],\n",
      "        [ 0.1368],\n",
      "        [ 0.1962],\n",
      "        [ 0.2910],\n",
      "        [ 0.6550],\n",
      "        [ 0.2344],\n",
      "        [ 0.0865],\n",
      "        [ 0.3032],\n",
      "        [ 0.1509],\n",
      "        [ 0.1137],\n",
      "        [ 0.6644],\n",
      "        [ 0.6424],\n",
      "        [ 0.3709],\n",
      "        [ 0.3312],\n",
      "        [-0.0184],\n",
      "        [ 0.7836],\n",
      "        [ 0.3409],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.15105533599854\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 135\n",
      "剩餘X 資料 torch.Size([25, 18])\n",
      "剩餘Y 資料 torch.Size([25, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16020047664642334, 24)\n",
      "The second_loss value of k: (0.16278193891048431, 16)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引24，y= tensor([0.1057])\n",
      "目前模型的Data狀態 torch.Size([135, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6383],\n",
      "        [ 0.9158],\n",
      "        [ 0.2124],\n",
      "        [ 0.6738],\n",
      "        [ 0.5743],\n",
      "        [ 0.1918],\n",
      "        [ 0.9575],\n",
      "        [ 0.2576],\n",
      "        [ 0.2086],\n",
      "        [ 0.6470],\n",
      "        [ 0.6790],\n",
      "        [ 0.3425],\n",
      "        [ 0.6143],\n",
      "        [ 0.6378],\n",
      "        [ 0.9085],\n",
      "        [ 0.6254],\n",
      "        [ 0.7100],\n",
      "        [ 0.5091],\n",
      "        [ 0.6564],\n",
      "        [ 0.3726],\n",
      "        [ 0.7670],\n",
      "        [ 0.5725],\n",
      "        [ 0.0415],\n",
      "        [ 0.5955],\n",
      "        [ 0.2582],\n",
      "        [ 0.5932],\n",
      "        [ 0.5420],\n",
      "        [ 0.1343],\n",
      "        [ 0.0766],\n",
      "        [ 0.8413],\n",
      "        [ 0.1472],\n",
      "        [ 0.1527],\n",
      "        [ 0.2129],\n",
      "        [ 0.5964],\n",
      "        [ 0.8185],\n",
      "        [ 0.2258],\n",
      "        [ 0.4193],\n",
      "        [ 0.6533],\n",
      "        [ 0.6857],\n",
      "        [ 0.6459],\n",
      "        [ 0.8713],\n",
      "        [ 0.0770],\n",
      "        [ 0.7268],\n",
      "        [ 0.8796],\n",
      "        [ 0.5059],\n",
      "        [ 0.7212],\n",
      "        [ 0.1680],\n",
      "        [ 0.2812],\n",
      "        [ 0.9037],\n",
      "        [ 0.5202],\n",
      "        [ 0.6026],\n",
      "        [ 0.8362],\n",
      "        [ 0.9837],\n",
      "        [ 0.0771],\n",
      "        [ 0.8093],\n",
      "        [ 0.1930],\n",
      "        [ 0.7881],\n",
      "        [ 0.7729],\n",
      "        [ 0.7141],\n",
      "        [ 0.2186],\n",
      "        [ 0.6854],\n",
      "        [ 0.8604],\n",
      "        [-0.1779],\n",
      "        [ 0.6516],\n",
      "        [ 0.4822],\n",
      "        [ 0.8213],\n",
      "        [ 0.4365],\n",
      "        [ 0.8201],\n",
      "        [ 0.6824],\n",
      "        [ 0.8277],\n",
      "        [ 0.3859],\n",
      "        [ 0.8772],\n",
      "        [-0.1753],\n",
      "        [ 0.6260],\n",
      "        [ 0.6234],\n",
      "        [ 0.6829],\n",
      "        [ 0.7606],\n",
      "        [ 0.4307],\n",
      "        [ 0.2045],\n",
      "        [ 0.1956],\n",
      "        [ 0.0285],\n",
      "        [ 0.1485],\n",
      "        [ 0.1851],\n",
      "        [ 0.2084],\n",
      "        [ 0.8210],\n",
      "        [ 0.1878],\n",
      "        [ 0.6802],\n",
      "        [ 0.6982],\n",
      "        [ 0.6406],\n",
      "        [ 0.1899],\n",
      "        [ 0.6117],\n",
      "        [ 0.0299],\n",
      "        [ 0.5881],\n",
      "        [ 0.6233],\n",
      "        [ 0.6085],\n",
      "        [ 0.7125],\n",
      "        [ 0.9828],\n",
      "        [ 0.1670],\n",
      "        [ 0.8017],\n",
      "        [ 0.9754],\n",
      "        [ 0.2088],\n",
      "        [ 0.2264],\n",
      "        [ 0.2119],\n",
      "        [ 0.4422],\n",
      "        [ 0.6835],\n",
      "        [ 0.6975],\n",
      "        [ 0.0762],\n",
      "        [ 0.5402],\n",
      "        [ 0.5572],\n",
      "        [ 0.5523],\n",
      "        [ 0.2396],\n",
      "        [ 0.7226],\n",
      "        [ 0.1368],\n",
      "        [ 0.1962],\n",
      "        [ 0.2910],\n",
      "        [ 0.6550],\n",
      "        [ 0.2344],\n",
      "        [ 0.0865],\n",
      "        [ 0.3032],\n",
      "        [ 0.1509],\n",
      "        [ 0.1137],\n",
      "        [ 0.6644],\n",
      "        [ 0.6424],\n",
      "        [ 0.3709],\n",
      "        [ 0.3312],\n",
      "        [-0.0184],\n",
      "        [ 0.7836],\n",
      "        [ 0.3409],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992],\n",
      "        [0.4003]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992],\n",
      "        [0.4003]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.19045209884644\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 136\n",
      "剩餘X 資料 torch.Size([24, 18])\n",
      "剩餘Y 資料 torch.Size([24, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16278193891048431, 16)\n",
      "The second_loss value of k: (0.17853839695453644, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.1025])\n",
      "目前模型的Data狀態 torch.Size([136, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6383],\n",
      "        [ 0.9158],\n",
      "        [ 0.2124],\n",
      "        [ 0.6738],\n",
      "        [ 0.5743],\n",
      "        [ 0.1918],\n",
      "        [ 0.9575],\n",
      "        [ 0.2576],\n",
      "        [ 0.2086],\n",
      "        [ 0.6470],\n",
      "        [ 0.6790],\n",
      "        [ 0.3425],\n",
      "        [ 0.6143],\n",
      "        [ 0.6378],\n",
      "        [ 0.9085],\n",
      "        [ 0.6254],\n",
      "        [ 0.7100],\n",
      "        [ 0.5091],\n",
      "        [ 0.6564],\n",
      "        [ 0.3726],\n",
      "        [ 0.7670],\n",
      "        [ 0.5725],\n",
      "        [ 0.0415],\n",
      "        [ 0.5955],\n",
      "        [ 0.2582],\n",
      "        [ 0.5932],\n",
      "        [ 0.5420],\n",
      "        [ 0.1343],\n",
      "        [ 0.0766],\n",
      "        [ 0.8413],\n",
      "        [ 0.1472],\n",
      "        [ 0.1527],\n",
      "        [ 0.2129],\n",
      "        [ 0.5964],\n",
      "        [ 0.8185],\n",
      "        [ 0.2258],\n",
      "        [ 0.4193],\n",
      "        [ 0.6533],\n",
      "        [ 0.6857],\n",
      "        [ 0.6459],\n",
      "        [ 0.8713],\n",
      "        [ 0.0770],\n",
      "        [ 0.7268],\n",
      "        [ 0.8796],\n",
      "        [ 0.5059],\n",
      "        [ 0.7212],\n",
      "        [ 0.1680],\n",
      "        [ 0.2812],\n",
      "        [ 0.9037],\n",
      "        [ 0.5202],\n",
      "        [ 0.6026],\n",
      "        [ 0.8362],\n",
      "        [ 0.9837],\n",
      "        [ 0.0771],\n",
      "        [ 0.8093],\n",
      "        [ 0.1930],\n",
      "        [ 0.7881],\n",
      "        [ 0.7729],\n",
      "        [ 0.7141],\n",
      "        [ 0.2186],\n",
      "        [ 0.6854],\n",
      "        [ 0.8604],\n",
      "        [-0.1779],\n",
      "        [ 0.6516],\n",
      "        [ 0.4822],\n",
      "        [ 0.8213],\n",
      "        [ 0.4365],\n",
      "        [ 0.8201],\n",
      "        [ 0.6824],\n",
      "        [ 0.8277],\n",
      "        [ 0.3859],\n",
      "        [ 0.8772],\n",
      "        [-0.1753],\n",
      "        [ 0.6260],\n",
      "        [ 0.6234],\n",
      "        [ 0.6829],\n",
      "        [ 0.7606],\n",
      "        [ 0.4307],\n",
      "        [ 0.2045],\n",
      "        [ 0.1956],\n",
      "        [ 0.0285],\n",
      "        [ 0.1485],\n",
      "        [ 0.1851],\n",
      "        [ 0.2084],\n",
      "        [ 0.8210],\n",
      "        [ 0.1878],\n",
      "        [ 0.6802],\n",
      "        [ 0.6982],\n",
      "        [ 0.6406],\n",
      "        [ 0.1899],\n",
      "        [ 0.6117],\n",
      "        [ 0.0299],\n",
      "        [ 0.5881],\n",
      "        [ 0.6233],\n",
      "        [ 0.6085],\n",
      "        [ 0.7125],\n",
      "        [ 0.9828],\n",
      "        [ 0.1670],\n",
      "        [ 0.8017],\n",
      "        [ 0.9754],\n",
      "        [ 0.2088],\n",
      "        [ 0.2264],\n",
      "        [ 0.2119],\n",
      "        [ 0.4422],\n",
      "        [ 0.6835],\n",
      "        [ 0.6975],\n",
      "        [ 0.0762],\n",
      "        [ 0.5402],\n",
      "        [ 0.5572],\n",
      "        [ 0.5523],\n",
      "        [ 0.2396],\n",
      "        [ 0.7226],\n",
      "        [ 0.1368],\n",
      "        [ 0.1962],\n",
      "        [ 0.2910],\n",
      "        [ 0.6550],\n",
      "        [ 0.2344],\n",
      "        [ 0.0865],\n",
      "        [ 0.3032],\n",
      "        [ 0.1509],\n",
      "        [ 0.1137],\n",
      "        [ 0.6644],\n",
      "        [ 0.6424],\n",
      "        [ 0.3709],\n",
      "        [ 0.3312],\n",
      "        [-0.0184],\n",
      "        [ 0.7836],\n",
      "        [ 0.3409],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992],\n",
      "        [0.4003],\n",
      "        [0.4035]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992],\n",
      "        [0.4003],\n",
      "        [0.4035]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.22750115394592\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 137\n",
      "剩餘X 資料 torch.Size([23, 18])\n",
      "剩餘Y 資料 torch.Size([23, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.17853839695453644, 14)\n",
      "The second_loss value of k: (0.17936688661575317, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0834])\n",
      "目前模型的Data狀態 torch.Size([137, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6383],\n",
      "        [ 0.9158],\n",
      "        [ 0.2124],\n",
      "        [ 0.6738],\n",
      "        [ 0.5743],\n",
      "        [ 0.1918],\n",
      "        [ 0.9575],\n",
      "        [ 0.2576],\n",
      "        [ 0.2086],\n",
      "        [ 0.6470],\n",
      "        [ 0.6790],\n",
      "        [ 0.3425],\n",
      "        [ 0.6143],\n",
      "        [ 0.6378],\n",
      "        [ 0.9085],\n",
      "        [ 0.6254],\n",
      "        [ 0.7100],\n",
      "        [ 0.5091],\n",
      "        [ 0.6564],\n",
      "        [ 0.3726],\n",
      "        [ 0.7670],\n",
      "        [ 0.5725],\n",
      "        [ 0.0415],\n",
      "        [ 0.5955],\n",
      "        [ 0.2582],\n",
      "        [ 0.5932],\n",
      "        [ 0.5420],\n",
      "        [ 0.1343],\n",
      "        [ 0.0766],\n",
      "        [ 0.8413],\n",
      "        [ 0.1472],\n",
      "        [ 0.1527],\n",
      "        [ 0.2129],\n",
      "        [ 0.5964],\n",
      "        [ 0.8185],\n",
      "        [ 0.2258],\n",
      "        [ 0.4193],\n",
      "        [ 0.6533],\n",
      "        [ 0.6857],\n",
      "        [ 0.6459],\n",
      "        [ 0.8713],\n",
      "        [ 0.0770],\n",
      "        [ 0.7268],\n",
      "        [ 0.8796],\n",
      "        [ 0.5059],\n",
      "        [ 0.7212],\n",
      "        [ 0.1680],\n",
      "        [ 0.2812],\n",
      "        [ 0.9037],\n",
      "        [ 0.5202],\n",
      "        [ 0.6026],\n",
      "        [ 0.8362],\n",
      "        [ 0.9837],\n",
      "        [ 0.0771],\n",
      "        [ 0.8093],\n",
      "        [ 0.1930],\n",
      "        [ 0.7881],\n",
      "        [ 0.7729],\n",
      "        [ 0.7141],\n",
      "        [ 0.2186],\n",
      "        [ 0.6854],\n",
      "        [ 0.8604],\n",
      "        [-0.1779],\n",
      "        [ 0.6516],\n",
      "        [ 0.4822],\n",
      "        [ 0.8213],\n",
      "        [ 0.4365],\n",
      "        [ 0.8201],\n",
      "        [ 0.6824],\n",
      "        [ 0.8277],\n",
      "        [ 0.3859],\n",
      "        [ 0.8772],\n",
      "        [-0.1753],\n",
      "        [ 0.6260],\n",
      "        [ 0.6234],\n",
      "        [ 0.6829],\n",
      "        [ 0.7606],\n",
      "        [ 0.4307],\n",
      "        [ 0.2045],\n",
      "        [ 0.1956],\n",
      "        [ 0.0285],\n",
      "        [ 0.1485],\n",
      "        [ 0.1851],\n",
      "        [ 0.2084],\n",
      "        [ 0.8210],\n",
      "        [ 0.1878],\n",
      "        [ 0.6802],\n",
      "        [ 0.6982],\n",
      "        [ 0.6406],\n",
      "        [ 0.1899],\n",
      "        [ 0.6117],\n",
      "        [ 0.0299],\n",
      "        [ 0.5881],\n",
      "        [ 0.6233],\n",
      "        [ 0.6085],\n",
      "        [ 0.7125],\n",
      "        [ 0.9828],\n",
      "        [ 0.1670],\n",
      "        [ 0.8017],\n",
      "        [ 0.9754],\n",
      "        [ 0.2088],\n",
      "        [ 0.2264],\n",
      "        [ 0.2119],\n",
      "        [ 0.4422],\n",
      "        [ 0.6835],\n",
      "        [ 0.6975],\n",
      "        [ 0.0762],\n",
      "        [ 0.5402],\n",
      "        [ 0.5572],\n",
      "        [ 0.5523],\n",
      "        [ 0.2396],\n",
      "        [ 0.7226],\n",
      "        [ 0.1368],\n",
      "        [ 0.1962],\n",
      "        [ 0.2910],\n",
      "        [ 0.6550],\n",
      "        [ 0.2344],\n",
      "        [ 0.0865],\n",
      "        [ 0.3032],\n",
      "        [ 0.1509],\n",
      "        [ 0.1137],\n",
      "        [ 0.6644],\n",
      "        [ 0.6424],\n",
      "        [ 0.3709],\n",
      "        [ 0.3312],\n",
      "        [-0.0184],\n",
      "        [ 0.7836],\n",
      "        [ 0.3409],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992],\n",
      "        [0.4003],\n",
      "        [0.4035],\n",
      "        [0.4225]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992],\n",
      "        [0.4003],\n",
      "        [0.4035],\n",
      "        [0.4225]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.26487612724304\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 138\n",
      "剩餘X 資料 torch.Size([22, 18])\n",
      "剩餘Y 資料 torch.Size([22, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.17936688661575317, 14)\n",
      "The second_loss value of k: (0.1852874457836151, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0825])\n",
      "目前模型的Data狀態 torch.Size([138, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6383],\n",
      "        [ 0.9158],\n",
      "        [ 0.2124],\n",
      "        [ 0.6738],\n",
      "        [ 0.5743],\n",
      "        [ 0.1918],\n",
      "        [ 0.9575],\n",
      "        [ 0.2576],\n",
      "        [ 0.2086],\n",
      "        [ 0.6470],\n",
      "        [ 0.6790],\n",
      "        [ 0.3425],\n",
      "        [ 0.6143],\n",
      "        [ 0.6378],\n",
      "        [ 0.9085],\n",
      "        [ 0.6254],\n",
      "        [ 0.7100],\n",
      "        [ 0.5091],\n",
      "        [ 0.6564],\n",
      "        [ 0.3726],\n",
      "        [ 0.7670],\n",
      "        [ 0.5725],\n",
      "        [ 0.0415],\n",
      "        [ 0.5955],\n",
      "        [ 0.2582],\n",
      "        [ 0.5932],\n",
      "        [ 0.5420],\n",
      "        [ 0.1343],\n",
      "        [ 0.0766],\n",
      "        [ 0.8413],\n",
      "        [ 0.1472],\n",
      "        [ 0.1527],\n",
      "        [ 0.2129],\n",
      "        [ 0.5964],\n",
      "        [ 0.8185],\n",
      "        [ 0.2258],\n",
      "        [ 0.4193],\n",
      "        [ 0.6533],\n",
      "        [ 0.6857],\n",
      "        [ 0.6459],\n",
      "        [ 0.8713],\n",
      "        [ 0.0770],\n",
      "        [ 0.7268],\n",
      "        [ 0.8796],\n",
      "        [ 0.5059],\n",
      "        [ 0.7212],\n",
      "        [ 0.1680],\n",
      "        [ 0.2812],\n",
      "        [ 0.9037],\n",
      "        [ 0.5202],\n",
      "        [ 0.6026],\n",
      "        [ 0.8362],\n",
      "        [ 0.9837],\n",
      "        [ 0.0771],\n",
      "        [ 0.8093],\n",
      "        [ 0.1930],\n",
      "        [ 0.7881],\n",
      "        [ 0.7729],\n",
      "        [ 0.7141],\n",
      "        [ 0.2186],\n",
      "        [ 0.6854],\n",
      "        [ 0.8604],\n",
      "        [-0.1779],\n",
      "        [ 0.6516],\n",
      "        [ 0.4822],\n",
      "        [ 0.8213],\n",
      "        [ 0.4365],\n",
      "        [ 0.8201],\n",
      "        [ 0.6824],\n",
      "        [ 0.8277],\n",
      "        [ 0.3859],\n",
      "        [ 0.8772],\n",
      "        [-0.1753],\n",
      "        [ 0.6260],\n",
      "        [ 0.6234],\n",
      "        [ 0.6829],\n",
      "        [ 0.7606],\n",
      "        [ 0.4307],\n",
      "        [ 0.2045],\n",
      "        [ 0.1956],\n",
      "        [ 0.0285],\n",
      "        [ 0.1485],\n",
      "        [ 0.1851],\n",
      "        [ 0.2084],\n",
      "        [ 0.8210],\n",
      "        [ 0.1878],\n",
      "        [ 0.6802],\n",
      "        [ 0.6982],\n",
      "        [ 0.6406],\n",
      "        [ 0.1899],\n",
      "        [ 0.6117],\n",
      "        [ 0.0299],\n",
      "        [ 0.5881],\n",
      "        [ 0.6233],\n",
      "        [ 0.6085],\n",
      "        [ 0.7125],\n",
      "        [ 0.9828],\n",
      "        [ 0.1670],\n",
      "        [ 0.8017],\n",
      "        [ 0.9754],\n",
      "        [ 0.2088],\n",
      "        [ 0.2264],\n",
      "        [ 0.2119],\n",
      "        [ 0.4422],\n",
      "        [ 0.6835],\n",
      "        [ 0.6975],\n",
      "        [ 0.0762],\n",
      "        [ 0.5402],\n",
      "        [ 0.5572],\n",
      "        [ 0.5523],\n",
      "        [ 0.2396],\n",
      "        [ 0.7226],\n",
      "        [ 0.1368],\n",
      "        [ 0.1962],\n",
      "        [ 0.2910],\n",
      "        [ 0.6550],\n",
      "        [ 0.2344],\n",
      "        [ 0.0865],\n",
      "        [ 0.3032],\n",
      "        [ 0.1509],\n",
      "        [ 0.1137],\n",
      "        [ 0.6644],\n",
      "        [ 0.6424],\n",
      "        [ 0.3709],\n",
      "        [ 0.3312],\n",
      "        [-0.0184],\n",
      "        [ 0.7836],\n",
      "        [ 0.3409],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992],\n",
      "        [0.4003],\n",
      "        [0.4035],\n",
      "        [0.4225],\n",
      "        [0.4235]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "Regularizing結束-因為沒有顧好預測誤差\n",
      "Number of enlarge: 0\n",
      "Number of shrink: 0\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992],\n",
      "        [0.4003],\n",
      "        [0.4035],\n",
      "        [0.4225],\n",
      "        [0.4235]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.3012342453003\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 139\n",
      "剩餘X 資料 torch.Size([21, 18])\n",
      "剩餘Y 資料 torch.Size([21, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1852874457836151, 4)\n",
      "The second_loss value of k: (0.18908345699310303, 13)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引4，y= tensor([0.0755])\n",
      "目前模型的Data狀態 torch.Size([139, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6383],\n",
      "        [ 0.9158],\n",
      "        [ 0.2124],\n",
      "        [ 0.6738],\n",
      "        [ 0.5743],\n",
      "        [ 0.1918],\n",
      "        [ 0.9575],\n",
      "        [ 0.2576],\n",
      "        [ 0.2086],\n",
      "        [ 0.6470],\n",
      "        [ 0.6790],\n",
      "        [ 0.3425],\n",
      "        [ 0.6143],\n",
      "        [ 0.6378],\n",
      "        [ 0.9085],\n",
      "        [ 0.6254],\n",
      "        [ 0.7100],\n",
      "        [ 0.5091],\n",
      "        [ 0.6564],\n",
      "        [ 0.3726],\n",
      "        [ 0.7670],\n",
      "        [ 0.5725],\n",
      "        [ 0.0415],\n",
      "        [ 0.5955],\n",
      "        [ 0.2582],\n",
      "        [ 0.5932],\n",
      "        [ 0.5420],\n",
      "        [ 0.1343],\n",
      "        [ 0.0766],\n",
      "        [ 0.8413],\n",
      "        [ 0.1472],\n",
      "        [ 0.1527],\n",
      "        [ 0.2129],\n",
      "        [ 0.5964],\n",
      "        [ 0.8185],\n",
      "        [ 0.2258],\n",
      "        [ 0.4193],\n",
      "        [ 0.6533],\n",
      "        [ 0.6857],\n",
      "        [ 0.6459],\n",
      "        [ 0.8713],\n",
      "        [ 0.0770],\n",
      "        [ 0.7268],\n",
      "        [ 0.8796],\n",
      "        [ 0.5059],\n",
      "        [ 0.7212],\n",
      "        [ 0.1680],\n",
      "        [ 0.2812],\n",
      "        [ 0.9037],\n",
      "        [ 0.5202],\n",
      "        [ 0.6026],\n",
      "        [ 0.8362],\n",
      "        [ 0.9837],\n",
      "        [ 0.0771],\n",
      "        [ 0.8093],\n",
      "        [ 0.1930],\n",
      "        [ 0.7881],\n",
      "        [ 0.7729],\n",
      "        [ 0.7141],\n",
      "        [ 0.2186],\n",
      "        [ 0.6854],\n",
      "        [ 0.8604],\n",
      "        [-0.1779],\n",
      "        [ 0.6516],\n",
      "        [ 0.4822],\n",
      "        [ 0.8213],\n",
      "        [ 0.4365],\n",
      "        [ 0.8201],\n",
      "        [ 0.6824],\n",
      "        [ 0.8277],\n",
      "        [ 0.3859],\n",
      "        [ 0.8772],\n",
      "        [-0.1753],\n",
      "        [ 0.6260],\n",
      "        [ 0.6234],\n",
      "        [ 0.6829],\n",
      "        [ 0.7606],\n",
      "        [ 0.4307],\n",
      "        [ 0.2045],\n",
      "        [ 0.1956],\n",
      "        [ 0.0285],\n",
      "        [ 0.1485],\n",
      "        [ 0.1851],\n",
      "        [ 0.2084],\n",
      "        [ 0.8210],\n",
      "        [ 0.1878],\n",
      "        [ 0.6802],\n",
      "        [ 0.6982],\n",
      "        [ 0.6406],\n",
      "        [ 0.1899],\n",
      "        [ 0.6117],\n",
      "        [ 0.0299],\n",
      "        [ 0.5881],\n",
      "        [ 0.6233],\n",
      "        [ 0.6085],\n",
      "        [ 0.7125],\n",
      "        [ 0.9828],\n",
      "        [ 0.1670],\n",
      "        [ 0.8017],\n",
      "        [ 0.9754],\n",
      "        [ 0.2088],\n",
      "        [ 0.2264],\n",
      "        [ 0.2119],\n",
      "        [ 0.4422],\n",
      "        [ 0.6835],\n",
      "        [ 0.6975],\n",
      "        [ 0.0762],\n",
      "        [ 0.5402],\n",
      "        [ 0.5572],\n",
      "        [ 0.5523],\n",
      "        [ 0.2396],\n",
      "        [ 0.7226],\n",
      "        [ 0.1368],\n",
      "        [ 0.1962],\n",
      "        [ 0.2910],\n",
      "        [ 0.6550],\n",
      "        [ 0.2344],\n",
      "        [ 0.0865],\n",
      "        [ 0.3032],\n",
      "        [ 0.1509],\n",
      "        [ 0.1137],\n",
      "        [ 0.6644],\n",
      "        [ 0.6424],\n",
      "        [ 0.3709],\n",
      "        [ 0.3312],\n",
      "        [-0.0184],\n",
      "        [ 0.7836],\n",
      "        [ 0.3409],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060],\n",
      "        [ 0.5060]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0793],\n",
      "        [0.3088],\n",
      "        [0.0947],\n",
      "        [0.0146],\n",
      "        [0.0185],\n",
      "        [0.0116],\n",
      "        [0.3768],\n",
      "        [0.0655],\n",
      "        [0.2318],\n",
      "        [0.0120],\n",
      "        [0.0192],\n",
      "        [0.1447],\n",
      "        [0.0332],\n",
      "        [0.0768],\n",
      "        [0.2919],\n",
      "        [0.0161],\n",
      "        [0.0328],\n",
      "        [0.0173],\n",
      "        [0.0011],\n",
      "        [0.1835],\n",
      "        [0.2155],\n",
      "        [0.0635],\n",
      "        [0.2727],\n",
      "        [0.0499],\n",
      "        [0.2157],\n",
      "        [0.0432],\n",
      "        [0.0059],\n",
      "        [0.3376],\n",
      "        [0.2348],\n",
      "        [0.2305],\n",
      "        [0.1781],\n",
      "        [0.3118],\n",
      "        [0.1063],\n",
      "        [0.0448],\n",
      "        [0.1998],\n",
      "        [0.0820],\n",
      "        [0.0516],\n",
      "        [0.0078],\n",
      "        [0.0950],\n",
      "        [0.0105],\n",
      "        [0.3075],\n",
      "        [0.1566],\n",
      "        [0.0433],\n",
      "        [0.2945],\n",
      "        [0.0347],\n",
      "        [0.1139],\n",
      "        [0.0942],\n",
      "        [0.1221],\n",
      "        [0.2751],\n",
      "        [0.0164],\n",
      "        [0.0138],\n",
      "        [0.2146],\n",
      "        [0.3531],\n",
      "        [0.1146],\n",
      "        [0.1922],\n",
      "        [0.0780],\n",
      "        [0.1603],\n",
      "        [0.0779],\n",
      "        [0.1876],\n",
      "        [0.0840],\n",
      "        [0.0618],\n",
      "        [0.2164],\n",
      "        [0.3399],\n",
      "        [0.0278],\n",
      "        [0.0518],\n",
      "        [0.1879],\n",
      "        [0.0866],\n",
      "        [0.1151],\n",
      "        [0.0090],\n",
      "        [0.2172],\n",
      "        [0.1681],\n",
      "        [0.2342],\n",
      "        [0.4299],\n",
      "        [0.0460],\n",
      "        [0.0565],\n",
      "        [0.0205],\n",
      "        [0.0528],\n",
      "        [0.1022],\n",
      "        [0.1534],\n",
      "        [0.1631],\n",
      "        [0.3258],\n",
      "        [0.0356],\n",
      "        [0.1729],\n",
      "        [0.1502],\n",
      "        [0.1971],\n",
      "        [0.0324],\n",
      "        [0.0467],\n",
      "        [0.0179],\n",
      "        [0.0650],\n",
      "        [0.1735],\n",
      "        [0.1001],\n",
      "        [0.3345],\n",
      "        [0.1026],\n",
      "        [0.0733],\n",
      "        [0.0808],\n",
      "        [0.2012],\n",
      "        [0.3060],\n",
      "        [0.0138],\n",
      "        [0.1387],\n",
      "        [0.3037],\n",
      "        [0.0212],\n",
      "        [0.0645],\n",
      "        [0.0210],\n",
      "        [0.3241],\n",
      "        [0.1881],\n",
      "        [0.0832],\n",
      "        [0.3227],\n",
      "        [0.1766],\n",
      "        [0.1771],\n",
      "        [0.1832],\n",
      "        [0.0768],\n",
      "        [0.1012],\n",
      "        [0.2543],\n",
      "        [0.1302],\n",
      "        [0.0236],\n",
      "        [0.1981],\n",
      "        [0.0524],\n",
      "        [0.3500],\n",
      "        [0.0383],\n",
      "        [0.3013],\n",
      "        [0.3373],\n",
      "        [0.2407],\n",
      "        [0.2219],\n",
      "        [0.3439],\n",
      "        [0.1038],\n",
      "        [0.0184],\n",
      "        [0.3225],\n",
      "        [0.1434],\n",
      "        [0.3755],\n",
      "        [0.3850],\n",
      "        [0.3868],\n",
      "        [0.3953],\n",
      "        [0.3971],\n",
      "        [0.3992],\n",
      "        [0.4003],\n",
      "        [0.4035],\n",
      "        [0.4225],\n",
      "        [0.4235],\n",
      "        [0.4305]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 18\n",
      "Number of shrink: 0\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0400],\n",
      "        [0.2529],\n",
      "        [0.0748],\n",
      "        [0.0201],\n",
      "        [0.0180],\n",
      "        [0.0464],\n",
      "        [0.3234],\n",
      "        [0.1020],\n",
      "        [0.2374],\n",
      "        [0.0452],\n",
      "        [0.0135],\n",
      "        [0.1604],\n",
      "        [0.0635],\n",
      "        [0.0363],\n",
      "        [0.2360],\n",
      "        [0.0501],\n",
      "        [0.0004],\n",
      "        [0.0468],\n",
      "        [0.0310],\n",
      "        [0.2147],\n",
      "        [0.1762],\n",
      "        [0.0964],\n",
      "        [0.2458],\n",
      "        [0.0838],\n",
      "        [0.2281],\n",
      "        [0.0773],\n",
      "        [0.0398],\n",
      "        [0.3423],\n",
      "        [0.2085],\n",
      "        [0.1802],\n",
      "        [0.1554],\n",
      "        [0.3159],\n",
      "        [0.0906],\n",
      "        [0.0762],\n",
      "        [0.1517],\n",
      "        [0.0643],\n",
      "        [0.0705],\n",
      "        [0.0380],\n",
      "        [0.0522],\n",
      "        [0.0206],\n",
      "        [0.2569],\n",
      "        [0.1230],\n",
      "        [0.0109],\n",
      "        [0.2504],\n",
      "        [0.0632],\n",
      "        [0.0681],\n",
      "        [0.0697],\n",
      "        [0.1538],\n",
      "        [0.2196],\n",
      "        [0.0437],\n",
      "        [0.0190],\n",
      "        [0.1626],\n",
      "        [0.2971],\n",
      "        [0.0823],\n",
      "        [0.1479],\n",
      "        [0.0557],\n",
      "        [0.1116],\n",
      "        [0.0453],\n",
      "        [0.1537],\n",
      "        [0.0701],\n",
      "        [0.0282],\n",
      "        [0.1635],\n",
      "        [0.3070],\n",
      "        [0.0587],\n",
      "        [0.0770],\n",
      "        [0.1362],\n",
      "        [0.1063],\n",
      "        [0.0784],\n",
      "        [0.0417],\n",
      "        [0.1739],\n",
      "        [0.2019],\n",
      "        [0.1802],\n",
      "        [0.3983],\n",
      "        [0.0783],\n",
      "        [0.0224],\n",
      "        [0.0539],\n",
      "        [0.0181],\n",
      "        [0.1224],\n",
      "        [0.1344],\n",
      "        [0.1425],\n",
      "        [0.2967],\n",
      "        [0.0074],\n",
      "        [0.1520],\n",
      "        [0.1297],\n",
      "        [0.1540],\n",
      "        [0.0092],\n",
      "        [0.0140],\n",
      "        [0.0507],\n",
      "        [0.0948],\n",
      "        [0.1522],\n",
      "        [0.1312],\n",
      "        [0.3043],\n",
      "        [0.1323],\n",
      "        [0.1034],\n",
      "        [0.1113],\n",
      "        [0.1659],\n",
      "        [0.2484],\n",
      "        [0.0123],\n",
      "        [0.0972],\n",
      "        [0.2464],\n",
      "        [0.0442],\n",
      "        [0.0559],\n",
      "        [0.0422],\n",
      "        [0.3268],\n",
      "        [0.1544],\n",
      "        [0.0483],\n",
      "        [0.2939],\n",
      "        [0.1989],\n",
      "        [0.2038],\n",
      "        [0.2080],\n",
      "        [0.0973],\n",
      "        [0.0632],\n",
      "        [0.2268],\n",
      "        [0.1356],\n",
      "        [0.0261],\n",
      "        [0.1675],\n",
      "        [0.0540],\n",
      "        [0.3215],\n",
      "        [0.0387],\n",
      "        [0.2763],\n",
      "        [0.3091],\n",
      "        [0.2065],\n",
      "        [0.1881],\n",
      "        [0.3732],\n",
      "        [0.1015],\n",
      "        [0.0097],\n",
      "        [0.2800],\n",
      "        [0.1471],\n",
      "        [0.3750],\n",
      "        [0.3845],\n",
      "        [0.3863],\n",
      "        [0.3948],\n",
      "        [0.3966],\n",
      "        [0.3987],\n",
      "        [0.3998],\n",
      "        [0.4030],\n",
      "        [0.4220],\n",
      "        [0.4230],\n",
      "        [0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0232],\n",
      "        [0.1704],\n",
      "        [0.0042],\n",
      "        [0.0183],\n",
      "        [0.0186],\n",
      "        [0.1104],\n",
      "        [0.2245],\n",
      "        [0.1598],\n",
      "        [0.1711],\n",
      "        [0.0388],\n",
      "        [0.0084],\n",
      "        [0.1284],\n",
      "        [0.0477],\n",
      "        [0.0179],\n",
      "        [0.1596],\n",
      "        [0.0631],\n",
      "        [0.0045],\n",
      "        [0.0233],\n",
      "        [0.0051],\n",
      "        [0.2482],\n",
      "        [0.1154],\n",
      "        [0.1030],\n",
      "        [0.1052],\n",
      "        [0.0961],\n",
      "        [0.1765],\n",
      "        [0.0896],\n",
      "        [0.0276],\n",
      "        [0.2673],\n",
      "        [0.0694],\n",
      "        [0.1054],\n",
      "        [0.0346],\n",
      "        [0.2443],\n",
      "        [0.0326],\n",
      "        [0.0639],\n",
      "        [0.0787],\n",
      "        [0.0073],\n",
      "        [0.0543],\n",
      "        [0.0223],\n",
      "        [0.0235],\n",
      "        [0.0120],\n",
      "        [0.1815],\n",
      "        [0.0300],\n",
      "        [0.0217],\n",
      "        [0.1616],\n",
      "        [0.0438],\n",
      "        [0.0326],\n",
      "        [0.0325],\n",
      "        [0.2041],\n",
      "        [0.1336],\n",
      "        [0.0352],\n",
      "        [0.0332],\n",
      "        [0.0785],\n",
      "        [0.1914],\n",
      "        [0.0163],\n",
      "        [0.0741],\n",
      "        [0.0488],\n",
      "        [0.0616],\n",
      "        [0.0603],\n",
      "        [0.1079],\n",
      "        [0.0164],\n",
      "        [0.0424],\n",
      "        [0.0997],\n",
      "        [0.2508],\n",
      "        [0.0558],\n",
      "        [0.0685],\n",
      "        [0.0590],\n",
      "        [0.0913],\n",
      "        [0.0892],\n",
      "        [0.0316],\n",
      "        [0.0960],\n",
      "        [0.2347],\n",
      "        [0.0895],\n",
      "        [0.3118],\n",
      "        [0.0942],\n",
      "        [0.0082],\n",
      "        [0.0392],\n",
      "        [0.0383],\n",
      "        [0.1082],\n",
      "        [0.0503],\n",
      "        [0.0604],\n",
      "        [0.1613],\n",
      "        [0.0750],\n",
      "        [0.0622],\n",
      "        [0.0483],\n",
      "        [0.0714],\n",
      "        [0.0850],\n",
      "        [0.0461],\n",
      "        [0.0215],\n",
      "        [0.0726],\n",
      "        [0.0645],\n",
      "        [0.1120],\n",
      "        [0.1691],\n",
      "        [0.1339],\n",
      "        [0.1038],\n",
      "        [0.1212],\n",
      "        [0.1283],\n",
      "        [0.1456],\n",
      "        [0.0994],\n",
      "        [0.1005],\n",
      "        [0.1404],\n",
      "        [0.1248],\n",
      "        [0.0089],\n",
      "        [0.1275],\n",
      "        [0.3121],\n",
      "        [0.1230],\n",
      "        [0.0765],\n",
      "        [0.1724],\n",
      "        [0.1665],\n",
      "        [0.1718],\n",
      "        [0.1721],\n",
      "        [0.1716],\n",
      "        [0.0812],\n",
      "        [0.1195],\n",
      "        [0.0688],\n",
      "        [0.0603],\n",
      "        [0.1408],\n",
      "        [0.0092],\n",
      "        [0.2007],\n",
      "        [0.0786],\n",
      "        [0.1645],\n",
      "        [0.1926],\n",
      "        [0.1766],\n",
      "        [0.1710],\n",
      "        [0.4034],\n",
      "        [0.1476],\n",
      "        [0.0507],\n",
      "        [0.2880],\n",
      "        [0.1435],\n",
      "        [0.3708],\n",
      "        [0.3803],\n",
      "        [0.3821],\n",
      "        [0.3906],\n",
      "        [0.3924],\n",
      "        [0.3945],\n",
      "        [0.3956],\n",
      "        [0.3988],\n",
      "        [0.4179],\n",
      "        [0.4188],\n",
      "        [0.4258]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.5884599685669\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 140\n",
      "剩餘X 資料 torch.Size([20, 18])\n",
      "剩餘Y 資料 torch.Size([20, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.185034841299057, 12)\n",
      "The second_loss value of k: (0.18675744533538818, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引12，y= tensor([0.0711])\n",
      "目前模型的Data狀態 torch.Size([140, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.5822],\n",
      "        [ 0.7774],\n",
      "        [ 0.3113],\n",
      "        [ 0.6408],\n",
      "        [ 0.5373],\n",
      "        [ 0.2906],\n",
      "        [ 0.8051],\n",
      "        [ 0.3519],\n",
      "        [ 0.2693],\n",
      "        [ 0.6202],\n",
      "        [ 0.6514],\n",
      "        [ 0.3589],\n",
      "        [ 0.5998],\n",
      "        [ 0.5790],\n",
      "        [ 0.7762],\n",
      "        [ 0.5784],\n",
      "        [ 0.6817],\n",
      "        [ 0.5031],\n",
      "        [ 0.6604],\n",
      "        [ 0.4373],\n",
      "        [ 0.6668],\n",
      "        [ 0.5330],\n",
      "        [ 0.2090],\n",
      "        [ 0.5492],\n",
      "        [ 0.2975],\n",
      "        [ 0.5468],\n",
      "        [ 0.5203],\n",
      "        [ 0.2046],\n",
      "        [ 0.2421],\n",
      "        [ 0.7162],\n",
      "        [ 0.2907],\n",
      "        [ 0.2203],\n",
      "        [ 0.2867],\n",
      "        [ 0.5773],\n",
      "        [ 0.6974],\n",
      "        [ 0.3150],\n",
      "        [ 0.4166],\n",
      "        [ 0.6388],\n",
      "        [ 0.6141],\n",
      "        [ 0.6233],\n",
      "        [ 0.7453],\n",
      "        [ 0.2036],\n",
      "        [ 0.7052],\n",
      "        [ 0.7467],\n",
      "        [ 0.4968],\n",
      "        [ 0.6399],\n",
      "        [ 0.2947],\n",
      "        [ 0.3632],\n",
      "        [ 0.7622],\n",
      "        [ 0.5015],\n",
      "        [ 0.5557],\n",
      "        [ 0.7001],\n",
      "        [ 0.8220],\n",
      "        [ 0.2080],\n",
      "        [ 0.6912],\n",
      "        [ 0.3198],\n",
      "        [ 0.6894],\n",
      "        [ 0.7553],\n",
      "        [ 0.6344],\n",
      "        [ 0.2863],\n",
      "        [ 0.6660],\n",
      "        [ 0.7437],\n",
      "        [-0.0888],\n",
      "        [ 0.6237],\n",
      "        [ 0.4655],\n",
      "        [ 0.6923],\n",
      "        [ 0.4317],\n",
      "        [ 0.7943],\n",
      "        [ 0.6598],\n",
      "        [ 0.7065],\n",
      "        [ 0.4526],\n",
      "        [ 0.7324],\n",
      "        [-0.0573],\n",
      "        [ 0.5779],\n",
      "        [ 0.5751],\n",
      "        [ 0.6641],\n",
      "        [ 0.7461],\n",
      "        [ 0.4248],\n",
      "        [ 0.3076],\n",
      "        [ 0.2983],\n",
      "        [ 0.1930],\n",
      "        [ 0.2591],\n",
      "        [ 0.2958],\n",
      "        [ 0.3103],\n",
      "        [ 0.6953],\n",
      "        [ 0.3052],\n",
      "        [ 0.6796],\n",
      "        [ 0.6946],\n",
      "        [ 0.6331],\n",
      "        [ 0.2990],\n",
      "        [ 0.5998],\n",
      "        [ 0.1954],\n",
      "        [ 0.5569],\n",
      "        [ 0.5928],\n",
      "        [ 0.5681],\n",
      "        [ 0.6397],\n",
      "        [ 0.8223],\n",
      "        [ 0.2802],\n",
      "        [ 0.7635],\n",
      "        [ 0.8121],\n",
      "        [ 0.3125],\n",
      "        [ 0.2821],\n",
      "        [ 0.3184],\n",
      "        [ 0.4302],\n",
      "        [ 0.6184],\n",
      "        [ 0.6909],\n",
      "        [ 0.2266],\n",
      "        [ 0.5502],\n",
      "        [ 0.5625],\n",
      "        [ 0.5634],\n",
      "        [ 0.3344],\n",
      "        [ 0.7025],\n",
      "        [ 0.2716],\n",
      "        [ 0.2576],\n",
      "        [ 0.3278],\n",
      "        [ 0.5977],\n",
      "        [ 0.2960],\n",
      "        [ 0.2359],\n",
      "        [ 0.3436],\n",
      "        [ 0.2877],\n",
      "        [ 0.2585],\n",
      "        [ 0.6003],\n",
      "        [ 0.5915],\n",
      "        [ 0.4305],\n",
      "        [ 0.3750],\n",
      "        [ 0.0507],\n",
      "        [ 0.7492],\n",
      "        [ 0.3410],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013],\n",
      "        [ 0.5013]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0232],\n",
      "        [0.1704],\n",
      "        [0.0042],\n",
      "        [0.0183],\n",
      "        [0.0186],\n",
      "        [0.1104],\n",
      "        [0.2245],\n",
      "        [0.1598],\n",
      "        [0.1711],\n",
      "        [0.0388],\n",
      "        [0.0084],\n",
      "        [0.1284],\n",
      "        [0.0477],\n",
      "        [0.0179],\n",
      "        [0.1596],\n",
      "        [0.0631],\n",
      "        [0.0045],\n",
      "        [0.0233],\n",
      "        [0.0051],\n",
      "        [0.2482],\n",
      "        [0.1154],\n",
      "        [0.1030],\n",
      "        [0.1052],\n",
      "        [0.0961],\n",
      "        [0.1765],\n",
      "        [0.0896],\n",
      "        [0.0276],\n",
      "        [0.2673],\n",
      "        [0.0694],\n",
      "        [0.1054],\n",
      "        [0.0346],\n",
      "        [0.2443],\n",
      "        [0.0326],\n",
      "        [0.0639],\n",
      "        [0.0787],\n",
      "        [0.0073],\n",
      "        [0.0543],\n",
      "        [0.0223],\n",
      "        [0.0235],\n",
      "        [0.0120],\n",
      "        [0.1815],\n",
      "        [0.0300],\n",
      "        [0.0217],\n",
      "        [0.1616],\n",
      "        [0.0438],\n",
      "        [0.0326],\n",
      "        [0.0325],\n",
      "        [0.2041],\n",
      "        [0.1336],\n",
      "        [0.0352],\n",
      "        [0.0332],\n",
      "        [0.0785],\n",
      "        [0.1914],\n",
      "        [0.0163],\n",
      "        [0.0741],\n",
      "        [0.0488],\n",
      "        [0.0616],\n",
      "        [0.0603],\n",
      "        [0.1079],\n",
      "        [0.0164],\n",
      "        [0.0424],\n",
      "        [0.0997],\n",
      "        [0.2508],\n",
      "        [0.0558],\n",
      "        [0.0685],\n",
      "        [0.0590],\n",
      "        [0.0913],\n",
      "        [0.0892],\n",
      "        [0.0316],\n",
      "        [0.0960],\n",
      "        [0.2347],\n",
      "        [0.0895],\n",
      "        [0.3118],\n",
      "        [0.0942],\n",
      "        [0.0082],\n",
      "        [0.0392],\n",
      "        [0.0383],\n",
      "        [0.1082],\n",
      "        [0.0503],\n",
      "        [0.0604],\n",
      "        [0.1613],\n",
      "        [0.0750],\n",
      "        [0.0622],\n",
      "        [0.0483],\n",
      "        [0.0714],\n",
      "        [0.0850],\n",
      "        [0.0461],\n",
      "        [0.0215],\n",
      "        [0.0726],\n",
      "        [0.0645],\n",
      "        [0.1120],\n",
      "        [0.1691],\n",
      "        [0.1339],\n",
      "        [0.1038],\n",
      "        [0.1212],\n",
      "        [0.1283],\n",
      "        [0.1456],\n",
      "        [0.0994],\n",
      "        [0.1005],\n",
      "        [0.1404],\n",
      "        [0.1248],\n",
      "        [0.0089],\n",
      "        [0.1275],\n",
      "        [0.3121],\n",
      "        [0.1230],\n",
      "        [0.0765],\n",
      "        [0.1724],\n",
      "        [0.1665],\n",
      "        [0.1718],\n",
      "        [0.1721],\n",
      "        [0.1716],\n",
      "        [0.0812],\n",
      "        [0.1195],\n",
      "        [0.0688],\n",
      "        [0.0603],\n",
      "        [0.1408],\n",
      "        [0.0092],\n",
      "        [0.2007],\n",
      "        [0.0786],\n",
      "        [0.1645],\n",
      "        [0.1926],\n",
      "        [0.1766],\n",
      "        [0.1710],\n",
      "        [0.4034],\n",
      "        [0.1476],\n",
      "        [0.0507],\n",
      "        [0.2880],\n",
      "        [0.1435],\n",
      "        [0.3708],\n",
      "        [0.3803],\n",
      "        [0.3821],\n",
      "        [0.3906],\n",
      "        [0.3924],\n",
      "        [0.3945],\n",
      "        [0.3956],\n",
      "        [0.3988],\n",
      "        [0.4179],\n",
      "        [0.4188],\n",
      "        [0.4258],\n",
      "        [0.4302]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 13\n",
      "Number of shrink: 0\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0090],\n",
      "        [0.1514],\n",
      "        [0.0030],\n",
      "        [0.0265],\n",
      "        [0.0319],\n",
      "        [0.1075],\n",
      "        [0.2044],\n",
      "        [0.1512],\n",
      "        [0.1778],\n",
      "        [0.0471],\n",
      "        [0.0149],\n",
      "        [0.1372],\n",
      "        [0.0537],\n",
      "        [0.0035],\n",
      "        [0.1407],\n",
      "        [0.0717],\n",
      "        [0.0024],\n",
      "        [0.0353],\n",
      "        [0.0028],\n",
      "        [0.2360],\n",
      "        [0.0961],\n",
      "        [0.1106],\n",
      "        [0.1047],\n",
      "        [0.1041],\n",
      "        [0.1837],\n",
      "        [0.0975],\n",
      "        [0.0408],\n",
      "        [0.2721],\n",
      "        [0.0691],\n",
      "        [0.0864],\n",
      "        [0.0360],\n",
      "        [0.2497],\n",
      "        [0.0401],\n",
      "        [0.0702],\n",
      "        [0.0601],\n",
      "        [0.0005],\n",
      "        [0.0644],\n",
      "        [0.0318],\n",
      "        [0.0079],\n",
      "        [0.0181],\n",
      "        [0.1626],\n",
      "        [0.0380],\n",
      "        [0.0147],\n",
      "        [0.1402],\n",
      "        [0.0559],\n",
      "        [0.0161],\n",
      "        [0.0294],\n",
      "        [0.1928],\n",
      "        [0.1140],\n",
      "        [0.0475],\n",
      "        [0.0415],\n",
      "        [0.0591],\n",
      "        [0.1702],\n",
      "        [0.0090],\n",
      "        [0.0554],\n",
      "        [0.0464],\n",
      "        [0.0443],\n",
      "        [0.0527],\n",
      "        [0.0924],\n",
      "        [0.0228],\n",
      "        [0.0328],\n",
      "        [0.0816],\n",
      "        [0.2542],\n",
      "        [0.0638],\n",
      "        [0.0807],\n",
      "        [0.0402],\n",
      "        [0.1020],\n",
      "        [0.0807],\n",
      "        [0.0394],\n",
      "        [0.0742],\n",
      "        [0.2234],\n",
      "        [0.0691],\n",
      "        [0.3150],\n",
      "        [0.1029],\n",
      "        [0.0009],\n",
      "        [0.0476],\n",
      "        [0.0309],\n",
      "        [0.1189],\n",
      "        [0.0582],\n",
      "        [0.0668],\n",
      "        [0.1609],\n",
      "        [0.0669],\n",
      "        [0.0689],\n",
      "        [0.0546],\n",
      "        [0.0517],\n",
      "        [0.0799],\n",
      "        [0.0420],\n",
      "        [0.0277],\n",
      "        [0.0781],\n",
      "        [0.0701],\n",
      "        [0.1199],\n",
      "        [0.1689],\n",
      "        [0.1420],\n",
      "        [0.1122],\n",
      "        [0.1297],\n",
      "        [0.1146],\n",
      "        [0.1245],\n",
      "        [0.0922],\n",
      "        [0.0945],\n",
      "        [0.1189],\n",
      "        [0.1176],\n",
      "        [0.0155],\n",
      "        [0.1211],\n",
      "        [0.3060],\n",
      "        [0.1118],\n",
      "        [0.0730],\n",
      "        [0.1733],\n",
      "        [0.1734],\n",
      "        [0.1787],\n",
      "        [0.1783],\n",
      "        [0.1641],\n",
      "        [0.0773],\n",
      "        [0.1231],\n",
      "        [0.0759],\n",
      "        [0.0527],\n",
      "        [0.1311],\n",
      "        [0.0022],\n",
      "        [0.2022],\n",
      "        [0.0714],\n",
      "        [0.1673],\n",
      "        [0.1954],\n",
      "        [0.1657],\n",
      "        [0.1608],\n",
      "        [0.3923],\n",
      "        [0.1398],\n",
      "        [0.0459],\n",
      "        [0.2828],\n",
      "        [0.1378],\n",
      "        [0.3706],\n",
      "        [0.3801],\n",
      "        [0.3819],\n",
      "        [0.3904],\n",
      "        [0.3922],\n",
      "        [0.3943],\n",
      "        [0.3954],\n",
      "        [0.3986],\n",
      "        [0.4177],\n",
      "        [0.4186],\n",
      "        [0.4256],\n",
      "        [0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 71\n",
      "Number of shrink: 29\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0212],\n",
      "        [0.1234],\n",
      "        [0.0038],\n",
      "        [0.0137],\n",
      "        [0.0096],\n",
      "        [0.1088],\n",
      "        [0.1590],\n",
      "        [0.1187],\n",
      "        [0.1239],\n",
      "        [0.0330],\n",
      "        [0.0011],\n",
      "        [0.1015],\n",
      "        [0.0302],\n",
      "        [0.0142],\n",
      "        [0.1172],\n",
      "        [0.0540],\n",
      "        [0.0089],\n",
      "        [0.0033],\n",
      "        [0.0150],\n",
      "        [0.1806],\n",
      "        [0.0482],\n",
      "        [0.0818],\n",
      "        [0.0447],\n",
      "        [0.0812],\n",
      "        [0.1325],\n",
      "        [0.0727],\n",
      "        [0.0136],\n",
      "        [0.2043],\n",
      "        [0.0080],\n",
      "        [0.0509],\n",
      "        [0.0097],\n",
      "        [0.1874],\n",
      "        [0.0402],\n",
      "        [0.0459],\n",
      "        [0.0238],\n",
      "        [0.0034],\n",
      "        [0.0385],\n",
      "        [0.0325],\n",
      "        [0.0122],\n",
      "        [0.0023],\n",
      "        [0.1296],\n",
      "        [0.0457],\n",
      "        [0.0238],\n",
      "        [0.0729],\n",
      "        [0.0279],\n",
      "        [0.0154],\n",
      "        [0.0529],\n",
      "        [0.1494],\n",
      "        [0.0823],\n",
      "        [0.0269],\n",
      "        [0.0210],\n",
      "        [0.0296],\n",
      "        [0.1213],\n",
      "        [0.0068],\n",
      "        [0.0138],\n",
      "        [0.0755],\n",
      "        [0.0349],\n",
      "        [0.0569],\n",
      "        [0.0605],\n",
      "        [0.0122],\n",
      "        [0.0339],\n",
      "        [0.0644],\n",
      "        [0.2573],\n",
      "        [0.0522],\n",
      "        [0.0612],\n",
      "        [0.0139],\n",
      "        [0.0773],\n",
      "        [0.0843],\n",
      "        [0.0282],\n",
      "        [0.0098],\n",
      "        [0.1708],\n",
      "        [0.0330],\n",
      "        [0.3023],\n",
      "        [0.0911],\n",
      "        [0.0172],\n",
      "        [0.0378],\n",
      "        [0.0470],\n",
      "        [0.0950],\n",
      "        [0.0551],\n",
      "        [0.0595],\n",
      "        [0.1088],\n",
      "        [0.0576],\n",
      "        [0.0605],\n",
      "        [0.0477],\n",
      "        [0.0026],\n",
      "        [0.0944],\n",
      "        [0.0694],\n",
      "        [0.0021],\n",
      "        [0.0526],\n",
      "        [0.0599],\n",
      "        [0.1068],\n",
      "        [0.1209],\n",
      "        [0.1287],\n",
      "        [0.1018],\n",
      "        [0.1190],\n",
      "        [0.0953],\n",
      "        [0.0798],\n",
      "        [0.0917],\n",
      "        [0.1010],\n",
      "        [0.0714],\n",
      "        [0.1167],\n",
      "        [0.0051],\n",
      "        [0.1298],\n",
      "        [0.2693],\n",
      "        [0.1065],\n",
      "        [0.1027],\n",
      "        [0.1365],\n",
      "        [0.1598],\n",
      "        [0.1594],\n",
      "        [0.1551],\n",
      "        [0.1605],\n",
      "        [0.1019],\n",
      "        [0.1022],\n",
      "        [0.0243],\n",
      "        [0.0723],\n",
      "        [0.1332],\n",
      "        [0.0447],\n",
      "        [0.1676],\n",
      "        [0.0983],\n",
      "        [0.1392],\n",
      "        [0.1674],\n",
      "        [0.1690],\n",
      "        [0.1732],\n",
      "        [0.3430],\n",
      "        [0.1687],\n",
      "        [0.0313],\n",
      "        [0.2992],\n",
      "        [0.1090],\n",
      "        [0.3680],\n",
      "        [0.3775],\n",
      "        [0.3793],\n",
      "        [0.3878],\n",
      "        [0.3896],\n",
      "        [0.3917],\n",
      "        [0.3927],\n",
      "        [0.3960],\n",
      "        [0.4150],\n",
      "        [0.4160],\n",
      "        [0.4229],\n",
      "        [0.4273]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 70.95228886604309\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 141\n",
      "剩餘X 資料 torch.Size([19, 18])\n",
      "剩餘Y 資料 torch.Size([19, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1843247264623642, 3)\n",
      "The second_loss value of k: (0.1850654035806656, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.0691])\n",
      "目前模型的Data狀態 torch.Size([141, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.5802],\n",
      "        [ 0.7304],\n",
      "        [ 0.3033],\n",
      "        [ 0.6455],\n",
      "        [ 0.5462],\n",
      "        [ 0.2890],\n",
      "        [ 0.7397],\n",
      "        [ 0.3108],\n",
      "        [ 0.3165],\n",
      "        [ 0.6260],\n",
      "        [ 0.6608],\n",
      "        [ 0.3858],\n",
      "        [ 0.6172],\n",
      "        [ 0.5753],\n",
      "        [ 0.7337],\n",
      "        [ 0.5876],\n",
      "        [ 0.6860],\n",
      "        [ 0.5231],\n",
      "        [ 0.6703],\n",
      "        [ 0.3698],\n",
      "        [ 0.5997],\n",
      "        [ 0.5542],\n",
      "        [ 0.2695],\n",
      "        [ 0.5642],\n",
      "        [ 0.3414],\n",
      "        [ 0.5637],\n",
      "        [ 0.5343],\n",
      "        [ 0.2677],\n",
      "        [ 0.3034],\n",
      "        [ 0.6617],\n",
      "        [ 0.3350],\n",
      "        [ 0.2771],\n",
      "        [ 0.2790],\n",
      "        [ 0.5952],\n",
      "        [ 0.6425],\n",
      "        [ 0.3044],\n",
      "        [ 0.4325],\n",
      "        [ 0.6285],\n",
      "        [ 0.6029],\n",
      "        [ 0.6377],\n",
      "        [ 0.6934],\n",
      "        [ 0.1879],\n",
      "        [ 0.7073],\n",
      "        [ 0.6580],\n",
      "        [ 0.5126],\n",
      "        [ 0.6227],\n",
      "        [ 0.3151],\n",
      "        [ 0.3085],\n",
      "        [ 0.7109],\n",
      "        [ 0.5097],\n",
      "        [ 0.5678],\n",
      "        [ 0.6512],\n",
      "        [ 0.7519],\n",
      "        [ 0.1985],\n",
      "        [ 0.6309],\n",
      "        [ 0.3465],\n",
      "        [ 0.6627],\n",
      "        [ 0.7519],\n",
      "        [ 0.5870],\n",
      "        [ 0.2905],\n",
      "        [ 0.6575],\n",
      "        [ 0.7083],\n",
      "        [-0.0952],\n",
      "        [ 0.6272],\n",
      "        [ 0.4728],\n",
      "        [ 0.6472],\n",
      "        [ 0.4457],\n",
      "        [ 0.7893],\n",
      "        [ 0.6632],\n",
      "        [ 0.6203],\n",
      "        [ 0.3886],\n",
      "        [ 0.6760],\n",
      "        [-0.0477],\n",
      "        [ 0.5810],\n",
      "        [ 0.5841],\n",
      "        [ 0.6656],\n",
      "        [ 0.7548],\n",
      "        [ 0.4379],\n",
      "        [ 0.3028],\n",
      "        [ 0.2992],\n",
      "        [ 0.2455],\n",
      "        [ 0.2417],\n",
      "        [ 0.2975],\n",
      "        [ 0.3108],\n",
      "        [ 0.6213],\n",
      "        [ 0.3145],\n",
      "        [ 0.7029],\n",
      "        [ 0.7140],\n",
      "        [ 0.6531],\n",
      "        [ 0.3035],\n",
      "        [ 0.6049],\n",
      "        [ 0.2436],\n",
      "        [ 0.5620],\n",
      "        [ 0.5948],\n",
      "        [ 0.5703],\n",
      "        [ 0.6067],\n",
      "        [ 0.7565],\n",
      "        [ 0.2725],\n",
      "        [ 0.7640],\n",
      "        [ 0.7431],\n",
      "        [ 0.3043],\n",
      "        [ 0.2961],\n",
      "        [ 0.3206],\n",
      "        [ 0.3874],\n",
      "        [ 0.6018],\n",
      "        [ 0.7170],\n",
      "        [ 0.2624],\n",
      "        [ 0.5570],\n",
      "        [ 0.5749],\n",
      "        [ 0.5804],\n",
      "        [ 0.3233],\n",
      "        [ 0.7232],\n",
      "        [ 0.2889],\n",
      "        [ 0.3021],\n",
      "        [ 0.3397],\n",
      "        [ 0.5901],\n",
      "        [ 0.3314],\n",
      "        [ 0.2689],\n",
      "        [ 0.3632],\n",
      "        [ 0.3130],\n",
      "        [ 0.2836],\n",
      "        [ 0.5927],\n",
      "        [ 0.5937],\n",
      "        [ 0.3700],\n",
      "        [ 0.3961],\n",
      "        [ 0.0313],\n",
      "        [ 0.7603],\n",
      "        [ 0.3066],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985],\n",
      "        [ 0.4985]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0212],\n",
      "        [0.1234],\n",
      "        [0.0038],\n",
      "        [0.0137],\n",
      "        [0.0096],\n",
      "        [0.1088],\n",
      "        [0.1590],\n",
      "        [0.1187],\n",
      "        [0.1239],\n",
      "        [0.0330],\n",
      "        [0.0011],\n",
      "        [0.1015],\n",
      "        [0.0302],\n",
      "        [0.0142],\n",
      "        [0.1172],\n",
      "        [0.0540],\n",
      "        [0.0089],\n",
      "        [0.0033],\n",
      "        [0.0150],\n",
      "        [0.1806],\n",
      "        [0.0482],\n",
      "        [0.0818],\n",
      "        [0.0447],\n",
      "        [0.0812],\n",
      "        [0.1325],\n",
      "        [0.0727],\n",
      "        [0.0136],\n",
      "        [0.2043],\n",
      "        [0.0080],\n",
      "        [0.0509],\n",
      "        [0.0097],\n",
      "        [0.1874],\n",
      "        [0.0402],\n",
      "        [0.0459],\n",
      "        [0.0238],\n",
      "        [0.0034],\n",
      "        [0.0385],\n",
      "        [0.0325],\n",
      "        [0.0122],\n",
      "        [0.0023],\n",
      "        [0.1296],\n",
      "        [0.0457],\n",
      "        [0.0238],\n",
      "        [0.0729],\n",
      "        [0.0279],\n",
      "        [0.0154],\n",
      "        [0.0529],\n",
      "        [0.1494],\n",
      "        [0.0823],\n",
      "        [0.0269],\n",
      "        [0.0210],\n",
      "        [0.0296],\n",
      "        [0.1213],\n",
      "        [0.0068],\n",
      "        [0.0138],\n",
      "        [0.0755],\n",
      "        [0.0349],\n",
      "        [0.0569],\n",
      "        [0.0605],\n",
      "        [0.0122],\n",
      "        [0.0339],\n",
      "        [0.0644],\n",
      "        [0.2573],\n",
      "        [0.0522],\n",
      "        [0.0612],\n",
      "        [0.0139],\n",
      "        [0.0773],\n",
      "        [0.0843],\n",
      "        [0.0282],\n",
      "        [0.0098],\n",
      "        [0.1708],\n",
      "        [0.0330],\n",
      "        [0.3023],\n",
      "        [0.0911],\n",
      "        [0.0172],\n",
      "        [0.0378],\n",
      "        [0.0470],\n",
      "        [0.0950],\n",
      "        [0.0551],\n",
      "        [0.0595],\n",
      "        [0.1088],\n",
      "        [0.0576],\n",
      "        [0.0605],\n",
      "        [0.0477],\n",
      "        [0.0026],\n",
      "        [0.0944],\n",
      "        [0.0694],\n",
      "        [0.0021],\n",
      "        [0.0526],\n",
      "        [0.0599],\n",
      "        [0.1068],\n",
      "        [0.1209],\n",
      "        [0.1287],\n",
      "        [0.1018],\n",
      "        [0.1190],\n",
      "        [0.0953],\n",
      "        [0.0798],\n",
      "        [0.0917],\n",
      "        [0.1010],\n",
      "        [0.0714],\n",
      "        [0.1167],\n",
      "        [0.0051],\n",
      "        [0.1298],\n",
      "        [0.2693],\n",
      "        [0.1065],\n",
      "        [0.1027],\n",
      "        [0.1365],\n",
      "        [0.1598],\n",
      "        [0.1594],\n",
      "        [0.1551],\n",
      "        [0.1605],\n",
      "        [0.1019],\n",
      "        [0.1022],\n",
      "        [0.0243],\n",
      "        [0.0723],\n",
      "        [0.1332],\n",
      "        [0.0447],\n",
      "        [0.1676],\n",
      "        [0.0983],\n",
      "        [0.1392],\n",
      "        [0.1674],\n",
      "        [0.1690],\n",
      "        [0.1732],\n",
      "        [0.3430],\n",
      "        [0.1687],\n",
      "        [0.0313],\n",
      "        [0.2992],\n",
      "        [0.1090],\n",
      "        [0.3680],\n",
      "        [0.3775],\n",
      "        [0.3793],\n",
      "        [0.3878],\n",
      "        [0.3896],\n",
      "        [0.3917],\n",
      "        [0.3927],\n",
      "        [0.3960],\n",
      "        [0.4150],\n",
      "        [0.4160],\n",
      "        [0.4229],\n",
      "        [0.4273],\n",
      "        [0.4293]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0260],\n",
      "        [    0.1047],\n",
      "        [    0.0174],\n",
      "        [    0.0116],\n",
      "        [    0.0020],\n",
      "        [    0.0893],\n",
      "        [    0.1304],\n",
      "        [    0.0878],\n",
      "        [    0.0940],\n",
      "        [    0.0298],\n",
      "        [    0.0055],\n",
      "        [    0.0824],\n",
      "        [    0.0214],\n",
      "        [    0.0185],\n",
      "        [    0.1008],\n",
      "        [    0.0454],\n",
      "        [    0.0101],\n",
      "        [    0.0130],\n",
      "        [    0.0200],\n",
      "        [    0.1371],\n",
      "        [    0.0165],\n",
      "        [    0.0660],\n",
      "        [    0.0196],\n",
      "        [    0.0687],\n",
      "        [    0.1027],\n",
      "        [    0.0591],\n",
      "        [    0.0009],\n",
      "        [    0.1645],\n",
      "        [    0.0168],\n",
      "        [    0.0293],\n",
      "        [    0.0248],\n",
      "        [    0.1516],\n",
      "        [    0.0525],\n",
      "        [    0.0356],\n",
      "        [    0.0013],\n",
      "        [    0.0187],\n",
      "        [    0.0256],\n",
      "        [    0.0343],\n",
      "        [    0.0134],\n",
      "        [    0.0091],\n",
      "        [    0.1090],\n",
      "        [    0.0559],\n",
      "        [    0.0226],\n",
      "        [    0.0303],\n",
      "        [    0.0132],\n",
      "        [    0.0141],\n",
      "        [    0.0548],\n",
      "        [    0.1153],\n",
      "        [    0.0633],\n",
      "        [    0.0164],\n",
      "        [    0.0113],\n",
      "        [    0.0141],\n",
      "        [    0.0922],\n",
      "        [    0.0016],\n",
      "        [    0.0120],\n",
      "        [    0.0800],\n",
      "        [    0.0280],\n",
      "        [    0.0508],\n",
      "        [    0.0341],\n",
      "        [    0.0181],\n",
      "        [    0.0304],\n",
      "        [    0.0518],\n",
      "        [    0.2560],\n",
      "        [    0.0504],\n",
      "        [    0.0508],\n",
      "        [    0.0002],\n",
      "        [    0.0649],\n",
      "        [    0.0769],\n",
      "        [    0.0233],\n",
      "        [    0.0307],\n",
      "        [    0.1281],\n",
      "        [    0.0136],\n",
      "        [    0.2879],\n",
      "        [    0.0857],\n",
      "        [    0.0255],\n",
      "        [    0.0332],\n",
      "        [    0.0487],\n",
      "        [    0.0826],\n",
      "        [    0.0633],\n",
      "        [    0.0649],\n",
      "        [    0.0882],\n",
      "        [    0.0440],\n",
      "        [    0.0652],\n",
      "        [    0.0534],\n",
      "        [    0.0361],\n",
      "        [    0.0927],\n",
      "        [    0.0768],\n",
      "        [    0.0072],\n",
      "        [    0.0406],\n",
      "        [    0.0633],\n",
      "        [    0.0984],\n",
      "        [    0.1030],\n",
      "        [    0.1232],\n",
      "        [    0.0993],\n",
      "        [    0.1149],\n",
      "        [    0.0763],\n",
      "        [    0.0535],\n",
      "        [    0.0826],\n",
      "        [    0.0946],\n",
      "        [    0.0436],\n",
      "        [    0.1067],\n",
      "        [    0.0088],\n",
      "        [    0.1257],\n",
      "        [    0.2472],\n",
      "        [    0.0966],\n",
      "        [    0.1120],\n",
      "        [    0.1260],\n",
      "        [    0.1498],\n",
      "        [    0.1468],\n",
      "        [    0.1400],\n",
      "        [    0.1487],\n",
      "        [    0.1079],\n",
      "        [    0.0988],\n",
      "        [    0.0049],\n",
      "        [    0.0775],\n",
      "        [    0.1282],\n",
      "        [    0.0656],\n",
      "        [    0.1582],\n",
      "        [    0.1081],\n",
      "        [    0.1332],\n",
      "        [    0.1604],\n",
      "        [    0.1684],\n",
      "        [    0.1781],\n",
      "        [    0.2999],\n",
      "        [    0.1792],\n",
      "        [    0.0247],\n",
      "        [    0.2989],\n",
      "        [    0.0929],\n",
      "        [    0.3661],\n",
      "        [    0.3756],\n",
      "        [    0.3774],\n",
      "        [    0.3859],\n",
      "        [    0.3877],\n",
      "        [    0.3898],\n",
      "        [    0.3909],\n",
      "        [    0.3941],\n",
      "        [    0.4132],\n",
      "        [    0.4142],\n",
      "        [    0.4211],\n",
      "        [    0.4255],\n",
      "        [    0.4275]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 71.19483518600464\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 142\n",
      "剩餘X 資料 torch.Size([18, 18])\n",
      "剩餘Y 資料 torch.Size([18, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18346625566482544, 3)\n",
      "The second_loss value of k: (0.18731147050857544, 17)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.0683])\n",
      "目前模型的Data狀態 torch.Size([142, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.5850],\n",
      "        [ 0.7116],\n",
      "        [ 0.2897],\n",
      "        [ 0.6475],\n",
      "        [ 0.5579],\n",
      "        [ 0.2695],\n",
      "        [ 0.7110],\n",
      "        [ 0.2798],\n",
      "        [ 0.3464],\n",
      "        [ 0.6292],\n",
      "        [ 0.6652],\n",
      "        [ 0.4048],\n",
      "        [ 0.6261],\n",
      "        [ 0.5796],\n",
      "        [ 0.7174],\n",
      "        [ 0.5961],\n",
      "        [ 0.6872],\n",
      "        [ 0.5394],\n",
      "        [ 0.6753],\n",
      "        [ 0.3262],\n",
      "        [ 0.5680],\n",
      "        [ 0.5700],\n",
      "        [ 0.2946],\n",
      "        [ 0.5767],\n",
      "        [ 0.3713],\n",
      "        [ 0.5772],\n",
      "        [ 0.5489],\n",
      "        [ 0.3074],\n",
      "        [ 0.3282],\n",
      "        [ 0.6400],\n",
      "        [ 0.3500],\n",
      "        [ 0.3129],\n",
      "        [ 0.2667],\n",
      "        [ 0.6056],\n",
      "        [ 0.6200],\n",
      "        [ 0.2891],\n",
      "        [ 0.4453],\n",
      "        [ 0.6267],\n",
      "        [ 0.6040],\n",
      "        [ 0.6445],\n",
      "        [ 0.6727],\n",
      "        [ 0.1777],\n",
      "        [ 0.7061],\n",
      "        [ 0.6154],\n",
      "        [ 0.5274],\n",
      "        [ 0.6214],\n",
      "        [ 0.3170],\n",
      "        [ 0.2744],\n",
      "        [ 0.6919],\n",
      "        [ 0.5202],\n",
      "        [ 0.5775],\n",
      "        [ 0.6357],\n",
      "        [ 0.7227],\n",
      "        [ 0.1901],\n",
      "        [ 0.6051],\n",
      "        [ 0.3510],\n",
      "        [ 0.6557],\n",
      "        [ 0.7459],\n",
      "        [ 0.5606],\n",
      "        [ 0.2846],\n",
      "        [ 0.6540],\n",
      "        [ 0.6957],\n",
      "        [-0.0940],\n",
      "        [ 0.6291],\n",
      "        [ 0.4832],\n",
      "        [ 0.6332],\n",
      "        [ 0.4582],\n",
      "        [ 0.7819],\n",
      "        [ 0.6681],\n",
      "        [ 0.5798],\n",
      "        [ 0.3459],\n",
      "        [ 0.6566],\n",
      "        [-0.0334],\n",
      "        [ 0.5863],\n",
      "        [ 0.5924],\n",
      "        [ 0.6701],\n",
      "        [ 0.7565],\n",
      "        [ 0.4504],\n",
      "        [ 0.2946],\n",
      "        [ 0.2938],\n",
      "        [ 0.2661],\n",
      "        [ 0.2281],\n",
      "        [ 0.2928],\n",
      "        [ 0.3052],\n",
      "        [ 0.5878],\n",
      "        [ 0.3129],\n",
      "        [ 0.7103],\n",
      "        [ 0.7233],\n",
      "        [ 0.6650],\n",
      "        [ 0.3002],\n",
      "        [ 0.6134],\n",
      "        [ 0.2614],\n",
      "        [ 0.5675],\n",
      "        [ 0.5973],\n",
      "        [ 0.5743],\n",
      "        [ 0.5877],\n",
      "        [ 0.7303],\n",
      "        [ 0.2634],\n",
      "        [ 0.7576],\n",
      "        [ 0.7154],\n",
      "        [ 0.2943],\n",
      "        [ 0.2997],\n",
      "        [ 0.3165],\n",
      "        [ 0.3653],\n",
      "        [ 0.5919],\n",
      "        [ 0.7264],\n",
      "        [ 0.2729],\n",
      "        [ 0.5670],\n",
      "        [ 0.5874],\n",
      "        [ 0.5955],\n",
      "        [ 0.3115],\n",
      "        [ 0.7293],\n",
      "        [ 0.2923],\n",
      "        [ 0.3312],\n",
      "        [ 0.3449],\n",
      "        [ 0.5850],\n",
      "        [ 0.3523],\n",
      "        [ 0.2783],\n",
      "        [ 0.3730],\n",
      "        [ 0.3190],\n",
      "        [ 0.2907],\n",
      "        [ 0.5921],\n",
      "        [ 0.5986],\n",
      "        [ 0.3270],\n",
      "        [ 0.4065],\n",
      "        [ 0.0247],\n",
      "        [ 0.7600],\n",
      "        [ 0.2904],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966],\n",
      "        [ 0.4966]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0260],\n",
      "        [    0.1047],\n",
      "        [    0.0174],\n",
      "        [    0.0116],\n",
      "        [    0.0020],\n",
      "        [    0.0893],\n",
      "        [    0.1304],\n",
      "        [    0.0878],\n",
      "        [    0.0940],\n",
      "        [    0.0298],\n",
      "        [    0.0055],\n",
      "        [    0.0824],\n",
      "        [    0.0214],\n",
      "        [    0.0185],\n",
      "        [    0.1008],\n",
      "        [    0.0454],\n",
      "        [    0.0101],\n",
      "        [    0.0130],\n",
      "        [    0.0200],\n",
      "        [    0.1371],\n",
      "        [    0.0165],\n",
      "        [    0.0660],\n",
      "        [    0.0196],\n",
      "        [    0.0687],\n",
      "        [    0.1027],\n",
      "        [    0.0591],\n",
      "        [    0.0009],\n",
      "        [    0.1645],\n",
      "        [    0.0168],\n",
      "        [    0.0293],\n",
      "        [    0.0248],\n",
      "        [    0.1516],\n",
      "        [    0.0525],\n",
      "        [    0.0356],\n",
      "        [    0.0013],\n",
      "        [    0.0187],\n",
      "        [    0.0256],\n",
      "        [    0.0343],\n",
      "        [    0.0134],\n",
      "        [    0.0091],\n",
      "        [    0.1090],\n",
      "        [    0.0559],\n",
      "        [    0.0226],\n",
      "        [    0.0303],\n",
      "        [    0.0132],\n",
      "        [    0.0141],\n",
      "        [    0.0548],\n",
      "        [    0.1153],\n",
      "        [    0.0633],\n",
      "        [    0.0164],\n",
      "        [    0.0113],\n",
      "        [    0.0141],\n",
      "        [    0.0922],\n",
      "        [    0.0016],\n",
      "        [    0.0120],\n",
      "        [    0.0800],\n",
      "        [    0.0280],\n",
      "        [    0.0508],\n",
      "        [    0.0341],\n",
      "        [    0.0181],\n",
      "        [    0.0304],\n",
      "        [    0.0518],\n",
      "        [    0.2560],\n",
      "        [    0.0504],\n",
      "        [    0.0508],\n",
      "        [    0.0002],\n",
      "        [    0.0649],\n",
      "        [    0.0769],\n",
      "        [    0.0233],\n",
      "        [    0.0307],\n",
      "        [    0.1281],\n",
      "        [    0.0136],\n",
      "        [    0.2879],\n",
      "        [    0.0857],\n",
      "        [    0.0255],\n",
      "        [    0.0332],\n",
      "        [    0.0487],\n",
      "        [    0.0826],\n",
      "        [    0.0633],\n",
      "        [    0.0649],\n",
      "        [    0.0882],\n",
      "        [    0.0440],\n",
      "        [    0.0652],\n",
      "        [    0.0534],\n",
      "        [    0.0361],\n",
      "        [    0.0927],\n",
      "        [    0.0768],\n",
      "        [    0.0072],\n",
      "        [    0.0406],\n",
      "        [    0.0633],\n",
      "        [    0.0984],\n",
      "        [    0.1030],\n",
      "        [    0.1232],\n",
      "        [    0.0993],\n",
      "        [    0.1149],\n",
      "        [    0.0763],\n",
      "        [    0.0535],\n",
      "        [    0.0826],\n",
      "        [    0.0946],\n",
      "        [    0.0436],\n",
      "        [    0.1067],\n",
      "        [    0.0088],\n",
      "        [    0.1257],\n",
      "        [    0.2472],\n",
      "        [    0.0966],\n",
      "        [    0.1120],\n",
      "        [    0.1260],\n",
      "        [    0.1498],\n",
      "        [    0.1468],\n",
      "        [    0.1400],\n",
      "        [    0.1487],\n",
      "        [    0.1079],\n",
      "        [    0.0988],\n",
      "        [    0.0049],\n",
      "        [    0.0775],\n",
      "        [    0.1282],\n",
      "        [    0.0656],\n",
      "        [    0.1582],\n",
      "        [    0.1081],\n",
      "        [    0.1332],\n",
      "        [    0.1604],\n",
      "        [    0.1684],\n",
      "        [    0.1781],\n",
      "        [    0.2999],\n",
      "        [    0.1792],\n",
      "        [    0.0247],\n",
      "        [    0.2989],\n",
      "        [    0.0929],\n",
      "        [    0.3661],\n",
      "        [    0.3756],\n",
      "        [    0.3774],\n",
      "        [    0.3859],\n",
      "        [    0.3877],\n",
      "        [    0.3898],\n",
      "        [    0.3909],\n",
      "        [    0.3941],\n",
      "        [    0.4132],\n",
      "        [    0.4142],\n",
      "        [    0.4211],\n",
      "        [    0.4255],\n",
      "        [    0.4275],\n",
      "        [    0.4283]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0166],\n",
      "        [    0.0795],\n",
      "        [    0.0324],\n",
      "        [    0.0207],\n",
      "        [    0.0024],\n",
      "        [    0.0693],\n",
      "        [    0.1020],\n",
      "        [    0.0671],\n",
      "        [    0.0832],\n",
      "        [    0.0379],\n",
      "        [    0.0029],\n",
      "        [    0.0800],\n",
      "        [    0.0264],\n",
      "        [    0.0090],\n",
      "        [    0.0772],\n",
      "        [    0.0488],\n",
      "        [    0.0004],\n",
      "        [    0.0124],\n",
      "        [    0.0125],\n",
      "        [    0.1090],\n",
      "        [    0.0096],\n",
      "        [    0.0652],\n",
      "        [    0.0111],\n",
      "        [    0.0698],\n",
      "        [    0.0928],\n",
      "        [    0.0596],\n",
      "        [    0.0012],\n",
      "        [    0.1477],\n",
      "        [    0.0252],\n",
      "        [    0.0068],\n",
      "        [    0.0276],\n",
      "        [    0.1371],\n",
      "        [    0.0665],\n",
      "        [    0.0392],\n",
      "        [    0.0212],\n",
      "        [    0.0350],\n",
      "        [    0.0271],\n",
      "        [    0.0413],\n",
      "        [    0.0017],\n",
      "        [    0.0026],\n",
      "        [    0.0866],\n",
      "        [    0.0601],\n",
      "        [    0.0108],\n",
      "        [    0.0033],\n",
      "        [    0.0140],\n",
      "        [    0.0008],\n",
      "        [    0.0504],\n",
      "        [    0.0944],\n",
      "        [    0.0393],\n",
      "        [    0.0194],\n",
      "        [    0.0148],\n",
      "        [    0.0061],\n",
      "        [    0.0633],\n",
      "        [    0.0065],\n",
      "        [    0.0354],\n",
      "        [    0.0763],\n",
      "        [    0.0107],\n",
      "        [    0.0356],\n",
      "        [    0.0096],\n",
      "        [    0.0294],\n",
      "        [    0.0199],\n",
      "        [    0.0306],\n",
      "        [    0.2512],\n",
      "        [    0.0590],\n",
      "        [    0.0532],\n",
      "        [    0.0199],\n",
      "        [    0.0665],\n",
      "        [    0.0607],\n",
      "        [    0.0284],\n",
      "        [    0.0615],\n",
      "        [    0.1004],\n",
      "        [    0.0094],\n",
      "        [    0.2713],\n",
      "        [    0.0905],\n",
      "        [    0.0210],\n",
      "        [    0.0384],\n",
      "        [    0.0385],\n",
      "        [    0.0841],\n",
      "        [    0.0741],\n",
      "        [    0.0733],\n",
      "        [    0.0825],\n",
      "        [    0.0352],\n",
      "        [    0.0733],\n",
      "        [    0.0622],\n",
      "        [    0.0641],\n",
      "        [    0.0875],\n",
      "        [    0.0680],\n",
      "        [    0.0023],\n",
      "        [    0.0424],\n",
      "        [    0.0707],\n",
      "        [    0.0996],\n",
      "        [    0.0989],\n",
      "        [    0.1279],\n",
      "        [    0.1065],\n",
      "        [    0.1206],\n",
      "        [    0.0557],\n",
      "        [    0.0254],\n",
      "        [    0.0754],\n",
      "        [    0.0753],\n",
      "        [    0.0147],\n",
      "        [    0.0981],\n",
      "        [    0.0032],\n",
      "        [    0.1199],\n",
      "        [    0.2328],\n",
      "        [    0.0810],\n",
      "        [    0.1039],\n",
      "        [    0.1264],\n",
      "        [    0.1480],\n",
      "        [    0.1448],\n",
      "        [    0.1362],\n",
      "        [    0.1386],\n",
      "        [    0.0967],\n",
      "        [    0.1024],\n",
      "        [    0.0154],\n",
      "        [    0.0719],\n",
      "        [    0.1156],\n",
      "        [    0.0703],\n",
      "        [    0.1588],\n",
      "        [    0.1047],\n",
      "        [    0.1359],\n",
      "        [    0.1615],\n",
      "        [    0.1589],\n",
      "        [    0.1720],\n",
      "        [    0.2709],\n",
      "        [    0.1761],\n",
      "        [    0.0245],\n",
      "        [    0.2826],\n",
      "        [    0.0831],\n",
      "        [    0.3646],\n",
      "        [    0.3741],\n",
      "        [    0.3759],\n",
      "        [    0.3844],\n",
      "        [    0.3862],\n",
      "        [    0.3883],\n",
      "        [    0.3893],\n",
      "        [    0.3925],\n",
      "        [    0.4116],\n",
      "        [    0.4126],\n",
      "        [    0.4195],\n",
      "        [    0.4239],\n",
      "        [    0.4259],\n",
      "        [    0.4268]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 71.4362473487854\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 143\n",
      "剩餘X 資料 torch.Size([17, 18])\n",
      "剩餘Y 資料 torch.Size([17, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18596625328063965, 16)\n",
      "The second_loss value of k: (0.19244037568569183, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.0638])\n",
      "目前模型的Data狀態 torch.Size([143, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.5756],\n",
      "        [ 0.6865],\n",
      "        [ 0.2747],\n",
      "        [ 0.6384],\n",
      "        [ 0.5534],\n",
      "        [ 0.2496],\n",
      "        [ 0.6827],\n",
      "        [ 0.2592],\n",
      "        [ 0.3572],\n",
      "        [ 0.6211],\n",
      "        [ 0.6568],\n",
      "        [ 0.4072],\n",
      "        [ 0.6211],\n",
      "        [ 0.5701],\n",
      "        [ 0.6938],\n",
      "        [ 0.5928],\n",
      "        [ 0.6768],\n",
      "        [ 0.5388],\n",
      "        [ 0.6678],\n",
      "        [ 0.2982],\n",
      "        [ 0.5419],\n",
      "        [ 0.5708],\n",
      "        [ 0.3030],\n",
      "        [ 0.5755],\n",
      "        [ 0.3811],\n",
      "        [ 0.5767],\n",
      "        [ 0.5468],\n",
      "        [ 0.3242],\n",
      "        [ 0.3367],\n",
      "        [ 0.6176],\n",
      "        [ 0.3528],\n",
      "        [ 0.3275],\n",
      "        [ 0.2527],\n",
      "        [ 0.6020],\n",
      "        [ 0.5975],\n",
      "        [ 0.2728],\n",
      "        [ 0.4439],\n",
      "        [ 0.6197],\n",
      "        [ 0.5924],\n",
      "        [ 0.6379],\n",
      "        [ 0.6504],\n",
      "        [ 0.1736],\n",
      "        [ 0.6944],\n",
      "        [ 0.5819],\n",
      "        [ 0.5266],\n",
      "        [ 0.6081],\n",
      "        [ 0.3126],\n",
      "        [ 0.2535],\n",
      "        [ 0.6679],\n",
      "        [ 0.5173],\n",
      "        [ 0.5740],\n",
      "        [ 0.6155],\n",
      "        [ 0.6938],\n",
      "        [ 0.1852],\n",
      "        [ 0.5817],\n",
      "        [ 0.3473],\n",
      "        [ 0.6385],\n",
      "        [ 0.7306],\n",
      "        [ 0.5361],\n",
      "        [ 0.2733],\n",
      "        [ 0.6435],\n",
      "        [ 0.6745],\n",
      "        [-0.0891],\n",
      "        [ 0.6204],\n",
      "        [ 0.4808],\n",
      "        [ 0.6135],\n",
      "        [ 0.4565],\n",
      "        [ 0.7657],\n",
      "        [ 0.6630],\n",
      "        [ 0.5490],\n",
      "        [ 0.3183],\n",
      "        [ 0.6336],\n",
      "        [-0.0168],\n",
      "        [ 0.5816],\n",
      "        [ 0.5879],\n",
      "        [ 0.6650],\n",
      "        [ 0.7463],\n",
      "        [ 0.4489],\n",
      "        [ 0.2838],\n",
      "        [ 0.2854],\n",
      "        [ 0.2718],\n",
      "        [ 0.2193],\n",
      "        [ 0.2847],\n",
      "        [ 0.2963],\n",
      "        [ 0.5597],\n",
      "        [ 0.3076],\n",
      "        [ 0.7015],\n",
      "        [ 0.7184],\n",
      "        [ 0.6632],\n",
      "        [ 0.2927],\n",
      "        [ 0.6122],\n",
      "        [ 0.2655],\n",
      "        [ 0.5628],\n",
      "        [ 0.5901],\n",
      "        [ 0.5686],\n",
      "        [ 0.5671],\n",
      "        [ 0.7021],\n",
      "        [ 0.2562],\n",
      "        [ 0.7383],\n",
      "        [ 0.6864],\n",
      "        [ 0.2857],\n",
      "        [ 0.2942],\n",
      "        [ 0.3108],\n",
      "        [ 0.3509],\n",
      "        [ 0.5764],\n",
      "        [ 0.7182],\n",
      "        [ 0.2725],\n",
      "        [ 0.5687],\n",
      "        [ 0.5894],\n",
      "        [ 0.5993],\n",
      "        [ 0.3013],\n",
      "        [ 0.7181],\n",
      "        [ 0.2887],\n",
      "        [ 0.3418],\n",
      "        [ 0.3393],\n",
      "        [ 0.5724],\n",
      "        [ 0.3570],\n",
      "        [ 0.2777],\n",
      "        [ 0.3696],\n",
      "        [ 0.3162],\n",
      "        [ 0.2895],\n",
      "        [ 0.5826],\n",
      "        [ 0.5925],\n",
      "        [ 0.2979],\n",
      "        [ 0.4035],\n",
      "        [ 0.0245],\n",
      "        [ 0.7438],\n",
      "        [ 0.2806],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950],\n",
      "        [ 0.4950]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0166],\n",
      "        [    0.0795],\n",
      "        [    0.0324],\n",
      "        [    0.0207],\n",
      "        [    0.0024],\n",
      "        [    0.0693],\n",
      "        [    0.1020],\n",
      "        [    0.0671],\n",
      "        [    0.0832],\n",
      "        [    0.0379],\n",
      "        [    0.0029],\n",
      "        [    0.0800],\n",
      "        [    0.0264],\n",
      "        [    0.0090],\n",
      "        [    0.0772],\n",
      "        [    0.0488],\n",
      "        [    0.0004],\n",
      "        [    0.0124],\n",
      "        [    0.0125],\n",
      "        [    0.1090],\n",
      "        [    0.0096],\n",
      "        [    0.0652],\n",
      "        [    0.0111],\n",
      "        [    0.0698],\n",
      "        [    0.0928],\n",
      "        [    0.0596],\n",
      "        [    0.0012],\n",
      "        [    0.1477],\n",
      "        [    0.0252],\n",
      "        [    0.0068],\n",
      "        [    0.0276],\n",
      "        [    0.1371],\n",
      "        [    0.0665],\n",
      "        [    0.0392],\n",
      "        [    0.0212],\n",
      "        [    0.0350],\n",
      "        [    0.0271],\n",
      "        [    0.0413],\n",
      "        [    0.0017],\n",
      "        [    0.0026],\n",
      "        [    0.0866],\n",
      "        [    0.0601],\n",
      "        [    0.0108],\n",
      "        [    0.0033],\n",
      "        [    0.0140],\n",
      "        [    0.0008],\n",
      "        [    0.0504],\n",
      "        [    0.0944],\n",
      "        [    0.0393],\n",
      "        [    0.0194],\n",
      "        [    0.0148],\n",
      "        [    0.0061],\n",
      "        [    0.0633],\n",
      "        [    0.0065],\n",
      "        [    0.0354],\n",
      "        [    0.0763],\n",
      "        [    0.0107],\n",
      "        [    0.0356],\n",
      "        [    0.0096],\n",
      "        [    0.0294],\n",
      "        [    0.0199],\n",
      "        [    0.0306],\n",
      "        [    0.2512],\n",
      "        [    0.0590],\n",
      "        [    0.0532],\n",
      "        [    0.0199],\n",
      "        [    0.0665],\n",
      "        [    0.0607],\n",
      "        [    0.0284],\n",
      "        [    0.0615],\n",
      "        [    0.1004],\n",
      "        [    0.0094],\n",
      "        [    0.2713],\n",
      "        [    0.0905],\n",
      "        [    0.0210],\n",
      "        [    0.0384],\n",
      "        [    0.0385],\n",
      "        [    0.0841],\n",
      "        [    0.0741],\n",
      "        [    0.0733],\n",
      "        [    0.0825],\n",
      "        [    0.0352],\n",
      "        [    0.0733],\n",
      "        [    0.0622],\n",
      "        [    0.0641],\n",
      "        [    0.0875],\n",
      "        [    0.0680],\n",
      "        [    0.0023],\n",
      "        [    0.0424],\n",
      "        [    0.0707],\n",
      "        [    0.0996],\n",
      "        [    0.0989],\n",
      "        [    0.1279],\n",
      "        [    0.1065],\n",
      "        [    0.1206],\n",
      "        [    0.0557],\n",
      "        [    0.0254],\n",
      "        [    0.0754],\n",
      "        [    0.0753],\n",
      "        [    0.0147],\n",
      "        [    0.0981],\n",
      "        [    0.0032],\n",
      "        [    0.1199],\n",
      "        [    0.2328],\n",
      "        [    0.0810],\n",
      "        [    0.1039],\n",
      "        [    0.1264],\n",
      "        [    0.1480],\n",
      "        [    0.1448],\n",
      "        [    0.1362],\n",
      "        [    0.1386],\n",
      "        [    0.0967],\n",
      "        [    0.1024],\n",
      "        [    0.0154],\n",
      "        [    0.0719],\n",
      "        [    0.1156],\n",
      "        [    0.0703],\n",
      "        [    0.1588],\n",
      "        [    0.1047],\n",
      "        [    0.1359],\n",
      "        [    0.1615],\n",
      "        [    0.1589],\n",
      "        [    0.1720],\n",
      "        [    0.2709],\n",
      "        [    0.1761],\n",
      "        [    0.0245],\n",
      "        [    0.2826],\n",
      "        [    0.0831],\n",
      "        [    0.3646],\n",
      "        [    0.3741],\n",
      "        [    0.3759],\n",
      "        [    0.3844],\n",
      "        [    0.3862],\n",
      "        [    0.3883],\n",
      "        [    0.3893],\n",
      "        [    0.3925],\n",
      "        [    0.4116],\n",
      "        [    0.4126],\n",
      "        [    0.4195],\n",
      "        [    0.4239],\n",
      "        [    0.4259],\n",
      "        [    0.4268],\n",
      "        [    0.4312]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 66\n",
      "Number of shrink: 29\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0175],\n",
      "        [    0.0691],\n",
      "        [    0.0385],\n",
      "        [    0.0200],\n",
      "        [    0.0022],\n",
      "        [    0.0599],\n",
      "        [    0.0896],\n",
      "        [    0.0573],\n",
      "        [    0.0684],\n",
      "        [    0.0365],\n",
      "        [    0.0014],\n",
      "        [    0.0714],\n",
      "        [    0.0226],\n",
      "        [    0.0098],\n",
      "        [    0.0680],\n",
      "        [    0.0435],\n",
      "        [    0.0003],\n",
      "        [    0.0196],\n",
      "        [    0.0146],\n",
      "        [    0.0943],\n",
      "        [    0.0214],\n",
      "        [    0.0568],\n",
      "        [    0.0011],\n",
      "        [    0.0628],\n",
      "        [    0.0784],\n",
      "        [    0.0521],\n",
      "        [    0.0052],\n",
      "        [    0.1282],\n",
      "        [    0.0374],\n",
      "        [    0.0013],\n",
      "        [    0.0354],\n",
      "        [    0.1194],\n",
      "        [    0.0722],\n",
      "        [    0.0342],\n",
      "        [    0.0296],\n",
      "        [    0.0424],\n",
      "        [    0.0211],\n",
      "        [    0.0377],\n",
      "        [    0.0012],\n",
      "        [    0.0053],\n",
      "        [    0.0788],\n",
      "        [    0.0568],\n",
      "        [    0.0098],\n",
      "        [    0.0201],\n",
      "        [    0.0068],\n",
      "        [    0.0006],\n",
      "        [    0.0525],\n",
      "        [    0.0851],\n",
      "        [    0.0302],\n",
      "        [    0.0139],\n",
      "        [    0.0100],\n",
      "        [    0.0121],\n",
      "        [    0.0509],\n",
      "        [    0.0041],\n",
      "        [    0.0445],\n",
      "        [    0.0789],\n",
      "        [    0.0063],\n",
      "        [    0.0317],\n",
      "        [    0.0023],\n",
      "        [    0.0330],\n",
      "        [    0.0202],\n",
      "        [    0.0231],\n",
      "        [    0.2398],\n",
      "        [    0.0581],\n",
      "        [    0.0473],\n",
      "        [    0.0256],\n",
      "        [    0.0604],\n",
      "        [    0.0560],\n",
      "        [    0.0234],\n",
      "        [    0.0765],\n",
      "        [    0.0859],\n",
      "        [    0.0174],\n",
      "        [    0.2503],\n",
      "        [    0.0861],\n",
      "        [    0.0252],\n",
      "        [    0.0332],\n",
      "        [    0.0386],\n",
      "        [    0.0779],\n",
      "        [    0.0762],\n",
      "        [    0.0737],\n",
      "        [    0.0724],\n",
      "        [    0.0345],\n",
      "        [    0.0733],\n",
      "        [    0.0629],\n",
      "        [    0.0767],\n",
      "        [    0.0894],\n",
      "        [    0.0685],\n",
      "        [    0.0067],\n",
      "        [    0.0352],\n",
      "        [    0.0703],\n",
      "        [    0.0913],\n",
      "        [    0.0903],\n",
      "        [    0.1237],\n",
      "        [    0.1044],\n",
      "        [    0.1172],\n",
      "        [    0.0467],\n",
      "        [    0.0137],\n",
      "        [    0.0758],\n",
      "        [    0.0680],\n",
      "        [    0.0023],\n",
      "        [    0.0976],\n",
      "        [    0.0045],\n",
      "        [    0.1217],\n",
      "        [    0.2226],\n",
      "        [    0.0760],\n",
      "        [    0.1051],\n",
      "        [    0.1213],\n",
      "        [    0.1371],\n",
      "        [    0.1342],\n",
      "        [    0.1240],\n",
      "        [    0.1369],\n",
      "        [    0.0956],\n",
      "        [    0.0990],\n",
      "        [    0.0303],\n",
      "        [    0.0737],\n",
      "        [    0.1128],\n",
      "        [    0.0805],\n",
      "        [    0.1538],\n",
      "        [    0.1085],\n",
      "        [    0.1322],\n",
      "        [    0.1564],\n",
      "        [    0.1594],\n",
      "        [    0.1751],\n",
      "        [    0.2542],\n",
      "        [    0.1802],\n",
      "        [    0.0321],\n",
      "        [    0.2775],\n",
      "        [    0.0767],\n",
      "        [    0.3633],\n",
      "        [    0.3728],\n",
      "        [    0.3746],\n",
      "        [    0.3831],\n",
      "        [    0.3850],\n",
      "        [    0.3870],\n",
      "        [    0.3881],\n",
      "        [    0.3913],\n",
      "        [    0.4104],\n",
      "        [    0.4114],\n",
      "        [    0.4183],\n",
      "        [    0.4227],\n",
      "        [    0.4247],\n",
      "        [    0.4255],\n",
      "        [    0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0246],\n",
      "        [0.0673],\n",
      "        [0.0406],\n",
      "        [0.0136],\n",
      "        [0.0122],\n",
      "        [0.0559],\n",
      "        [0.0864],\n",
      "        [0.0535],\n",
      "        [0.0513],\n",
      "        [0.0296],\n",
      "        [0.0061],\n",
      "        [0.0589],\n",
      "        [0.0136],\n",
      "        [0.0167],\n",
      "        [0.0670],\n",
      "        [0.0330],\n",
      "        [0.0062],\n",
      "        [0.0311],\n",
      "        [0.0220],\n",
      "        [0.0874],\n",
      "        [0.0250],\n",
      "        [0.0439],\n",
      "        [0.0151],\n",
      "        [0.0507],\n",
      "        [0.0610],\n",
      "        [0.0396],\n",
      "        [0.0165],\n",
      "        [0.1070],\n",
      "        [0.0509],\n",
      "        [0.0010],\n",
      "        [0.0449],\n",
      "        [0.1000],\n",
      "        [0.0746],\n",
      "        [0.0240],\n",
      "        [0.0297],\n",
      "        [0.0460],\n",
      "        [0.0108],\n",
      "        [0.0273],\n",
      "        [0.0076],\n",
      "        [0.0135],\n",
      "        [0.0792],\n",
      "        [0.0493],\n",
      "        [0.0152],\n",
      "        [0.0270],\n",
      "        [0.0048],\n",
      "        [0.0052],\n",
      "        [0.0572],\n",
      "        [0.0829],\n",
      "        [0.0302],\n",
      "        [0.0036],\n",
      "        [0.0005],\n",
      "        [0.0092],\n",
      "        [0.0483],\n",
      "        [0.0021],\n",
      "        [0.0454],\n",
      "        [0.0838],\n",
      "        [0.0097],\n",
      "        [0.0344],\n",
      "        [0.0079],\n",
      "        [0.0341],\n",
      "        [0.0267],\n",
      "        [0.0237],\n",
      "        [0.2222],\n",
      "        [0.0515],\n",
      "        [0.0367],\n",
      "        [0.0225],\n",
      "        [0.0498],\n",
      "        [0.0575],\n",
      "        [0.0118],\n",
      "        [0.0821],\n",
      "        [0.0791],\n",
      "        [0.0163],\n",
      "        [0.2242],\n",
      "        [0.0764],\n",
      "        [0.0344],\n",
      "        [0.0212],\n",
      "        [0.0443],\n",
      "        [0.0673],\n",
      "        [0.0737],\n",
      "        [0.0699],\n",
      "        [0.0601],\n",
      "        [0.0379],\n",
      "        [0.0691],\n",
      "        [0.0594],\n",
      "        [0.0800],\n",
      "        [0.0946],\n",
      "        [0.0738],\n",
      "        [0.0165],\n",
      "        [0.0226],\n",
      "        [0.0656],\n",
      "        [0.0766],\n",
      "        [0.0792],\n",
      "        [0.1140],\n",
      "        [0.0966],\n",
      "        [0.1081],\n",
      "        [0.0430],\n",
      "        [0.0116],\n",
      "        [0.0799],\n",
      "        [0.0674],\n",
      "        [0.0005],\n",
      "        [0.1008],\n",
      "        [0.0086],\n",
      "        [0.1272],\n",
      "        [0.2117],\n",
      "        [0.0761],\n",
      "        [0.1113],\n",
      "        [0.1135],\n",
      "        [0.1197],\n",
      "        [0.1176],\n",
      "        [0.1059],\n",
      "        [0.1393],\n",
      "        [0.1000],\n",
      "        [0.0917],\n",
      "        [0.0478],\n",
      "        [0.0793],\n",
      "        [0.1148],\n",
      "        [0.0936],\n",
      "        [0.1462],\n",
      "        [0.1160],\n",
      "        [0.1254],\n",
      "        [0.1480],\n",
      "        [0.1656],\n",
      "        [0.1835],\n",
      "        [0.2438],\n",
      "        [0.1880],\n",
      "        [0.0461],\n",
      "        [0.2783],\n",
      "        [0.0695],\n",
      "        [0.3622],\n",
      "        [0.3717],\n",
      "        [0.3735],\n",
      "        [0.3821],\n",
      "        [0.3839],\n",
      "        [0.3859],\n",
      "        [0.3870],\n",
      "        [0.3902],\n",
      "        [0.4093],\n",
      "        [0.4103],\n",
      "        [0.4172],\n",
      "        [0.4216],\n",
      "        [0.4236],\n",
      "        [0.4244],\n",
      "        [0.4289]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 71.89296388626099\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 144\n",
      "剩餘X 資料 torch.Size([16, 18])\n",
      "剩餘Y 資料 torch.Size([16, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.19039379060268402, 14)\n",
      "The second_loss value of k: (0.19183219969272614, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0564])\n",
      "目前模型的Data狀態 torch.Size([144, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.5836],\n",
      "        [ 0.6743],\n",
      "        [ 0.2665],\n",
      "        [ 0.6456],\n",
      "        [ 0.5680],\n",
      "        [ 0.2361],\n",
      "        [ 0.6671],\n",
      "        [ 0.2456],\n",
      "        [ 0.3892],\n",
      "        [ 0.6294],\n",
      "        [ 0.6658],\n",
      "        [ 0.4283],\n",
      "        [ 0.6338],\n",
      "        [ 0.5777],\n",
      "        [ 0.6835],\n",
      "        [ 0.6086],\n",
      "        [ 0.6834],\n",
      "        [ 0.5575],\n",
      "        [ 0.6774],\n",
      "        [ 0.2766],\n",
      "        [ 0.5264],\n",
      "        [ 0.5921],\n",
      "        [ 0.3292],\n",
      "        [ 0.5947],\n",
      "        [ 0.4130],\n",
      "        [ 0.5968],\n",
      "        [ 0.5645],\n",
      "        [ 0.3649],\n",
      "        [ 0.3623],\n",
      "        [ 0.6098],\n",
      "        [ 0.3702],\n",
      "        [ 0.3646],\n",
      "        [ 0.2446],\n",
      "        [ 0.6171],\n",
      "        [ 0.5890],\n",
      "        [ 0.2618],\n",
      "        [ 0.4601],\n",
      "        [ 0.6338],\n",
      "        [ 0.5983],\n",
      "        [ 0.6489],\n",
      "        [ 0.6430],\n",
      "        [ 0.1843],\n",
      "        [ 0.6988],\n",
      "        [ 0.5582],\n",
      "        [ 0.5453],\n",
      "        [ 0.6125],\n",
      "        [ 0.3194],\n",
      "        [ 0.2420],\n",
      "        [ 0.6588],\n",
      "        [ 0.5330],\n",
      "        [ 0.5883],\n",
      "        [ 0.6124],\n",
      "        [ 0.6789],\n",
      "        [ 0.1938],\n",
      "        [ 0.5717],\n",
      "        [ 0.3548],\n",
      "        [ 0.6375],\n",
      "        [ 0.7295],\n",
      "        [ 0.5187],\n",
      "        [ 0.2686],\n",
      "        [ 0.6503],\n",
      "        [ 0.6676],\n",
      "        [-0.0601],\n",
      "        [ 0.6279],\n",
      "        [ 0.4973],\n",
      "        [ 0.6109],\n",
      "        [ 0.4733],\n",
      "        [ 0.7625],\n",
      "        [ 0.6796],\n",
      "        [ 0.5284],\n",
      "        [ 0.2969],\n",
      "        [ 0.6267],\n",
      "        [ 0.0304],\n",
      "        [ 0.5957],\n",
      "        [ 0.6013],\n",
      "        [ 0.6822],\n",
      "        [ 0.7521],\n",
      "        [ 0.4657],\n",
      "        [ 0.2842],\n",
      "        [ 0.2888],\n",
      "        [ 0.2942],\n",
      "        [ 0.2220],\n",
      "        [ 0.2889],\n",
      "        [ 0.2992],\n",
      "        [ 0.5439],\n",
      "        [ 0.3147],\n",
      "        [ 0.7073],\n",
      "        [ 0.7325],\n",
      "        [ 0.6831],\n",
      "        [ 0.2978],\n",
      "        [ 0.6351],\n",
      "        [ 0.2852],\n",
      "        [ 0.5767],\n",
      "        [ 0.6000],\n",
      "        [ 0.5812],\n",
      "        [ 0.5544],\n",
      "        [ 0.6884],\n",
      "        [ 0.2607],\n",
      "        [ 0.7305],\n",
      "        [ 0.6712],\n",
      "        [ 0.2884],\n",
      "        [ 0.2995],\n",
      "        [ 0.3181],\n",
      "        [ 0.3298],\n",
      "        [ 0.5714],\n",
      "        [ 0.7256],\n",
      "        [ 0.2855],\n",
      "        [ 0.5971],\n",
      "        [ 0.6166],\n",
      "        [ 0.6296],\n",
      "        [ 0.3021],\n",
      "        [ 0.7214],\n",
      "        [ 0.2994],\n",
      "        [ 0.3741],\n",
      "        [ 0.3468],\n",
      "        [ 0.5717],\n",
      "        [ 0.3804],\n",
      "        [ 0.2903],\n",
      "        [ 0.3810],\n",
      "        [ 0.3268],\n",
      "        [ 0.3030],\n",
      "        [ 0.5893],\n",
      "        [ 0.6040],\n",
      "        [ 0.2708],\n",
      "        [ 0.4153],\n",
      "        [ 0.0461],\n",
      "        [ 0.7395],\n",
      "        [ 0.2670],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927],\n",
      "        [ 0.4927]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0246],\n",
      "        [0.0673],\n",
      "        [0.0406],\n",
      "        [0.0136],\n",
      "        [0.0122],\n",
      "        [0.0559],\n",
      "        [0.0864],\n",
      "        [0.0535],\n",
      "        [0.0513],\n",
      "        [0.0296],\n",
      "        [0.0061],\n",
      "        [0.0589],\n",
      "        [0.0136],\n",
      "        [0.0167],\n",
      "        [0.0670],\n",
      "        [0.0330],\n",
      "        [0.0062],\n",
      "        [0.0311],\n",
      "        [0.0220],\n",
      "        [0.0874],\n",
      "        [0.0250],\n",
      "        [0.0439],\n",
      "        [0.0151],\n",
      "        [0.0507],\n",
      "        [0.0610],\n",
      "        [0.0396],\n",
      "        [0.0165],\n",
      "        [0.1070],\n",
      "        [0.0509],\n",
      "        [0.0010],\n",
      "        [0.0449],\n",
      "        [0.1000],\n",
      "        [0.0746],\n",
      "        [0.0240],\n",
      "        [0.0297],\n",
      "        [0.0460],\n",
      "        [0.0108],\n",
      "        [0.0273],\n",
      "        [0.0076],\n",
      "        [0.0135],\n",
      "        [0.0792],\n",
      "        [0.0493],\n",
      "        [0.0152],\n",
      "        [0.0270],\n",
      "        [0.0048],\n",
      "        [0.0052],\n",
      "        [0.0572],\n",
      "        [0.0829],\n",
      "        [0.0302],\n",
      "        [0.0036],\n",
      "        [0.0005],\n",
      "        [0.0092],\n",
      "        [0.0483],\n",
      "        [0.0021],\n",
      "        [0.0454],\n",
      "        [0.0838],\n",
      "        [0.0097],\n",
      "        [0.0344],\n",
      "        [0.0079],\n",
      "        [0.0341],\n",
      "        [0.0267],\n",
      "        [0.0237],\n",
      "        [0.2222],\n",
      "        [0.0515],\n",
      "        [0.0367],\n",
      "        [0.0225],\n",
      "        [0.0498],\n",
      "        [0.0575],\n",
      "        [0.0118],\n",
      "        [0.0821],\n",
      "        [0.0791],\n",
      "        [0.0163],\n",
      "        [0.2242],\n",
      "        [0.0764],\n",
      "        [0.0344],\n",
      "        [0.0212],\n",
      "        [0.0443],\n",
      "        [0.0673],\n",
      "        [0.0737],\n",
      "        [0.0699],\n",
      "        [0.0601],\n",
      "        [0.0379],\n",
      "        [0.0691],\n",
      "        [0.0594],\n",
      "        [0.0800],\n",
      "        [0.0946],\n",
      "        [0.0738],\n",
      "        [0.0165],\n",
      "        [0.0226],\n",
      "        [0.0656],\n",
      "        [0.0766],\n",
      "        [0.0792],\n",
      "        [0.1140],\n",
      "        [0.0966],\n",
      "        [0.1081],\n",
      "        [0.0430],\n",
      "        [0.0116],\n",
      "        [0.0799],\n",
      "        [0.0674],\n",
      "        [0.0005],\n",
      "        [0.1008],\n",
      "        [0.0086],\n",
      "        [0.1272],\n",
      "        [0.2117],\n",
      "        [0.0761],\n",
      "        [0.1113],\n",
      "        [0.1135],\n",
      "        [0.1197],\n",
      "        [0.1176],\n",
      "        [0.1059],\n",
      "        [0.1393],\n",
      "        [0.1000],\n",
      "        [0.0917],\n",
      "        [0.0478],\n",
      "        [0.0793],\n",
      "        [0.1148],\n",
      "        [0.0936],\n",
      "        [0.1462],\n",
      "        [0.1160],\n",
      "        [0.1254],\n",
      "        [0.1480],\n",
      "        [0.1656],\n",
      "        [0.1835],\n",
      "        [0.2438],\n",
      "        [0.1880],\n",
      "        [0.0461],\n",
      "        [0.2783],\n",
      "        [0.0695],\n",
      "        [0.3622],\n",
      "        [0.3717],\n",
      "        [0.3735],\n",
      "        [0.3821],\n",
      "        [0.3839],\n",
      "        [0.3859],\n",
      "        [0.3870],\n",
      "        [0.3902],\n",
      "        [0.4093],\n",
      "        [0.4103],\n",
      "        [0.4172],\n",
      "        [0.4216],\n",
      "        [0.4236],\n",
      "        [0.4244],\n",
      "        [0.4289],\n",
      "        [0.4363]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 541\n",
      "Number of shrink: 274\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0206],\n",
      "        [    0.0354],\n",
      "        [    0.0604],\n",
      "        [    0.0151],\n",
      "        [    0.0172],\n",
      "        [    0.0273],\n",
      "        [    0.0507],\n",
      "        [    0.0279],\n",
      "        [    0.0237],\n",
      "        [    0.0290],\n",
      "        [    0.0027],\n",
      "        [    0.0456],\n",
      "        [    0.0098],\n",
      "        [    0.0133],\n",
      "        [    0.0354],\n",
      "        [    0.0247],\n",
      "        [    0.0001],\n",
      "        [    0.0368],\n",
      "        [    0.0112],\n",
      "        [    0.0559],\n",
      "        [    0.0496],\n",
      "        [    0.0314],\n",
      "        [    0.0306],\n",
      "        [    0.0374],\n",
      "        [    0.0312],\n",
      "        [    0.0275],\n",
      "        [    0.0241],\n",
      "        [    0.0654],\n",
      "        [    0.0590],\n",
      "        [    0.0177],\n",
      "        [    0.0415],\n",
      "        [    0.0644],\n",
      "        [    0.1001],\n",
      "        [    0.0151],\n",
      "        [    0.0457],\n",
      "        [    0.0722],\n",
      "        [    0.0064],\n",
      "        [    0.0148],\n",
      "        [    0.0034],\n",
      "        [    0.0127],\n",
      "        [    0.0585],\n",
      "        [    0.0347],\n",
      "        [    0.0037],\n",
      "        [    0.0543],\n",
      "        [    0.0122],\n",
      "        [    0.0020],\n",
      "        [    0.0413],\n",
      "        [    0.0690],\n",
      "        [    0.0084],\n",
      "        [    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0198],\n",
      "        [    0.0187],\n",
      "        [    0.0086],\n",
      "        [    0.0613],\n",
      "        [    0.0658],\n",
      "        [    0.0035],\n",
      "        [    0.0118],\n",
      "        [    0.0477],\n",
      "        [    0.0636],\n",
      "        [    0.0167],\n",
      "        [    0.0018],\n",
      "        [    0.1484],\n",
      "        [    0.0557],\n",
      "        [    0.0314],\n",
      "        [    0.0308],\n",
      "        [    0.0457],\n",
      "        [    0.0253],\n",
      "        [    0.0037],\n",
      "        [    0.1026],\n",
      "        [    0.0453],\n",
      "        [    0.0333],\n",
      "        [    0.1250],\n",
      "        [    0.0712],\n",
      "        [    0.0348],\n",
      "        [    0.0118],\n",
      "        [    0.0263],\n",
      "        [    0.0643],\n",
      "        [    0.0768],\n",
      "        [    0.0722],\n",
      "        [    0.0488],\n",
      "        [    0.0330],\n",
      "        [    0.0679],\n",
      "        [    0.0646],\n",
      "        [    0.0980],\n",
      "        [    0.0849],\n",
      "        [    0.0479],\n",
      "        [    0.0129],\n",
      "        [    0.0148],\n",
      "        [    0.0697],\n",
      "        [    0.0560],\n",
      "        [    0.0708],\n",
      "        [    0.1047],\n",
      "        [    0.0952],\n",
      "        [    0.1018],\n",
      "        [    0.0060],\n",
      "        [    0.0185],\n",
      "        [    0.0728],\n",
      "        [    0.0315],\n",
      "        [    0.0321],\n",
      "        [    0.0898],\n",
      "        [    0.0047],\n",
      "        [    0.1213],\n",
      "        [    0.1376],\n",
      "        [    0.0456],\n",
      "        [    0.0879],\n",
      "        [    0.1144],\n",
      "        [    0.0762],\n",
      "        [    0.0886],\n",
      "        [    0.0719],\n",
      "        [    0.1258],\n",
      "        [    0.0723],\n",
      "        [    0.0904],\n",
      "        [    0.0799],\n",
      "        [    0.0722],\n",
      "        [    0.0886],\n",
      "        [    0.1130],\n",
      "        [    0.1468],\n",
      "        [    0.1142],\n",
      "        [    0.1281],\n",
      "        [    0.1437],\n",
      "        [    0.1573],\n",
      "        [    0.1822],\n",
      "        [    0.1954],\n",
      "        [    0.1871],\n",
      "        [    0.1066],\n",
      "        [    0.2406],\n",
      "        [    0.0144],\n",
      "        [    0.3559],\n",
      "        [    0.3654],\n",
      "        [    0.3672],\n",
      "        [    0.3757],\n",
      "        [    0.3775],\n",
      "        [    0.3796],\n",
      "        [    0.3806],\n",
      "        [    0.3839],\n",
      "        [    0.4029],\n",
      "        [    0.4039],\n",
      "        [    0.4108],\n",
      "        [    0.4152],\n",
      "        [    0.4172],\n",
      "        [    0.4181],\n",
      "        [    0.4226],\n",
      "        [    0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0252],\n",
      "        [0.0378],\n",
      "        [0.0579],\n",
      "        [0.0101],\n",
      "        [0.0220],\n",
      "        [0.0253],\n",
      "        [0.0530],\n",
      "        [0.0268],\n",
      "        [0.0198],\n",
      "        [0.0240],\n",
      "        [0.0069],\n",
      "        [0.0426],\n",
      "        [0.0053],\n",
      "        [0.0179],\n",
      "        [0.0380],\n",
      "        [0.0206],\n",
      "        [0.0043],\n",
      "        [0.0408],\n",
      "        [0.0138],\n",
      "        [0.0557],\n",
      "        [0.0449],\n",
      "        [0.0279],\n",
      "        [0.0343],\n",
      "        [0.0332],\n",
      "        [0.0270],\n",
      "        [0.0234],\n",
      "        [0.0290],\n",
      "        [0.0607],\n",
      "        [0.0612],\n",
      "        [0.0130],\n",
      "        [0.0430],\n",
      "        [0.0604],\n",
      "        [0.0993],\n",
      "        [0.0098],\n",
      "        [0.0409],\n",
      "        [0.0705],\n",
      "        [0.0038],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0168],\n",
      "        [0.0625],\n",
      "        [0.0309],\n",
      "        [0.0076],\n",
      "        [0.0488],\n",
      "        [0.0162],\n",
      "        [0.0072],\n",
      "        [0.0414],\n",
      "        [0.0704],\n",
      "        [0.0123],\n",
      "        [0.0038],\n",
      "        [0.0019],\n",
      "        [0.0150],\n",
      "        [0.0220],\n",
      "        [0.0115],\n",
      "        [0.0569],\n",
      "        [0.0658],\n",
      "        [0.0008],\n",
      "        [0.0149],\n",
      "        [0.0463],\n",
      "        [0.0645],\n",
      "        [0.0209],\n",
      "        [0.0016],\n",
      "        [0.1416],\n",
      "        [0.0519],\n",
      "        [0.0280],\n",
      "        [0.0257],\n",
      "        [0.0430],\n",
      "        [0.0270],\n",
      "        [0.0017],\n",
      "        [0.0961],\n",
      "        [0.0440],\n",
      "        [0.0290],\n",
      "        [0.1146],\n",
      "        [0.0677],\n",
      "        [0.0376],\n",
      "        [0.0059],\n",
      "        [0.0290],\n",
      "        [0.0621],\n",
      "        [0.0723],\n",
      "        [0.0688],\n",
      "        [0.0454],\n",
      "        [0.0348],\n",
      "        [0.0636],\n",
      "        [0.0617],\n",
      "        [0.0925],\n",
      "        [0.0862],\n",
      "        [0.0492],\n",
      "        [0.0168],\n",
      "        [0.0101],\n",
      "        [0.0665],\n",
      "        [0.0496],\n",
      "        [0.0675],\n",
      "        [0.0999],\n",
      "        [0.0911],\n",
      "        [0.0976],\n",
      "        [0.0066],\n",
      "        [0.0151],\n",
      "        [0.0742],\n",
      "        [0.0344],\n",
      "        [0.0291],\n",
      "        [0.0912],\n",
      "        [0.0051],\n",
      "        [0.1235],\n",
      "        [0.1291],\n",
      "        [0.0464],\n",
      "        [0.0896],\n",
      "        [0.1121],\n",
      "        [0.0671],\n",
      "        [0.0820],\n",
      "        [0.0650],\n",
      "        [0.1274],\n",
      "        [0.0742],\n",
      "        [0.0875],\n",
      "        [0.0846],\n",
      "        [0.0723],\n",
      "        [0.0888],\n",
      "        [0.1172],\n",
      "        [0.1447],\n",
      "        [0.1155],\n",
      "        [0.1257],\n",
      "        [0.1409],\n",
      "        [0.1595],\n",
      "        [0.1851],\n",
      "        [0.1924],\n",
      "        [0.1892],\n",
      "        [0.1123],\n",
      "        [0.2424],\n",
      "        [0.0080],\n",
      "        [0.3553],\n",
      "        [0.3648],\n",
      "        [0.3666],\n",
      "        [0.3751],\n",
      "        [0.3769],\n",
      "        [0.3790],\n",
      "        [0.3801],\n",
      "        [0.3833],\n",
      "        [0.4023],\n",
      "        [0.4033],\n",
      "        [0.4103],\n",
      "        [0.4146],\n",
      "        [0.4166],\n",
      "        [0.4175],\n",
      "        [0.4220],\n",
      "        [0.4294]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 73.83035111427307\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 145\n",
      "剩餘X 資料 torch.Size([15, 18])\n",
      "剩餘Y 資料 torch.Size([15, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1858064979314804, 2)\n",
      "The second_loss value of k: (0.1864486187696457, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.0547])\n",
      "目前模型的Data狀態 torch.Size([145, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5842],\n",
      "        [0.6448],\n",
      "        [0.2492],\n",
      "        [0.6490],\n",
      "        [0.5778],\n",
      "        [0.2055],\n",
      "        [0.6337],\n",
      "        [0.2189],\n",
      "        [0.4207],\n",
      "        [0.6350],\n",
      "        [0.6666],\n",
      "        [0.4447],\n",
      "        [0.6421],\n",
      "        [0.5790],\n",
      "        [0.6546],\n",
      "        [0.6209],\n",
      "        [0.6815],\n",
      "        [0.5672],\n",
      "        [0.6691],\n",
      "        [0.2448],\n",
      "        [0.5065],\n",
      "        [0.6081],\n",
      "        [0.3485],\n",
      "        [0.6122],\n",
      "        [0.4470],\n",
      "        [0.6129],\n",
      "        [0.5769],\n",
      "        [0.4112],\n",
      "        [0.3726],\n",
      "        [0.5978],\n",
      "        [0.3682],\n",
      "        [0.4041],\n",
      "        [0.2199],\n",
      "        [0.6313],\n",
      "        [0.5779],\n",
      "        [0.2373],\n",
      "        [0.4671],\n",
      "        [0.6531],\n",
      "        [0.5988],\n",
      "        [0.6521],\n",
      "        [0.6263],\n",
      "        [0.2027],\n",
      "        [0.6912],\n",
      "        [0.5363],\n",
      "        [0.5568],\n",
      "        [0.6145],\n",
      "        [0.3036],\n",
      "        [0.2295],\n",
      "        [0.6409],\n",
      "        [0.5404],\n",
      "        [0.5907],\n",
      "        [0.6066],\n",
      "        [0.6526],\n",
      "        [0.2032],\n",
      "        [0.5602],\n",
      "        [0.3368],\n",
      "        [0.6286],\n",
      "        [0.7100],\n",
      "        [0.4802],\n",
      "        [0.2382],\n",
      "        [0.6444],\n",
      "        [0.6456],\n",
      "        [0.0204],\n",
      "        [0.6275],\n",
      "        [0.5060],\n",
      "        [0.6077],\n",
      "        [0.4800],\n",
      "        [0.7321],\n",
      "        [0.6931],\n",
      "        [0.5144],\n",
      "        [0.2619],\n",
      "        [0.6139],\n",
      "        [0.1400],\n",
      "        [0.6044],\n",
      "        [0.6045],\n",
      "        [0.6974],\n",
      "        [0.7367],\n",
      "        [0.4709],\n",
      "        [0.2856],\n",
      "        [0.2899],\n",
      "        [0.3088],\n",
      "        [0.2188],\n",
      "        [0.2944],\n",
      "        [0.2968],\n",
      "        [0.5313],\n",
      "        [0.3063],\n",
      "        [0.6827],\n",
      "        [0.7329],\n",
      "        [0.6955],\n",
      "        [0.2970],\n",
      "        [0.6621],\n",
      "        [0.2970],\n",
      "        [0.5908],\n",
      "        [0.6055],\n",
      "        [0.5917],\n",
      "        [0.5180],\n",
      "        [0.6616],\n",
      "        [0.2550],\n",
      "        [0.6975],\n",
      "        [0.6427],\n",
      "        [0.2788],\n",
      "        [0.2859],\n",
      "        [0.3144],\n",
      "        [0.2472],\n",
      "        [0.5418],\n",
      "        [0.7039],\n",
      "        [0.2869],\n",
      "        [0.6497],\n",
      "        [0.6522],\n",
      "        [0.6705],\n",
      "        [0.2901],\n",
      "        [0.6956],\n",
      "        [0.3036],\n",
      "        [0.4110],\n",
      "        [0.3397],\n",
      "        [0.5457],\n",
      "        [0.4039],\n",
      "        [0.2918],\n",
      "        [0.3805],\n",
      "        [0.3265],\n",
      "        [0.3102],\n",
      "        [0.5832],\n",
      "        [0.6056],\n",
      "        [0.2194],\n",
      "        [0.4165],\n",
      "        [0.1123],\n",
      "        [0.7036],\n",
      "        [0.2055],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858],\n",
      "        [0.4858]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0252],\n",
      "        [0.0378],\n",
      "        [0.0579],\n",
      "        [0.0101],\n",
      "        [0.0220],\n",
      "        [0.0253],\n",
      "        [0.0530],\n",
      "        [0.0268],\n",
      "        [0.0198],\n",
      "        [0.0240],\n",
      "        [0.0069],\n",
      "        [0.0426],\n",
      "        [0.0053],\n",
      "        [0.0179],\n",
      "        [0.0380],\n",
      "        [0.0206],\n",
      "        [0.0043],\n",
      "        [0.0408],\n",
      "        [0.0138],\n",
      "        [0.0557],\n",
      "        [0.0449],\n",
      "        [0.0279],\n",
      "        [0.0343],\n",
      "        [0.0332],\n",
      "        [0.0270],\n",
      "        [0.0234],\n",
      "        [0.0290],\n",
      "        [0.0607],\n",
      "        [0.0612],\n",
      "        [0.0130],\n",
      "        [0.0430],\n",
      "        [0.0604],\n",
      "        [0.0993],\n",
      "        [0.0098],\n",
      "        [0.0409],\n",
      "        [0.0705],\n",
      "        [0.0038],\n",
      "        [0.0080],\n",
      "        [0.0081],\n",
      "        [0.0168],\n",
      "        [0.0625],\n",
      "        [0.0309],\n",
      "        [0.0076],\n",
      "        [0.0488],\n",
      "        [0.0162],\n",
      "        [0.0072],\n",
      "        [0.0414],\n",
      "        [0.0704],\n",
      "        [0.0123],\n",
      "        [0.0038],\n",
      "        [0.0019],\n",
      "        [0.0150],\n",
      "        [0.0220],\n",
      "        [0.0115],\n",
      "        [0.0569],\n",
      "        [0.0658],\n",
      "        [0.0008],\n",
      "        [0.0149],\n",
      "        [0.0463],\n",
      "        [0.0645],\n",
      "        [0.0209],\n",
      "        [0.0016],\n",
      "        [0.1416],\n",
      "        [0.0519],\n",
      "        [0.0280],\n",
      "        [0.0257],\n",
      "        [0.0430],\n",
      "        [0.0270],\n",
      "        [0.0017],\n",
      "        [0.0961],\n",
      "        [0.0440],\n",
      "        [0.0290],\n",
      "        [0.1146],\n",
      "        [0.0677],\n",
      "        [0.0376],\n",
      "        [0.0059],\n",
      "        [0.0290],\n",
      "        [0.0621],\n",
      "        [0.0723],\n",
      "        [0.0688],\n",
      "        [0.0454],\n",
      "        [0.0348],\n",
      "        [0.0636],\n",
      "        [0.0617],\n",
      "        [0.0925],\n",
      "        [0.0862],\n",
      "        [0.0492],\n",
      "        [0.0168],\n",
      "        [0.0101],\n",
      "        [0.0665],\n",
      "        [0.0496],\n",
      "        [0.0675],\n",
      "        [0.0999],\n",
      "        [0.0911],\n",
      "        [0.0976],\n",
      "        [0.0066],\n",
      "        [0.0151],\n",
      "        [0.0742],\n",
      "        [0.0344],\n",
      "        [0.0291],\n",
      "        [0.0912],\n",
      "        [0.0051],\n",
      "        [0.1235],\n",
      "        [0.1291],\n",
      "        [0.0464],\n",
      "        [0.0896],\n",
      "        [0.1121],\n",
      "        [0.0671],\n",
      "        [0.0820],\n",
      "        [0.0650],\n",
      "        [0.1274],\n",
      "        [0.0742],\n",
      "        [0.0875],\n",
      "        [0.0846],\n",
      "        [0.0723],\n",
      "        [0.0888],\n",
      "        [0.1172],\n",
      "        [0.1447],\n",
      "        [0.1155],\n",
      "        [0.1257],\n",
      "        [0.1409],\n",
      "        [0.1595],\n",
      "        [0.1851],\n",
      "        [0.1924],\n",
      "        [0.1892],\n",
      "        [0.1123],\n",
      "        [0.2424],\n",
      "        [0.0080],\n",
      "        [0.3553],\n",
      "        [0.3648],\n",
      "        [0.3666],\n",
      "        [0.3751],\n",
      "        [0.3769],\n",
      "        [0.3790],\n",
      "        [0.3801],\n",
      "        [0.3833],\n",
      "        [0.4023],\n",
      "        [0.4033],\n",
      "        [0.4103],\n",
      "        [0.4146],\n",
      "        [0.4166],\n",
      "        [0.4175],\n",
      "        [0.4220],\n",
      "        [0.4294],\n",
      "        [0.4311]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 127\n",
      "Number of shrink: 62\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0257],\n",
      "        [    0.0349],\n",
      "        [    0.0568],\n",
      "        [    0.0089],\n",
      "        [    0.0230],\n",
      "        [    0.0208],\n",
      "        [    0.0492],\n",
      "        [    0.0237],\n",
      "        [    0.0184],\n",
      "        [    0.0225],\n",
      "        [    0.0067],\n",
      "        [    0.0423],\n",
      "        [    0.0047],\n",
      "        [    0.0187],\n",
      "        [    0.0350],\n",
      "        [    0.0206],\n",
      "        [    0.0043],\n",
      "        [    0.0408],\n",
      "        [    0.0114],\n",
      "        [    0.0534],\n",
      "        [    0.0441],\n",
      "        [    0.0285],\n",
      "        [    0.0362],\n",
      "        [    0.0329],\n",
      "        [    0.0251],\n",
      "        [    0.0236],\n",
      "        [    0.0299],\n",
      "        [    0.0582],\n",
      "        [    0.0607],\n",
      "        [    0.0130],\n",
      "        [    0.0415],\n",
      "        [    0.0588],\n",
      "        [    0.1008],\n",
      "        [    0.0083],\n",
      "        [    0.0405],\n",
      "        [    0.0705],\n",
      "        [    0.0046],\n",
      "        [    0.0051],\n",
      "        [    0.0087],\n",
      "        [    0.0166],\n",
      "        [    0.0612],\n",
      "        [    0.0278],\n",
      "        [    0.0070],\n",
      "        [    0.0468],\n",
      "        [    0.0163],\n",
      "        [    0.0085],\n",
      "        [    0.0387],\n",
      "        [    0.0704],\n",
      "        [    0.0114],\n",
      "        [    0.0032],\n",
      "        [    0.0003],\n",
      "        [    0.0151],\n",
      "        [    0.0200],\n",
      "        [    0.0135],\n",
      "        [    0.0567],\n",
      "        [    0.0626],\n",
      "        [    0.0006],\n",
      "        [    0.0133],\n",
      "        [    0.0497],\n",
      "        [    0.0686],\n",
      "        [    0.0199],\n",
      "        [    0.0001],\n",
      "        [    0.1344],\n",
      "        [    0.0522],\n",
      "        [    0.0284],\n",
      "        [    0.0248],\n",
      "        [    0.0441],\n",
      "        [    0.0231],\n",
      "        [    0.0026],\n",
      "        [    0.0925],\n",
      "        [    0.0405],\n",
      "        [    0.0298],\n",
      "        [    0.1026],\n",
      "        [    0.0684],\n",
      "        [    0.0362],\n",
      "        [    0.0046],\n",
      "        [    0.0262],\n",
      "        [    0.0636],\n",
      "        [    0.0688],\n",
      "        [    0.0669],\n",
      "        [    0.0437],\n",
      "        [    0.0349],\n",
      "        [    0.0603],\n",
      "        [    0.0606],\n",
      "        [    0.0906],\n",
      "        [    0.0850],\n",
      "        [    0.0444],\n",
      "        [    0.0157],\n",
      "        [    0.0102],\n",
      "        [    0.0652],\n",
      "        [    0.0475],\n",
      "        [    0.0657],\n",
      "        [    0.0987],\n",
      "        [    0.0908],\n",
      "        [    0.0971],\n",
      "        [    0.0024],\n",
      "        [    0.0172],\n",
      "        [    0.0737],\n",
      "        [    0.0319],\n",
      "        [    0.0314],\n",
      "        [    0.0903],\n",
      "        [    0.0084],\n",
      "        [    0.1234],\n",
      "        [    0.1166],\n",
      "        [    0.0421],\n",
      "        [    0.0852],\n",
      "        [    0.1115],\n",
      "        [    0.0603],\n",
      "        [    0.0791],\n",
      "        [    0.0616],\n",
      "        [    0.1265],\n",
      "        [    0.0700],\n",
      "        [    0.0865],\n",
      "        [    0.0872],\n",
      "        [    0.0695],\n",
      "        [    0.0839],\n",
      "        [    0.1192],\n",
      "        [    0.1443],\n",
      "        [    0.1139],\n",
      "        [    0.1253],\n",
      "        [    0.1397],\n",
      "        [    0.1573],\n",
      "        [    0.1838],\n",
      "        [    0.1866],\n",
      "        [    0.1884],\n",
      "        [    0.1181],\n",
      "        [    0.2381],\n",
      "        [    0.0016],\n",
      "        [    0.3542],\n",
      "        [    0.3637],\n",
      "        [    0.3655],\n",
      "        [    0.3741],\n",
      "        [    0.3759],\n",
      "        [    0.3779],\n",
      "        [    0.3790],\n",
      "        [    0.3822],\n",
      "        [    0.4013],\n",
      "        [    0.4023],\n",
      "        [    0.4092],\n",
      "        [    0.4136],\n",
      "        [    0.4156],\n",
      "        [    0.4164],\n",
      "        [    0.4209],\n",
      "        [    0.4284],\n",
      "        [    0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0229],\n",
      "        [    0.0303],\n",
      "        [    0.0579],\n",
      "        [    0.0114],\n",
      "        [    0.0205],\n",
      "        [    0.0166],\n",
      "        [    0.0443],\n",
      "        [    0.0201],\n",
      "        [    0.0204],\n",
      "        [    0.0249],\n",
      "        [    0.0035],\n",
      "        [    0.0451],\n",
      "        [    0.0075],\n",
      "        [    0.0160],\n",
      "        [    0.0304],\n",
      "        [    0.0239],\n",
      "        [    0.0011],\n",
      "        [    0.0378],\n",
      "        [    0.0070],\n",
      "        [    0.0501],\n",
      "        [    0.0464],\n",
      "        [    0.0320],\n",
      "        [    0.0351],\n",
      "        [    0.0359],\n",
      "        [    0.0271],\n",
      "        [    0.0269],\n",
      "        [    0.0274],\n",
      "        [    0.0597],\n",
      "        [    0.0583],\n",
      "        [    0.0160],\n",
      "        [    0.0387],\n",
      "        [    0.0607],\n",
      "        [    0.1031],\n",
      "        [    0.0106],\n",
      "        [    0.0433],\n",
      "        [    0.0722],\n",
      "        [    0.0080],\n",
      "        [    0.0066],\n",
      "        [    0.0059],\n",
      "        [    0.0134],\n",
      "        [    0.0575],\n",
      "        [    0.0280],\n",
      "        [    0.0036],\n",
      "        [    0.0485],\n",
      "        [    0.0135],\n",
      "        [    0.0060],\n",
      "        [    0.0354],\n",
      "        [    0.0685],\n",
      "        [    0.0077],\n",
      "        [    0.0001],\n",
      "        [    0.0047],\n",
      "        [    0.0181],\n",
      "        [    0.0159],\n",
      "        [    0.0127],\n",
      "        [    0.0596],\n",
      "        [    0.0590],\n",
      "        [    0.0026],\n",
      "        [    0.0093],\n",
      "        [    0.0541],\n",
      "        [    0.0724],\n",
      "        [    0.0165],\n",
      "        [    0.0041],\n",
      "        [    0.1321],\n",
      "        [    0.0554],\n",
      "        [    0.0315],\n",
      "        [    0.0274],\n",
      "        [    0.0474],\n",
      "        [    0.0178],\n",
      "        [    0.0002],\n",
      "        [    0.0934],\n",
      "        [    0.0365],\n",
      "        [    0.0331],\n",
      "        [    0.0976],\n",
      "        [    0.0719],\n",
      "        [    0.0323],\n",
      "        [    0.0072],\n",
      "        [    0.0215],\n",
      "        [    0.0672],\n",
      "        [    0.0688],\n",
      "        [    0.0678],\n",
      "        [    0.0448],\n",
      "        [    0.0332],\n",
      "        [    0.0604],\n",
      "        [    0.0618],\n",
      "        [    0.0924],\n",
      "        [    0.0825],\n",
      "        [    0.0390],\n",
      "        [    0.0118],\n",
      "        [    0.0134],\n",
      "        [    0.0663],\n",
      "        [    0.0495],\n",
      "        [    0.0667],\n",
      "        [    0.1011],\n",
      "        [    0.0937],\n",
      "        [    0.0999],\n",
      "        [    0.0026],\n",
      "        [    0.0212],\n",
      "        [    0.0715],\n",
      "        [    0.0277],\n",
      "        [    0.0356],\n",
      "        [    0.0880],\n",
      "        [    0.0122],\n",
      "        [    0.1214],\n",
      "        [    0.1096],\n",
      "        [    0.0371],\n",
      "        [    0.0799],\n",
      "        [    0.1130],\n",
      "        [    0.0599],\n",
      "        [    0.0807],\n",
      "        [    0.0631],\n",
      "        [    0.1242],\n",
      "        [    0.0649],\n",
      "        [    0.0877],\n",
      "        [    0.0858],\n",
      "        [    0.0657],\n",
      "        [    0.0786],\n",
      "        [    0.1178],\n",
      "        [    0.1460],\n",
      "        [    0.1107],\n",
      "        [    0.1270],\n",
      "        [    0.1410],\n",
      "        [    0.1531],\n",
      "        [    0.1800],\n",
      "        [    0.1817],\n",
      "        [    0.1855],\n",
      "        [    0.1196],\n",
      "        [    0.2329],\n",
      "        [    0.0070],\n",
      "        [    0.3537],\n",
      "        [    0.3632],\n",
      "        [    0.3650],\n",
      "        [    0.3735],\n",
      "        [    0.3753],\n",
      "        [    0.3774],\n",
      "        [    0.3785],\n",
      "        [    0.3817],\n",
      "        [    0.4007],\n",
      "        [    0.4017],\n",
      "        [    0.4087],\n",
      "        [    0.4130],\n",
      "        [    0.4150],\n",
      "        [    0.4159],\n",
      "        [    0.4204],\n",
      "        [    0.4278],\n",
      "        [    0.4295]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 74.47790288925171\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 146\n",
      "剩餘X 資料 torch.Size([14, 18])\n",
      "剩餘Y 資料 torch.Size([14, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18507254123687744, 8)\n",
      "The second_loss value of k: (0.18510623276233673, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.0540])\n",
      "目前模型的Data狀態 torch.Size([146, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5819],\n",
      "        [0.6372],\n",
      "        [0.2492],\n",
      "        [0.6478],\n",
      "        [0.5763],\n",
      "        [0.1968],\n",
      "        [0.6250],\n",
      "        [0.2122],\n",
      "        [0.4200],\n",
      "        [0.6341],\n",
      "        [0.6632],\n",
      "        [0.4421],\n",
      "        [0.6399],\n",
      "        [0.5771],\n",
      "        [0.6470],\n",
      "        [0.6177],\n",
      "        [0.6783],\n",
      "        [0.5642],\n",
      "        [0.6623],\n",
      "        [0.2393],\n",
      "        [0.5050],\n",
      "        [0.6040],\n",
      "        [0.3492],\n",
      "        [0.6095],\n",
      "        [0.4469],\n",
      "        [0.6095],\n",
      "        [0.5753],\n",
      "        [0.4122],\n",
      "        [0.3698],\n",
      "        [0.5947],\n",
      "        [0.3639],\n",
      "        [0.4038],\n",
      "        [0.2161],\n",
      "        [0.6306],\n",
      "        [0.5755],\n",
      "        [0.2356],\n",
      "        [0.4630],\n",
      "        [0.6544],\n",
      "        [0.5965],\n",
      "        [0.6487],\n",
      "        [0.6213],\n",
      "        [0.2057],\n",
      "        [0.6872],\n",
      "        [0.5367],\n",
      "        [0.5541],\n",
      "        [0.6132],\n",
      "        [0.2976],\n",
      "        [0.2276],\n",
      "        [0.6364],\n",
      "        [0.5367],\n",
      "        [0.5842],\n",
      "        [0.6035],\n",
      "        [0.6465],\n",
      "        [0.2044],\n",
      "        [0.5575],\n",
      "        [0.3300],\n",
      "        [0.6252],\n",
      "        [0.7043],\n",
      "        [0.4725],\n",
      "        [0.2303],\n",
      "        [0.6400],\n",
      "        [0.6398],\n",
      "        [0.0299],\n",
      "        [0.6241],\n",
      "        [0.5025],\n",
      "        [0.6059],\n",
      "        [0.4756],\n",
      "        [0.7228],\n",
      "        [0.6912],\n",
      "        [0.5171],\n",
      "        [0.2543],\n",
      "        [0.6099],\n",
      "        [0.1569],\n",
      "        [0.6001],\n",
      "        [0.5992],\n",
      "        [0.6962],\n",
      "        [0.7292],\n",
      "        [0.4657],\n",
      "        [0.2891],\n",
      "        [0.2909],\n",
      "        [0.3095],\n",
      "        [0.2172],\n",
      "        [0.2976],\n",
      "        [0.2967],\n",
      "        [0.5315],\n",
      "        [0.3027],\n",
      "        [0.6725],\n",
      "        [0.7279],\n",
      "        [0.6922],\n",
      "        [0.2971],\n",
      "        [0.6623],\n",
      "        [0.2978],\n",
      "        [0.5896],\n",
      "        [0.6029],\n",
      "        [0.5894],\n",
      "        [0.5088],\n",
      "        [0.6555],\n",
      "        [0.2523],\n",
      "        [0.6907],\n",
      "        [0.6361],\n",
      "        [0.2756],\n",
      "        [0.2787],\n",
      "        [0.3122],\n",
      "        [0.2277],\n",
      "        [0.5325],\n",
      "        [0.6943],\n",
      "        [0.2859],\n",
      "        [0.6569],\n",
      "        [0.6535],\n",
      "        [0.6724],\n",
      "        [0.2870],\n",
      "        [0.6862],\n",
      "        [0.3034],\n",
      "        [0.4121],\n",
      "        [0.3331],\n",
      "        [0.5355],\n",
      "        [0.4045],\n",
      "        [0.2905],\n",
      "        [0.3756],\n",
      "        [0.3252],\n",
      "        [0.3101],\n",
      "        [0.5768],\n",
      "        [0.6005],\n",
      "        [0.2088],\n",
      "        [0.4129],\n",
      "        [0.1196],\n",
      "        [0.6941],\n",
      "        [0.1906],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842],\n",
      "        [0.4842]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0229],\n",
      "        [    0.0303],\n",
      "        [    0.0579],\n",
      "        [    0.0114],\n",
      "        [    0.0205],\n",
      "        [    0.0166],\n",
      "        [    0.0443],\n",
      "        [    0.0201],\n",
      "        [    0.0204],\n",
      "        [    0.0249],\n",
      "        [    0.0035],\n",
      "        [    0.0451],\n",
      "        [    0.0075],\n",
      "        [    0.0160],\n",
      "        [    0.0304],\n",
      "        [    0.0239],\n",
      "        [    0.0011],\n",
      "        [    0.0378],\n",
      "        [    0.0070],\n",
      "        [    0.0501],\n",
      "        [    0.0464],\n",
      "        [    0.0320],\n",
      "        [    0.0351],\n",
      "        [    0.0359],\n",
      "        [    0.0271],\n",
      "        [    0.0269],\n",
      "        [    0.0274],\n",
      "        [    0.0597],\n",
      "        [    0.0583],\n",
      "        [    0.0160],\n",
      "        [    0.0387],\n",
      "        [    0.0607],\n",
      "        [    0.1031],\n",
      "        [    0.0106],\n",
      "        [    0.0433],\n",
      "        [    0.0722],\n",
      "        [    0.0080],\n",
      "        [    0.0066],\n",
      "        [    0.0059],\n",
      "        [    0.0134],\n",
      "        [    0.0575],\n",
      "        [    0.0280],\n",
      "        [    0.0036],\n",
      "        [    0.0485],\n",
      "        [    0.0135],\n",
      "        [    0.0060],\n",
      "        [    0.0354],\n",
      "        [    0.0685],\n",
      "        [    0.0077],\n",
      "        [    0.0001],\n",
      "        [    0.0047],\n",
      "        [    0.0181],\n",
      "        [    0.0159],\n",
      "        [    0.0127],\n",
      "        [    0.0596],\n",
      "        [    0.0590],\n",
      "        [    0.0026],\n",
      "        [    0.0093],\n",
      "        [    0.0541],\n",
      "        [    0.0724],\n",
      "        [    0.0165],\n",
      "        [    0.0041],\n",
      "        [    0.1321],\n",
      "        [    0.0554],\n",
      "        [    0.0315],\n",
      "        [    0.0274],\n",
      "        [    0.0474],\n",
      "        [    0.0178],\n",
      "        [    0.0002],\n",
      "        [    0.0934],\n",
      "        [    0.0365],\n",
      "        [    0.0331],\n",
      "        [    0.0976],\n",
      "        [    0.0719],\n",
      "        [    0.0323],\n",
      "        [    0.0072],\n",
      "        [    0.0215],\n",
      "        [    0.0672],\n",
      "        [    0.0688],\n",
      "        [    0.0678],\n",
      "        [    0.0448],\n",
      "        [    0.0332],\n",
      "        [    0.0604],\n",
      "        [    0.0618],\n",
      "        [    0.0924],\n",
      "        [    0.0825],\n",
      "        [    0.0390],\n",
      "        [    0.0118],\n",
      "        [    0.0134],\n",
      "        [    0.0663],\n",
      "        [    0.0495],\n",
      "        [    0.0667],\n",
      "        [    0.1011],\n",
      "        [    0.0937],\n",
      "        [    0.0999],\n",
      "        [    0.0026],\n",
      "        [    0.0212],\n",
      "        [    0.0715],\n",
      "        [    0.0277],\n",
      "        [    0.0356],\n",
      "        [    0.0880],\n",
      "        [    0.0122],\n",
      "        [    0.1214],\n",
      "        [    0.1096],\n",
      "        [    0.0371],\n",
      "        [    0.0799],\n",
      "        [    0.1130],\n",
      "        [    0.0599],\n",
      "        [    0.0807],\n",
      "        [    0.0631],\n",
      "        [    0.1242],\n",
      "        [    0.0649],\n",
      "        [    0.0877],\n",
      "        [    0.0858],\n",
      "        [    0.0657],\n",
      "        [    0.0786],\n",
      "        [    0.1178],\n",
      "        [    0.1460],\n",
      "        [    0.1107],\n",
      "        [    0.1270],\n",
      "        [    0.1410],\n",
      "        [    0.1531],\n",
      "        [    0.1800],\n",
      "        [    0.1817],\n",
      "        [    0.1855],\n",
      "        [    0.1196],\n",
      "        [    0.2329],\n",
      "        [    0.0070],\n",
      "        [    0.3537],\n",
      "        [    0.3632],\n",
      "        [    0.3650],\n",
      "        [    0.3735],\n",
      "        [    0.3753],\n",
      "        [    0.3774],\n",
      "        [    0.3785],\n",
      "        [    0.3817],\n",
      "        [    0.4007],\n",
      "        [    0.4017],\n",
      "        [    0.4087],\n",
      "        [    0.4130],\n",
      "        [    0.4150],\n",
      "        [    0.4159],\n",
      "        [    0.4204],\n",
      "        [    0.4278],\n",
      "        [    0.4295],\n",
      "        [    0.4302]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 27\n",
      "Number of shrink: 11\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0263],\n",
      "        [0.0330],\n",
      "        [0.0554],\n",
      "        [0.0078],\n",
      "        [0.0239],\n",
      "        [0.0173],\n",
      "        [0.0469],\n",
      "        [0.0211],\n",
      "        [0.0176],\n",
      "        [0.0213],\n",
      "        [0.0067],\n",
      "        [0.0427],\n",
      "        [0.0041],\n",
      "        [0.0194],\n",
      "        [0.0332],\n",
      "        [0.0207],\n",
      "        [0.0044],\n",
      "        [0.0409],\n",
      "        [0.0095],\n",
      "        [0.0516],\n",
      "        [0.0428],\n",
      "        [0.0291],\n",
      "        [0.0381],\n",
      "        [0.0327],\n",
      "        [0.0242],\n",
      "        [0.0238],\n",
      "        [0.0308],\n",
      "        [0.0567],\n",
      "        [0.0608],\n",
      "        [0.0126],\n",
      "        [0.0409],\n",
      "        [0.0580],\n",
      "        [0.1012],\n",
      "        [0.0069],\n",
      "        [0.0398],\n",
      "        [0.0699],\n",
      "        [0.0056],\n",
      "        [0.0025],\n",
      "        [0.0093],\n",
      "        [0.0166],\n",
      "        [0.0608],\n",
      "        [0.0250],\n",
      "        [0.0068],\n",
      "        [0.0446],\n",
      "        [0.0166],\n",
      "        [0.0096],\n",
      "        [0.0371],\n",
      "        [0.0706],\n",
      "        [0.0110],\n",
      "        [0.0029],\n",
      "        [0.0023],\n",
      "        [0.0145],\n",
      "        [0.0190],\n",
      "        [0.0154],\n",
      "        [0.0563],\n",
      "        [0.0608],\n",
      "        [0.0008],\n",
      "        [0.0122],\n",
      "        [0.0518],\n",
      "        [0.0711],\n",
      "        [0.0196],\n",
      "        [0.0011],\n",
      "        [0.1291],\n",
      "        [0.0523],\n",
      "        [0.0286],\n",
      "        [0.0237],\n",
      "        [0.0449],\n",
      "        [0.0202],\n",
      "        [0.0034],\n",
      "        [0.0891],\n",
      "        [0.0375],\n",
      "        [0.0297],\n",
      "        [0.0931],\n",
      "        [0.0690],\n",
      "        [0.0350],\n",
      "        [0.0034],\n",
      "        [0.0242],\n",
      "        [0.0649],\n",
      "        [0.0655],\n",
      "        [0.0649],\n",
      "        [0.0419],\n",
      "        [0.0354],\n",
      "        [0.0572],\n",
      "        [0.0591],\n",
      "        [0.0885],\n",
      "        [0.0847],\n",
      "        [0.0411],\n",
      "        [0.0149],\n",
      "        [0.0101],\n",
      "        [0.0634],\n",
      "        [0.0455],\n",
      "        [0.0638],\n",
      "        [0.0977],\n",
      "        [0.0905],\n",
      "        [0.0966],\n",
      "        [0.0006],\n",
      "        [0.0181],\n",
      "        [0.0737],\n",
      "        [0.0306],\n",
      "        [0.0326],\n",
      "        [0.0902],\n",
      "        [0.0109],\n",
      "        [0.1239],\n",
      "        [0.1072],\n",
      "        [0.0392],\n",
      "        [0.0823],\n",
      "        [0.1105],\n",
      "        [0.0550],\n",
      "        [0.0768],\n",
      "        [0.0590],\n",
      "        [0.1265],\n",
      "        [0.0673],\n",
      "        [0.0849],\n",
      "        [0.0888],\n",
      "        [0.0671],\n",
      "        [0.0804],\n",
      "        [0.1208],\n",
      "        [0.1436],\n",
      "        [0.1126],\n",
      "        [0.1244],\n",
      "        [0.1383],\n",
      "        [0.1555],\n",
      "        [0.1827],\n",
      "        [0.1820],\n",
      "        [0.1877],\n",
      "        [0.1223],\n",
      "        [0.2354],\n",
      "        [0.0087],\n",
      "        [0.3535],\n",
      "        [0.3630],\n",
      "        [0.3648],\n",
      "        [0.3733],\n",
      "        [0.3751],\n",
      "        [0.3772],\n",
      "        [0.3783],\n",
      "        [0.3815],\n",
      "        [0.4005],\n",
      "        [0.4015],\n",
      "        [0.4085],\n",
      "        [0.4128],\n",
      "        [0.4148],\n",
      "        [0.4157],\n",
      "        [0.4202],\n",
      "        [0.4276],\n",
      "        [0.4293],\n",
      "        [0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0235],\n",
      "        [    0.0285],\n",
      "        [    0.0569],\n",
      "        [    0.0099],\n",
      "        [    0.0213],\n",
      "        [    0.0128],\n",
      "        [    0.0422],\n",
      "        [    0.0173],\n",
      "        [    0.0200],\n",
      "        [    0.0233],\n",
      "        [    0.0039],\n",
      "        [    0.0457],\n",
      "        [    0.0066],\n",
      "        [    0.0167],\n",
      "        [    0.0288],\n",
      "        [    0.0236],\n",
      "        [    0.0017],\n",
      "        [    0.0377],\n",
      "        [    0.0051],\n",
      "        [    0.0484],\n",
      "        [    0.0448],\n",
      "        [    0.0323],\n",
      "        [    0.0366],\n",
      "        [    0.0354],\n",
      "        [    0.0265],\n",
      "        [    0.0267],\n",
      "        [    0.0281],\n",
      "        [    0.0586],\n",
      "        [    0.0580],\n",
      "        [    0.0155],\n",
      "        [    0.0377],\n",
      "        [    0.0604],\n",
      "        [    0.1038],\n",
      "        [    0.0089],\n",
      "        [    0.0424],\n",
      "        [    0.0719],\n",
      "        [    0.0091],\n",
      "        [    0.0040],\n",
      "        [    0.0065],\n",
      "        [    0.0137],\n",
      "        [    0.0573],\n",
      "        [    0.0255],\n",
      "        [    0.0038],\n",
      "        [    0.0459],\n",
      "        [    0.0136],\n",
      "        [    0.0071],\n",
      "        [    0.0335],\n",
      "        [    0.0685],\n",
      "        [    0.0076],\n",
      "        [    0.0003],\n",
      "        [    0.0064],\n",
      "        [    0.0174],\n",
      "        [    0.0153],\n",
      "        [    0.0142],\n",
      "        [    0.0589],\n",
      "        [    0.0569],\n",
      "        [    0.0023],\n",
      "        [    0.0086],\n",
      "        [    0.0558],\n",
      "        [    0.0752],\n",
      "        [    0.0163],\n",
      "        [    0.0049],\n",
      "        [    0.1279],\n",
      "        [    0.0552],\n",
      "        [    0.0318],\n",
      "        [    0.0262],\n",
      "        [    0.0485],\n",
      "        [    0.0152],\n",
      "        [    0.0008],\n",
      "        [    0.0897],\n",
      "        [    0.0335],\n",
      "        [    0.0328],\n",
      "        [    0.0893],\n",
      "        [    0.0722],\n",
      "        [    0.0314],\n",
      "        [    0.0058],\n",
      "        [    0.0197],\n",
      "        [    0.0686],\n",
      "        [    0.0659],\n",
      "        [    0.0662],\n",
      "        [    0.0433],\n",
      "        [    0.0333],\n",
      "        [    0.0577],\n",
      "        [    0.0607],\n",
      "        [    0.0900],\n",
      "        [    0.0819],\n",
      "        [    0.0359],\n",
      "        [    0.0113],\n",
      "        [    0.0131],\n",
      "        [    0.0649],\n",
      "        [    0.0474],\n",
      "        [    0.0651],\n",
      "        [    0.0998],\n",
      "        [    0.0931],\n",
      "        [    0.0991],\n",
      "        [    0.0052],\n",
      "        [    0.0218],\n",
      "        [    0.0712],\n",
      "        [    0.0266],\n",
      "        [    0.0365],\n",
      "        [    0.0875],\n",
      "        [    0.0149],\n",
      "        [    0.1216],\n",
      "        [    0.1010],\n",
      "        [    0.0346],\n",
      "        [    0.0772],\n",
      "        [    0.1123],\n",
      "        [    0.0545],\n",
      "        [    0.0784],\n",
      "        [    0.0604],\n",
      "        [    0.1239],\n",
      "        [    0.0624],\n",
      "        [    0.0865],\n",
      "        [    0.0870],\n",
      "        [    0.0631],\n",
      "        [    0.0754],\n",
      "        [    0.1190],\n",
      "        [    0.1456],\n",
      "        [    0.1091],\n",
      "        [    0.1264],\n",
      "        [    0.1398],\n",
      "        [    0.1516],\n",
      "        [    0.1792],\n",
      "        [    0.1770],\n",
      "        [    0.1846],\n",
      "        [    0.1228],\n",
      "        [    0.2304],\n",
      "        [    0.0134],\n",
      "        [    0.3530],\n",
      "        [    0.3625],\n",
      "        [    0.3643],\n",
      "        [    0.3728],\n",
      "        [    0.3746],\n",
      "        [    0.3767],\n",
      "        [    0.3778],\n",
      "        [    0.3810],\n",
      "        [    0.4000],\n",
      "        [    0.4010],\n",
      "        [    0.4080],\n",
      "        [    0.4123],\n",
      "        [    0.4143],\n",
      "        [    0.4152],\n",
      "        [    0.4197],\n",
      "        [    0.4271],\n",
      "        [    0.4288],\n",
      "        [    0.4295]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 74.79885268211365\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 147\n",
      "剩餘X 資料 torch.Size([13, 18])\n",
      "剩餘Y 資料 torch.Size([13, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18450333178043365, 8)\n",
      "The second_loss value of k: (0.1868494153022766, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.0539])\n",
      "目前模型的Data狀態 torch.Size([147, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5825],\n",
      "        [0.6355],\n",
      "        [0.2502],\n",
      "        [0.6492],\n",
      "        [0.5771],\n",
      "        [0.1930],\n",
      "        [0.6229],\n",
      "        [0.2094],\n",
      "        [0.4204],\n",
      "        [0.6357],\n",
      "        [0.6636],\n",
      "        [0.4415],\n",
      "        [0.6408],\n",
      "        [0.5778],\n",
      "        [0.6453],\n",
      "        [0.6179],\n",
      "        [0.6789],\n",
      "        [0.5641],\n",
      "        [0.6604],\n",
      "        [0.2375],\n",
      "        [0.5066],\n",
      "        [0.6037],\n",
      "        [0.3508],\n",
      "        [0.6099],\n",
      "        [0.4474],\n",
      "        [0.6096],\n",
      "        [0.5761],\n",
      "        [0.4133],\n",
      "        [0.3694],\n",
      "        [0.5953],\n",
      "        [0.3629],\n",
      "        [0.4042],\n",
      "        [0.2154],\n",
      "        [0.6323],\n",
      "        [0.5763],\n",
      "        [0.2358],\n",
      "        [0.4619],\n",
      "        [0.6571],\n",
      "        [0.5972],\n",
      "        [0.6490],\n",
      "        [0.6210],\n",
      "        [0.2081],\n",
      "        [0.6873],\n",
      "        [0.5392],\n",
      "        [0.5542],\n",
      "        [0.6144],\n",
      "        [0.2957],\n",
      "        [0.2276],\n",
      "        [0.6362],\n",
      "        [0.5363],\n",
      "        [0.5825],\n",
      "        [0.6042],\n",
      "        [0.6459],\n",
      "        [0.2059],\n",
      "        [0.5582],\n",
      "        [0.3279],\n",
      "        [0.6255],\n",
      "        [0.7036],\n",
      "        [0.4707],\n",
      "        [0.2275],\n",
      "        [0.6399],\n",
      "        [0.6391],\n",
      "        [0.0342],\n",
      "        [0.6242],\n",
      "        [0.5022],\n",
      "        [0.6072],\n",
      "        [0.4746],\n",
      "        [0.7202],\n",
      "        [0.6922],\n",
      "        [0.5208],\n",
      "        [0.2513],\n",
      "        [0.6102],\n",
      "        [0.1653],\n",
      "        [0.5998],\n",
      "        [0.5983],\n",
      "        [0.6975],\n",
      "        [0.7275],\n",
      "        [0.4643],\n",
      "        [0.2920],\n",
      "        [0.2926],\n",
      "        [0.3110],\n",
      "        [0.2174],\n",
      "        [0.3003],\n",
      "        [0.2979],\n",
      "        [0.5339],\n",
      "        [0.3020],\n",
      "        [0.6694],\n",
      "        [0.7274],\n",
      "        [0.6926],\n",
      "        [0.2986],\n",
      "        [0.6643],\n",
      "        [0.2993],\n",
      "        [0.5910],\n",
      "        [0.6035],\n",
      "        [0.5902],\n",
      "        [0.5061],\n",
      "        [0.6549],\n",
      "        [0.2520],\n",
      "        [0.6897],\n",
      "        [0.6352],\n",
      "        [0.2752],\n",
      "        [0.2760],\n",
      "        [0.3125],\n",
      "        [0.2191],\n",
      "        [0.5299],\n",
      "        [0.6916],\n",
      "        [0.2866],\n",
      "        [0.6623],\n",
      "        [0.6559],\n",
      "        [0.6751],\n",
      "        [0.2866],\n",
      "        [0.6838],\n",
      "        [0.3047],\n",
      "        [0.4134],\n",
      "        [0.3306],\n",
      "        [0.5323],\n",
      "        [0.4058],\n",
      "        [0.2909],\n",
      "        [0.3741],\n",
      "        [0.3257],\n",
      "        [0.3112],\n",
      "        [0.5753],\n",
      "        [0.5997],\n",
      "        [0.2041],\n",
      "        [0.4120],\n",
      "        [0.1228],\n",
      "        [0.6916],\n",
      "        [0.1841],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835],\n",
      "        [0.4835]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0235],\n",
      "        [    0.0285],\n",
      "        [    0.0569],\n",
      "        [    0.0099],\n",
      "        [    0.0213],\n",
      "        [    0.0128],\n",
      "        [    0.0422],\n",
      "        [    0.0173],\n",
      "        [    0.0200],\n",
      "        [    0.0233],\n",
      "        [    0.0039],\n",
      "        [    0.0457],\n",
      "        [    0.0066],\n",
      "        [    0.0167],\n",
      "        [    0.0288],\n",
      "        [    0.0236],\n",
      "        [    0.0017],\n",
      "        [    0.0377],\n",
      "        [    0.0051],\n",
      "        [    0.0484],\n",
      "        [    0.0448],\n",
      "        [    0.0323],\n",
      "        [    0.0366],\n",
      "        [    0.0354],\n",
      "        [    0.0265],\n",
      "        [    0.0267],\n",
      "        [    0.0281],\n",
      "        [    0.0586],\n",
      "        [    0.0580],\n",
      "        [    0.0155],\n",
      "        [    0.0377],\n",
      "        [    0.0604],\n",
      "        [    0.1038],\n",
      "        [    0.0089],\n",
      "        [    0.0424],\n",
      "        [    0.0719],\n",
      "        [    0.0091],\n",
      "        [    0.0040],\n",
      "        [    0.0065],\n",
      "        [    0.0137],\n",
      "        [    0.0573],\n",
      "        [    0.0255],\n",
      "        [    0.0038],\n",
      "        [    0.0459],\n",
      "        [    0.0136],\n",
      "        [    0.0071],\n",
      "        [    0.0335],\n",
      "        [    0.0685],\n",
      "        [    0.0076],\n",
      "        [    0.0003],\n",
      "        [    0.0064],\n",
      "        [    0.0174],\n",
      "        [    0.0153],\n",
      "        [    0.0142],\n",
      "        [    0.0589],\n",
      "        [    0.0569],\n",
      "        [    0.0023],\n",
      "        [    0.0086],\n",
      "        [    0.0558],\n",
      "        [    0.0752],\n",
      "        [    0.0163],\n",
      "        [    0.0049],\n",
      "        [    0.1279],\n",
      "        [    0.0552],\n",
      "        [    0.0318],\n",
      "        [    0.0262],\n",
      "        [    0.0485],\n",
      "        [    0.0152],\n",
      "        [    0.0008],\n",
      "        [    0.0897],\n",
      "        [    0.0335],\n",
      "        [    0.0328],\n",
      "        [    0.0893],\n",
      "        [    0.0722],\n",
      "        [    0.0314],\n",
      "        [    0.0058],\n",
      "        [    0.0197],\n",
      "        [    0.0686],\n",
      "        [    0.0659],\n",
      "        [    0.0662],\n",
      "        [    0.0433],\n",
      "        [    0.0333],\n",
      "        [    0.0577],\n",
      "        [    0.0607],\n",
      "        [    0.0900],\n",
      "        [    0.0819],\n",
      "        [    0.0359],\n",
      "        [    0.0113],\n",
      "        [    0.0131],\n",
      "        [    0.0649],\n",
      "        [    0.0474],\n",
      "        [    0.0651],\n",
      "        [    0.0998],\n",
      "        [    0.0931],\n",
      "        [    0.0991],\n",
      "        [    0.0052],\n",
      "        [    0.0218],\n",
      "        [    0.0712],\n",
      "        [    0.0266],\n",
      "        [    0.0365],\n",
      "        [    0.0875],\n",
      "        [    0.0149],\n",
      "        [    0.1216],\n",
      "        [    0.1010],\n",
      "        [    0.0346],\n",
      "        [    0.0772],\n",
      "        [    0.1123],\n",
      "        [    0.0545],\n",
      "        [    0.0784],\n",
      "        [    0.0604],\n",
      "        [    0.1239],\n",
      "        [    0.0624],\n",
      "        [    0.0865],\n",
      "        [    0.0870],\n",
      "        [    0.0631],\n",
      "        [    0.0754],\n",
      "        [    0.1190],\n",
      "        [    0.1456],\n",
      "        [    0.1091],\n",
      "        [    0.1264],\n",
      "        [    0.1398],\n",
      "        [    0.1516],\n",
      "        [    0.1792],\n",
      "        [    0.1770],\n",
      "        [    0.1846],\n",
      "        [    0.1228],\n",
      "        [    0.2304],\n",
      "        [    0.0134],\n",
      "        [    0.3530],\n",
      "        [    0.3625],\n",
      "        [    0.3643],\n",
      "        [    0.3728],\n",
      "        [    0.3746],\n",
      "        [    0.3767],\n",
      "        [    0.3778],\n",
      "        [    0.3810],\n",
      "        [    0.4000],\n",
      "        [    0.4010],\n",
      "        [    0.4080],\n",
      "        [    0.4123],\n",
      "        [    0.4143],\n",
      "        [    0.4152],\n",
      "        [    0.4197],\n",
      "        [    0.4271],\n",
      "        [    0.4288],\n",
      "        [    0.4295],\n",
      "        [    0.4295]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0264],\n",
      "        [    0.0301],\n",
      "        [    0.0545],\n",
      "        [    0.0067],\n",
      "        [    0.0243],\n",
      "        [    0.0121],\n",
      "        [    0.0436],\n",
      "        [    0.0172],\n",
      "        [    0.0174],\n",
      "        [    0.0200],\n",
      "        [    0.0064],\n",
      "        [    0.0437],\n",
      "        [    0.0038],\n",
      "        [    0.0197],\n",
      "        [    0.0304],\n",
      "        [    0.0210],\n",
      "        [    0.0044],\n",
      "        [    0.0402],\n",
      "        [    0.0063],\n",
      "        [    0.0487],\n",
      "        [    0.0418],\n",
      "        [    0.0298],\n",
      "        [    0.0397],\n",
      "        [    0.0326],\n",
      "        [    0.0239],\n",
      "        [    0.0240],\n",
      "        [    0.0311],\n",
      "        [    0.0556],\n",
      "        [    0.0601],\n",
      "        [    0.0126],\n",
      "        [    0.0394],\n",
      "        [    0.0578],\n",
      "        [    0.1024],\n",
      "        [    0.0056],\n",
      "        [    0.0395],\n",
      "        [    0.0700],\n",
      "        [    0.0073],\n",
      "        [    0.0003],\n",
      "        [    0.0094],\n",
      "        [    0.0163],\n",
      "        [    0.0597],\n",
      "        [    0.0226],\n",
      "        [    0.0062],\n",
      "        [    0.0423],\n",
      "        [    0.0162],\n",
      "        [    0.0103],\n",
      "        [    0.0345],\n",
      "        [    0.0699],\n",
      "        [    0.0100],\n",
      "        [    0.0020],\n",
      "        [    0.0047],\n",
      "        [    0.0146],\n",
      "        [    0.0174],\n",
      "        [    0.0166],\n",
      "        [    0.0561],\n",
      "        [    0.0578],\n",
      "        [    0.0003],\n",
      "        [    0.0106],\n",
      "        [    0.0545],\n",
      "        [    0.0749],\n",
      "        [    0.0184],\n",
      "        [    0.0027],\n",
      "        [    0.1248],\n",
      "        [    0.0527],\n",
      "        [    0.0295],\n",
      "        [    0.0231],\n",
      "        [    0.0466],\n",
      "        [    0.0164],\n",
      "        [    0.0039],\n",
      "        [    0.0854],\n",
      "        [    0.0331],\n",
      "        [    0.0304],\n",
      "        [    0.0837],\n",
      "        [    0.0699],\n",
      "        [    0.0334],\n",
      "        [    0.0027],\n",
      "        [    0.0213],\n",
      "        [    0.0670],\n",
      "        [    0.0625],\n",
      "        [    0.0634],\n",
      "        [    0.0405],\n",
      "        [    0.0350],\n",
      "        [    0.0543],\n",
      "        [    0.0583],\n",
      "        [    0.0864],\n",
      "        [    0.0835],\n",
      "        [    0.0367],\n",
      "        [    0.0136],\n",
      "        [    0.0103],\n",
      "        [    0.0623],\n",
      "        [    0.0440],\n",
      "        [    0.0623],\n",
      "        [    0.0967],\n",
      "        [    0.0904],\n",
      "        [    0.0963],\n",
      "        [    0.0042],\n",
      "        [    0.0197],\n",
      "        [    0.0728],\n",
      "        [    0.0285],\n",
      "        [    0.0346],\n",
      "        [    0.0892],\n",
      "        [    0.0145],\n",
      "        [    0.1238],\n",
      "        [    0.0964],\n",
      "        [    0.0357],\n",
      "        [    0.0783],\n",
      "        [    0.1101],\n",
      "        [    0.0491],\n",
      "        [    0.0747],\n",
      "        [    0.0566],\n",
      "        [    0.1257],\n",
      "        [    0.0636],\n",
      "        [    0.0840],\n",
      "        [    0.0900],\n",
      "        [    0.0637],\n",
      "        [    0.0763],\n",
      "        [    0.1220],\n",
      "        [    0.1435],\n",
      "        [    0.1104],\n",
      "        [    0.1242],\n",
      "        [    0.1374],\n",
      "        [    0.1533],\n",
      "        [    0.1813],\n",
      "        [    0.1759],\n",
      "        [    0.1864],\n",
      "        [    0.1255],\n",
      "        [    0.2316],\n",
      "        [    0.0166],\n",
      "        [    0.3525],\n",
      "        [    0.3620],\n",
      "        [    0.3638],\n",
      "        [    0.3723],\n",
      "        [    0.3741],\n",
      "        [    0.3762],\n",
      "        [    0.3772],\n",
      "        [    0.3804],\n",
      "        [    0.3995],\n",
      "        [    0.4005],\n",
      "        [    0.4074],\n",
      "        [    0.4118],\n",
      "        [    0.4138],\n",
      "        [    0.4147],\n",
      "        [    0.4191],\n",
      "        [    0.4266],\n",
      "        [    0.4282],\n",
      "        [    0.4290],\n",
      "        [    0.4290]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 75.03206300735474\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 148\n",
      "剩餘X 資料 torch.Size([12, 18])\n",
      "剩餘Y 資料 torch.Size([12, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18639424443244934, 6)\n",
      "The second_loss value of k: (0.18926312029361725, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.0512])\n",
      "目前模型的Data狀態 torch.Size([148, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5854],\n",
      "        [0.6371],\n",
      "        [0.2526],\n",
      "        [0.6524],\n",
      "        [0.5802],\n",
      "        [0.1923],\n",
      "        [0.6243],\n",
      "        [0.2093],\n",
      "        [0.4230],\n",
      "        [0.6390],\n",
      "        [0.6661],\n",
      "        [0.4435],\n",
      "        [0.6436],\n",
      "        [0.5808],\n",
      "        [0.6470],\n",
      "        [0.6206],\n",
      "        [0.6815],\n",
      "        [0.5667],\n",
      "        [0.6616],\n",
      "        [0.2378],\n",
      "        [0.5097],\n",
      "        [0.6062],\n",
      "        [0.3538],\n",
      "        [0.6128],\n",
      "        [0.4501],\n",
      "        [0.6124],\n",
      "        [0.5791],\n",
      "        [0.4163],\n",
      "        [0.3715],\n",
      "        [0.5982],\n",
      "        [0.3647],\n",
      "        [0.4068],\n",
      "        [0.2168],\n",
      "        [0.6356],\n",
      "        [0.5793],\n",
      "        [0.2378],\n",
      "        [0.4637],\n",
      "        [0.6608],\n",
      "        [0.6001],\n",
      "        [0.6516],\n",
      "        [0.6235],\n",
      "        [0.2111],\n",
      "        [0.6897],\n",
      "        [0.5428],\n",
      "        [0.5568],\n",
      "        [0.6175],\n",
      "        [0.2967],\n",
      "        [0.2289],\n",
      "        [0.6387],\n",
      "        [0.5386],\n",
      "        [0.5841],\n",
      "        [0.6070],\n",
      "        [0.6479],\n",
      "        [0.2083],\n",
      "        [0.5609],\n",
      "        [0.3288],\n",
      "        [0.6281],\n",
      "        [0.7056],\n",
      "        [0.4720],\n",
      "        [0.2278],\n",
      "        [0.6419],\n",
      "        [0.6412],\n",
      "        [0.0373],\n",
      "        [0.6267],\n",
      "        [0.5045],\n",
      "        [0.6103],\n",
      "        [0.4764],\n",
      "        [0.7214],\n",
      "        [0.6953],\n",
      "        [0.5251],\n",
      "        [0.2509],\n",
      "        [0.6126],\n",
      "        [0.1708],\n",
      "        [0.6021],\n",
      "        [0.6003],\n",
      "        [0.7007],\n",
      "        [0.7291],\n",
      "        [0.4659],\n",
      "        [0.2954],\n",
      "        [0.2953],\n",
      "        [0.3138],\n",
      "        [0.2191],\n",
      "        [0.3037],\n",
      "        [0.3003],\n",
      "        [0.5374],\n",
      "        [0.3036],\n",
      "        [0.6702],\n",
      "        [0.7297],\n",
      "        [0.6953],\n",
      "        [0.3012],\n",
      "        [0.6678],\n",
      "        [0.3021],\n",
      "        [0.5941],\n",
      "        [0.6062],\n",
      "        [0.5930],\n",
      "        [0.5072],\n",
      "        [0.6570],\n",
      "        [0.2536],\n",
      "        [0.6916],\n",
      "        [0.6371],\n",
      "        [0.2768],\n",
      "        [0.2765],\n",
      "        [0.3146],\n",
      "        [0.2145],\n",
      "        [0.5310],\n",
      "        [0.6927],\n",
      "        [0.2889],\n",
      "        [0.6676],\n",
      "        [0.6595],\n",
      "        [0.6789],\n",
      "        [0.2884],\n",
      "        [0.6849],\n",
      "        [0.3071],\n",
      "        [0.4164],\n",
      "        [0.3311],\n",
      "        [0.5331],\n",
      "        [0.4088],\n",
      "        [0.2930],\n",
      "        [0.3753],\n",
      "        [0.3280],\n",
      "        [0.3136],\n",
      "        [0.5770],\n",
      "        [0.6018],\n",
      "        [0.2029],\n",
      "        [0.4137],\n",
      "        [0.1255],\n",
      "        [0.6927],\n",
      "        [0.1810],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829],\n",
      "        [0.4829]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0264],\n",
      "        [    0.0301],\n",
      "        [    0.0545],\n",
      "        [    0.0067],\n",
      "        [    0.0243],\n",
      "        [    0.0121],\n",
      "        [    0.0436],\n",
      "        [    0.0172],\n",
      "        [    0.0174],\n",
      "        [    0.0200],\n",
      "        [    0.0064],\n",
      "        [    0.0437],\n",
      "        [    0.0038],\n",
      "        [    0.0197],\n",
      "        [    0.0304],\n",
      "        [    0.0210],\n",
      "        [    0.0044],\n",
      "        [    0.0402],\n",
      "        [    0.0063],\n",
      "        [    0.0487],\n",
      "        [    0.0418],\n",
      "        [    0.0298],\n",
      "        [    0.0397],\n",
      "        [    0.0326],\n",
      "        [    0.0239],\n",
      "        [    0.0240],\n",
      "        [    0.0311],\n",
      "        [    0.0556],\n",
      "        [    0.0601],\n",
      "        [    0.0126],\n",
      "        [    0.0394],\n",
      "        [    0.0578],\n",
      "        [    0.1024],\n",
      "        [    0.0056],\n",
      "        [    0.0395],\n",
      "        [    0.0700],\n",
      "        [    0.0073],\n",
      "        [    0.0003],\n",
      "        [    0.0094],\n",
      "        [    0.0163],\n",
      "        [    0.0597],\n",
      "        [    0.0226],\n",
      "        [    0.0062],\n",
      "        [    0.0423],\n",
      "        [    0.0162],\n",
      "        [    0.0103],\n",
      "        [    0.0345],\n",
      "        [    0.0699],\n",
      "        [    0.0100],\n",
      "        [    0.0020],\n",
      "        [    0.0047],\n",
      "        [    0.0146],\n",
      "        [    0.0174],\n",
      "        [    0.0166],\n",
      "        [    0.0561],\n",
      "        [    0.0578],\n",
      "        [    0.0003],\n",
      "        [    0.0106],\n",
      "        [    0.0545],\n",
      "        [    0.0749],\n",
      "        [    0.0184],\n",
      "        [    0.0027],\n",
      "        [    0.1248],\n",
      "        [    0.0527],\n",
      "        [    0.0295],\n",
      "        [    0.0231],\n",
      "        [    0.0466],\n",
      "        [    0.0164],\n",
      "        [    0.0039],\n",
      "        [    0.0854],\n",
      "        [    0.0331],\n",
      "        [    0.0304],\n",
      "        [    0.0837],\n",
      "        [    0.0699],\n",
      "        [    0.0334],\n",
      "        [    0.0027],\n",
      "        [    0.0213],\n",
      "        [    0.0670],\n",
      "        [    0.0625],\n",
      "        [    0.0634],\n",
      "        [    0.0405],\n",
      "        [    0.0350],\n",
      "        [    0.0543],\n",
      "        [    0.0583],\n",
      "        [    0.0864],\n",
      "        [    0.0835],\n",
      "        [    0.0367],\n",
      "        [    0.0136],\n",
      "        [    0.0103],\n",
      "        [    0.0623],\n",
      "        [    0.0440],\n",
      "        [    0.0623],\n",
      "        [    0.0967],\n",
      "        [    0.0904],\n",
      "        [    0.0963],\n",
      "        [    0.0042],\n",
      "        [    0.0197],\n",
      "        [    0.0728],\n",
      "        [    0.0285],\n",
      "        [    0.0346],\n",
      "        [    0.0892],\n",
      "        [    0.0145],\n",
      "        [    0.1238],\n",
      "        [    0.0964],\n",
      "        [    0.0357],\n",
      "        [    0.0783],\n",
      "        [    0.1101],\n",
      "        [    0.0491],\n",
      "        [    0.0747],\n",
      "        [    0.0566],\n",
      "        [    0.1257],\n",
      "        [    0.0636],\n",
      "        [    0.0840],\n",
      "        [    0.0900],\n",
      "        [    0.0637],\n",
      "        [    0.0763],\n",
      "        [    0.1220],\n",
      "        [    0.1435],\n",
      "        [    0.1104],\n",
      "        [    0.1242],\n",
      "        [    0.1374],\n",
      "        [    0.1533],\n",
      "        [    0.1813],\n",
      "        [    0.1759],\n",
      "        [    0.1864],\n",
      "        [    0.1255],\n",
      "        [    0.2316],\n",
      "        [    0.0166],\n",
      "        [    0.3525],\n",
      "        [    0.3620],\n",
      "        [    0.3638],\n",
      "        [    0.3723],\n",
      "        [    0.3741],\n",
      "        [    0.3762],\n",
      "        [    0.3772],\n",
      "        [    0.3804],\n",
      "        [    0.3995],\n",
      "        [    0.4005],\n",
      "        [    0.4074],\n",
      "        [    0.4118],\n",
      "        [    0.4138],\n",
      "        [    0.4147],\n",
      "        [    0.4191],\n",
      "        [    0.4266],\n",
      "        [    0.4282],\n",
      "        [    0.4290],\n",
      "        [    0.4290],\n",
      "        [    0.4317]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 242\n",
      "Number of shrink: 122\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0249],\n",
      "        [    0.0246],\n",
      "        [    0.0537],\n",
      "        [    0.0060],\n",
      "        [    0.0234],\n",
      "        [    0.0049],\n",
      "        [    0.0372],\n",
      "        [    0.0120],\n",
      "        [    0.0187],\n",
      "        [    0.0192],\n",
      "        [    0.0048],\n",
      "        [    0.0466],\n",
      "        [    0.0043],\n",
      "        [    0.0186],\n",
      "        [    0.0248],\n",
      "        [    0.0222],\n",
      "        [    0.0031],\n",
      "        [    0.0375],\n",
      "        [    0.0003],\n",
      "        [    0.0446],\n",
      "        [    0.0414],\n",
      "        [    0.0319],\n",
      "        [    0.0411],\n",
      "        [    0.0333],\n",
      "        [    0.0249],\n",
      "        [    0.0252],\n",
      "        [    0.0297],\n",
      "        [    0.0556],\n",
      "        [    0.0580],\n",
      "        [    0.0140],\n",
      "        [    0.0364],\n",
      "        [    0.0588],\n",
      "        [    0.1049],\n",
      "        [    0.0047],\n",
      "        [    0.0404],\n",
      "        [    0.0708],\n",
      "        [    0.0110],\n",
      "        [    0.0008],\n",
      "        [    0.0079],\n",
      "        [    0.0148],\n",
      "        [    0.0566],\n",
      "        [    0.0198],\n",
      "        [    0.0040],\n",
      "        [    0.0395],\n",
      "        [    0.0138],\n",
      "        [    0.0097],\n",
      "        [    0.0300],\n",
      "        [    0.0688],\n",
      "        [    0.0073],\n",
      "        [    0.0011],\n",
      "        [    0.0091],\n",
      "        [    0.0166],\n",
      "        [    0.0136],\n",
      "        [    0.0176],\n",
      "        [    0.0571],\n",
      "        [    0.0526],\n",
      "        [    0.0021],\n",
      "        [    0.0070],\n",
      "        [    0.0592],\n",
      "        [    0.0810],\n",
      "        [    0.0139],\n",
      "        [    0.0069],\n",
      "        [    0.1198],\n",
      "        [    0.0543],\n",
      "        [    0.0325],\n",
      "        [    0.0238],\n",
      "        [    0.0506],\n",
      "        [    0.0094],\n",
      "        [    0.0030],\n",
      "        [    0.0806],\n",
      "        [    0.0273],\n",
      "        [    0.0331],\n",
      "        [    0.0718],\n",
      "        [    0.0722],\n",
      "        [    0.0302],\n",
      "        [    0.0038],\n",
      "        [    0.0158],\n",
      "        [    0.0717],\n",
      "        [    0.0588],\n",
      "        [    0.0618],\n",
      "        [    0.0393],\n",
      "        [    0.0339],\n",
      "        [    0.0508],\n",
      "        [    0.0576],\n",
      "        [    0.0841],\n",
      "        [    0.0810],\n",
      "        [    0.0283],\n",
      "        [    0.0102],\n",
      "        [    0.0121],\n",
      "        [    0.0615],\n",
      "        [    0.0439],\n",
      "        [    0.0612],\n",
      "        [    0.0961],\n",
      "        [    0.0912],\n",
      "        [    0.0966],\n",
      "        [    0.0099],\n",
      "        [    0.0237],\n",
      "        [    0.0708],\n",
      "        [    0.0240],\n",
      "        [    0.0390],\n",
      "        [    0.0872],\n",
      "        [    0.0205],\n",
      "        [    0.1227],\n",
      "        [    0.0820],\n",
      "        [    0.0300],\n",
      "        [    0.0706],\n",
      "        [    0.1103],\n",
      "        [    0.0419],\n",
      "        [    0.0738],\n",
      "        [    0.0551],\n",
      "        [    0.1239],\n",
      "        [    0.0561],\n",
      "        [    0.0836],\n",
      "        [    0.0901],\n",
      "        [    0.0579],\n",
      "        [    0.0697],\n",
      "        [    0.1227],\n",
      "        [    0.1440],\n",
      "        [    0.1063],\n",
      "        [    0.1245],\n",
      "        [    0.1369],\n",
      "        [    0.1492],\n",
      "        [    0.1784],\n",
      "        [    0.1673],\n",
      "        [    0.1835],\n",
      "        [    0.1291],\n",
      "        [    0.2241],\n",
      "        [    0.0267],\n",
      "        [    0.3507],\n",
      "        [    0.3602],\n",
      "        [    0.3620],\n",
      "        [    0.3706],\n",
      "        [    0.3724],\n",
      "        [    0.3744],\n",
      "        [    0.3755],\n",
      "        [    0.3787],\n",
      "        [    0.3978],\n",
      "        [    0.3988],\n",
      "        [    0.4057],\n",
      "        [    0.4101],\n",
      "        [    0.4121],\n",
      "        [    0.4129],\n",
      "        [    0.4174],\n",
      "        [    0.4248],\n",
      "        [    0.4265],\n",
      "        [    0.4272],\n",
      "        [    0.4273],\n",
      "        [    0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0280],\n",
      "        [0.0264],\n",
      "        [0.0513],\n",
      "        [0.0024],\n",
      "        [0.0266],\n",
      "        [0.0043],\n",
      "        [0.0388],\n",
      "        [0.0119],\n",
      "        [0.0163],\n",
      "        [0.0155],\n",
      "        [0.0078],\n",
      "        [0.0447],\n",
      "        [0.0012],\n",
      "        [0.0218],\n",
      "        [0.0268],\n",
      "        [0.0195],\n",
      "        [0.0061],\n",
      "        [0.0400],\n",
      "        [0.0013],\n",
      "        [0.0449],\n",
      "        [0.0381],\n",
      "        [0.0295],\n",
      "        [0.0441],\n",
      "        [0.0304],\n",
      "        [0.0224],\n",
      "        [0.0225],\n",
      "        [0.0329],\n",
      "        [0.0528],\n",
      "        [0.0600],\n",
      "        [0.0111],\n",
      "        [0.0379],\n",
      "        [0.0564],\n",
      "        [0.1034],\n",
      "        [0.0011],\n",
      "        [0.0375],\n",
      "        [0.0688],\n",
      "        [0.0094],\n",
      "        [0.0044],\n",
      "        [0.0111],\n",
      "        [0.0177],\n",
      "        [0.0591],\n",
      "        [0.0171],\n",
      "        [0.0068],\n",
      "        [0.0357],\n",
      "        [0.0164],\n",
      "        [0.0131],\n",
      "        [0.0308],\n",
      "        [0.0701],\n",
      "        [0.0099],\n",
      "        [0.0011],\n",
      "        [0.0074],\n",
      "        [0.0137],\n",
      "        [0.0160],\n",
      "        [0.0198],\n",
      "        [0.0544],\n",
      "        [0.0535],\n",
      "        [0.0009],\n",
      "        [0.0094],\n",
      "        [0.0576],\n",
      "        [0.0805],\n",
      "        [0.0161],\n",
      "        [0.0045],\n",
      "        [0.1173],\n",
      "        [0.0516],\n",
      "        [0.0303],\n",
      "        [0.0205],\n",
      "        [0.0489],\n",
      "        [0.0107],\n",
      "        [0.0059],\n",
      "        [0.0762],\n",
      "        [0.0270],\n",
      "        [0.0303],\n",
      "        [0.0671],\n",
      "        [0.0699],\n",
      "        [0.0323],\n",
      "        [0.0008],\n",
      "        [0.0175],\n",
      "        [0.0702],\n",
      "        [0.0553],\n",
      "        [0.0590],\n",
      "        [0.0366],\n",
      "        [0.0354],\n",
      "        [0.0475],\n",
      "        [0.0552],\n",
      "        [0.0804],\n",
      "        [0.0824],\n",
      "        [0.0292],\n",
      "        [0.0126],\n",
      "        [0.0094],\n",
      "        [0.0589],\n",
      "        [0.0406],\n",
      "        [0.0585],\n",
      "        [0.0929],\n",
      "        [0.0883],\n",
      "        [0.0937],\n",
      "        [0.0087],\n",
      "        [0.0213],\n",
      "        [0.0722],\n",
      "        [0.0262],\n",
      "        [0.0367],\n",
      "        [0.0887],\n",
      "        [0.0199],\n",
      "        [0.1247],\n",
      "        [0.0786],\n",
      "        [0.0312],\n",
      "        [0.0718],\n",
      "        [0.1080],\n",
      "        [0.0369],\n",
      "        [0.0703],\n",
      "        [0.0515],\n",
      "        [0.1255],\n",
      "        [0.0575],\n",
      "        [0.0811],\n",
      "        [0.0929],\n",
      "        [0.0587],\n",
      "        [0.0706],\n",
      "        [0.1257],\n",
      "        [0.1419],\n",
      "        [0.1077],\n",
      "        [0.1223],\n",
      "        [0.1345],\n",
      "        [0.1509],\n",
      "        [0.1805],\n",
      "        [0.1661],\n",
      "        [0.1853],\n",
      "        [0.1310],\n",
      "        [0.2255],\n",
      "        [0.0289],\n",
      "        [0.3503],\n",
      "        [0.3598],\n",
      "        [0.3616],\n",
      "        [0.3701],\n",
      "        [0.3719],\n",
      "        [0.3740],\n",
      "        [0.3750],\n",
      "        [0.3783],\n",
      "        [0.3973],\n",
      "        [0.3983],\n",
      "        [0.4052],\n",
      "        [0.4096],\n",
      "        [0.4116],\n",
      "        [0.4125],\n",
      "        [0.4170],\n",
      "        [0.4244],\n",
      "        [0.4260],\n",
      "        [0.4268],\n",
      "        [0.4268],\n",
      "        [0.4295]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 76.02016234397888\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 149\n",
      "剩餘X 資料 torch.Size([11, 18])\n",
      "剩餘Y 資料 torch.Size([11, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18736454844474792, 9)\n",
      "The second_loss value of k: (0.1887234002351761, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.0479])\n",
      "目前模型的Data狀態 torch.Size([149, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5870],\n",
      "        [0.6334],\n",
      "        [0.2558],\n",
      "        [0.6568],\n",
      "        [0.5824],\n",
      "        [0.1845],\n",
      "        [0.6194],\n",
      "        [0.2040],\n",
      "        [0.4241],\n",
      "        [0.6435],\n",
      "        [0.6675],\n",
      "        [0.4425],\n",
      "        [0.6463],\n",
      "        [0.5829],\n",
      "        [0.6433],\n",
      "        [0.6221],\n",
      "        [0.6833],\n",
      "        [0.5665],\n",
      "        [0.6566],\n",
      "        [0.2341],\n",
      "        [0.5133],\n",
      "        [0.6065],\n",
      "        [0.3582],\n",
      "        [0.6150],\n",
      "        [0.4516],\n",
      "        [0.6139],\n",
      "        [0.5808],\n",
      "        [0.4191],\n",
      "        [0.3714],\n",
      "        [0.5996],\n",
      "        [0.3632],\n",
      "        [0.4081],\n",
      "        [0.2158],\n",
      "        [0.6401],\n",
      "        [0.5812],\n",
      "        [0.2390],\n",
      "        [0.4615],\n",
      "        [0.6655],\n",
      "        [0.6017],\n",
      "        [0.6531],\n",
      "        [0.6229],\n",
      "        [0.2165],\n",
      "        [0.6903],\n",
      "        [0.5494],\n",
      "        [0.5570],\n",
      "        [0.6203],\n",
      "        [0.2930],\n",
      "        [0.2292],\n",
      "        [0.6385],\n",
      "        [0.5378],\n",
      "        [0.5814],\n",
      "        [0.6079],\n",
      "        [0.6465],\n",
      "        [0.2115],\n",
      "        [0.5627],\n",
      "        [0.3244],\n",
      "        [0.6286],\n",
      "        [0.7044],\n",
      "        [0.4690],\n",
      "        [0.2222],\n",
      "        [0.6397],\n",
      "        [0.6395],\n",
      "        [0.0447],\n",
      "        [0.6278],\n",
      "        [0.5037],\n",
      "        [0.6128],\n",
      "        [0.4741],\n",
      "        [0.7157],\n",
      "        [0.6973],\n",
      "        [0.5343],\n",
      "        [0.2448],\n",
      "        [0.6127],\n",
      "        [0.1875],\n",
      "        [0.6021],\n",
      "        [0.5992],\n",
      "        [0.7026],\n",
      "        [0.7253],\n",
      "        [0.4627],\n",
      "        [0.3026],\n",
      "        [0.2997],\n",
      "        [0.3177],\n",
      "        [0.2195],\n",
      "        [0.3105],\n",
      "        [0.3034],\n",
      "        [0.5435],\n",
      "        [0.3026],\n",
      "        [0.6627],\n",
      "        [0.7287],\n",
      "        [0.6962],\n",
      "        [0.3045],\n",
      "        [0.6711],\n",
      "        [0.3060],\n",
      "        [0.5978],\n",
      "        [0.6083],\n",
      "        [0.5956],\n",
      "        [0.5027],\n",
      "        [0.6554],\n",
      "        [0.2530],\n",
      "        [0.6892],\n",
      "        [0.6350],\n",
      "        [0.2763],\n",
      "        [0.2711],\n",
      "        [0.3156],\n",
      "        [0.1967],\n",
      "        [0.5266],\n",
      "        [0.6861],\n",
      "        [0.2909],\n",
      "        [0.6799],\n",
      "        [0.6639],\n",
      "        [0.6840],\n",
      "        [0.2883],\n",
      "        [0.6788],\n",
      "        [0.3100],\n",
      "        [0.4193],\n",
      "        [0.3261],\n",
      "        [0.5275],\n",
      "        [0.4124],\n",
      "        [0.2946],\n",
      "        [0.3726],\n",
      "        [0.3299],\n",
      "        [0.3166],\n",
      "        [0.5746],\n",
      "        [0.6010],\n",
      "        [0.1931],\n",
      "        [0.4127],\n",
      "        [0.1310],\n",
      "        [0.6867],\n",
      "        [0.1686],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808],\n",
      "        [0.4808]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0280],\n",
      "        [0.0264],\n",
      "        [0.0513],\n",
      "        [0.0024],\n",
      "        [0.0266],\n",
      "        [0.0043],\n",
      "        [0.0388],\n",
      "        [0.0119],\n",
      "        [0.0163],\n",
      "        [0.0155],\n",
      "        [0.0078],\n",
      "        [0.0447],\n",
      "        [0.0012],\n",
      "        [0.0218],\n",
      "        [0.0268],\n",
      "        [0.0195],\n",
      "        [0.0061],\n",
      "        [0.0400],\n",
      "        [0.0013],\n",
      "        [0.0449],\n",
      "        [0.0381],\n",
      "        [0.0295],\n",
      "        [0.0441],\n",
      "        [0.0304],\n",
      "        [0.0224],\n",
      "        [0.0225],\n",
      "        [0.0329],\n",
      "        [0.0528],\n",
      "        [0.0600],\n",
      "        [0.0111],\n",
      "        [0.0379],\n",
      "        [0.0564],\n",
      "        [0.1034],\n",
      "        [0.0011],\n",
      "        [0.0375],\n",
      "        [0.0688],\n",
      "        [0.0094],\n",
      "        [0.0044],\n",
      "        [0.0111],\n",
      "        [0.0177],\n",
      "        [0.0591],\n",
      "        [0.0171],\n",
      "        [0.0068],\n",
      "        [0.0357],\n",
      "        [0.0164],\n",
      "        [0.0131],\n",
      "        [0.0308],\n",
      "        [0.0701],\n",
      "        [0.0099],\n",
      "        [0.0011],\n",
      "        [0.0074],\n",
      "        [0.0137],\n",
      "        [0.0160],\n",
      "        [0.0198],\n",
      "        [0.0544],\n",
      "        [0.0535],\n",
      "        [0.0009],\n",
      "        [0.0094],\n",
      "        [0.0576],\n",
      "        [0.0805],\n",
      "        [0.0161],\n",
      "        [0.0045],\n",
      "        [0.1173],\n",
      "        [0.0516],\n",
      "        [0.0303],\n",
      "        [0.0205],\n",
      "        [0.0489],\n",
      "        [0.0107],\n",
      "        [0.0059],\n",
      "        [0.0762],\n",
      "        [0.0270],\n",
      "        [0.0303],\n",
      "        [0.0671],\n",
      "        [0.0699],\n",
      "        [0.0323],\n",
      "        [0.0008],\n",
      "        [0.0175],\n",
      "        [0.0702],\n",
      "        [0.0553],\n",
      "        [0.0590],\n",
      "        [0.0366],\n",
      "        [0.0354],\n",
      "        [0.0475],\n",
      "        [0.0552],\n",
      "        [0.0804],\n",
      "        [0.0824],\n",
      "        [0.0292],\n",
      "        [0.0126],\n",
      "        [0.0094],\n",
      "        [0.0589],\n",
      "        [0.0406],\n",
      "        [0.0585],\n",
      "        [0.0929],\n",
      "        [0.0883],\n",
      "        [0.0937],\n",
      "        [0.0087],\n",
      "        [0.0213],\n",
      "        [0.0722],\n",
      "        [0.0262],\n",
      "        [0.0367],\n",
      "        [0.0887],\n",
      "        [0.0199],\n",
      "        [0.1247],\n",
      "        [0.0786],\n",
      "        [0.0312],\n",
      "        [0.0718],\n",
      "        [0.1080],\n",
      "        [0.0369],\n",
      "        [0.0703],\n",
      "        [0.0515],\n",
      "        [0.1255],\n",
      "        [0.0575],\n",
      "        [0.0811],\n",
      "        [0.0929],\n",
      "        [0.0587],\n",
      "        [0.0706],\n",
      "        [0.1257],\n",
      "        [0.1419],\n",
      "        [0.1077],\n",
      "        [0.1223],\n",
      "        [0.1345],\n",
      "        [0.1509],\n",
      "        [0.1805],\n",
      "        [0.1661],\n",
      "        [0.1853],\n",
      "        [0.1310],\n",
      "        [0.2255],\n",
      "        [0.0289],\n",
      "        [0.3503],\n",
      "        [0.3598],\n",
      "        [0.3616],\n",
      "        [0.3701],\n",
      "        [0.3719],\n",
      "        [0.3740],\n",
      "        [0.3750],\n",
      "        [0.3783],\n",
      "        [0.3973],\n",
      "        [0.3983],\n",
      "        [0.4052],\n",
      "        [0.4096],\n",
      "        [0.4116],\n",
      "        [0.4125],\n",
      "        [0.4170],\n",
      "        [0.4244],\n",
      "        [0.4260],\n",
      "        [0.4268],\n",
      "        [0.4268],\n",
      "        [0.4295],\n",
      "        [0.4329]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 445\n",
      "Number of shrink: 225\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0298],\n",
      "        [    0.0224],\n",
      "        [    0.0481],\n",
      "        [    0.0029],\n",
      "        [    0.0290],\n",
      "        [    0.0041],\n",
      "        [    0.0337],\n",
      "        [    0.0059],\n",
      "        [    0.0155],\n",
      "        [    0.0101],\n",
      "        [    0.0096],\n",
      "        [    0.0465],\n",
      "        [    0.0022],\n",
      "        [    0.0240],\n",
      "        [    0.0230],\n",
      "        [    0.0174],\n",
      "        [    0.0085],\n",
      "        [    0.0396],\n",
      "        [    0.0047],\n",
      "        [    0.0405],\n",
      "        [    0.0340],\n",
      "        [    0.0286],\n",
      "        [    0.0489],\n",
      "        [    0.0275],\n",
      "        [    0.0216],\n",
      "        [    0.0202],\n",
      "        [    0.0346],\n",
      "        [    0.0503],\n",
      "        [    0.0600],\n",
      "        [    0.0097],\n",
      "        [    0.0365],\n",
      "        [    0.0554],\n",
      "        [    0.1041],\n",
      "        [    0.0044],\n",
      "        [    0.0357],\n",
      "        [    0.0675],\n",
      "        [    0.0122],\n",
      "        [    0.0088],\n",
      "        [    0.0127],\n",
      "        [    0.0199],\n",
      "        [    0.0585],\n",
      "        [    0.0119],\n",
      "        [    0.0077],\n",
      "        [    0.0286],\n",
      "        [    0.0164],\n",
      "        [    0.0159],\n",
      "        [    0.0269],\n",
      "        [    0.0700],\n",
      "        [    0.0096],\n",
      "        [    0.0002],\n",
      "        [    0.0099],\n",
      "        [    0.0127],\n",
      "        [    0.0146],\n",
      "        [    0.0226],\n",
      "        [    0.0527],\n",
      "        [    0.0489],\n",
      "        [    0.0014],\n",
      "        [    0.0082],\n",
      "        [    0.0601],\n",
      "        [    0.0858],\n",
      "        [    0.0132],\n",
      "        [    0.0062],\n",
      "        [    0.1112],\n",
      "        [    0.0500],\n",
      "        [    0.0313],\n",
      "        [    0.0181],\n",
      "        [    0.0517],\n",
      "        [    0.0047],\n",
      "        [    0.0076],\n",
      "        [    0.0663],\n",
      "        [    0.0200],\n",
      "        [    0.0301],\n",
      "        [    0.0513],\n",
      "        [    0.0695],\n",
      "        [    0.0315],\n",
      "        [    0.0006],\n",
      "        [    0.0136],\n",
      "        [    0.0740],\n",
      "        [    0.0479],\n",
      "        [    0.0545],\n",
      "        [    0.0326],\n",
      "        [    0.0354],\n",
      "        [    0.0404],\n",
      "        [    0.0519],\n",
      "        [    0.0737],\n",
      "        [    0.0813],\n",
      "        [    0.0214],\n",
      "        [    0.0116],\n",
      "        [    0.0084],\n",
      "        [    0.0553],\n",
      "        [    0.0377],\n",
      "        [    0.0545],\n",
      "        [    0.0885],\n",
      "        [    0.0858],\n",
      "        [    0.0903],\n",
      "        [    0.0126],\n",
      "        [    0.0228],\n",
      "        [    0.0713],\n",
      "        [    0.0237],\n",
      "        [    0.0389],\n",
      "        [    0.0879],\n",
      "        [    0.0253],\n",
      "        [    0.1257],\n",
      "        [    0.0621],\n",
      "        [    0.0274],\n",
      "        [    0.0650],\n",
      "        [    0.1059],\n",
      "        [    0.0244],\n",
      "        [    0.0664],\n",
      "        [    0.0468],\n",
      "        [    0.1253],\n",
      "        [    0.0511],\n",
      "        [    0.0781],\n",
      "        [    0.0956],\n",
      "        [    0.0533],\n",
      "        [    0.0654],\n",
      "        [    0.1296],\n",
      "        [    0.1403],\n",
      "        [    0.1049],\n",
      "        [    0.1201],\n",
      "        [    0.1313],\n",
      "        [    0.1488],\n",
      "        [    0.1801],\n",
      "        [    0.1553],\n",
      "        [    0.1843],\n",
      "        [    0.1350],\n",
      "        [    0.2192],\n",
      "        [    0.0394],\n",
      "        [    0.3474],\n",
      "        [    0.3569],\n",
      "        [    0.3587],\n",
      "        [    0.3672],\n",
      "        [    0.3691],\n",
      "        [    0.3711],\n",
      "        [    0.3722],\n",
      "        [    0.3754],\n",
      "        [    0.3945],\n",
      "        [    0.3955],\n",
      "        [    0.4024],\n",
      "        [    0.4068],\n",
      "        [    0.4088],\n",
      "        [    0.4096],\n",
      "        [    0.4141],\n",
      "        [    0.4215],\n",
      "        [    0.4232],\n",
      "        [    0.4239],\n",
      "        [    0.4240],\n",
      "        [    0.4267],\n",
      "        [    0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0295],\n",
      "        [    0.0212],\n",
      "        [    0.0478],\n",
      "        [    0.0028],\n",
      "        [    0.0287],\n",
      "        [    0.0058],\n",
      "        [    0.0323],\n",
      "        [    0.0045],\n",
      "        [    0.0158],\n",
      "        [    0.0102],\n",
      "        [    0.0089],\n",
      "        [    0.0474],\n",
      "        [    0.0018],\n",
      "        [    0.0237],\n",
      "        [    0.0219],\n",
      "        [    0.0180],\n",
      "        [    0.0079],\n",
      "        [    0.0391],\n",
      "        [    0.0063],\n",
      "        [    0.0394],\n",
      "        [    0.0340],\n",
      "        [    0.0294],\n",
      "        [    0.0491],\n",
      "        [    0.0280],\n",
      "        [    0.0221],\n",
      "        [    0.0208],\n",
      "        [    0.0343],\n",
      "        [    0.0504],\n",
      "        [    0.0596],\n",
      "        [    0.0102],\n",
      "        [    0.0359],\n",
      "        [    0.0558],\n",
      "        [    0.1044],\n",
      "        [    0.0044],\n",
      "        [    0.0362],\n",
      "        [    0.0675],\n",
      "        [    0.0132],\n",
      "        [    0.0086],\n",
      "        [    0.0124],\n",
      "        [    0.0193],\n",
      "        [    0.0577],\n",
      "        [    0.0114],\n",
      "        [    0.0069],\n",
      "        [    0.0283],\n",
      "        [    0.0159],\n",
      "        [    0.0157],\n",
      "        [    0.0260],\n",
      "        [    0.0696],\n",
      "        [    0.0089],\n",
      "        [    0.0005],\n",
      "        [    0.0112],\n",
      "        [    0.0131],\n",
      "        [    0.0137],\n",
      "        [    0.0226],\n",
      "        [    0.0532],\n",
      "        [    0.0478],\n",
      "        [    0.0009],\n",
      "        [    0.0071],\n",
      "        [    0.0610],\n",
      "        [    0.0866],\n",
      "        [    0.0122],\n",
      "        [    0.0070],\n",
      "        [    0.1109],\n",
      "        [    0.0506],\n",
      "        [    0.0320],\n",
      "        [    0.0183],\n",
      "        [    0.0527],\n",
      "        [    0.0028],\n",
      "        [    0.0067],\n",
      "        [    0.0656],\n",
      "        [    0.0183],\n",
      "        [    0.0305],\n",
      "        [    0.0496],\n",
      "        [    0.0704],\n",
      "        [    0.0305],\n",
      "        [    0.0002],\n",
      "        [    0.0120],\n",
      "        [    0.0751],\n",
      "        [    0.0471],\n",
      "        [    0.0541],\n",
      "        [    0.0324],\n",
      "        [    0.0350],\n",
      "        [    0.0397],\n",
      "        [    0.0518],\n",
      "        [    0.0734],\n",
      "        [    0.0807],\n",
      "        [    0.0194],\n",
      "        [    0.0105],\n",
      "        [    0.0092],\n",
      "        [    0.0550],\n",
      "        [    0.0382],\n",
      "        [    0.0543],\n",
      "        [    0.0887],\n",
      "        [    0.0862],\n",
      "        [    0.0907],\n",
      "        [    0.0139],\n",
      "        [    0.0236],\n",
      "        [    0.0708],\n",
      "        [    0.0225],\n",
      "        [    0.0397],\n",
      "        [    0.0874],\n",
      "        [    0.0263],\n",
      "        [    0.1254],\n",
      "        [    0.0602],\n",
      "        [    0.0260],\n",
      "        [    0.0632],\n",
      "        [    0.1059],\n",
      "        [    0.0236],\n",
      "        [    0.0668],\n",
      "        [    0.0470],\n",
      "        [    0.1248],\n",
      "        [    0.0494],\n",
      "        [    0.0779],\n",
      "        [    0.0955],\n",
      "        [    0.0522],\n",
      "        [    0.0639],\n",
      "        [    0.1298],\n",
      "        [    0.1404],\n",
      "        [    0.1041],\n",
      "        [    0.1201],\n",
      "        [    0.1312],\n",
      "        [    0.1476],\n",
      "        [    0.1791],\n",
      "        [    0.1533],\n",
      "        [    0.1838],\n",
      "        [    0.1350],\n",
      "        [    0.2175],\n",
      "        [    0.0406],\n",
      "        [    0.3470],\n",
      "        [    0.3565],\n",
      "        [    0.3583],\n",
      "        [    0.3668],\n",
      "        [    0.3686],\n",
      "        [    0.3707],\n",
      "        [    0.3718],\n",
      "        [    0.3750],\n",
      "        [    0.3941],\n",
      "        [    0.3950],\n",
      "        [    0.4020],\n",
      "        [    0.4064],\n",
      "        [    0.4084],\n",
      "        [    0.4092],\n",
      "        [    0.4137],\n",
      "        [    0.4211],\n",
      "        [    0.4228],\n",
      "        [    0.4235],\n",
      "        [    0.4236],\n",
      "        [    0.4263],\n",
      "        [    0.4296]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 77.68016576766968\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 150\n",
      "剩餘X 資料 torch.Size([10, 18])\n",
      "剩餘Y 資料 torch.Size([10, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18589581549167633, 9)\n",
      "The second_loss value of k: (0.1873849332332611, 8)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.0463])\n",
      "目前模型的Data狀態 torch.Size([150, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5885],\n",
      "        [0.6281],\n",
      "        [0.2592],\n",
      "        [0.6620],\n",
      "        [0.5846],\n",
      "        [0.1744],\n",
      "        [0.6130],\n",
      "        [0.1966],\n",
      "        [0.4246],\n",
      "        [0.6488],\n",
      "        [0.6687],\n",
      "        [0.4398],\n",
      "        [0.6493],\n",
      "        [0.5848],\n",
      "        [0.6384],\n",
      "        [0.6235],\n",
      "        [0.6851],\n",
      "        [0.5655],\n",
      "        [0.6490],\n",
      "        [0.2285],\n",
      "        [0.5174],\n",
      "        [0.6066],\n",
      "        [0.3633],\n",
      "        [0.6174],\n",
      "        [0.4518],\n",
      "        [0.6156],\n",
      "        [0.5823],\n",
      "        [0.4215],\n",
      "        [0.3710],\n",
      "        [0.6005],\n",
      "        [0.3612],\n",
      "        [0.4088],\n",
      "        [0.2149],\n",
      "        [0.6455],\n",
      "        [0.5825],\n",
      "        [0.2402],\n",
      "        [0.4577],\n",
      "        [0.6696],\n",
      "        [0.6031],\n",
      "        [0.6546],\n",
      "        [0.6214],\n",
      "        [0.2222],\n",
      "        [0.6905],\n",
      "        [0.5569],\n",
      "        [0.5565],\n",
      "        [0.6230],\n",
      "        [0.2882],\n",
      "        [0.2286],\n",
      "        [0.6376],\n",
      "        [0.5361],\n",
      "        [0.5776],\n",
      "        [0.6085],\n",
      "        [0.6443],\n",
      "        [0.2143],\n",
      "        [0.5639],\n",
      "        [0.3188],\n",
      "        [0.6287],\n",
      "        [0.7021],\n",
      "        [0.4655],\n",
      "        [0.2161],\n",
      "        [0.6358],\n",
      "        [0.6369],\n",
      "        [0.0512],\n",
      "        [0.6288],\n",
      "        [0.5020],\n",
      "        [0.6150],\n",
      "        [0.4704],\n",
      "        [0.7078],\n",
      "        [0.6982],\n",
      "        [0.5448],\n",
      "        [0.2362],\n",
      "        [0.6124],\n",
      "        [0.2050],\n",
      "        [0.6017],\n",
      "        [0.5974],\n",
      "        [0.7031],\n",
      "        [0.7198],\n",
      "        [0.4578],\n",
      "        [0.3107],\n",
      "        [0.3046],\n",
      "        [0.3219],\n",
      "        [0.2191],\n",
      "        [0.3184],\n",
      "        [0.3068],\n",
      "        [0.5504],\n",
      "        [0.3009],\n",
      "        [0.6529],\n",
      "        [0.7265],\n",
      "        [0.6964],\n",
      "        [0.3084],\n",
      "        [0.6735],\n",
      "        [0.3102],\n",
      "        [0.6020],\n",
      "        [0.6104],\n",
      "        [0.5985],\n",
      "        [0.4974],\n",
      "        [0.6532],\n",
      "        [0.2516],\n",
      "        [0.6855],\n",
      "        [0.6320],\n",
      "        [0.2750],\n",
      "        [0.2646],\n",
      "        [0.3163],\n",
      "        [0.1783],\n",
      "        [0.5214],\n",
      "        [0.6775],\n",
      "        [0.2930],\n",
      "        [0.6931],\n",
      "        [0.6674],\n",
      "        [0.6884],\n",
      "        [0.2876],\n",
      "        [0.6708],\n",
      "        [0.3132],\n",
      "        [0.4219],\n",
      "        [0.3197],\n",
      "        [0.5208],\n",
      "        [0.4165],\n",
      "        [0.2961],\n",
      "        [0.3691],\n",
      "        [0.3320],\n",
      "        [0.3199],\n",
      "        [0.5713],\n",
      "        [0.5996],\n",
      "        [0.1804],\n",
      "        [0.4111],\n",
      "        [0.1350],\n",
      "        [0.6786],\n",
      "        [0.1569],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775],\n",
      "        [0.4775]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0295],\n",
      "        [    0.0212],\n",
      "        [    0.0478],\n",
      "        [    0.0028],\n",
      "        [    0.0287],\n",
      "        [    0.0058],\n",
      "        [    0.0323],\n",
      "        [    0.0045],\n",
      "        [    0.0158],\n",
      "        [    0.0102],\n",
      "        [    0.0089],\n",
      "        [    0.0474],\n",
      "        [    0.0018],\n",
      "        [    0.0237],\n",
      "        [    0.0219],\n",
      "        [    0.0180],\n",
      "        [    0.0079],\n",
      "        [    0.0391],\n",
      "        [    0.0063],\n",
      "        [    0.0394],\n",
      "        [    0.0340],\n",
      "        [    0.0294],\n",
      "        [    0.0491],\n",
      "        [    0.0280],\n",
      "        [    0.0221],\n",
      "        [    0.0208],\n",
      "        [    0.0343],\n",
      "        [    0.0504],\n",
      "        [    0.0596],\n",
      "        [    0.0102],\n",
      "        [    0.0359],\n",
      "        [    0.0558],\n",
      "        [    0.1044],\n",
      "        [    0.0044],\n",
      "        [    0.0362],\n",
      "        [    0.0675],\n",
      "        [    0.0132],\n",
      "        [    0.0086],\n",
      "        [    0.0124],\n",
      "        [    0.0193],\n",
      "        [    0.0577],\n",
      "        [    0.0114],\n",
      "        [    0.0069],\n",
      "        [    0.0283],\n",
      "        [    0.0159],\n",
      "        [    0.0157],\n",
      "        [    0.0260],\n",
      "        [    0.0696],\n",
      "        [    0.0089],\n",
      "        [    0.0005],\n",
      "        [    0.0112],\n",
      "        [    0.0131],\n",
      "        [    0.0137],\n",
      "        [    0.0226],\n",
      "        [    0.0532],\n",
      "        [    0.0478],\n",
      "        [    0.0009],\n",
      "        [    0.0071],\n",
      "        [    0.0610],\n",
      "        [    0.0866],\n",
      "        [    0.0122],\n",
      "        [    0.0070],\n",
      "        [    0.1109],\n",
      "        [    0.0506],\n",
      "        [    0.0320],\n",
      "        [    0.0183],\n",
      "        [    0.0527],\n",
      "        [    0.0028],\n",
      "        [    0.0067],\n",
      "        [    0.0656],\n",
      "        [    0.0183],\n",
      "        [    0.0305],\n",
      "        [    0.0496],\n",
      "        [    0.0704],\n",
      "        [    0.0305],\n",
      "        [    0.0002],\n",
      "        [    0.0120],\n",
      "        [    0.0751],\n",
      "        [    0.0471],\n",
      "        [    0.0541],\n",
      "        [    0.0324],\n",
      "        [    0.0350],\n",
      "        [    0.0397],\n",
      "        [    0.0518],\n",
      "        [    0.0734],\n",
      "        [    0.0807],\n",
      "        [    0.0194],\n",
      "        [    0.0105],\n",
      "        [    0.0092],\n",
      "        [    0.0550],\n",
      "        [    0.0382],\n",
      "        [    0.0543],\n",
      "        [    0.0887],\n",
      "        [    0.0862],\n",
      "        [    0.0907],\n",
      "        [    0.0139],\n",
      "        [    0.0236],\n",
      "        [    0.0708],\n",
      "        [    0.0225],\n",
      "        [    0.0397],\n",
      "        [    0.0874],\n",
      "        [    0.0263],\n",
      "        [    0.1254],\n",
      "        [    0.0602],\n",
      "        [    0.0260],\n",
      "        [    0.0632],\n",
      "        [    0.1059],\n",
      "        [    0.0236],\n",
      "        [    0.0668],\n",
      "        [    0.0470],\n",
      "        [    0.1248],\n",
      "        [    0.0494],\n",
      "        [    0.0779],\n",
      "        [    0.0955],\n",
      "        [    0.0522],\n",
      "        [    0.0639],\n",
      "        [    0.1298],\n",
      "        [    0.1404],\n",
      "        [    0.1041],\n",
      "        [    0.1201],\n",
      "        [    0.1312],\n",
      "        [    0.1476],\n",
      "        [    0.1791],\n",
      "        [    0.1533],\n",
      "        [    0.1838],\n",
      "        [    0.1350],\n",
      "        [    0.2175],\n",
      "        [    0.0406],\n",
      "        [    0.3470],\n",
      "        [    0.3565],\n",
      "        [    0.3583],\n",
      "        [    0.3668],\n",
      "        [    0.3686],\n",
      "        [    0.3707],\n",
      "        [    0.3718],\n",
      "        [    0.3750],\n",
      "        [    0.3941],\n",
      "        [    0.3950],\n",
      "        [    0.4020],\n",
      "        [    0.4064],\n",
      "        [    0.4084],\n",
      "        [    0.4092],\n",
      "        [    0.4137],\n",
      "        [    0.4211],\n",
      "        [    0.4228],\n",
      "        [    0.4235],\n",
      "        [    0.4236],\n",
      "        [    0.4263],\n",
      "        [    0.4296],\n",
      "        [    0.4312]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 204\n",
      "Number of shrink: 103\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0280],\n",
      "        [0.0172],\n",
      "        [0.0477],\n",
      "        [0.0029],\n",
      "        [0.0273],\n",
      "        [0.0103],\n",
      "        [0.0283],\n",
      "        [0.0008],\n",
      "        [0.0176],\n",
      "        [0.0101],\n",
      "        [0.0075],\n",
      "        [0.0506],\n",
      "        [0.0011],\n",
      "        [0.0223],\n",
      "        [0.0181],\n",
      "        [0.0196],\n",
      "        [0.0066],\n",
      "        [0.0365],\n",
      "        [0.0112],\n",
      "        [0.0364],\n",
      "        [0.0340],\n",
      "        [0.0314],\n",
      "        [0.0492],\n",
      "        [0.0291],\n",
      "        [0.0243],\n",
      "        [0.0223],\n",
      "        [0.0327],\n",
      "        [0.0516],\n",
      "        [0.0578],\n",
      "        [0.0118],\n",
      "        [0.0336],\n",
      "        [0.0576],\n",
      "        [0.1057],\n",
      "        [0.0045],\n",
      "        [0.0375],\n",
      "        [0.0682],\n",
      "        [0.0167],\n",
      "        [0.0082],\n",
      "        [0.0108],\n",
      "        [0.0180],\n",
      "        [0.0553],\n",
      "        [0.0105],\n",
      "        [0.0050],\n",
      "        [0.0271],\n",
      "        [0.0136],\n",
      "        [0.0146],\n",
      "        [0.0228],\n",
      "        [0.0684],\n",
      "        [0.0065],\n",
      "        [0.0031],\n",
      "        [0.0147],\n",
      "        [0.0145],\n",
      "        [0.0110],\n",
      "        [0.0224],\n",
      "        [0.0545],\n",
      "        [0.0443],\n",
      "        [0.0011],\n",
      "        [0.0043],\n",
      "        [0.0637],\n",
      "        [0.0897],\n",
      "        [0.0090],\n",
      "        [0.0099],\n",
      "        [0.1100],\n",
      "        [0.0522],\n",
      "        [0.0347],\n",
      "        [0.0194],\n",
      "        [0.0561],\n",
      "        [0.0023],\n",
      "        [0.0049],\n",
      "        [0.0635],\n",
      "        [0.0141],\n",
      "        [0.0323],\n",
      "        [0.0449],\n",
      "        [0.0725],\n",
      "        [0.0278],\n",
      "        [0.0022],\n",
      "        [0.0078],\n",
      "        [0.0789],\n",
      "        [0.0454],\n",
      "        [0.0536],\n",
      "        [0.0325],\n",
      "        [0.0335],\n",
      "        [0.0381],\n",
      "        [0.0517],\n",
      "        [0.0725],\n",
      "        [0.0787],\n",
      "        [0.0139],\n",
      "        [0.0075],\n",
      "        [0.0113],\n",
      "        [0.0547],\n",
      "        [0.0394],\n",
      "        [0.0542],\n",
      "        [0.0890],\n",
      "        [0.0874],\n",
      "        [0.0915],\n",
      "        [0.0175],\n",
      "        [0.0262],\n",
      "        [0.0689],\n",
      "        [0.0192],\n",
      "        [0.0426],\n",
      "        [0.0856],\n",
      "        [0.0299],\n",
      "        [0.1243],\n",
      "        [0.0549],\n",
      "        [0.0223],\n",
      "        [0.0582],\n",
      "        [0.1065],\n",
      "        [0.0209],\n",
      "        [0.0677],\n",
      "        [0.0475],\n",
      "        [0.1232],\n",
      "        [0.0447],\n",
      "        [0.0779],\n",
      "        [0.0945],\n",
      "        [0.0484],\n",
      "        [0.0596],\n",
      "        [0.1295],\n",
      "        [0.1412],\n",
      "        [0.1013],\n",
      "        [0.1206],\n",
      "        [0.1312],\n",
      "        [0.1443],\n",
      "        [0.1765],\n",
      "        [0.1477],\n",
      "        [0.1815],\n",
      "        [0.1348],\n",
      "        [0.2127],\n",
      "        [0.0437],\n",
      "        [0.3459],\n",
      "        [0.3553],\n",
      "        [0.3571],\n",
      "        [0.3657],\n",
      "        [0.3675],\n",
      "        [0.3696],\n",
      "        [0.3706],\n",
      "        [0.3738],\n",
      "        [0.3929],\n",
      "        [0.3939],\n",
      "        [0.4008],\n",
      "        [0.4052],\n",
      "        [0.4072],\n",
      "        [0.4081],\n",
      "        [0.4125],\n",
      "        [0.4200],\n",
      "        [0.4216],\n",
      "        [0.4224],\n",
      "        [0.4224],\n",
      "        [0.4251],\n",
      "        [0.4284],\n",
      "        [0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0306],\n",
      "        [    0.0191],\n",
      "        [    0.0460],\n",
      "        [    0.0057],\n",
      "        [    0.0299],\n",
      "        [    0.0102],\n",
      "        [    0.0299],\n",
      "        [    0.0010],\n",
      "        [    0.0156],\n",
      "        [    0.0072],\n",
      "        [    0.0099],\n",
      "        [    0.0491],\n",
      "        [    0.0037],\n",
      "        [    0.0248],\n",
      "        [    0.0201],\n",
      "        [    0.0173],\n",
      "        [    0.0090],\n",
      "        [    0.0388],\n",
      "        [    0.0095],\n",
      "        [    0.0370],\n",
      "        [    0.0314],\n",
      "        [    0.0293],\n",
      "        [    0.0514],\n",
      "        [    0.0268],\n",
      "        [    0.0223],\n",
      "        [    0.0200],\n",
      "        [    0.0353],\n",
      "        [    0.0494],\n",
      "        [    0.0594],\n",
      "        [    0.0095],\n",
      "        [    0.0351],\n",
      "        [    0.0556],\n",
      "        [    0.1042],\n",
      "        [    0.0074],\n",
      "        [    0.0353],\n",
      "        [    0.0666],\n",
      "        [    0.0152],\n",
      "        [    0.0108],\n",
      "        [    0.0134],\n",
      "        [    0.0205],\n",
      "        [    0.0575],\n",
      "        [    0.0087],\n",
      "        [    0.0073],\n",
      "        [    0.0244],\n",
      "        [    0.0158],\n",
      "        [    0.0174],\n",
      "        [    0.0238],\n",
      "        [    0.0695],\n",
      "        [    0.0088],\n",
      "        [    0.0011],\n",
      "        [    0.0130],\n",
      "        [    0.0120],\n",
      "        [    0.0132],\n",
      "        [    0.0241],\n",
      "        [    0.0523],\n",
      "        [    0.0453],\n",
      "        [    0.0014],\n",
      "        [    0.0064],\n",
      "        [    0.0620],\n",
      "        [    0.0886],\n",
      "        [    0.0111],\n",
      "        [    0.0076],\n",
      "        [    0.1089],\n",
      "        [    0.0499],\n",
      "        [    0.0328],\n",
      "        [    0.0169],\n",
      "        [    0.0545],\n",
      "        [    0.0008],\n",
      "        [    0.0070],\n",
      "        [    0.0605],\n",
      "        [    0.0143],\n",
      "        [    0.0298],\n",
      "        [    0.0426],\n",
      "        [    0.0705],\n",
      "        [    0.0297],\n",
      "        [    0.0001],\n",
      "        [    0.0095],\n",
      "        [    0.0774],\n",
      "        [    0.0430],\n",
      "        [    0.0516],\n",
      "        [    0.0305],\n",
      "        [    0.0348],\n",
      "        [    0.0357],\n",
      "        [    0.0498],\n",
      "        [    0.0697],\n",
      "        [    0.0801],\n",
      "        [    0.0152],\n",
      "        [    0.0096],\n",
      "        [    0.0092],\n",
      "        [    0.0527],\n",
      "        [    0.0370],\n",
      "        [    0.0522],\n",
      "        [    0.0865],\n",
      "        [    0.0850],\n",
      "        [    0.0891],\n",
      "        [    0.0160],\n",
      "        [    0.0239],\n",
      "        [    0.0702],\n",
      "        [    0.0211],\n",
      "        [    0.0404],\n",
      "        [    0.0869],\n",
      "        [    0.0288],\n",
      "        [    0.1260],\n",
      "        [    0.0539],\n",
      "        [    0.0238],\n",
      "        [    0.0597],\n",
      "        [    0.1047],\n",
      "        [    0.0177],\n",
      "        [    0.0652],\n",
      "        [    0.0449],\n",
      "        [    0.1245],\n",
      "        [    0.0463],\n",
      "        [    0.0760],\n",
      "        [    0.0968],\n",
      "        [    0.0495],\n",
      "        [    0.0609],\n",
      "        [    0.1318],\n",
      "        [    0.1395],\n",
      "        [    0.1027],\n",
      "        [    0.1188],\n",
      "        [    0.1294],\n",
      "        [    0.1459],\n",
      "        [    0.1785],\n",
      "        [    0.1474],\n",
      "        [    0.1832],\n",
      "        [    0.1355],\n",
      "        [    0.2143],\n",
      "        [    0.0440],\n",
      "        [    0.3455],\n",
      "        [    0.3549],\n",
      "        [    0.3567],\n",
      "        [    0.3653],\n",
      "        [    0.3671],\n",
      "        [    0.3692],\n",
      "        [    0.3702],\n",
      "        [    0.3734],\n",
      "        [    0.3925],\n",
      "        [    0.3935],\n",
      "        [    0.4004],\n",
      "        [    0.4048],\n",
      "        [    0.4068],\n",
      "        [    0.4077],\n",
      "        [    0.4121],\n",
      "        [    0.4196],\n",
      "        [    0.4212],\n",
      "        [    0.4220],\n",
      "        [    0.4220],\n",
      "        [    0.4247],\n",
      "        [    0.4280],\n",
      "        [    0.4296]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 78.57303071022034\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 151\n",
      "剩餘X 資料 torch.Size([9, 18])\n",
      "剩餘Y 資料 torch.Size([9, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18604019284248352, 8)\n",
      "The second_loss value of k: (0.18658123910427094, 7)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.0446])\n",
      "目前模型的Data狀態 torch.Size([151, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5896],\n",
      "        [0.6261],\n",
      "        [0.2611],\n",
      "        [0.6648],\n",
      "        [0.5857],\n",
      "        [0.1700],\n",
      "        [0.6106],\n",
      "        [0.1931],\n",
      "        [0.4248],\n",
      "        [0.6518],\n",
      "        [0.6696],\n",
      "        [0.4382],\n",
      "        [0.6511],\n",
      "        [0.5859],\n",
      "        [0.6367],\n",
      "        [0.6242],\n",
      "        [0.6862],\n",
      "        [0.5652],\n",
      "        [0.6458],\n",
      "        [0.2261],\n",
      "        [0.5200],\n",
      "        [0.6067],\n",
      "        [0.3656],\n",
      "        [0.6186],\n",
      "        [0.4516],\n",
      "        [0.6164],\n",
      "        [0.5833],\n",
      "        [0.4225],\n",
      "        [0.3709],\n",
      "        [0.6012],\n",
      "        [0.3604],\n",
      "        [0.4089],\n",
      "        [0.2150],\n",
      "        [0.6486],\n",
      "        [0.5834],\n",
      "        [0.2412],\n",
      "        [0.4557],\n",
      "        [0.6719],\n",
      "        [0.6041],\n",
      "        [0.6558],\n",
      "        [0.6212],\n",
      "        [0.2249],\n",
      "        [0.6908],\n",
      "        [0.5607],\n",
      "        [0.5564],\n",
      "        [0.6246],\n",
      "        [0.2860],\n",
      "        [0.2286],\n",
      "        [0.6374],\n",
      "        [0.5355],\n",
      "        [0.5758],\n",
      "        [0.6096],\n",
      "        [0.6437],\n",
      "        [0.2158],\n",
      "        [0.5648],\n",
      "        [0.3163],\n",
      "        [0.6292],\n",
      "        [0.7014],\n",
      "        [0.4645],\n",
      "        [0.2140],\n",
      "        [0.6347],\n",
      "        [0.6363],\n",
      "        [0.0532],\n",
      "        [0.6296],\n",
      "        [0.5012],\n",
      "        [0.6165],\n",
      "        [0.4685],\n",
      "        [0.7042],\n",
      "        [0.6984],\n",
      "        [0.5500],\n",
      "        [0.2321],\n",
      "        [0.6132],\n",
      "        [0.2120],\n",
      "        [0.6015],\n",
      "        [0.5966],\n",
      "        [0.7035],\n",
      "        [0.7172],\n",
      "        [0.4555],\n",
      "        [0.3149],\n",
      "        [0.3072],\n",
      "        [0.3238],\n",
      "        [0.2189],\n",
      "        [0.3223],\n",
      "        [0.3088],\n",
      "        [0.5541],\n",
      "        [0.3002],\n",
      "        [0.6487],\n",
      "        [0.7256],\n",
      "        [0.6965],\n",
      "        [0.3108],\n",
      "        [0.6748],\n",
      "        [0.3123],\n",
      "        [0.6042],\n",
      "        [0.6116],\n",
      "        [0.6001],\n",
      "        [0.4953],\n",
      "        [0.6528],\n",
      "        [0.2510],\n",
      "        [0.6842],\n",
      "        [0.6313],\n",
      "        [0.2745],\n",
      "        [0.2621],\n",
      "        [0.3168],\n",
      "        [0.1720],\n",
      "        [0.5191],\n",
      "        [0.6740],\n",
      "        [0.2942],\n",
      "        [0.6991],\n",
      "        [0.6690],\n",
      "        [0.6905],\n",
      "        [0.2873],\n",
      "        [0.6676],\n",
      "        [0.3151],\n",
      "        [0.4231],\n",
      "        [0.3169],\n",
      "        [0.5177],\n",
      "        [0.4186],\n",
      "        [0.2970],\n",
      "        [0.3677],\n",
      "        [0.3333],\n",
      "        [0.3217],\n",
      "        [0.5696],\n",
      "        [0.5989],\n",
      "        [0.1745],\n",
      "        [0.4106],\n",
      "        [0.1355],\n",
      "        [0.6754],\n",
      "        [0.1535],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759],\n",
      "        [0.4759]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0306],\n",
      "        [    0.0191],\n",
      "        [    0.0460],\n",
      "        [    0.0057],\n",
      "        [    0.0299],\n",
      "        [    0.0102],\n",
      "        [    0.0299],\n",
      "        [    0.0010],\n",
      "        [    0.0156],\n",
      "        [    0.0072],\n",
      "        [    0.0099],\n",
      "        [    0.0491],\n",
      "        [    0.0037],\n",
      "        [    0.0248],\n",
      "        [    0.0201],\n",
      "        [    0.0173],\n",
      "        [    0.0090],\n",
      "        [    0.0388],\n",
      "        [    0.0095],\n",
      "        [    0.0370],\n",
      "        [    0.0314],\n",
      "        [    0.0293],\n",
      "        [    0.0514],\n",
      "        [    0.0268],\n",
      "        [    0.0223],\n",
      "        [    0.0200],\n",
      "        [    0.0353],\n",
      "        [    0.0494],\n",
      "        [    0.0594],\n",
      "        [    0.0095],\n",
      "        [    0.0351],\n",
      "        [    0.0556],\n",
      "        [    0.1042],\n",
      "        [    0.0074],\n",
      "        [    0.0353],\n",
      "        [    0.0666],\n",
      "        [    0.0152],\n",
      "        [    0.0108],\n",
      "        [    0.0134],\n",
      "        [    0.0205],\n",
      "        [    0.0575],\n",
      "        [    0.0087],\n",
      "        [    0.0073],\n",
      "        [    0.0244],\n",
      "        [    0.0158],\n",
      "        [    0.0174],\n",
      "        [    0.0238],\n",
      "        [    0.0695],\n",
      "        [    0.0088],\n",
      "        [    0.0011],\n",
      "        [    0.0130],\n",
      "        [    0.0120],\n",
      "        [    0.0132],\n",
      "        [    0.0241],\n",
      "        [    0.0523],\n",
      "        [    0.0453],\n",
      "        [    0.0014],\n",
      "        [    0.0064],\n",
      "        [    0.0620],\n",
      "        [    0.0886],\n",
      "        [    0.0111],\n",
      "        [    0.0076],\n",
      "        [    0.1089],\n",
      "        [    0.0499],\n",
      "        [    0.0328],\n",
      "        [    0.0169],\n",
      "        [    0.0545],\n",
      "        [    0.0008],\n",
      "        [    0.0070],\n",
      "        [    0.0605],\n",
      "        [    0.0143],\n",
      "        [    0.0298],\n",
      "        [    0.0426],\n",
      "        [    0.0705],\n",
      "        [    0.0297],\n",
      "        [    0.0001],\n",
      "        [    0.0095],\n",
      "        [    0.0774],\n",
      "        [    0.0430],\n",
      "        [    0.0516],\n",
      "        [    0.0305],\n",
      "        [    0.0348],\n",
      "        [    0.0357],\n",
      "        [    0.0498],\n",
      "        [    0.0697],\n",
      "        [    0.0801],\n",
      "        [    0.0152],\n",
      "        [    0.0096],\n",
      "        [    0.0092],\n",
      "        [    0.0527],\n",
      "        [    0.0370],\n",
      "        [    0.0522],\n",
      "        [    0.0865],\n",
      "        [    0.0850],\n",
      "        [    0.0891],\n",
      "        [    0.0160],\n",
      "        [    0.0239],\n",
      "        [    0.0702],\n",
      "        [    0.0211],\n",
      "        [    0.0404],\n",
      "        [    0.0869],\n",
      "        [    0.0288],\n",
      "        [    0.1260],\n",
      "        [    0.0539],\n",
      "        [    0.0238],\n",
      "        [    0.0597],\n",
      "        [    0.1047],\n",
      "        [    0.0177],\n",
      "        [    0.0652],\n",
      "        [    0.0449],\n",
      "        [    0.1245],\n",
      "        [    0.0463],\n",
      "        [    0.0760],\n",
      "        [    0.0968],\n",
      "        [    0.0495],\n",
      "        [    0.0609],\n",
      "        [    0.1318],\n",
      "        [    0.1395],\n",
      "        [    0.1027],\n",
      "        [    0.1188],\n",
      "        [    0.1294],\n",
      "        [    0.1459],\n",
      "        [    0.1785],\n",
      "        [    0.1474],\n",
      "        [    0.1832],\n",
      "        [    0.1355],\n",
      "        [    0.2143],\n",
      "        [    0.0440],\n",
      "        [    0.3455],\n",
      "        [    0.3549],\n",
      "        [    0.3567],\n",
      "        [    0.3653],\n",
      "        [    0.3671],\n",
      "        [    0.3692],\n",
      "        [    0.3702],\n",
      "        [    0.3734],\n",
      "        [    0.3925],\n",
      "        [    0.3935],\n",
      "        [    0.4004],\n",
      "        [    0.4048],\n",
      "        [    0.4068],\n",
      "        [    0.4077],\n",
      "        [    0.4121],\n",
      "        [    0.4196],\n",
      "        [    0.4212],\n",
      "        [    0.4220],\n",
      "        [    0.4220],\n",
      "        [    0.4247],\n",
      "        [    0.4280],\n",
      "        [    0.4296],\n",
      "        [    0.4313]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 234\n",
      "Number of shrink: 118\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0285],\n",
      "        [0.0146],\n",
      "        [0.0466],\n",
      "        [0.0050],\n",
      "        [0.0279],\n",
      "        [0.0148],\n",
      "        [0.0255],\n",
      "        [0.0030],\n",
      "        [0.0178],\n",
      "        [0.0079],\n",
      "        [0.0078],\n",
      "        [0.0527],\n",
      "        [0.0023],\n",
      "        [0.0227],\n",
      "        [0.0159],\n",
      "        [0.0193],\n",
      "        [0.0071],\n",
      "        [0.0358],\n",
      "        [0.0147],\n",
      "        [0.0336],\n",
      "        [0.0322],\n",
      "        [0.0318],\n",
      "        [0.0510],\n",
      "        [0.0283],\n",
      "        [0.0249],\n",
      "        [0.0218],\n",
      "        [0.0331],\n",
      "        [0.0510],\n",
      "        [0.0573],\n",
      "        [0.0117],\n",
      "        [0.0325],\n",
      "        [0.0578],\n",
      "        [0.1059],\n",
      "        [0.0069],\n",
      "        [0.0372],\n",
      "        [0.0678],\n",
      "        [0.0192],\n",
      "        [0.0097],\n",
      "        [0.0111],\n",
      "        [0.0187],\n",
      "        [0.0547],\n",
      "        [0.0084],\n",
      "        [0.0047],\n",
      "        [0.0243],\n",
      "        [0.0131],\n",
      "        [0.0155],\n",
      "        [0.0204],\n",
      "        [0.0678],\n",
      "        [0.0057],\n",
      "        [0.0041],\n",
      "        [0.0168],\n",
      "        [0.0140],\n",
      "        [0.0100],\n",
      "        [0.0233],\n",
      "        [0.0542],\n",
      "        [0.0417],\n",
      "        [0.0012],\n",
      "        [0.0029],\n",
      "        [0.0650],\n",
      "        [0.0917],\n",
      "        [0.0075],\n",
      "        [0.0111],\n",
      "        [0.1088],\n",
      "        [0.0519],\n",
      "        [0.0359],\n",
      "        [0.0186],\n",
      "        [0.0583],\n",
      "        [0.0064],\n",
      "        [0.0045],\n",
      "        [0.0595],\n",
      "        [0.0097],\n",
      "        [0.0320],\n",
      "        [0.0391],\n",
      "        [0.0730],\n",
      "        [0.0266],\n",
      "        [0.0025],\n",
      "        [0.0048],\n",
      "        [0.0815],\n",
      "        [0.0421],\n",
      "        [0.0517],\n",
      "        [0.0311],\n",
      "        [0.0329],\n",
      "        [0.0350],\n",
      "        [0.0503],\n",
      "        [0.0697],\n",
      "        [0.0777],\n",
      "        [0.0096],\n",
      "        [0.0061],\n",
      "        [0.0117],\n",
      "        [0.0529],\n",
      "        [0.0387],\n",
      "        [0.0527],\n",
      "        [0.0874],\n",
      "        [0.0867],\n",
      "        [0.0904],\n",
      "        [0.0199],\n",
      "        [0.0271],\n",
      "        [0.0679],\n",
      "        [0.0174],\n",
      "        [0.0438],\n",
      "        [0.0847],\n",
      "        [0.0324],\n",
      "        [0.1244],\n",
      "        [0.0494],\n",
      "        [0.0199],\n",
      "        [0.0545],\n",
      "        [0.1059],\n",
      "        [0.0159],\n",
      "        [0.0667],\n",
      "        [0.0460],\n",
      "        [0.1224],\n",
      "        [0.0414],\n",
      "        [0.0765],\n",
      "        [0.0953],\n",
      "        [0.0455],\n",
      "        [0.0565],\n",
      "        [0.1310],\n",
      "        [0.1408],\n",
      "        [0.0996],\n",
      "        [0.1198],\n",
      "        [0.1299],\n",
      "        [0.1423],\n",
      "        [0.1754],\n",
      "        [0.1417],\n",
      "        [0.1806],\n",
      "        [0.1344],\n",
      "        [0.2093],\n",
      "        [0.0464],\n",
      "        [0.3441],\n",
      "        [0.3536],\n",
      "        [0.3554],\n",
      "        [0.3640],\n",
      "        [0.3658],\n",
      "        [0.3678],\n",
      "        [0.3689],\n",
      "        [0.3721],\n",
      "        [0.3912],\n",
      "        [0.3922],\n",
      "        [0.3991],\n",
      "        [0.4035],\n",
      "        [0.4055],\n",
      "        [0.4063],\n",
      "        [0.4108],\n",
      "        [0.4182],\n",
      "        [0.4199],\n",
      "        [0.4206],\n",
      "        [0.4207],\n",
      "        [0.4234],\n",
      "        [0.4267],\n",
      "        [0.4283],\n",
      "        [0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0291],\n",
      "        [0.0146],\n",
      "        [0.0461],\n",
      "        [0.0057],\n",
      "        [0.0284],\n",
      "        [0.0156],\n",
      "        [0.0255],\n",
      "        [0.0037],\n",
      "        [0.0174],\n",
      "        [0.0072],\n",
      "        [0.0082],\n",
      "        [0.0527],\n",
      "        [0.0029],\n",
      "        [0.0233],\n",
      "        [0.0160],\n",
      "        [0.0189],\n",
      "        [0.0075],\n",
      "        [0.0361],\n",
      "        [0.0150],\n",
      "        [0.0330],\n",
      "        [0.0315],\n",
      "        [0.0314],\n",
      "        [0.0516],\n",
      "        [0.0278],\n",
      "        [0.0247],\n",
      "        [0.0213],\n",
      "        [0.0336],\n",
      "        [0.0504],\n",
      "        [0.0576],\n",
      "        [0.0111],\n",
      "        [0.0327],\n",
      "        [0.0574],\n",
      "        [0.1056],\n",
      "        [0.0077],\n",
      "        [0.0367],\n",
      "        [0.0674],\n",
      "        [0.0192],\n",
      "        [0.0104],\n",
      "        [0.0117],\n",
      "        [0.0191],\n",
      "        [0.0551],\n",
      "        [0.0078],\n",
      "        [0.0050],\n",
      "        [0.0234],\n",
      "        [0.0136],\n",
      "        [0.0162],\n",
      "        [0.0202],\n",
      "        [0.0677],\n",
      "        [0.0061],\n",
      "        [0.0038],\n",
      "        [0.0169],\n",
      "        [0.0133],\n",
      "        [0.0102],\n",
      "        [0.0236],\n",
      "        [0.0538],\n",
      "        [0.0415],\n",
      "        [0.0007],\n",
      "        [0.0029],\n",
      "        [0.0649],\n",
      "        [0.0917],\n",
      "        [0.0077],\n",
      "        [0.0108],\n",
      "        [0.1087],\n",
      "        [0.0516],\n",
      "        [0.0356],\n",
      "        [0.0180],\n",
      "        [0.0583],\n",
      "        [0.0068],\n",
      "        [0.0048],\n",
      "        [0.0584],\n",
      "        [0.0088],\n",
      "        [0.0315],\n",
      "        [0.0381],\n",
      "        [0.0728],\n",
      "        [0.0267],\n",
      "        [0.0021],\n",
      "        [0.0046],\n",
      "        [0.0816],\n",
      "        [0.0413],\n",
      "        [0.0511],\n",
      "        [0.0306],\n",
      "        [0.0329],\n",
      "        [0.0341],\n",
      "        [0.0498],\n",
      "        [0.0687],\n",
      "        [0.0777],\n",
      "        [0.0093],\n",
      "        [0.0062],\n",
      "        [0.0114],\n",
      "        [0.0522],\n",
      "        [0.0381],\n",
      "        [0.0522],\n",
      "        [0.0868],\n",
      "        [0.0863],\n",
      "        [0.0899],\n",
      "        [0.0200],\n",
      "        [0.0267],\n",
      "        [0.0679],\n",
      "        [0.0175],\n",
      "        [0.0435],\n",
      "        [0.0847],\n",
      "        [0.0326],\n",
      "        [0.1247],\n",
      "        [0.0484],\n",
      "        [0.0198],\n",
      "        [0.0543],\n",
      "        [0.1055],\n",
      "        [0.0147],\n",
      "        [0.0661],\n",
      "        [0.0453],\n",
      "        [0.1225],\n",
      "        [0.0413],\n",
      "        [0.0760],\n",
      "        [0.0959],\n",
      "        [0.0453],\n",
      "        [0.0562],\n",
      "        [0.1317],\n",
      "        [0.1405],\n",
      "        [0.0996],\n",
      "        [0.1193],\n",
      "        [0.1294],\n",
      "        [0.1422],\n",
      "        [0.1755],\n",
      "        [0.1405],\n",
      "        [0.1807],\n",
      "        [0.1342],\n",
      "        [0.2091],\n",
      "        [0.0469],\n",
      "        [0.3437],\n",
      "        [0.3532],\n",
      "        [0.3550],\n",
      "        [0.3635],\n",
      "        [0.3653],\n",
      "        [0.3674],\n",
      "        [0.3685],\n",
      "        [0.3717],\n",
      "        [0.3908],\n",
      "        [0.3917],\n",
      "        [0.3987],\n",
      "        [0.4031],\n",
      "        [0.4051],\n",
      "        [0.4059],\n",
      "        [0.4104],\n",
      "        [0.4178],\n",
      "        [0.4195],\n",
      "        [0.4202],\n",
      "        [0.4203],\n",
      "        [0.4230],\n",
      "        [0.4263],\n",
      "        [0.4279],\n",
      "        [0.4296]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 79.68488049507141\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 152\n",
      "剩餘X 資料 torch.Size([8, 18])\n",
      "剩餘Y 資料 torch.Size([8, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18507809937000275, 7)\n",
      "The second_loss value of k: (0.18642862141132355, 5)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引7，y= tensor([0.0440])\n",
      "目前模型的Data狀態 torch.Size([152, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5880],\n",
      "        [0.6216],\n",
      "        [0.2610],\n",
      "        [0.6649],\n",
      "        [0.5843],\n",
      "        [0.1646],\n",
      "        [0.6062],\n",
      "        [0.1884],\n",
      "        [0.4231],\n",
      "        [0.6518],\n",
      "        [0.6679],\n",
      "        [0.4345],\n",
      "        [0.6503],\n",
      "        [0.5844],\n",
      "        [0.6326],\n",
      "        [0.6226],\n",
      "        [0.6847],\n",
      "        [0.5626],\n",
      "        [0.6403],\n",
      "        [0.2222],\n",
      "        [0.5199],\n",
      "        [0.6046],\n",
      "        [0.3658],\n",
      "        [0.6176],\n",
      "        [0.4493],\n",
      "        [0.6151],\n",
      "        [0.5816],\n",
      "        [0.4215],\n",
      "        [0.3690],\n",
      "        [0.5996],\n",
      "        [0.3579],\n",
      "        [0.4071],\n",
      "        [0.2137],\n",
      "        [0.6489],\n",
      "        [0.5820],\n",
      "        [0.2404],\n",
      "        [0.4517],\n",
      "        [0.6715],\n",
      "        [0.6023],\n",
      "        [0.6545],\n",
      "        [0.6189],\n",
      "        [0.2258],\n",
      "        [0.6885],\n",
      "        [0.5617],\n",
      "        [0.5542],\n",
      "        [0.6234],\n",
      "        [0.2824],\n",
      "        [0.2268],\n",
      "        [0.6347],\n",
      "        [0.5328],\n",
      "        [0.5720],\n",
      "        [0.6083],\n",
      "        [0.6407],\n",
      "        [0.2153],\n",
      "        [0.5633],\n",
      "        [0.3124],\n",
      "        [0.6271],\n",
      "        [0.6979],\n",
      "        [0.4617],\n",
      "        [0.2110],\n",
      "        [0.6312],\n",
      "        [0.6331],\n",
      "        [0.0534],\n",
      "        [0.6278],\n",
      "        [0.4984],\n",
      "        [0.6153],\n",
      "        [0.4648],\n",
      "        [0.6982],\n",
      "        [0.6962],\n",
      "        [0.5521],\n",
      "        [0.2266],\n",
      "        [0.6115],\n",
      "        [0.2165],\n",
      "        [0.5993],\n",
      "        [0.5936],\n",
      "        [0.7012],\n",
      "        [0.7124],\n",
      "        [0.4514],\n",
      "        [0.3166],\n",
      "        [0.3076],\n",
      "        [0.3237],\n",
      "        [0.2170],\n",
      "        [0.3239],\n",
      "        [0.3087],\n",
      "        [0.5551],\n",
      "        [0.2979],\n",
      "        [0.6428],\n",
      "        [0.7223],\n",
      "        [0.6942],\n",
      "        [0.3112],\n",
      "        [0.6736],\n",
      "        [0.3123],\n",
      "        [0.6039],\n",
      "        [0.6103],\n",
      "        [0.5994],\n",
      "        [0.4913],\n",
      "        [0.6500],\n",
      "        [0.2487],\n",
      "        [0.6805],\n",
      "        [0.6282],\n",
      "        [0.2724],\n",
      "        [0.2584],\n",
      "        [0.3156],\n",
      "        [0.1665],\n",
      "        [0.5151],\n",
      "        [0.6687],\n",
      "        [0.2935],\n",
      "        [0.7021],\n",
      "        [0.6681],\n",
      "        [0.6901],\n",
      "        [0.2852],\n",
      "        [0.6627],\n",
      "        [0.3151],\n",
      "        [0.4223],\n",
      "        [0.3127],\n",
      "        [0.5131],\n",
      "        [0.4184],\n",
      "        [0.2960],\n",
      "        [0.3645],\n",
      "        [0.3328],\n",
      "        [0.3216],\n",
      "        [0.5659],\n",
      "        [0.5960],\n",
      "        [0.1676],\n",
      "        [0.4081],\n",
      "        [0.1342],\n",
      "        [0.6703],\n",
      "        [0.1506],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742],\n",
      "        [0.4742]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0291],\n",
      "        [0.0146],\n",
      "        [0.0461],\n",
      "        [0.0057],\n",
      "        [0.0284],\n",
      "        [0.0156],\n",
      "        [0.0255],\n",
      "        [0.0037],\n",
      "        [0.0174],\n",
      "        [0.0072],\n",
      "        [0.0082],\n",
      "        [0.0527],\n",
      "        [0.0029],\n",
      "        [0.0233],\n",
      "        [0.0160],\n",
      "        [0.0189],\n",
      "        [0.0075],\n",
      "        [0.0361],\n",
      "        [0.0150],\n",
      "        [0.0330],\n",
      "        [0.0315],\n",
      "        [0.0314],\n",
      "        [0.0516],\n",
      "        [0.0278],\n",
      "        [0.0247],\n",
      "        [0.0213],\n",
      "        [0.0336],\n",
      "        [0.0504],\n",
      "        [0.0576],\n",
      "        [0.0111],\n",
      "        [0.0327],\n",
      "        [0.0574],\n",
      "        [0.1056],\n",
      "        [0.0077],\n",
      "        [0.0367],\n",
      "        [0.0674],\n",
      "        [0.0192],\n",
      "        [0.0104],\n",
      "        [0.0117],\n",
      "        [0.0191],\n",
      "        [0.0551],\n",
      "        [0.0078],\n",
      "        [0.0050],\n",
      "        [0.0234],\n",
      "        [0.0136],\n",
      "        [0.0162],\n",
      "        [0.0202],\n",
      "        [0.0677],\n",
      "        [0.0061],\n",
      "        [0.0038],\n",
      "        [0.0169],\n",
      "        [0.0133],\n",
      "        [0.0102],\n",
      "        [0.0236],\n",
      "        [0.0538],\n",
      "        [0.0415],\n",
      "        [0.0007],\n",
      "        [0.0029],\n",
      "        [0.0649],\n",
      "        [0.0917],\n",
      "        [0.0077],\n",
      "        [0.0108],\n",
      "        [0.1087],\n",
      "        [0.0516],\n",
      "        [0.0356],\n",
      "        [0.0180],\n",
      "        [0.0583],\n",
      "        [0.0068],\n",
      "        [0.0048],\n",
      "        [0.0584],\n",
      "        [0.0088],\n",
      "        [0.0315],\n",
      "        [0.0381],\n",
      "        [0.0728],\n",
      "        [0.0267],\n",
      "        [0.0021],\n",
      "        [0.0046],\n",
      "        [0.0816],\n",
      "        [0.0413],\n",
      "        [0.0511],\n",
      "        [0.0306],\n",
      "        [0.0329],\n",
      "        [0.0341],\n",
      "        [0.0498],\n",
      "        [0.0687],\n",
      "        [0.0777],\n",
      "        [0.0093],\n",
      "        [0.0062],\n",
      "        [0.0114],\n",
      "        [0.0522],\n",
      "        [0.0381],\n",
      "        [0.0522],\n",
      "        [0.0868],\n",
      "        [0.0863],\n",
      "        [0.0899],\n",
      "        [0.0200],\n",
      "        [0.0267],\n",
      "        [0.0679],\n",
      "        [0.0175],\n",
      "        [0.0435],\n",
      "        [0.0847],\n",
      "        [0.0326],\n",
      "        [0.1247],\n",
      "        [0.0484],\n",
      "        [0.0198],\n",
      "        [0.0543],\n",
      "        [0.1055],\n",
      "        [0.0147],\n",
      "        [0.0661],\n",
      "        [0.0453],\n",
      "        [0.1225],\n",
      "        [0.0413],\n",
      "        [0.0760],\n",
      "        [0.0959],\n",
      "        [0.0453],\n",
      "        [0.0562],\n",
      "        [0.1317],\n",
      "        [0.1405],\n",
      "        [0.0996],\n",
      "        [0.1193],\n",
      "        [0.1294],\n",
      "        [0.1422],\n",
      "        [0.1755],\n",
      "        [0.1405],\n",
      "        [0.1807],\n",
      "        [0.1342],\n",
      "        [0.2091],\n",
      "        [0.0469],\n",
      "        [0.3437],\n",
      "        [0.3532],\n",
      "        [0.3550],\n",
      "        [0.3635],\n",
      "        [0.3653],\n",
      "        [0.3674],\n",
      "        [0.3685],\n",
      "        [0.3717],\n",
      "        [0.3908],\n",
      "        [0.3917],\n",
      "        [0.3987],\n",
      "        [0.4031],\n",
      "        [0.4051],\n",
      "        [0.4059],\n",
      "        [0.4104],\n",
      "        [0.4178],\n",
      "        [0.4195],\n",
      "        [0.4202],\n",
      "        [0.4203],\n",
      "        [0.4230],\n",
      "        [0.4263],\n",
      "        [0.4279],\n",
      "        [0.4296],\n",
      "        [0.4302]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 38\n",
      "Number of shrink: 18\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0314],\n",
      "        [    0.0166],\n",
      "        [    0.0444],\n",
      "        [    0.0082],\n",
      "        [    0.0308],\n",
      "        [    0.0148],\n",
      "        [    0.0273],\n",
      "        [    0.0028],\n",
      "        [    0.0155],\n",
      "        [    0.0047],\n",
      "        [    0.0104],\n",
      "        [    0.0511],\n",
      "        [    0.0052],\n",
      "        [    0.0256],\n",
      "        [    0.0181],\n",
      "        [    0.0169],\n",
      "        [    0.0098],\n",
      "        [    0.0382],\n",
      "        [    0.0132],\n",
      "        [    0.0342],\n",
      "        [    0.0291],\n",
      "        [    0.0295],\n",
      "        [    0.0535],\n",
      "        [    0.0257],\n",
      "        [    0.0228],\n",
      "        [    0.0192],\n",
      "        [    0.0360],\n",
      "        [    0.0485],\n",
      "        [    0.0593],\n",
      "        [    0.0090],\n",
      "        [    0.0342],\n",
      "        [    0.0556],\n",
      "        [    0.1040],\n",
      "        [    0.0102],\n",
      "        [    0.0346],\n",
      "        [    0.0658],\n",
      "        [    0.0176],\n",
      "        [    0.0128],\n",
      "        [    0.0140],\n",
      "        [    0.0214],\n",
      "        [    0.0572],\n",
      "        [    0.0061],\n",
      "        [    0.0071],\n",
      "        [    0.0210],\n",
      "        [    0.0156],\n",
      "        [    0.0186],\n",
      "        [    0.0215],\n",
      "        [    0.0691],\n",
      "        [    0.0082],\n",
      "        [    0.0019],\n",
      "        [    0.0152],\n",
      "        [    0.0109],\n",
      "        [    0.0123],\n",
      "        [    0.0253],\n",
      "        [    0.0517],\n",
      "        [    0.0428],\n",
      "        [    0.0016],\n",
      "        [    0.0049],\n",
      "        [    0.0630],\n",
      "        [    0.0903],\n",
      "        [    0.0098],\n",
      "        [    0.0086],\n",
      "        [    0.1076],\n",
      "        [    0.0495],\n",
      "        [    0.0337],\n",
      "        [    0.0157],\n",
      "        [    0.0566],\n",
      "        [    0.0051],\n",
      "        [    0.0068],\n",
      "        [    0.0558],\n",
      "        [    0.0096],\n",
      "        [    0.0291],\n",
      "        [    0.0364],\n",
      "        [    0.0709],\n",
      "        [    0.0286],\n",
      "        [    0.0000],\n",
      "        [    0.0064],\n",
      "        [    0.0800],\n",
      "        [    0.0392],\n",
      "        [    0.0492],\n",
      "        [    0.0288],\n",
      "        [    0.0343],\n",
      "        [    0.0321],\n",
      "        [    0.0480],\n",
      "        [    0.0663],\n",
      "        [    0.0792],\n",
      "        [    0.0109],\n",
      "        [    0.0082],\n",
      "        [    0.0094],\n",
      "        [    0.0503],\n",
      "        [    0.0359],\n",
      "        [    0.0503],\n",
      "        [    0.0846],\n",
      "        [    0.0841],\n",
      "        [    0.0878],\n",
      "        [    0.0184],\n",
      "        [    0.0245],\n",
      "        [    0.0693],\n",
      "        [    0.0194],\n",
      "        [    0.0413],\n",
      "        [    0.0862],\n",
      "        [    0.0312],\n",
      "        [    0.1264],\n",
      "        [    0.0483],\n",
      "        [    0.0214],\n",
      "        [    0.0560],\n",
      "        [    0.1038],\n",
      "        [    0.0121],\n",
      "        [    0.0639],\n",
      "        [    0.0431],\n",
      "        [    0.1239],\n",
      "        [    0.0431],\n",
      "        [    0.0741],\n",
      "        [    0.0979],\n",
      "        [    0.0467],\n",
      "        [    0.0577],\n",
      "        [    0.1337],\n",
      "        [    0.1388],\n",
      "        [    0.1011],\n",
      "        [    0.1176],\n",
      "        [    0.1276],\n",
      "        [    0.1439],\n",
      "        [    0.1774],\n",
      "        [    0.1411],\n",
      "        [    0.1824],\n",
      "        [    0.1350],\n",
      "        [    0.2109],\n",
      "        [    0.0466],\n",
      "        [    0.3435],\n",
      "        [    0.3530],\n",
      "        [    0.3548],\n",
      "        [    0.3633],\n",
      "        [    0.3651],\n",
      "        [    0.3672],\n",
      "        [    0.3683],\n",
      "        [    0.3715],\n",
      "        [    0.3906],\n",
      "        [    0.3915],\n",
      "        [    0.3985],\n",
      "        [    0.4029],\n",
      "        [    0.4048],\n",
      "        [    0.4057],\n",
      "        [    0.4102],\n",
      "        [    0.4176],\n",
      "        [    0.4193],\n",
      "        [    0.4200],\n",
      "        [    0.4200],\n",
      "        [    0.4228],\n",
      "        [    0.4261],\n",
      "        [    0.4276],\n",
      "        [    0.4294],\n",
      "        [    0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0321],\n",
      "        [0.0168],\n",
      "        [0.0440],\n",
      "        [0.0091],\n",
      "        [0.0315],\n",
      "        [0.0153],\n",
      "        [0.0275],\n",
      "        [0.0033],\n",
      "        [0.0149],\n",
      "        [0.0037],\n",
      "        [0.0111],\n",
      "        [0.0509],\n",
      "        [0.0061],\n",
      "        [0.0263],\n",
      "        [0.0185],\n",
      "        [0.0162],\n",
      "        [0.0105],\n",
      "        [0.0388],\n",
      "        [0.0129],\n",
      "        [0.0338],\n",
      "        [0.0283],\n",
      "        [0.0288],\n",
      "        [0.0542],\n",
      "        [0.0250],\n",
      "        [0.0224],\n",
      "        [0.0185],\n",
      "        [0.0367],\n",
      "        [0.0478],\n",
      "        [0.0596],\n",
      "        [0.0084],\n",
      "        [0.0345],\n",
      "        [0.0551],\n",
      "        [0.1037],\n",
      "        [0.0112],\n",
      "        [0.0340],\n",
      "        [0.0655],\n",
      "        [0.0175],\n",
      "        [0.0137],\n",
      "        [0.0148],\n",
      "        [0.0222],\n",
      "        [0.0577],\n",
      "        [0.0055],\n",
      "        [0.0078],\n",
      "        [0.0202],\n",
      "        [0.0162],\n",
      "        [0.0194],\n",
      "        [0.0215],\n",
      "        [0.0692],\n",
      "        [0.0087],\n",
      "        [0.0014],\n",
      "        [0.0148],\n",
      "        [0.0102],\n",
      "        [0.0127],\n",
      "        [0.0257],\n",
      "        [0.0512],\n",
      "        [0.0428],\n",
      "        [0.0023],\n",
      "        [0.0054],\n",
      "        [0.0626],\n",
      "        [0.0902],\n",
      "        [0.0104],\n",
      "        [0.0080],\n",
      "        [0.1074],\n",
      "        [0.0488],\n",
      "        [0.0333],\n",
      "        [0.0149],\n",
      "        [0.0563],\n",
      "        [0.0051],\n",
      "        [0.0074],\n",
      "        [0.0548],\n",
      "        [0.0090],\n",
      "        [0.0283],\n",
      "        [0.0355],\n",
      "        [0.0703],\n",
      "        [0.0291],\n",
      "        [0.0006],\n",
      "        [0.0067],\n",
      "        [0.0798],\n",
      "        [0.0384],\n",
      "        [0.0486],\n",
      "        [0.0283],\n",
      "        [0.0345],\n",
      "        [0.0313],\n",
      "        [0.0474],\n",
      "        [0.0654],\n",
      "        [0.0794],\n",
      "        [0.0110],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0497],\n",
      "        [0.0351],\n",
      "        [0.0498],\n",
      "        [0.0837],\n",
      "        [0.0834],\n",
      "        [0.0870],\n",
      "        [0.0182],\n",
      "        [0.0240],\n",
      "        [0.0695],\n",
      "        [0.0198],\n",
      "        [0.0409],\n",
      "        [0.0864],\n",
      "        [0.0311],\n",
      "        [0.1267],\n",
      "        [0.0476],\n",
      "        [0.0216],\n",
      "        [0.0563],\n",
      "        [0.1033],\n",
      "        [0.0108],\n",
      "        [0.0631],\n",
      "        [0.0422],\n",
      "        [0.1241],\n",
      "        [0.0434],\n",
      "        [0.0736],\n",
      "        [0.0986],\n",
      "        [0.0467],\n",
      "        [0.0578],\n",
      "        [0.1344],\n",
      "        [0.1384],\n",
      "        [0.1014],\n",
      "        [0.1171],\n",
      "        [0.1271],\n",
      "        [0.1442],\n",
      "        [0.1779],\n",
      "        [0.1403],\n",
      "        [0.1828],\n",
      "        [0.1349],\n",
      "        [0.2111],\n",
      "        [0.0470],\n",
      "        [0.3431],\n",
      "        [0.3526],\n",
      "        [0.3544],\n",
      "        [0.3629],\n",
      "        [0.3647],\n",
      "        [0.3668],\n",
      "        [0.3678],\n",
      "        [0.3710],\n",
      "        [0.3901],\n",
      "        [0.3911],\n",
      "        [0.3980],\n",
      "        [0.4024],\n",
      "        [0.4044],\n",
      "        [0.4053],\n",
      "        [0.4097],\n",
      "        [0.4172],\n",
      "        [0.4188],\n",
      "        [0.4196],\n",
      "        [0.4196],\n",
      "        [0.4223],\n",
      "        [0.4256],\n",
      "        [0.4272],\n",
      "        [0.4289],\n",
      "        [0.4296]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 80.10770797729492\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 153\n",
      "剩餘X 資料 torch.Size([7, 18])\n",
      "剩餘Y 資料 torch.Size([7, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.18587127327919006, 5)\n",
      "The second_loss value of k: (0.18831098079681396, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.0424])\n",
      "目前模型的Data狀態 torch.Size([153, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.5911],\n",
      "        [0.6238],\n",
      "        [0.2631],\n",
      "        [0.6683],\n",
      "        [0.5873],\n",
      "        [0.1649],\n",
      "        [0.6082],\n",
      "        [0.1888],\n",
      "        [0.4255],\n",
      "        [0.6553],\n",
      "        [0.6709],\n",
      "        [0.4363],\n",
      "        [0.6535],\n",
      "        [0.5873],\n",
      "        [0.6351],\n",
      "        [0.6253],\n",
      "        [0.6877],\n",
      "        [0.5652],\n",
      "        [0.6424],\n",
      "        [0.2230],\n",
      "        [0.5231],\n",
      "        [0.6071],\n",
      "        [0.3683],\n",
      "        [0.6204],\n",
      "        [0.4516],\n",
      "        [0.6179],\n",
      "        [0.5846],\n",
      "        [0.4241],\n",
      "        [0.3710],\n",
      "        [0.6023],\n",
      "        [0.3598],\n",
      "        [0.4095],\n",
      "        [0.2155],\n",
      "        [0.6524],\n",
      "        [0.5847],\n",
      "        [0.2423],\n",
      "        [0.4535],\n",
      "        [0.6747],\n",
      "        [0.6054],\n",
      "        [0.6575],\n",
      "        [0.6214],\n",
      "        [0.2281],\n",
      "        [0.6913],\n",
      "        [0.5649],\n",
      "        [0.5568],\n",
      "        [0.6267],\n",
      "        [0.2837],\n",
      "        [0.2283],\n",
      "        [0.6373],\n",
      "        [0.5352],\n",
      "        [0.5740],\n",
      "        [0.6114],\n",
      "        [0.6433],\n",
      "        [0.2174],\n",
      "        [0.5659],\n",
      "        [0.3138],\n",
      "        [0.6301],\n",
      "        [0.7004],\n",
      "        [0.4639],\n",
      "        [0.2125],\n",
      "        [0.6339],\n",
      "        [0.6359],\n",
      "        [0.0546],\n",
      "        [0.6306],\n",
      "        [0.5007],\n",
      "        [0.6184],\n",
      "        [0.4667],\n",
      "        [0.7000],\n",
      "        [0.6988],\n",
      "        [0.5557],\n",
      "        [0.2268],\n",
      "        [0.6146],\n",
      "        [0.2191],\n",
      "        [0.6017],\n",
      "        [0.5960],\n",
      "        [0.7040],\n",
      "        [0.7145],\n",
      "        [0.4532],\n",
      "        [0.3195],\n",
      "        [0.3101],\n",
      "        [0.3260],\n",
      "        [0.2185],\n",
      "        [0.3267],\n",
      "        [0.3111],\n",
      "        [0.5584],\n",
      "        [0.2996],\n",
      "        [0.6445],\n",
      "        [0.7248],\n",
      "        [0.6968],\n",
      "        [0.3138],\n",
      "        [0.6767],\n",
      "        [0.3147],\n",
      "        [0.6070],\n",
      "        [0.6132],\n",
      "        [0.6023],\n",
      "        [0.4932],\n",
      "        [0.6528],\n",
      "        [0.2503],\n",
      "        [0.6828],\n",
      "        [0.6309],\n",
      "        [0.2740],\n",
      "        [0.2599],\n",
      "        [0.3176],\n",
      "        [0.1657],\n",
      "        [0.5170],\n",
      "        [0.6706],\n",
      "        [0.2956],\n",
      "        [0.7059],\n",
      "        [0.6711],\n",
      "        [0.6933],\n",
      "        [0.2869],\n",
      "        [0.6647],\n",
      "        [0.3175],\n",
      "        [0.4250],\n",
      "        [0.3142],\n",
      "        [0.5147],\n",
      "        [0.4212],\n",
      "        [0.2981],\n",
      "        [0.3663],\n",
      "        [0.3351],\n",
      "        [0.3239],\n",
      "        [0.5679],\n",
      "        [0.5984],\n",
      "        [0.1674],\n",
      "        [0.4102],\n",
      "        [0.1349],\n",
      "        [0.6723],\n",
      "        [0.1506],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735],\n",
      "        [0.4735]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0321],\n",
      "        [0.0168],\n",
      "        [0.0440],\n",
      "        [0.0091],\n",
      "        [0.0315],\n",
      "        [0.0153],\n",
      "        [0.0275],\n",
      "        [0.0033],\n",
      "        [0.0149],\n",
      "        [0.0037],\n",
      "        [0.0111],\n",
      "        [0.0509],\n",
      "        [0.0061],\n",
      "        [0.0263],\n",
      "        [0.0185],\n",
      "        [0.0162],\n",
      "        [0.0105],\n",
      "        [0.0388],\n",
      "        [0.0129],\n",
      "        [0.0338],\n",
      "        [0.0283],\n",
      "        [0.0288],\n",
      "        [0.0542],\n",
      "        [0.0250],\n",
      "        [0.0224],\n",
      "        [0.0185],\n",
      "        [0.0367],\n",
      "        [0.0478],\n",
      "        [0.0596],\n",
      "        [0.0084],\n",
      "        [0.0345],\n",
      "        [0.0551],\n",
      "        [0.1037],\n",
      "        [0.0112],\n",
      "        [0.0340],\n",
      "        [0.0655],\n",
      "        [0.0175],\n",
      "        [0.0137],\n",
      "        [0.0148],\n",
      "        [0.0222],\n",
      "        [0.0577],\n",
      "        [0.0055],\n",
      "        [0.0078],\n",
      "        [0.0202],\n",
      "        [0.0162],\n",
      "        [0.0194],\n",
      "        [0.0215],\n",
      "        [0.0692],\n",
      "        [0.0087],\n",
      "        [0.0014],\n",
      "        [0.0148],\n",
      "        [0.0102],\n",
      "        [0.0127],\n",
      "        [0.0257],\n",
      "        [0.0512],\n",
      "        [0.0428],\n",
      "        [0.0023],\n",
      "        [0.0054],\n",
      "        [0.0626],\n",
      "        [0.0902],\n",
      "        [0.0104],\n",
      "        [0.0080],\n",
      "        [0.1074],\n",
      "        [0.0488],\n",
      "        [0.0333],\n",
      "        [0.0149],\n",
      "        [0.0563],\n",
      "        [0.0051],\n",
      "        [0.0074],\n",
      "        [0.0548],\n",
      "        [0.0090],\n",
      "        [0.0283],\n",
      "        [0.0355],\n",
      "        [0.0703],\n",
      "        [0.0291],\n",
      "        [0.0006],\n",
      "        [0.0067],\n",
      "        [0.0798],\n",
      "        [0.0384],\n",
      "        [0.0486],\n",
      "        [0.0283],\n",
      "        [0.0345],\n",
      "        [0.0313],\n",
      "        [0.0474],\n",
      "        [0.0654],\n",
      "        [0.0794],\n",
      "        [0.0110],\n",
      "        [0.0087],\n",
      "        [0.0088],\n",
      "        [0.0497],\n",
      "        [0.0351],\n",
      "        [0.0498],\n",
      "        [0.0837],\n",
      "        [0.0834],\n",
      "        [0.0870],\n",
      "        [0.0182],\n",
      "        [0.0240],\n",
      "        [0.0695],\n",
      "        [0.0198],\n",
      "        [0.0409],\n",
      "        [0.0864],\n",
      "        [0.0311],\n",
      "        [0.1267],\n",
      "        [0.0476],\n",
      "        [0.0216],\n",
      "        [0.0563],\n",
      "        [0.1033],\n",
      "        [0.0108],\n",
      "        [0.0631],\n",
      "        [0.0422],\n",
      "        [0.1241],\n",
      "        [0.0434],\n",
      "        [0.0736],\n",
      "        [0.0986],\n",
      "        [0.0467],\n",
      "        [0.0578],\n",
      "        [0.1344],\n",
      "        [0.1384],\n",
      "        [0.1014],\n",
      "        [0.1171],\n",
      "        [0.1271],\n",
      "        [0.1442],\n",
      "        [0.1779],\n",
      "        [0.1403],\n",
      "        [0.1828],\n",
      "        [0.1349],\n",
      "        [0.2111],\n",
      "        [0.0470],\n",
      "        [0.3431],\n",
      "        [0.3526],\n",
      "        [0.3544],\n",
      "        [0.3629],\n",
      "        [0.3647],\n",
      "        [0.3668],\n",
      "        [0.3678],\n",
      "        [0.3710],\n",
      "        [0.3901],\n",
      "        [0.3911],\n",
      "        [0.3980],\n",
      "        [0.4024],\n",
      "        [0.4044],\n",
      "        [0.4053],\n",
      "        [0.4097],\n",
      "        [0.4172],\n",
      "        [0.4188],\n",
      "        [0.4196],\n",
      "        [0.4196],\n",
      "        [0.4223],\n",
      "        [0.4256],\n",
      "        [0.4272],\n",
      "        [0.4289],\n",
      "        [0.4296],\n",
      "        [0.4311]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.43\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 187\n",
      "Number of shrink: 94\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0293],\n",
      "        [0.0127],\n",
      "        [0.0455],\n",
      "        [0.0071],\n",
      "        [0.0287],\n",
      "        [0.0184],\n",
      "        [0.0235],\n",
      "        [0.0063],\n",
      "        [0.0174],\n",
      "        [0.0057],\n",
      "        [0.0085],\n",
      "        [0.0544],\n",
      "        [0.0038],\n",
      "        [0.0234],\n",
      "        [0.0146],\n",
      "        [0.0190],\n",
      "        [0.0079],\n",
      "        [0.0356],\n",
      "        [0.0172],\n",
      "        [0.0313],\n",
      "        [0.0303],\n",
      "        [0.0318],\n",
      "        [0.0527],\n",
      "        [0.0274],\n",
      "        [0.0253],\n",
      "        [0.0211],\n",
      "        [0.0339],\n",
      "        [0.0500],\n",
      "        [0.0573],\n",
      "        [0.0114],\n",
      "        [0.0319],\n",
      "        [0.0576],\n",
      "        [0.1056],\n",
      "        [0.0094],\n",
      "        [0.0368],\n",
      "        [0.0674],\n",
      "        [0.0211],\n",
      "        [0.0115],\n",
      "        [0.0119],\n",
      "        [0.0196],\n",
      "        [0.0544],\n",
      "        [0.0061],\n",
      "        [0.0047],\n",
      "        [0.0219],\n",
      "        [0.0132],\n",
      "        [0.0167],\n",
      "        [0.0187],\n",
      "        [0.0675],\n",
      "        [0.0053],\n",
      "        [0.0045],\n",
      "        [0.0184],\n",
      "        [0.0127],\n",
      "        [0.0094],\n",
      "        [0.0244],\n",
      "        [0.0539],\n",
      "        [0.0397],\n",
      "        [0.0008],\n",
      "        [0.0019],\n",
      "        [0.0658],\n",
      "        [0.0927],\n",
      "        [0.0070],\n",
      "        [0.0116],\n",
      "        [0.1082],\n",
      "        [0.0515],\n",
      "        [0.0365],\n",
      "        [0.0175],\n",
      "        [0.0598],\n",
      "        [0.0098],\n",
      "        [0.0043],\n",
      "        [0.0561],\n",
      "        [0.0057],\n",
      "        [0.0310],\n",
      "        [0.0346],\n",
      "        [0.0732],\n",
      "        [0.0258],\n",
      "        [0.0024],\n",
      "        [0.0024],\n",
      "        [0.0834],\n",
      "        [0.0391],\n",
      "        [0.0498],\n",
      "        [0.0297],\n",
      "        [0.0326],\n",
      "        [0.0321],\n",
      "        [0.0488],\n",
      "        [0.0671],\n",
      "        [0.0771],\n",
      "        [0.0063],\n",
      "        [0.0051],\n",
      "        [0.0119],\n",
      "        [0.0508],\n",
      "        [0.0376],\n",
      "        [0.0512],\n",
      "        [0.0857],\n",
      "        [0.0859],\n",
      "        [0.0892],\n",
      "        [0.0219],\n",
      "        [0.0272],\n",
      "        [0.0674],\n",
      "        [0.0159],\n",
      "        [0.0442],\n",
      "        [0.0842],\n",
      "        [0.0339],\n",
      "        [0.1249],\n",
      "        [0.0453],\n",
      "        [0.0179],\n",
      "        [0.0519],\n",
      "        [0.1050],\n",
      "        [0.0116],\n",
      "        [0.0655],\n",
      "        [0.0443],\n",
      "        [0.1219],\n",
      "        [0.0392],\n",
      "        [0.0748],\n",
      "        [0.0965],\n",
      "        [0.0435],\n",
      "        [0.0539],\n",
      "        [0.1327],\n",
      "        [0.1401],\n",
      "        [0.0985],\n",
      "        [0.1187],\n",
      "        [0.1284],\n",
      "        [0.1406],\n",
      "        [0.1746],\n",
      "        [0.1364],\n",
      "        [0.1800],\n",
      "        [0.1333],\n",
      "        [0.2066],\n",
      "        [0.0481],\n",
      "        [0.3419],\n",
      "        [0.3514],\n",
      "        [0.3532],\n",
      "        [0.3618],\n",
      "        [0.3636],\n",
      "        [0.3656],\n",
      "        [0.3667],\n",
      "        [0.3699],\n",
      "        [0.3890],\n",
      "        [0.3900],\n",
      "        [0.3969],\n",
      "        [0.4013],\n",
      "        [0.4033],\n",
      "        [0.4041],\n",
      "        [0.4086],\n",
      "        [0.4161],\n",
      "        [0.4177],\n",
      "        [0.4184],\n",
      "        [0.4185],\n",
      "        [0.4212],\n",
      "        [0.4245],\n",
      "        [0.4261],\n",
      "        [0.4278],\n",
      "        [0.4284],\n",
      "        [0.4300]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: [0.43086565]\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0298],\n",
      "        [    0.0128],\n",
      "        [    0.0451],\n",
      "        [    0.0077],\n",
      "        [    0.0292],\n",
      "        [    0.0188],\n",
      "        [    0.0236],\n",
      "        [    0.0066],\n",
      "        [    0.0170],\n",
      "        [    0.0051],\n",
      "        [    0.0088],\n",
      "        [    0.0543],\n",
      "        [    0.0043],\n",
      "        [    0.0239],\n",
      "        [    0.0148],\n",
      "        [    0.0187],\n",
      "        [    0.0083],\n",
      "        [    0.0359],\n",
      "        [    0.0173],\n",
      "        [    0.0310],\n",
      "        [    0.0298],\n",
      "        [    0.0315],\n",
      "        [    0.0532],\n",
      "        [    0.0270],\n",
      "        [    0.0249],\n",
      "        [    0.0206],\n",
      "        [    0.0344],\n",
      "        [    0.0495],\n",
      "        [    0.0575],\n",
      "        [    0.0110],\n",
      "        [    0.0321],\n",
      "        [    0.0572],\n",
      "        [    0.1053],\n",
      "        [    0.0101],\n",
      "        [    0.0365],\n",
      "        [    0.0670],\n",
      "        [    0.0210],\n",
      "        [    0.0120],\n",
      "        [    0.0124],\n",
      "        [    0.0201],\n",
      "        [    0.0548],\n",
      "        [    0.0056],\n",
      "        [    0.0050],\n",
      "        [    0.0213],\n",
      "        [    0.0137],\n",
      "        [    0.0173],\n",
      "        [    0.0187],\n",
      "        [    0.0676],\n",
      "        [    0.0056],\n",
      "        [    0.0042],\n",
      "        [    0.0184],\n",
      "        [    0.0121],\n",
      "        [    0.0096],\n",
      "        [    0.0248],\n",
      "        [    0.0536],\n",
      "        [    0.0397],\n",
      "        [    0.0003],\n",
      "        [    0.0020],\n",
      "        [    0.0656],\n",
      "        [    0.0926],\n",
      "        [    0.0073],\n",
      "        [    0.0112],\n",
      "        [    0.1080],\n",
      "        [    0.0511],\n",
      "        [    0.0362],\n",
      "        [    0.0170],\n",
      "        [    0.0596],\n",
      "        [    0.0100],\n",
      "        [    0.0045],\n",
      "        [    0.0554],\n",
      "        [    0.0052],\n",
      "        [    0.0304],\n",
      "        [    0.0338],\n",
      "        [    0.0730],\n",
      "        [    0.0260],\n",
      "        [    0.0021],\n",
      "        [    0.0023],\n",
      "        [    0.0833],\n",
      "        [    0.0384],\n",
      "        [    0.0492],\n",
      "        [    0.0293],\n",
      "        [    0.0328],\n",
      "        [    0.0314],\n",
      "        [    0.0483],\n",
      "        [    0.0664],\n",
      "        [    0.0773],\n",
      "        [    0.0062],\n",
      "        [    0.0052],\n",
      "        [    0.0116],\n",
      "        [    0.0502],\n",
      "        [    0.0371],\n",
      "        [    0.0506],\n",
      "        [    0.0852],\n",
      "        [    0.0854],\n",
      "        [    0.0887],\n",
      "        [    0.0219],\n",
      "        [    0.0269],\n",
      "        [    0.0675],\n",
      "        [    0.0159],\n",
      "        [    0.0439],\n",
      "        [    0.0844],\n",
      "        [    0.0338],\n",
      "        [    0.1252],\n",
      "        [    0.0447],\n",
      "        [    0.0179],\n",
      "        [    0.0518],\n",
      "        [    0.1046],\n",
      "        [    0.0107],\n",
      "        [    0.0650],\n",
      "        [    0.0437],\n",
      "        [    0.1221],\n",
      "        [    0.0392],\n",
      "        [    0.0743],\n",
      "        [    0.0971],\n",
      "        [    0.0436],\n",
      "        [    0.0537],\n",
      "        [    0.1333],\n",
      "        [    0.1397],\n",
      "        [    0.0987],\n",
      "        [    0.1182],\n",
      "        [    0.1279],\n",
      "        [    0.1407],\n",
      "        [    0.1748],\n",
      "        [    0.1357],\n",
      "        [    0.1803],\n",
      "        [    0.1332],\n",
      "        [    0.2066],\n",
      "        [    0.0483],\n",
      "        [    0.3415],\n",
      "        [    0.3510],\n",
      "        [    0.3528],\n",
      "        [    0.3613],\n",
      "        [    0.3631],\n",
      "        [    0.3652],\n",
      "        [    0.3663],\n",
      "        [    0.3695],\n",
      "        [    0.3886],\n",
      "        [    0.3895],\n",
      "        [    0.3965],\n",
      "        [    0.4009],\n",
      "        [    0.4029],\n",
      "        [    0.4037],\n",
      "        [    0.4082],\n",
      "        [    0.4156],\n",
      "        [    0.4173],\n",
      "        [    0.4180],\n",
      "        [    0.4181],\n",
      "        [    0.4208],\n",
      "        [    0.4241],\n",
      "        [    0.4257],\n",
      "        [    0.4274],\n",
      "        [    0.4280],\n",
      "        [    0.4296]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 80.9551784992218\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 4 個區塊累積花費時間(s) 13.25072979927063\n",
      "<<The performance of 4 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 13.25072979927063\n",
      "<<The percentage of each step>>\n",
      "Step 4: 50.00%\n",
      "Step 6.1: 46.15%\n",
      "Step 6.2: 3.85%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 1\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 3\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 2632.99\n",
      "The MAPE for l = 1: 0.07%\n",
      "The RMSE for l = 1: 4340.13\n",
      "The accuracy(2000) for l = 1: 68.63%\n",
      "The accuracy(3000) for l = 1: 75.82%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in toutlier>>\n",
      "The MAE for l = 1: 11251.82\n",
      "The MAPE for l = 1: 0.32%\n",
      "The RMSE for l = 1: 11253.22\n",
      "The accuracy(2000) for l = 1: 0.00%\n",
      "The accuracy(3000) for l = 1: 0.00%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 3925.8\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 5279.1\n",
      "The accuracy(2000) for l = 1: 46.2%\n",
      "The accuracy(3000) for l = 1: 53.8%\n",
      "------------------------------------------------------------\n",
      "0.6862745098039216\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "0.46153846153846156\n",
      "<class 'float'>\n",
      "The <<5>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (1.1445769132478745e-07, 37)\n",
      "The second_loss value of k: (5.565913852478843e-06, 140)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [37, 140, 46, 0, 95, 90, 33, 96, 27, 24, 36, 150, 31, 39, 34, 148, 45, 35, 32, 55, 38, 142, 15, 10, 6, 82, 97, 144, 8, 50, 20, 16, 143, 26, 41, 89, 52, 139, 11, 28, 7, 42, 77, 18, 40, 30, 94, 67, 9, 80, 88, 141, 91, 61, 43, 44, 47, 23, 149, 147, 66, 81, 146, 60, 29, 145, 99, 65, 92, 69, 68, 53, 70, 76, 1, 78, 22, 14, 51, 25, 19, 54, 79, 49, 17, 21, 64, 87, 93, 5, 71, 83, 48, 86, 3, 2, 4, 62, 56, 59, 158, 75, 63, 98, 151, 73, 85, 157, 84, 72, 101, 57, 102, 74, 13, 12, 58, 152, 156, 153, 155, 154, 135, 136, 137, 134, 118]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0003],\n",
      "        [    0.0024],\n",
      "        [    0.0042],\n",
      "        [    0.0051],\n",
      "        [    0.0052],\n",
      "        [    0.0056],\n",
      "        [    0.0056],\n",
      "        [    0.0066],\n",
      "        [    0.0096],\n",
      "        [    0.0110],\n",
      "        [    0.0112],\n",
      "        [    0.0112],\n",
      "        [    0.0121],\n",
      "        [    0.0124],\n",
      "        [    0.0128],\n",
      "        [    0.0133],\n",
      "        [    0.0137],\n",
      "        [    0.0148],\n",
      "        [    0.0170],\n",
      "        [    0.0170],\n",
      "        [    0.0173],\n",
      "        [    0.0178],\n",
      "        [    0.0179],\n",
      "        [    0.0184],\n",
      "        [    0.0187],\n",
      "        [    0.0187],\n",
      "        [    0.0188],\n",
      "        [    0.0195],\n",
      "        [    0.0206],\n",
      "        [    0.0210],\n",
      "        [    0.0213],\n",
      "        [    0.0219],\n",
      "        [    0.0235],\n",
      "        [    0.0236],\n",
      "        [    0.0239],\n",
      "        [    0.0248],\n",
      "        [    0.0249],\n",
      "        [    0.0256],\n",
      "        [    0.0260],\n",
      "        [    0.0269],\n",
      "        [    0.0270],\n",
      "        [    0.0292],\n",
      "        [    0.0293],\n",
      "        [    0.0298],\n",
      "        [    0.0298],\n",
      "        [    0.0304],\n",
      "        [    0.0310],\n",
      "        [    0.0314],\n",
      "        [    0.0315],\n",
      "        [    0.0321],\n",
      "        [    0.0328],\n",
      "        [    0.0328],\n",
      "        [    0.0338],\n",
      "        [    0.0338],\n",
      "        [    0.0344],\n",
      "        [    0.0359],\n",
      "        [    0.0362],\n",
      "        [    0.0365],\n",
      "        [    0.0375],\n",
      "        [    0.0383],\n",
      "        [    0.0384],\n",
      "        [    0.0397],\n",
      "        [    0.0428],\n",
      "        [    0.0436],\n",
      "        [    0.0439],\n",
      "        [    0.0442],\n",
      "        [    0.0447],\n",
      "        [    0.0451],\n",
      "        [    0.0483],\n",
      "        [    0.0483],\n",
      "        [    0.0492],\n",
      "        [    0.0495],\n",
      "        [    0.0502],\n",
      "        [    0.0506],\n",
      "        [    0.0511],\n",
      "        [    0.0532],\n",
      "        [    0.0536],\n",
      "        [    0.0537],\n",
      "        [    0.0543],\n",
      "        [    0.0548],\n",
      "        [    0.0554],\n",
      "        [    0.0572],\n",
      "        [    0.0575],\n",
      "        [    0.0596],\n",
      "        [    0.0656],\n",
      "        [    0.0664],\n",
      "        [    0.0670],\n",
      "        [    0.0675],\n",
      "        [    0.0676],\n",
      "        [    0.0730],\n",
      "        [    0.0743],\n",
      "        [    0.0773],\n",
      "        [    0.0833],\n",
      "        [    0.0844],\n",
      "        [    0.0852],\n",
      "        [    0.0854],\n",
      "        [    0.0887],\n",
      "        [    0.0926],\n",
      "        [    0.0971],\n",
      "        [    0.0987],\n",
      "        [    0.0997],\n",
      "        [    0.1046],\n",
      "        [    0.1053],\n",
      "        [    0.1080],\n",
      "        [    0.1093],\n",
      "        [    0.1182],\n",
      "        [    0.1221],\n",
      "        [    0.1227],\n",
      "        [    0.1252],\n",
      "        [    0.1279],\n",
      "        [    0.1332],\n",
      "        [    0.1333],\n",
      "        [    0.1357],\n",
      "        [    0.1397],\n",
      "        [    0.1407],\n",
      "        [    0.1748],\n",
      "        [    0.1803],\n",
      "        [    0.2002],\n",
      "        [    0.2317],\n",
      "        [    0.2601],\n",
      "        [    0.2713],\n",
      "        [    0.2811],\n",
      "        [    0.3216],\n",
      "        [    0.3356],\n",
      "        [    0.3386],\n",
      "        [    0.3405],\n",
      "        [    0.3415]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 128\n",
      "剩餘X 資料 torch.Size([32, 18])\n",
      "剩餘Y 資料 torch.Size([32, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12319870293140411, 20)\n",
      "The second_loss value of k: (0.12446679919958115, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引20，y= tensor([0.1210])\n",
      "目前模型的Data狀態 torch.Size([128, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6274],\n",
      "        [0.0884],\n",
      "        [0.5324],\n",
      "        [0.6539],\n",
      "        [0.2230],\n",
      "        [0.2280],\n",
      "        [0.6343],\n",
      "        [0.1855],\n",
      "        [0.6402],\n",
      "        [0.5998],\n",
      "        [0.6327],\n",
      "        [0.1868],\n",
      "        [0.6095],\n",
      "        [0.6031],\n",
      "        [0.6198],\n",
      "        [0.1315],\n",
      "        [0.5542],\n",
      "        [0.6313],\n",
      "        [0.6163],\n",
      "        [0.4234],\n",
      "        [0.6245],\n",
      "        [0.1084],\n",
      "        [0.5132],\n",
      "        [0.5704],\n",
      "        [0.6229],\n",
      "        [0.2809],\n",
      "        [0.1614],\n",
      "        [0.1001],\n",
      "        [0.6157],\n",
      "        [0.4499],\n",
      "        [0.5638],\n",
      "        [0.4895],\n",
      "        [0.1167],\n",
      "        [0.6043],\n",
      "        [0.5850],\n",
      "        [0.2165],\n",
      "        [0.4490],\n",
      "        [0.0830],\n",
      "        [0.5929],\n",
      "        [0.6499],\n",
      "        [0.6184],\n",
      "        [0.5851],\n",
      "        [0.3250],\n",
      "        [0.5217],\n",
      "        [0.5888],\n",
      "        [0.6126],\n",
      "        [0.2202],\n",
      "        [0.3266],\n",
      "        [0.6045],\n",
      "        [0.3574],\n",
      "        [0.2168],\n",
      "        [0.1137],\n",
      "        [0.2207],\n",
      "        [0.2571],\n",
      "        [0.5823],\n",
      "        [0.5624],\n",
      "        [0.4978],\n",
      "        [0.5823],\n",
      "        [0.1818],\n",
      "        [0.1000],\n",
      "        [0.3195],\n",
      "        [0.3107],\n",
      "        [0.0898],\n",
      "        [0.3110],\n",
      "        [0.6279],\n",
      "        [0.0889],\n",
      "        [0.1628],\n",
      "        [0.2620],\n",
      "        [0.1493],\n",
      "        [0.3103],\n",
      "        [0.3095],\n",
      "        [0.4224],\n",
      "        [0.3133],\n",
      "        [0.3138],\n",
      "        [0.6283],\n",
      "        [0.3674],\n",
      "        [0.5635],\n",
      "        [0.5106],\n",
      "        [0.4329],\n",
      "        [0.6185],\n",
      "        [0.5551],\n",
      "        [0.4074],\n",
      "        [0.3690],\n",
      "        [0.4634],\n",
      "        [0.4609],\n",
      "        [0.5575],\n",
      "        [0.2407],\n",
      "        [0.2483],\n",
      "        [0.2267],\n",
      "        [0.5991],\n",
      "        [0.3169],\n",
      "        [0.2975],\n",
      "        [0.4496],\n",
      "        [0.2720],\n",
      "        [0.6055],\n",
      "        [0.6112],\n",
      "        [0.6005],\n",
      "        [0.2101],\n",
      "        [0.4234],\n",
      "        [0.3636],\n",
      "        [0.2999],\n",
      "        [0.2944],\n",
      "        [0.2139],\n",
      "        [0.0540],\n",
      "        [0.2004],\n",
      "        [0.3340],\n",
      "        [0.2848],\n",
      "        [0.2914],\n",
      "        [0.3161],\n",
      "        [0.3231],\n",
      "        [0.1332],\n",
      "        [0.4200],\n",
      "        [0.1628],\n",
      "        [0.2968],\n",
      "        [0.5644],\n",
      "        [0.5953],\n",
      "        [0.4077],\n",
      "        [0.2090],\n",
      "        [0.2514],\n",
      "        [0.2070],\n",
      "        [0.2272],\n",
      "        [0.2147],\n",
      "        [0.4720],\n",
      "        [0.4720],\n",
      "        [0.4720],\n",
      "        [0.4720],\n",
      "        [0.4720],\n",
      "        [0.4720]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0003],\n",
      "        [    0.0024],\n",
      "        [    0.0042],\n",
      "        [    0.0051],\n",
      "        [    0.0052],\n",
      "        [    0.0056],\n",
      "        [    0.0056],\n",
      "        [    0.0066],\n",
      "        [    0.0096],\n",
      "        [    0.0110],\n",
      "        [    0.0112],\n",
      "        [    0.0112],\n",
      "        [    0.0121],\n",
      "        [    0.0124],\n",
      "        [    0.0128],\n",
      "        [    0.0133],\n",
      "        [    0.0137],\n",
      "        [    0.0148],\n",
      "        [    0.0170],\n",
      "        [    0.0170],\n",
      "        [    0.0173],\n",
      "        [    0.0178],\n",
      "        [    0.0179],\n",
      "        [    0.0184],\n",
      "        [    0.0187],\n",
      "        [    0.0187],\n",
      "        [    0.0188],\n",
      "        [    0.0195],\n",
      "        [    0.0206],\n",
      "        [    0.0210],\n",
      "        [    0.0213],\n",
      "        [    0.0219],\n",
      "        [    0.0235],\n",
      "        [    0.0236],\n",
      "        [    0.0239],\n",
      "        [    0.0248],\n",
      "        [    0.0249],\n",
      "        [    0.0256],\n",
      "        [    0.0260],\n",
      "        [    0.0269],\n",
      "        [    0.0270],\n",
      "        [    0.0292],\n",
      "        [    0.0293],\n",
      "        [    0.0298],\n",
      "        [    0.0298],\n",
      "        [    0.0304],\n",
      "        [    0.0310],\n",
      "        [    0.0314],\n",
      "        [    0.0315],\n",
      "        [    0.0321],\n",
      "        [    0.0328],\n",
      "        [    0.0328],\n",
      "        [    0.0338],\n",
      "        [    0.0338],\n",
      "        [    0.0344],\n",
      "        [    0.0359],\n",
      "        [    0.0362],\n",
      "        [    0.0365],\n",
      "        [    0.0375],\n",
      "        [    0.0383],\n",
      "        [    0.0384],\n",
      "        [    0.0397],\n",
      "        [    0.0428],\n",
      "        [    0.0436],\n",
      "        [    0.0439],\n",
      "        [    0.0442],\n",
      "        [    0.0447],\n",
      "        [    0.0451],\n",
      "        [    0.0483],\n",
      "        [    0.0483],\n",
      "        [    0.0492],\n",
      "        [    0.0495],\n",
      "        [    0.0502],\n",
      "        [    0.0506],\n",
      "        [    0.0511],\n",
      "        [    0.0532],\n",
      "        [    0.0536],\n",
      "        [    0.0537],\n",
      "        [    0.0543],\n",
      "        [    0.0548],\n",
      "        [    0.0554],\n",
      "        [    0.0572],\n",
      "        [    0.0575],\n",
      "        [    0.0596],\n",
      "        [    0.0656],\n",
      "        [    0.0664],\n",
      "        [    0.0670],\n",
      "        [    0.0675],\n",
      "        [    0.0676],\n",
      "        [    0.0730],\n",
      "        [    0.0743],\n",
      "        [    0.0773],\n",
      "        [    0.0833],\n",
      "        [    0.0844],\n",
      "        [    0.0852],\n",
      "        [    0.0854],\n",
      "        [    0.0887],\n",
      "        [    0.0926],\n",
      "        [    0.0971],\n",
      "        [    0.0987],\n",
      "        [    0.0997],\n",
      "        [    0.1046],\n",
      "        [    0.1053],\n",
      "        [    0.1080],\n",
      "        [    0.1093],\n",
      "        [    0.1182],\n",
      "        [    0.1221],\n",
      "        [    0.1227],\n",
      "        [    0.1252],\n",
      "        [    0.1279],\n",
      "        [    0.1332],\n",
      "        [    0.1333],\n",
      "        [    0.1357],\n",
      "        [    0.1397],\n",
      "        [    0.1407],\n",
      "        [    0.1748],\n",
      "        [    0.1803],\n",
      "        [    0.2002],\n",
      "        [    0.2317],\n",
      "        [    0.2601],\n",
      "        [    0.2713],\n",
      "        [    0.2811],\n",
      "        [    0.3216],\n",
      "        [    0.3356],\n",
      "        [    0.3386],\n",
      "        [    0.3405],\n",
      "        [    0.3415],\n",
      "        [    0.3510]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0004],\n",
      "        [    0.0542],\n",
      "        [    0.0122],\n",
      "        [    0.0072],\n",
      "        [    0.0343],\n",
      "        [    0.0107],\n",
      "        [    0.0090],\n",
      "        [    0.0159],\n",
      "        [    0.0163],\n",
      "        [    0.0021],\n",
      "        [    0.0127],\n",
      "        [    0.0906],\n",
      "        [    0.0069],\n",
      "        [    0.0095],\n",
      "        [    0.0147],\n",
      "        [    0.0854],\n",
      "        [    0.0045],\n",
      "        [    0.0120],\n",
      "        [    0.0107],\n",
      "        [    0.0259],\n",
      "        [    0.0164],\n",
      "        [    0.0758],\n",
      "        [    0.0107],\n",
      "        [    0.0136],\n",
      "        [    0.0097],\n",
      "        [    0.0235],\n",
      "        [    0.0002],\n",
      "        [    0.0389],\n",
      "        [    0.0157],\n",
      "        [    0.0232],\n",
      "        [    0.0115],\n",
      "        [    0.0305],\n",
      "        [    0.0785],\n",
      "        [    0.0318],\n",
      "        [    0.0144],\n",
      "        [    0.0398],\n",
      "        [    0.0261],\n",
      "        [    0.0290],\n",
      "        [    0.0301],\n",
      "        [    0.0228],\n",
      "        [    0.0202],\n",
      "        [    0.0182],\n",
      "        [    0.0241],\n",
      "        [    0.0246],\n",
      "        [    0.0220],\n",
      "        [    0.0275],\n",
      "        [    0.0597],\n",
      "        [    0.0239],\n",
      "        [    0.0272],\n",
      "        [    0.0334],\n",
      "        [    0.0450],\n",
      "        [    0.0910],\n",
      "        [    0.0243],\n",
      "        [    0.0440],\n",
      "        [    0.0228],\n",
      "        [    0.0256],\n",
      "        [    0.0429],\n",
      "        [    0.0233],\n",
      "        [    0.1181],\n",
      "        [    0.0237],\n",
      "        [    0.0327],\n",
      "        [    0.0442],\n",
      "        [    0.0157],\n",
      "        [    0.0376],\n",
      "        [    0.0415],\n",
      "        [    0.0145],\n",
      "        [    0.0477],\n",
      "        [    0.0406],\n",
      "        [    0.0459],\n",
      "        [    0.0415],\n",
      "        [    0.0420],\n",
      "        [    0.0584],\n",
      "        [    0.0430],\n",
      "        [    0.0430],\n",
      "        [    0.0353],\n",
      "        [    0.0547],\n",
      "        [    0.0408],\n",
      "        [    0.0467],\n",
      "        [    0.0546],\n",
      "        [    0.0625],\n",
      "        [    0.0488],\n",
      "        [    0.0667],\n",
      "        [    0.0588],\n",
      "        [    0.0629],\n",
      "        [    0.0696],\n",
      "        [    0.0574],\n",
      "        [    0.0677],\n",
      "        [    0.0756],\n",
      "        [    0.0926],\n",
      "        [    0.0604],\n",
      "        [    0.0657],\n",
      "        [    0.0827],\n",
      "        [    0.0877],\n",
      "        [    0.0905],\n",
      "        [    0.0719],\n",
      "        [    0.0710],\n",
      "        [    0.0781],\n",
      "        [    0.1075],\n",
      "        [    0.0902],\n",
      "        [    0.0911],\n",
      "        [    0.0237],\n",
      "        [    0.0980],\n",
      "        [    0.1128],\n",
      "        [    0.0957],\n",
      "        [    0.0305],\n",
      "        [    0.1130],\n",
      "        [    0.1278],\n",
      "        [    0.0398],\n",
      "        [    0.1297],\n",
      "        [    0.1210],\n",
      "        [    0.1459],\n",
      "        [    0.1207],\n",
      "        [    0.1550],\n",
      "        [    0.1339],\n",
      "        [    0.1441],\n",
      "        [    0.1775],\n",
      "        [    0.1704],\n",
      "        [    0.1203],\n",
      "        [    0.1443],\n",
      "        [    0.1795],\n",
      "        [    0.1848],\n",
      "        [    0.1996],\n",
      "        [    0.3203],\n",
      "        [    0.3343],\n",
      "        [    0.3373],\n",
      "        [    0.3391],\n",
      "        [    0.3402],\n",
      "        [    0.3496]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 81.42997479438782\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 129\n",
      "剩餘X 資料 torch.Size([31, 18])\n",
      "剩餘Y 資料 torch.Size([31, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12351265549659729, 15)\n",
      "The second_loss value of k: (0.1243399828672409, 29)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.1192])\n",
      "目前模型的Data狀態 torch.Size([129, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6274],\n",
      "        [0.1450],\n",
      "        [0.5244],\n",
      "        [0.6662],\n",
      "        [0.2521],\n",
      "        [0.2443],\n",
      "        [0.6377],\n",
      "        [0.2080],\n",
      "        [0.6469],\n",
      "        [0.6086],\n",
      "        [0.6313],\n",
      "        [0.2662],\n",
      "        [0.6146],\n",
      "        [0.6002],\n",
      "        [0.6217],\n",
      "        [0.2037],\n",
      "        [0.5451],\n",
      "        [0.6286],\n",
      "        [0.6227],\n",
      "        [0.4146],\n",
      "        [0.6237],\n",
      "        [0.1664],\n",
      "        [0.5060],\n",
      "        [0.5753],\n",
      "        [0.6319],\n",
      "        [0.2857],\n",
      "        [0.1804],\n",
      "        [0.1585],\n",
      "        [0.6207],\n",
      "        [0.4478],\n",
      "        [0.5737],\n",
      "        [0.4808],\n",
      "        [0.1716],\n",
      "        [0.6124],\n",
      "        [0.5755],\n",
      "        [0.2315],\n",
      "        [0.4478],\n",
      "        [0.1376],\n",
      "        [0.5970],\n",
      "        [0.6539],\n",
      "        [0.6252],\n",
      "        [0.5740],\n",
      "        [0.3302],\n",
      "        [0.5268],\n",
      "        [0.5810],\n",
      "        [0.6154],\n",
      "        [0.2489],\n",
      "        [0.3341],\n",
      "        [0.6088],\n",
      "        [0.3587],\n",
      "        [0.2291],\n",
      "        [0.1719],\n",
      "        [0.2303],\n",
      "        [0.2470],\n",
      "        [0.5708],\n",
      "        [0.5520],\n",
      "        [0.4911],\n",
      "        [0.5954],\n",
      "        [0.2624],\n",
      "        [0.1620],\n",
      "        [0.3252],\n",
      "        [0.3152],\n",
      "        [0.1483],\n",
      "        [0.3051],\n",
      "        [0.6303],\n",
      "        [0.1476],\n",
      "        [0.1658],\n",
      "        [0.2665],\n",
      "        [0.1516],\n",
      "        [0.3171],\n",
      "        [0.3167],\n",
      "        [0.4135],\n",
      "        [0.3204],\n",
      "        [0.3215],\n",
      "        [0.6441],\n",
      "        [0.3689],\n",
      "        [0.5763],\n",
      "        [0.5036],\n",
      "        [0.4327],\n",
      "        [0.6263],\n",
      "        [0.5617],\n",
      "        [0.3978],\n",
      "        [0.3703],\n",
      "        [0.4602],\n",
      "        [0.4569],\n",
      "        [0.5665],\n",
      "        [0.2401],\n",
      "        [0.2564],\n",
      "        [0.2517],\n",
      "        [0.6116],\n",
      "        [0.3254],\n",
      "        [0.3028],\n",
      "        [0.4453],\n",
      "        [0.2782],\n",
      "        [0.6189],\n",
      "        [0.6256],\n",
      "        [0.6112],\n",
      "        [0.1951],\n",
      "        [0.4165],\n",
      "        [0.3560],\n",
      "        [0.3759],\n",
      "        [0.3010],\n",
      "        [0.2064],\n",
      "        [0.0663],\n",
      "        [0.2792],\n",
      "        [0.3392],\n",
      "        [0.2906],\n",
      "        [0.3744],\n",
      "        [0.3205],\n",
      "        [0.3300],\n",
      "        [0.1459],\n",
      "        [0.4074],\n",
      "        [0.1821],\n",
      "        [0.3026],\n",
      "        [0.5678],\n",
      "        [0.5980],\n",
      "        [0.3978],\n",
      "        [0.2888],\n",
      "        [0.3388],\n",
      "        [0.2877],\n",
      "        [0.3137],\n",
      "        [0.2961],\n",
      "        [0.4706],\n",
      "        [0.4706],\n",
      "        [0.4706],\n",
      "        [0.4706],\n",
      "        [0.4706],\n",
      "        [0.4706],\n",
      "        [0.4706]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0004],\n",
      "        [    0.0542],\n",
      "        [    0.0122],\n",
      "        [    0.0072],\n",
      "        [    0.0343],\n",
      "        [    0.0107],\n",
      "        [    0.0090],\n",
      "        [    0.0159],\n",
      "        [    0.0163],\n",
      "        [    0.0021],\n",
      "        [    0.0127],\n",
      "        [    0.0906],\n",
      "        [    0.0069],\n",
      "        [    0.0095],\n",
      "        [    0.0147],\n",
      "        [    0.0854],\n",
      "        [    0.0045],\n",
      "        [    0.0120],\n",
      "        [    0.0107],\n",
      "        [    0.0259],\n",
      "        [    0.0164],\n",
      "        [    0.0758],\n",
      "        [    0.0107],\n",
      "        [    0.0136],\n",
      "        [    0.0097],\n",
      "        [    0.0235],\n",
      "        [    0.0002],\n",
      "        [    0.0389],\n",
      "        [    0.0157],\n",
      "        [    0.0232],\n",
      "        [    0.0115],\n",
      "        [    0.0305],\n",
      "        [    0.0785],\n",
      "        [    0.0318],\n",
      "        [    0.0144],\n",
      "        [    0.0398],\n",
      "        [    0.0261],\n",
      "        [    0.0290],\n",
      "        [    0.0301],\n",
      "        [    0.0228],\n",
      "        [    0.0202],\n",
      "        [    0.0182],\n",
      "        [    0.0241],\n",
      "        [    0.0246],\n",
      "        [    0.0220],\n",
      "        [    0.0275],\n",
      "        [    0.0597],\n",
      "        [    0.0239],\n",
      "        [    0.0272],\n",
      "        [    0.0334],\n",
      "        [    0.0450],\n",
      "        [    0.0910],\n",
      "        [    0.0243],\n",
      "        [    0.0440],\n",
      "        [    0.0228],\n",
      "        [    0.0256],\n",
      "        [    0.0429],\n",
      "        [    0.0233],\n",
      "        [    0.1181],\n",
      "        [    0.0237],\n",
      "        [    0.0327],\n",
      "        [    0.0442],\n",
      "        [    0.0157],\n",
      "        [    0.0376],\n",
      "        [    0.0415],\n",
      "        [    0.0145],\n",
      "        [    0.0477],\n",
      "        [    0.0406],\n",
      "        [    0.0459],\n",
      "        [    0.0415],\n",
      "        [    0.0420],\n",
      "        [    0.0584],\n",
      "        [    0.0430],\n",
      "        [    0.0430],\n",
      "        [    0.0353],\n",
      "        [    0.0547],\n",
      "        [    0.0408],\n",
      "        [    0.0467],\n",
      "        [    0.0546],\n",
      "        [    0.0625],\n",
      "        [    0.0488],\n",
      "        [    0.0667],\n",
      "        [    0.0588],\n",
      "        [    0.0629],\n",
      "        [    0.0696],\n",
      "        [    0.0574],\n",
      "        [    0.0677],\n",
      "        [    0.0756],\n",
      "        [    0.0926],\n",
      "        [    0.0604],\n",
      "        [    0.0657],\n",
      "        [    0.0827],\n",
      "        [    0.0877],\n",
      "        [    0.0905],\n",
      "        [    0.0719],\n",
      "        [    0.0710],\n",
      "        [    0.0781],\n",
      "        [    0.1075],\n",
      "        [    0.0902],\n",
      "        [    0.0911],\n",
      "        [    0.0237],\n",
      "        [    0.0980],\n",
      "        [    0.1128],\n",
      "        [    0.0957],\n",
      "        [    0.0305],\n",
      "        [    0.1130],\n",
      "        [    0.1278],\n",
      "        [    0.0398],\n",
      "        [    0.1297],\n",
      "        [    0.1210],\n",
      "        [    0.1459],\n",
      "        [    0.1207],\n",
      "        [    0.1550],\n",
      "        [    0.1339],\n",
      "        [    0.1441],\n",
      "        [    0.1775],\n",
      "        [    0.1704],\n",
      "        [    0.1203],\n",
      "        [    0.1443],\n",
      "        [    0.1795],\n",
      "        [    0.1848],\n",
      "        [    0.1996],\n",
      "        [    0.3203],\n",
      "        [    0.3343],\n",
      "        [    0.3373],\n",
      "        [    0.3391],\n",
      "        [    0.3402],\n",
      "        [    0.3496],\n",
      "        [    0.3514]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0062],\n",
      "        [0.0559],\n",
      "        [0.0109],\n",
      "        [0.0294],\n",
      "        [0.0305],\n",
      "        [0.0068],\n",
      "        [0.0133],\n",
      "        [0.0133],\n",
      "        [0.0201],\n",
      "        [0.0068],\n",
      "        [0.0075],\n",
      "        [0.0978],\n",
      "        [0.0011],\n",
      "        [0.0148],\n",
      "        [0.0193],\n",
      "        [0.0901],\n",
      "        [0.0081],\n",
      "        [0.0161],\n",
      "        [0.0048],\n",
      "        [0.0227],\n",
      "        [0.0237],\n",
      "        [0.0784],\n",
      "        [0.0118],\n",
      "        [0.0048],\n",
      "        [0.0045],\n",
      "        [0.0245],\n",
      "        [0.0032],\n",
      "        [0.0386],\n",
      "        [0.0040],\n",
      "        [0.0227],\n",
      "        [0.0090],\n",
      "        [0.0305],\n",
      "        [0.0773],\n",
      "        [0.0378],\n",
      "        [0.0186],\n",
      "        [0.0379],\n",
      "        [0.0210],\n",
      "        [0.0277],\n",
      "        [0.0388],\n",
      "        [0.0192],\n",
      "        [0.0070],\n",
      "        [0.0226],\n",
      "        [0.0167],\n",
      "        [0.0234],\n",
      "        [0.0272],\n",
      "        [0.0235],\n",
      "        [0.0559],\n",
      "        [0.0197],\n",
      "        [0.0161],\n",
      "        [0.0399],\n",
      "        [0.0411],\n",
      "        [0.0949],\n",
      "        [0.0295],\n",
      "        [0.0480],\n",
      "        [0.0276],\n",
      "        [0.0305],\n",
      "        [0.0427],\n",
      "        [0.0136],\n",
      "        [0.1258],\n",
      "        [0.0259],\n",
      "        [0.0290],\n",
      "        [0.0477],\n",
      "        [0.0173],\n",
      "        [0.0355],\n",
      "        [0.0394],\n",
      "        [0.0152],\n",
      "        [0.0451],\n",
      "        [0.0384],\n",
      "        [0.0487],\n",
      "        [0.0378],\n",
      "        [0.0381],\n",
      "        [0.0530],\n",
      "        [0.0380],\n",
      "        [0.0356],\n",
      "        [0.0152],\n",
      "        [0.0635],\n",
      "        [0.0333],\n",
      "        [0.0476],\n",
      "        [0.0513],\n",
      "        [0.0714],\n",
      "        [0.0485],\n",
      "        [0.0631],\n",
      "        [0.0675],\n",
      "        [0.0628],\n",
      "        [0.0693],\n",
      "        [0.0530],\n",
      "        [0.0684],\n",
      "        [0.0723],\n",
      "        [0.0888],\n",
      "        [0.0460],\n",
      "        [0.0610],\n",
      "        [0.0821],\n",
      "        [0.0883],\n",
      "        [0.0877],\n",
      "        [0.0546],\n",
      "        [0.0528],\n",
      "        [0.0631],\n",
      "        [0.1136],\n",
      "        [0.0950],\n",
      "        [0.0903],\n",
      "        [0.0066],\n",
      "        [0.0929],\n",
      "        [0.1175],\n",
      "        [0.0993],\n",
      "        [0.0215],\n",
      "        [0.1079],\n",
      "        [0.1251],\n",
      "        [0.0213],\n",
      "        [0.1288],\n",
      "        [0.1160],\n",
      "        [0.1395],\n",
      "        [0.1218],\n",
      "        [0.1477],\n",
      "        [0.1292],\n",
      "        [0.1487],\n",
      "        [0.1841],\n",
      "        [0.1702],\n",
      "        [0.1080],\n",
      "        [0.1257],\n",
      "        [0.1671],\n",
      "        [0.1703],\n",
      "        [0.1863],\n",
      "        [0.3194],\n",
      "        [0.3334],\n",
      "        [0.3364],\n",
      "        [0.3383],\n",
      "        [0.3393],\n",
      "        [0.3488],\n",
      "        [0.3506]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 81.67024660110474\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 130\n",
      "剩餘X 資料 torch.Size([30, 18])\n",
      "剩餘Y 資料 torch.Size([30, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12374086678028107, 28)\n",
      "The second_loss value of k: (0.12897582352161407, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引28，y= tensor([0.1180])\n",
      "目前模型的Data狀態 torch.Size([130, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6340],\n",
      "        [0.1466],\n",
      "        [0.5257],\n",
      "        [0.6884],\n",
      "        [0.2483],\n",
      "        [0.2404],\n",
      "        [0.6419],\n",
      "        [0.2053],\n",
      "        [0.6506],\n",
      "        [0.6176],\n",
      "        [0.6364],\n",
      "        [0.2734],\n",
      "        [0.6205],\n",
      "        [0.6055],\n",
      "        [0.6263],\n",
      "        [0.2083],\n",
      "        [0.5486],\n",
      "        [0.6327],\n",
      "        [0.6286],\n",
      "        [0.4177],\n",
      "        [0.6309],\n",
      "        [0.1690],\n",
      "        [0.5071],\n",
      "        [0.5840],\n",
      "        [0.6460],\n",
      "        [0.2867],\n",
      "        [0.1770],\n",
      "        [0.1582],\n",
      "        [0.6324],\n",
      "        [0.4483],\n",
      "        [0.5761],\n",
      "        [0.4809],\n",
      "        [0.1704],\n",
      "        [0.6185],\n",
      "        [0.5797],\n",
      "        [0.2296],\n",
      "        [0.4529],\n",
      "        [0.1363],\n",
      "        [0.6057],\n",
      "        [0.6575],\n",
      "        [0.6384],\n",
      "        [0.5784],\n",
      "        [0.3376],\n",
      "        [0.5280],\n",
      "        [0.5861],\n",
      "        [0.6194],\n",
      "        [0.2451],\n",
      "        [0.3383],\n",
      "        [0.6199],\n",
      "        [0.3651],\n",
      "        [0.2251],\n",
      "        [0.1758],\n",
      "        [0.2251],\n",
      "        [0.2430],\n",
      "        [0.5755],\n",
      "        [0.5570],\n",
      "        [0.4912],\n",
      "        [0.6052],\n",
      "        [0.2701],\n",
      "        [0.1641],\n",
      "        [0.3289],\n",
      "        [0.3187],\n",
      "        [0.1499],\n",
      "        [0.3029],\n",
      "        [0.6323],\n",
      "        [0.1483],\n",
      "        [0.1632],\n",
      "        [0.2687],\n",
      "        [0.1489],\n",
      "        [0.3207],\n",
      "        [0.3206],\n",
      "        [0.4189],\n",
      "        [0.3254],\n",
      "        [0.3289],\n",
      "        [0.6642],\n",
      "        [0.3776],\n",
      "        [0.5838],\n",
      "        [0.5045],\n",
      "        [0.4359],\n",
      "        [0.6352],\n",
      "        [0.5620],\n",
      "        [0.4015],\n",
      "        [0.3789],\n",
      "        [0.4602],\n",
      "        [0.4572],\n",
      "        [0.5708],\n",
      "        [0.2394],\n",
      "        [0.2531],\n",
      "        [0.2478],\n",
      "        [0.6260],\n",
      "        [0.3301],\n",
      "        [0.3023],\n",
      "        [0.4447],\n",
      "        [0.2754],\n",
      "        [0.6362],\n",
      "        [0.6438],\n",
      "        [0.6262],\n",
      "        [0.1891],\n",
      "        [0.4213],\n",
      "        [0.3553],\n",
      "        [0.3930],\n",
      "        [0.3060],\n",
      "        [0.2018],\n",
      "        [0.0627],\n",
      "        [0.2882],\n",
      "        [0.3443],\n",
      "        [0.2879],\n",
      "        [0.3928],\n",
      "        [0.3196],\n",
      "        [0.3351],\n",
      "        [0.1395],\n",
      "        [0.4086],\n",
      "        [0.1748],\n",
      "        [0.3073],\n",
      "        [0.5724],\n",
      "        [0.6046],\n",
      "        [0.3975],\n",
      "        [0.3011],\n",
      "        [0.3574],\n",
      "        [0.3001],\n",
      "        [0.3282],\n",
      "        [0.3094],\n",
      "        [0.4698],\n",
      "        [0.4698],\n",
      "        [0.4698],\n",
      "        [0.4698],\n",
      "        [0.4698],\n",
      "        [0.4698],\n",
      "        [0.4698],\n",
      "        [0.4698]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0062],\n",
      "        [0.0559],\n",
      "        [0.0109],\n",
      "        [0.0294],\n",
      "        [0.0305],\n",
      "        [0.0068],\n",
      "        [0.0133],\n",
      "        [0.0133],\n",
      "        [0.0201],\n",
      "        [0.0068],\n",
      "        [0.0075],\n",
      "        [0.0978],\n",
      "        [0.0011],\n",
      "        [0.0148],\n",
      "        [0.0193],\n",
      "        [0.0901],\n",
      "        [0.0081],\n",
      "        [0.0161],\n",
      "        [0.0048],\n",
      "        [0.0227],\n",
      "        [0.0237],\n",
      "        [0.0784],\n",
      "        [0.0118],\n",
      "        [0.0048],\n",
      "        [0.0045],\n",
      "        [0.0245],\n",
      "        [0.0032],\n",
      "        [0.0386],\n",
      "        [0.0040],\n",
      "        [0.0227],\n",
      "        [0.0090],\n",
      "        [0.0305],\n",
      "        [0.0773],\n",
      "        [0.0378],\n",
      "        [0.0186],\n",
      "        [0.0379],\n",
      "        [0.0210],\n",
      "        [0.0277],\n",
      "        [0.0388],\n",
      "        [0.0192],\n",
      "        [0.0070],\n",
      "        [0.0226],\n",
      "        [0.0167],\n",
      "        [0.0234],\n",
      "        [0.0272],\n",
      "        [0.0235],\n",
      "        [0.0559],\n",
      "        [0.0197],\n",
      "        [0.0161],\n",
      "        [0.0399],\n",
      "        [0.0411],\n",
      "        [0.0949],\n",
      "        [0.0295],\n",
      "        [0.0480],\n",
      "        [0.0276],\n",
      "        [0.0305],\n",
      "        [0.0427],\n",
      "        [0.0136],\n",
      "        [0.1258],\n",
      "        [0.0259],\n",
      "        [0.0290],\n",
      "        [0.0477],\n",
      "        [0.0173],\n",
      "        [0.0355],\n",
      "        [0.0394],\n",
      "        [0.0152],\n",
      "        [0.0451],\n",
      "        [0.0384],\n",
      "        [0.0487],\n",
      "        [0.0378],\n",
      "        [0.0381],\n",
      "        [0.0530],\n",
      "        [0.0380],\n",
      "        [0.0356],\n",
      "        [0.0152],\n",
      "        [0.0635],\n",
      "        [0.0333],\n",
      "        [0.0476],\n",
      "        [0.0513],\n",
      "        [0.0714],\n",
      "        [0.0485],\n",
      "        [0.0631],\n",
      "        [0.0675],\n",
      "        [0.0628],\n",
      "        [0.0693],\n",
      "        [0.0530],\n",
      "        [0.0684],\n",
      "        [0.0723],\n",
      "        [0.0888],\n",
      "        [0.0460],\n",
      "        [0.0610],\n",
      "        [0.0821],\n",
      "        [0.0883],\n",
      "        [0.0877],\n",
      "        [0.0546],\n",
      "        [0.0528],\n",
      "        [0.0631],\n",
      "        [0.1136],\n",
      "        [0.0950],\n",
      "        [0.0903],\n",
      "        [0.0066],\n",
      "        [0.0929],\n",
      "        [0.1175],\n",
      "        [0.0993],\n",
      "        [0.0215],\n",
      "        [0.1079],\n",
      "        [0.1251],\n",
      "        [0.0213],\n",
      "        [0.1288],\n",
      "        [0.1160],\n",
      "        [0.1395],\n",
      "        [0.1218],\n",
      "        [0.1477],\n",
      "        [0.1292],\n",
      "        [0.1487],\n",
      "        [0.1841],\n",
      "        [0.1702],\n",
      "        [0.1080],\n",
      "        [0.1257],\n",
      "        [0.1671],\n",
      "        [0.1703],\n",
      "        [0.1863],\n",
      "        [0.3194],\n",
      "        [0.3334],\n",
      "        [0.3364],\n",
      "        [0.3383],\n",
      "        [0.3393],\n",
      "        [0.3488],\n",
      "        [0.3506],\n",
      "        [0.3518]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0512],\n",
      "        [0.0175],\n",
      "        [0.0353],\n",
      "        [0.0219],\n",
      "        [0.0005],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0121],\n",
      "        [0.0039],\n",
      "        [0.0136],\n",
      "        [0.0961],\n",
      "        [0.0064],\n",
      "        [0.0098],\n",
      "        [0.0123],\n",
      "        [0.0873],\n",
      "        [0.0034],\n",
      "        [0.0089],\n",
      "        [0.0103],\n",
      "        [0.0262],\n",
      "        [0.0201],\n",
      "        [0.0741],\n",
      "        [0.0025],\n",
      "        [0.0090],\n",
      "        [0.0043],\n",
      "        [0.0195],\n",
      "        [0.0103],\n",
      "        [0.0324],\n",
      "        [0.0062],\n",
      "        [0.0297],\n",
      "        [0.0169],\n",
      "        [0.0401],\n",
      "        [0.0701],\n",
      "        [0.0320],\n",
      "        [0.0133],\n",
      "        [0.0319],\n",
      "        [0.0239],\n",
      "        [0.0209],\n",
      "        [0.0346],\n",
      "        [0.0272],\n",
      "        [0.0078],\n",
      "        [0.0178],\n",
      "        [0.0167],\n",
      "        [0.0319],\n",
      "        [0.0225],\n",
      "        [0.0304],\n",
      "        [0.0475],\n",
      "        [0.0211],\n",
      "        [0.0186],\n",
      "        [0.0388],\n",
      "        [0.0331],\n",
      "        [0.0918],\n",
      "        [0.0352],\n",
      "        [0.0560],\n",
      "        [0.0233],\n",
      "        [0.0269],\n",
      "        [0.0501],\n",
      "        [0.0158],\n",
      "        [0.1246],\n",
      "        [0.0218],\n",
      "        [0.0306],\n",
      "        [0.0442],\n",
      "        [0.0129],\n",
      "        [0.0282],\n",
      "        [0.0483],\n",
      "        [0.0100],\n",
      "        [0.0427],\n",
      "        [0.0406],\n",
      "        [0.0511],\n",
      "        [0.0400],\n",
      "        [0.0401],\n",
      "        [0.0547],\n",
      "        [0.0394],\n",
      "        [0.0355],\n",
      "        [0.0110],\n",
      "        [0.0644],\n",
      "        [0.0372],\n",
      "        [0.0380],\n",
      "        [0.0559],\n",
      "        [0.0684],\n",
      "        [0.0576],\n",
      "        [0.0660],\n",
      "        [0.0684],\n",
      "        [0.0702],\n",
      "        [0.0783],\n",
      "        [0.0595],\n",
      "        [0.0728],\n",
      "        [0.0646],\n",
      "        [0.0808],\n",
      "        [0.0461],\n",
      "        [0.0623],\n",
      "        [0.0761],\n",
      "        [0.0962],\n",
      "        [0.0802],\n",
      "        [0.0523],\n",
      "        [0.0499],\n",
      "        [0.0626],\n",
      "        [0.1224],\n",
      "        [0.0931],\n",
      "        [0.0840],\n",
      "        [0.0026],\n",
      "        [0.0946],\n",
      "        [0.1249],\n",
      "        [0.1039],\n",
      "        [0.0218],\n",
      "        [0.1093],\n",
      "        [0.1174],\n",
      "        [0.0160],\n",
      "        [0.1224],\n",
      "        [0.1170],\n",
      "        [0.1324],\n",
      "        [0.1171],\n",
      "        [0.1372],\n",
      "        [0.1308],\n",
      "        [0.1415],\n",
      "        [0.1785],\n",
      "        [0.1641],\n",
      "        [0.1055],\n",
      "        [0.1196],\n",
      "        [0.1643],\n",
      "        [0.1666],\n",
      "        [0.1829],\n",
      "        [0.3187],\n",
      "        [0.3327],\n",
      "        [0.3357],\n",
      "        [0.3376],\n",
      "        [0.3386],\n",
      "        [0.3481],\n",
      "        [0.3499],\n",
      "        [0.3511]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 81.90980863571167\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 131\n",
      "剩餘X 資料 torch.Size([29, 18])\n",
      "剩餘Y 資料 torch.Size([29, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12846311926841736, 15)\n",
      "The second_loss value of k: (0.12975797057151794, 18)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.1107])\n",
      "目前模型的Data狀態 torch.Size([131, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6295],\n",
      "        [0.1419],\n",
      "        [0.5191],\n",
      "        [0.6942],\n",
      "        [0.2398],\n",
      "        [0.2331],\n",
      "        [0.6347],\n",
      "        [0.1985],\n",
      "        [0.6427],\n",
      "        [0.6147],\n",
      "        [0.6303],\n",
      "        [0.2717],\n",
      "        [0.6152],\n",
      "        [0.6005],\n",
      "        [0.6193],\n",
      "        [0.2056],\n",
      "        [0.5439],\n",
      "        [0.6255],\n",
      "        [0.6230],\n",
      "        [0.4143],\n",
      "        [0.6273],\n",
      "        [0.1647],\n",
      "        [0.4978],\n",
      "        [0.5798],\n",
      "        [0.6458],\n",
      "        [0.2817],\n",
      "        [0.1699],\n",
      "        [0.1520],\n",
      "        [0.6302],\n",
      "        [0.4412],\n",
      "        [0.5682],\n",
      "        [0.4713],\n",
      "        [0.1633],\n",
      "        [0.6127],\n",
      "        [0.5744],\n",
      "        [0.2236],\n",
      "        [0.4501],\n",
      "        [0.1295],\n",
      "        [0.6015],\n",
      "        [0.6496],\n",
      "        [0.6376],\n",
      "        [0.5736],\n",
      "        [0.3376],\n",
      "        [0.5195],\n",
      "        [0.5814],\n",
      "        [0.6125],\n",
      "        [0.2367],\n",
      "        [0.3369],\n",
      "        [0.6174],\n",
      "        [0.3641],\n",
      "        [0.2172],\n",
      "        [0.1727],\n",
      "        [0.2193],\n",
      "        [0.2350],\n",
      "        [0.5712],\n",
      "        [0.5533],\n",
      "        [0.4839],\n",
      "        [0.6029],\n",
      "        [0.2689],\n",
      "        [0.1601],\n",
      "        [0.3273],\n",
      "        [0.3152],\n",
      "        [0.1455],\n",
      "        [0.2956],\n",
      "        [0.6234],\n",
      "        [0.1431],\n",
      "        [0.1608],\n",
      "        [0.2665],\n",
      "        [0.1464],\n",
      "        [0.3185],\n",
      "        [0.3186],\n",
      "        [0.4172],\n",
      "        [0.3241],\n",
      "        [0.3289],\n",
      "        [0.6685],\n",
      "        [0.3786],\n",
      "        [0.5799],\n",
      "        [0.4949],\n",
      "        [0.4314],\n",
      "        [0.6321],\n",
      "        [0.5529],\n",
      "        [0.3985],\n",
      "        [0.3798],\n",
      "        [0.4528],\n",
      "        [0.4482],\n",
      "        [0.5643],\n",
      "        [0.2350],\n",
      "        [0.2455],\n",
      "        [0.2398],\n",
      "        [0.6259],\n",
      "        [0.3288],\n",
      "        [0.2962],\n",
      "        [0.4368],\n",
      "        [0.2678],\n",
      "        [0.6384],\n",
      "        [0.6467],\n",
      "        [0.6266],\n",
      "        [0.1803],\n",
      "        [0.4195],\n",
      "        [0.3489],\n",
      "        [0.3971],\n",
      "        [0.3044],\n",
      "        [0.1943],\n",
      "        [0.0582],\n",
      "        [0.2879],\n",
      "        [0.3429],\n",
      "        [0.2801],\n",
      "        [0.3982],\n",
      "        [0.3133],\n",
      "        [0.3341],\n",
      "        [0.1324],\n",
      "        [0.4039],\n",
      "        [0.1643],\n",
      "        [0.3057],\n",
      "        [0.5652],\n",
      "        [0.5990],\n",
      "        [0.3914],\n",
      "        [0.3036],\n",
      "        [0.3635],\n",
      "        [0.3028],\n",
      "        [0.3319],\n",
      "        [0.3129],\n",
      "        [0.4691],\n",
      "        [0.4691],\n",
      "        [0.4691],\n",
      "        [0.4691],\n",
      "        [0.4691],\n",
      "        [0.4691],\n",
      "        [0.4691],\n",
      "        [0.4691],\n",
      "        [0.4691]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0017],\n",
      "        [0.0512],\n",
      "        [0.0175],\n",
      "        [0.0353],\n",
      "        [0.0219],\n",
      "        [0.0005],\n",
      "        [0.0061],\n",
      "        [0.0064],\n",
      "        [0.0121],\n",
      "        [0.0039],\n",
      "        [0.0136],\n",
      "        [0.0961],\n",
      "        [0.0064],\n",
      "        [0.0098],\n",
      "        [0.0123],\n",
      "        [0.0873],\n",
      "        [0.0034],\n",
      "        [0.0089],\n",
      "        [0.0103],\n",
      "        [0.0262],\n",
      "        [0.0201],\n",
      "        [0.0741],\n",
      "        [0.0025],\n",
      "        [0.0090],\n",
      "        [0.0043],\n",
      "        [0.0195],\n",
      "        [0.0103],\n",
      "        [0.0324],\n",
      "        [0.0062],\n",
      "        [0.0297],\n",
      "        [0.0169],\n",
      "        [0.0401],\n",
      "        [0.0701],\n",
      "        [0.0320],\n",
      "        [0.0133],\n",
      "        [0.0319],\n",
      "        [0.0239],\n",
      "        [0.0209],\n",
      "        [0.0346],\n",
      "        [0.0272],\n",
      "        [0.0078],\n",
      "        [0.0178],\n",
      "        [0.0167],\n",
      "        [0.0319],\n",
      "        [0.0225],\n",
      "        [0.0304],\n",
      "        [0.0475],\n",
      "        [0.0211],\n",
      "        [0.0186],\n",
      "        [0.0388],\n",
      "        [0.0331],\n",
      "        [0.0918],\n",
      "        [0.0352],\n",
      "        [0.0560],\n",
      "        [0.0233],\n",
      "        [0.0269],\n",
      "        [0.0501],\n",
      "        [0.0158],\n",
      "        [0.1246],\n",
      "        [0.0218],\n",
      "        [0.0306],\n",
      "        [0.0442],\n",
      "        [0.0129],\n",
      "        [0.0282],\n",
      "        [0.0483],\n",
      "        [0.0100],\n",
      "        [0.0427],\n",
      "        [0.0406],\n",
      "        [0.0511],\n",
      "        [0.0400],\n",
      "        [0.0401],\n",
      "        [0.0547],\n",
      "        [0.0394],\n",
      "        [0.0355],\n",
      "        [0.0110],\n",
      "        [0.0644],\n",
      "        [0.0372],\n",
      "        [0.0380],\n",
      "        [0.0559],\n",
      "        [0.0684],\n",
      "        [0.0576],\n",
      "        [0.0660],\n",
      "        [0.0684],\n",
      "        [0.0702],\n",
      "        [0.0783],\n",
      "        [0.0595],\n",
      "        [0.0728],\n",
      "        [0.0646],\n",
      "        [0.0808],\n",
      "        [0.0461],\n",
      "        [0.0623],\n",
      "        [0.0761],\n",
      "        [0.0962],\n",
      "        [0.0802],\n",
      "        [0.0523],\n",
      "        [0.0499],\n",
      "        [0.0626],\n",
      "        [0.1224],\n",
      "        [0.0931],\n",
      "        [0.0840],\n",
      "        [0.0026],\n",
      "        [0.0946],\n",
      "        [0.1249],\n",
      "        [0.1039],\n",
      "        [0.0218],\n",
      "        [0.1093],\n",
      "        [0.1174],\n",
      "        [0.0160],\n",
      "        [0.1224],\n",
      "        [0.1170],\n",
      "        [0.1324],\n",
      "        [0.1171],\n",
      "        [0.1372],\n",
      "        [0.1308],\n",
      "        [0.1415],\n",
      "        [0.1785],\n",
      "        [0.1641],\n",
      "        [0.1055],\n",
      "        [0.1196],\n",
      "        [0.1643],\n",
      "        [0.1666],\n",
      "        [0.1829],\n",
      "        [0.3187],\n",
      "        [0.3327],\n",
      "        [0.3357],\n",
      "        [0.3376],\n",
      "        [0.3386],\n",
      "        [0.3481],\n",
      "        [0.3499],\n",
      "        [0.3511],\n",
      "        [0.3584]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0032],\n",
      "        [0.0489],\n",
      "        [0.0179],\n",
      "        [0.0429],\n",
      "        [0.0161],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0021],\n",
      "        [0.0101],\n",
      "        [0.0060],\n",
      "        [0.0135],\n",
      "        [0.0954],\n",
      "        [0.0059],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0861],\n",
      "        [0.0046],\n",
      "        [0.0081],\n",
      "        [0.0101],\n",
      "        [0.0245],\n",
      "        [0.0224],\n",
      "        [0.0720],\n",
      "        [0.0017],\n",
      "        [0.0093],\n",
      "        [0.0072],\n",
      "        [0.0174],\n",
      "        [0.0146],\n",
      "        [0.0288],\n",
      "        [0.0047],\n",
      "        [0.0312],\n",
      "        [0.0196],\n",
      "        [0.0443],\n",
      "        [0.0656],\n",
      "        [0.0317],\n",
      "        [0.0142],\n",
      "        [0.0287],\n",
      "        [0.0219],\n",
      "        [0.0169],\n",
      "        [0.0345],\n",
      "        [0.0290],\n",
      "        [0.0053],\n",
      "        [0.0191],\n",
      "        [0.0145],\n",
      "        [0.0350],\n",
      "        [0.0239],\n",
      "        [0.0313],\n",
      "        [0.0420],\n",
      "        [0.0198],\n",
      "        [0.0174],\n",
      "        [0.0403],\n",
      "        [0.0284],\n",
      "        [0.0908],\n",
      "        [0.0377],\n",
      "        [0.0591],\n",
      "        [0.0252],\n",
      "        [0.0291],\n",
      "        [0.0514],\n",
      "        [0.0135],\n",
      "        [0.1244],\n",
      "        [0.0199],\n",
      "        [0.0290],\n",
      "        [0.0433],\n",
      "        [0.0110],\n",
      "        [0.0259],\n",
      "        [0.0509],\n",
      "        [0.0072],\n",
      "        [0.0415],\n",
      "        [0.0397],\n",
      "        [0.0523],\n",
      "        [0.0394],\n",
      "        [0.0393],\n",
      "        [0.0516],\n",
      "        [0.0381],\n",
      "        [0.0334],\n",
      "        [0.0049],\n",
      "        [0.0676],\n",
      "        [0.0364],\n",
      "        [0.0335],\n",
      "        [0.0553],\n",
      "        [0.0704],\n",
      "        [0.0612],\n",
      "        [0.0640],\n",
      "        [0.0716],\n",
      "        [0.0718],\n",
      "        [0.0819],\n",
      "        [0.0611],\n",
      "        [0.0734],\n",
      "        [0.0603],\n",
      "        [0.0757],\n",
      "        [0.0434],\n",
      "        [0.0612],\n",
      "        [0.0733],\n",
      "        [0.0981],\n",
      "        [0.0760],\n",
      "        [0.0478],\n",
      "        [0.0450],\n",
      "        [0.0596],\n",
      "        [0.1263],\n",
      "        [0.0962],\n",
      "        [0.0827],\n",
      "        [0.0022],\n",
      "        [0.0939],\n",
      "        [0.1279],\n",
      "        [0.1053],\n",
      "        [0.0212],\n",
      "        [0.1082],\n",
      "        [0.1131],\n",
      "        [0.0104],\n",
      "        [0.1195],\n",
      "        [0.1156],\n",
      "        [0.1288],\n",
      "        [0.1176],\n",
      "        [0.1302],\n",
      "        [0.1300],\n",
      "        [0.1389],\n",
      "        [0.1774],\n",
      "        [0.1632],\n",
      "        [0.1026],\n",
      "        [0.1136],\n",
      "        [0.1612],\n",
      "        [0.1628],\n",
      "        [0.1790],\n",
      "        [0.3181],\n",
      "        [0.3321],\n",
      "        [0.3351],\n",
      "        [0.3369],\n",
      "        [0.3380],\n",
      "        [0.3474],\n",
      "        [0.3492],\n",
      "        [0.3504],\n",
      "        [0.3578]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 82.15042114257812\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 132\n",
      "剩餘X 資料 torch.Size([28, 18])\n",
      "剩餘Y 資料 torch.Size([28, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12929975986480713, 17)\n",
      "The second_loss value of k: (0.13079705834388733, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引17，y= tensor([0.1089])\n",
      "目前模型的Data狀態 torch.Size([132, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6310],\n",
      "        [0.1396],\n",
      "        [0.5187],\n",
      "        [0.7019],\n",
      "        [0.2339],\n",
      "        [0.2287],\n",
      "        [0.6337],\n",
      "        [0.1942],\n",
      "        [0.6407],\n",
      "        [0.6168],\n",
      "        [0.6304],\n",
      "        [0.2710],\n",
      "        [0.6156],\n",
      "        [0.6016],\n",
      "        [0.6184],\n",
      "        [0.2043],\n",
      "        [0.5452],\n",
      "        [0.6246],\n",
      "        [0.6233],\n",
      "        [0.4159],\n",
      "        [0.6297],\n",
      "        [0.1626],\n",
      "        [0.4936],\n",
      "        [0.5795],\n",
      "        [0.6487],\n",
      "        [0.2796],\n",
      "        [0.1657],\n",
      "        [0.1484],\n",
      "        [0.6317],\n",
      "        [0.4398],\n",
      "        [0.5655],\n",
      "        [0.4671],\n",
      "        [0.1587],\n",
      "        [0.6123],\n",
      "        [0.5753],\n",
      "        [0.2204],\n",
      "        [0.4521],\n",
      "        [0.1256],\n",
      "        [0.6014],\n",
      "        [0.6477],\n",
      "        [0.6401],\n",
      "        [0.5750],\n",
      "        [0.3398],\n",
      "        [0.5165],\n",
      "        [0.5829],\n",
      "        [0.6117],\n",
      "        [0.2312],\n",
      "        [0.3383],\n",
      "        [0.6186],\n",
      "        [0.3656],\n",
      "        [0.2124],\n",
      "        [0.1717],\n",
      "        [0.2169],\n",
      "        [0.2319],\n",
      "        [0.5731],\n",
      "        [0.5556],\n",
      "        [0.4826],\n",
      "        [0.6052],\n",
      "        [0.2687],\n",
      "        [0.1581],\n",
      "        [0.3289],\n",
      "        [0.3143],\n",
      "        [0.1436],\n",
      "        [0.2933],\n",
      "        [0.6208],\n",
      "        [0.1403],\n",
      "        [0.1596],\n",
      "        [0.2674],\n",
      "        [0.1452],\n",
      "        [0.3191],\n",
      "        [0.3194],\n",
      "        [0.4203],\n",
      "        [0.3254],\n",
      "        [0.3311],\n",
      "        [0.6745],\n",
      "        [0.3818],\n",
      "        [0.5807],\n",
      "        [0.4904],\n",
      "        [0.4320],\n",
      "        [0.6342],\n",
      "        [0.5493],\n",
      "        [0.4005],\n",
      "        [0.3830],\n",
      "        [0.4513],\n",
      "        [0.4446],\n",
      "        [0.5627],\n",
      "        [0.2344],\n",
      "        [0.2411],\n",
      "        [0.2348],\n",
      "        [0.6287],\n",
      "        [0.3299],\n",
      "        [0.2934],\n",
      "        [0.4349],\n",
      "        [0.2636],\n",
      "        [0.6429],\n",
      "        [0.6516],\n",
      "        [0.6297],\n",
      "        [0.1764],\n",
      "        [0.4226],\n",
      "        [0.3476],\n",
      "        [0.4018],\n",
      "        [0.3051],\n",
      "        [0.1913],\n",
      "        [0.0567],\n",
      "        [0.2885],\n",
      "        [0.3440],\n",
      "        [0.2758],\n",
      "        [0.4037],\n",
      "        [0.3104],\n",
      "        [0.3355],\n",
      "        [0.1288],\n",
      "        [0.4044],\n",
      "        [0.1573],\n",
      "        [0.3065],\n",
      "        [0.5626],\n",
      "        [0.5979],\n",
      "        [0.3906],\n",
      "        [0.3066],\n",
      "        [0.3695],\n",
      "        [0.3060],\n",
      "        [0.3357],\n",
      "        [0.3167],\n",
      "        [0.4684],\n",
      "        [0.4684],\n",
      "        [0.4684],\n",
      "        [0.4684],\n",
      "        [0.4684],\n",
      "        [0.4684],\n",
      "        [0.4684],\n",
      "        [0.4684],\n",
      "        [0.4684],\n",
      "        [0.4684]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0032],\n",
      "        [0.0489],\n",
      "        [0.0179],\n",
      "        [0.0429],\n",
      "        [0.0161],\n",
      "        [0.0049],\n",
      "        [0.0050],\n",
      "        [0.0021],\n",
      "        [0.0101],\n",
      "        [0.0060],\n",
      "        [0.0135],\n",
      "        [0.0954],\n",
      "        [0.0059],\n",
      "        [0.0110],\n",
      "        [0.0114],\n",
      "        [0.0861],\n",
      "        [0.0046],\n",
      "        [0.0081],\n",
      "        [0.0101],\n",
      "        [0.0245],\n",
      "        [0.0224],\n",
      "        [0.0720],\n",
      "        [0.0017],\n",
      "        [0.0093],\n",
      "        [0.0072],\n",
      "        [0.0174],\n",
      "        [0.0146],\n",
      "        [0.0288],\n",
      "        [0.0047],\n",
      "        [0.0312],\n",
      "        [0.0196],\n",
      "        [0.0443],\n",
      "        [0.0656],\n",
      "        [0.0317],\n",
      "        [0.0142],\n",
      "        [0.0287],\n",
      "        [0.0219],\n",
      "        [0.0169],\n",
      "        [0.0345],\n",
      "        [0.0290],\n",
      "        [0.0053],\n",
      "        [0.0191],\n",
      "        [0.0145],\n",
      "        [0.0350],\n",
      "        [0.0239],\n",
      "        [0.0313],\n",
      "        [0.0420],\n",
      "        [0.0198],\n",
      "        [0.0174],\n",
      "        [0.0403],\n",
      "        [0.0284],\n",
      "        [0.0908],\n",
      "        [0.0377],\n",
      "        [0.0591],\n",
      "        [0.0252],\n",
      "        [0.0291],\n",
      "        [0.0514],\n",
      "        [0.0135],\n",
      "        [0.1244],\n",
      "        [0.0199],\n",
      "        [0.0290],\n",
      "        [0.0433],\n",
      "        [0.0110],\n",
      "        [0.0259],\n",
      "        [0.0509],\n",
      "        [0.0072],\n",
      "        [0.0415],\n",
      "        [0.0397],\n",
      "        [0.0523],\n",
      "        [0.0394],\n",
      "        [0.0393],\n",
      "        [0.0516],\n",
      "        [0.0381],\n",
      "        [0.0334],\n",
      "        [0.0049],\n",
      "        [0.0676],\n",
      "        [0.0364],\n",
      "        [0.0335],\n",
      "        [0.0553],\n",
      "        [0.0704],\n",
      "        [0.0612],\n",
      "        [0.0640],\n",
      "        [0.0716],\n",
      "        [0.0718],\n",
      "        [0.0819],\n",
      "        [0.0611],\n",
      "        [0.0734],\n",
      "        [0.0603],\n",
      "        [0.0757],\n",
      "        [0.0434],\n",
      "        [0.0612],\n",
      "        [0.0733],\n",
      "        [0.0981],\n",
      "        [0.0760],\n",
      "        [0.0478],\n",
      "        [0.0450],\n",
      "        [0.0596],\n",
      "        [0.1263],\n",
      "        [0.0962],\n",
      "        [0.0827],\n",
      "        [0.0022],\n",
      "        [0.0939],\n",
      "        [0.1279],\n",
      "        [0.1053],\n",
      "        [0.0212],\n",
      "        [0.1082],\n",
      "        [0.1131],\n",
      "        [0.0104],\n",
      "        [0.1195],\n",
      "        [0.1156],\n",
      "        [0.1288],\n",
      "        [0.1176],\n",
      "        [0.1302],\n",
      "        [0.1300],\n",
      "        [0.1389],\n",
      "        [0.1774],\n",
      "        [0.1632],\n",
      "        [0.1026],\n",
      "        [0.1136],\n",
      "        [0.1612],\n",
      "        [0.1628],\n",
      "        [0.1790],\n",
      "        [0.3181],\n",
      "        [0.3321],\n",
      "        [0.3351],\n",
      "        [0.3369],\n",
      "        [0.3380],\n",
      "        [0.3474],\n",
      "        [0.3492],\n",
      "        [0.3504],\n",
      "        [0.3578],\n",
      "        [0.3596]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0085],\n",
      "        [    0.0500],\n",
      "        [    0.0145],\n",
      "        [    0.0529],\n",
      "        [    0.0144],\n",
      "        [    0.0054],\n",
      "        [    0.0080],\n",
      "        [    0.0018],\n",
      "        [    0.0126],\n",
      "        [    0.0118],\n",
      "        [    0.0094],\n",
      "        [    0.0983],\n",
      "        [    0.0016],\n",
      "        [    0.0159],\n",
      "        [    0.0146],\n",
      "        [    0.0884],\n",
      "        [    0.0096],\n",
      "        [    0.0114],\n",
      "        [    0.0060],\n",
      "        [    0.0197],\n",
      "        [    0.0285],\n",
      "        [    0.0734],\n",
      "        [    0.0019],\n",
      "        [    0.0063],\n",
      "        [    0.0131],\n",
      "        [    0.0188],\n",
      "        [    0.0149],\n",
      "        [    0.0288],\n",
      "        [    0.0001],\n",
      "        [    0.0290],\n",
      "        [    0.0180],\n",
      "        [    0.0443],\n",
      "        [    0.0649],\n",
      "        [    0.0355],\n",
      "        [    0.0189],\n",
      "        [    0.0293],\n",
      "        [    0.0169],\n",
      "        [    0.0166],\n",
      "        [    0.0378],\n",
      "        [    0.0264],\n",
      "        [    0.0002],\n",
      "        [    0.0243],\n",
      "        [    0.0096],\n",
      "        [    0.0337],\n",
      "        [    0.0290],\n",
      "        [    0.0278],\n",
      "        [    0.0407],\n",
      "        [    0.0152],\n",
      "        [    0.0130],\n",
      "        [    0.0448],\n",
      "        [    0.0276],\n",
      "        [    0.0931],\n",
      "        [    0.0367],\n",
      "        [    0.0585],\n",
      "        [    0.0308],\n",
      "        [    0.0351],\n",
      "        [    0.0487],\n",
      "        [    0.0077],\n",
      "        [    0.1278],\n",
      "        [    0.0215],\n",
      "        [    0.0243],\n",
      "        [    0.0457],\n",
      "        [    0.0125],\n",
      "        [    0.0272],\n",
      "        [    0.0489],\n",
      "        [    0.0079],\n",
      "        [    0.0416],\n",
      "        [    0.0357],\n",
      "        [    0.0522],\n",
      "        [    0.0356],\n",
      "        [    0.0354],\n",
      "        [    0.0459],\n",
      "        [    0.0337],\n",
      "        [    0.0286],\n",
      "        [    0.0037],\n",
      "        [    0.0735],\n",
      "        [    0.0321],\n",
      "        [    0.0330],\n",
      "        [    0.0515],\n",
      "        [    0.0763],\n",
      "        [    0.0604],\n",
      "        [    0.0591],\n",
      "        [    0.0777],\n",
      "        [    0.0695],\n",
      "        [    0.0814],\n",
      "        [    0.0587],\n",
      "        [    0.0707],\n",
      "        [    0.0599],\n",
      "        [    0.0747],\n",
      "        [    0.0377],\n",
      "        [    0.0569],\n",
      "        [    0.0742],\n",
      "        [    0.0961],\n",
      "        [    0.0756],\n",
      "        [    0.0407],\n",
      "        [    0.0373],\n",
      "        [    0.0536],\n",
      "        [    0.1263],\n",
      "        [    0.1023],\n",
      "        [    0.0849],\n",
      "        [    0.0096],\n",
      "        [    0.0903],\n",
      "        [    0.1273],\n",
      "        [    0.1036],\n",
      "        [    0.0171],\n",
      "        [    0.1039],\n",
      "        [    0.1127],\n",
      "        [    0.0025],\n",
      "        [    0.1203],\n",
      "        [    0.1110],\n",
      "        [    0.1286],\n",
      "        [    0.1215],\n",
      "        [    0.1273],\n",
      "        [    0.1263],\n",
      "        [    0.1401],\n",
      "        [    0.1800],\n",
      "        [    0.1659],\n",
      "        [    0.0964],\n",
      "        [    0.1052],\n",
      "        [    0.1549],\n",
      "        [    0.1561],\n",
      "        [    0.1722],\n",
      "        [    0.3175],\n",
      "        [    0.3315],\n",
      "        [    0.3345],\n",
      "        [    0.3364],\n",
      "        [    0.3374],\n",
      "        [    0.3469],\n",
      "        [    0.3487],\n",
      "        [    0.3498],\n",
      "        [    0.3572],\n",
      "        [    0.3590]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 82.39411282539368\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 133\n",
      "剩餘X 資料 torch.Size([27, 18])\n",
      "剩餘Y 資料 torch.Size([27, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1303863823413849, 14)\n",
      "The second_loss value of k: (0.13115127384662628, 25)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.1068])\n",
      "目前模型的Data狀態 torch.Size([133, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6363],\n",
      "        [0.1408],\n",
      "        [0.5221],\n",
      "        [0.7119],\n",
      "        [0.2323],\n",
      "        [0.2282],\n",
      "        [0.6367],\n",
      "        [0.1939],\n",
      "        [0.6432],\n",
      "        [0.6225],\n",
      "        [0.6346],\n",
      "        [0.2739],\n",
      "        [0.6200],\n",
      "        [0.6066],\n",
      "        [0.6216],\n",
      "        [0.2067],\n",
      "        [0.5502],\n",
      "        [0.6280],\n",
      "        [0.6273],\n",
      "        [0.4207],\n",
      "        [0.6357],\n",
      "        [0.1640],\n",
      "        [0.4935],\n",
      "        [0.5825],\n",
      "        [0.6546],\n",
      "        [0.2810],\n",
      "        [0.1653],\n",
      "        [0.1484],\n",
      "        [0.6363],\n",
      "        [0.4419],\n",
      "        [0.5671],\n",
      "        [0.4671],\n",
      "        [0.1580],\n",
      "        [0.6162],\n",
      "        [0.5800],\n",
      "        [0.2210],\n",
      "        [0.4570],\n",
      "        [0.1252],\n",
      "        [0.6047],\n",
      "        [0.6504],\n",
      "        [0.6456],\n",
      "        [0.5801],\n",
      "        [0.3447],\n",
      "        [0.5177],\n",
      "        [0.5880],\n",
      "        [0.6152],\n",
      "        [0.2299],\n",
      "        [0.3428],\n",
      "        [0.6230],\n",
      "        [0.3701],\n",
      "        [0.2116],\n",
      "        [0.1740],\n",
      "        [0.2179],\n",
      "        [0.2325],\n",
      "        [0.5788],\n",
      "        [0.5615],\n",
      "        [0.4853],\n",
      "        [0.6110],\n",
      "        [0.2721],\n",
      "        [0.1597],\n",
      "        [0.3336],\n",
      "        [0.3167],\n",
      "        [0.1451],\n",
      "        [0.2947],\n",
      "        [0.6228],\n",
      "        [0.1411],\n",
      "        [0.1597],\n",
      "        [0.2714],\n",
      "        [0.1454],\n",
      "        [0.3229],\n",
      "        [0.3234],\n",
      "        [0.4260],\n",
      "        [0.3297],\n",
      "        [0.3358],\n",
      "        [0.6832],\n",
      "        [0.3877],\n",
      "        [0.5850],\n",
      "        [0.4898],\n",
      "        [0.4357],\n",
      "        [0.6400],\n",
      "        [0.5501],\n",
      "        [0.4054],\n",
      "        [0.3891],\n",
      "        [0.4535],\n",
      "        [0.4451],\n",
      "        [0.5651],\n",
      "        [0.2371],\n",
      "        [0.2407],\n",
      "        [0.2338],\n",
      "        [0.6344],\n",
      "        [0.3342],\n",
      "        [0.2943],\n",
      "        [0.4368],\n",
      "        [0.2632],\n",
      "        [0.6501],\n",
      "        [0.6593],\n",
      "        [0.6357],\n",
      "        [0.1764],\n",
      "        [0.4287],\n",
      "        [0.3499],\n",
      "        [0.4092],\n",
      "        [0.3086],\n",
      "        [0.1919],\n",
      "        [0.0585],\n",
      "        [0.2926],\n",
      "        [0.3482],\n",
      "        [0.2755],\n",
      "        [0.4117],\n",
      "        [0.3112],\n",
      "        [0.3400],\n",
      "        [0.1286],\n",
      "        [0.4083],\n",
      "        [0.1544],\n",
      "        [0.3102],\n",
      "        [0.5638],\n",
      "        [0.6004],\n",
      "        [0.3932],\n",
      "        [0.3127],\n",
      "        [0.3779],\n",
      "        [0.3123],\n",
      "        [0.3424],\n",
      "        [0.3236],\n",
      "        [0.4679],\n",
      "        [0.4679],\n",
      "        [0.4679],\n",
      "        [0.4679],\n",
      "        [0.4679],\n",
      "        [0.4679],\n",
      "        [0.4679],\n",
      "        [0.4679],\n",
      "        [0.4679],\n",
      "        [0.4679],\n",
      "        [0.4679]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0085],\n",
      "        [    0.0500],\n",
      "        [    0.0145],\n",
      "        [    0.0529],\n",
      "        [    0.0144],\n",
      "        [    0.0054],\n",
      "        [    0.0080],\n",
      "        [    0.0018],\n",
      "        [    0.0126],\n",
      "        [    0.0118],\n",
      "        [    0.0094],\n",
      "        [    0.0983],\n",
      "        [    0.0016],\n",
      "        [    0.0159],\n",
      "        [    0.0146],\n",
      "        [    0.0884],\n",
      "        [    0.0096],\n",
      "        [    0.0114],\n",
      "        [    0.0060],\n",
      "        [    0.0197],\n",
      "        [    0.0285],\n",
      "        [    0.0734],\n",
      "        [    0.0019],\n",
      "        [    0.0063],\n",
      "        [    0.0131],\n",
      "        [    0.0188],\n",
      "        [    0.0149],\n",
      "        [    0.0288],\n",
      "        [    0.0001],\n",
      "        [    0.0290],\n",
      "        [    0.0180],\n",
      "        [    0.0443],\n",
      "        [    0.0649],\n",
      "        [    0.0355],\n",
      "        [    0.0189],\n",
      "        [    0.0293],\n",
      "        [    0.0169],\n",
      "        [    0.0166],\n",
      "        [    0.0378],\n",
      "        [    0.0264],\n",
      "        [    0.0002],\n",
      "        [    0.0243],\n",
      "        [    0.0096],\n",
      "        [    0.0337],\n",
      "        [    0.0290],\n",
      "        [    0.0278],\n",
      "        [    0.0407],\n",
      "        [    0.0152],\n",
      "        [    0.0130],\n",
      "        [    0.0448],\n",
      "        [    0.0276],\n",
      "        [    0.0931],\n",
      "        [    0.0367],\n",
      "        [    0.0585],\n",
      "        [    0.0308],\n",
      "        [    0.0351],\n",
      "        [    0.0487],\n",
      "        [    0.0077],\n",
      "        [    0.1278],\n",
      "        [    0.0215],\n",
      "        [    0.0243],\n",
      "        [    0.0457],\n",
      "        [    0.0125],\n",
      "        [    0.0272],\n",
      "        [    0.0489],\n",
      "        [    0.0079],\n",
      "        [    0.0416],\n",
      "        [    0.0357],\n",
      "        [    0.0522],\n",
      "        [    0.0356],\n",
      "        [    0.0354],\n",
      "        [    0.0459],\n",
      "        [    0.0337],\n",
      "        [    0.0286],\n",
      "        [    0.0037],\n",
      "        [    0.0735],\n",
      "        [    0.0321],\n",
      "        [    0.0330],\n",
      "        [    0.0515],\n",
      "        [    0.0763],\n",
      "        [    0.0604],\n",
      "        [    0.0591],\n",
      "        [    0.0777],\n",
      "        [    0.0695],\n",
      "        [    0.0814],\n",
      "        [    0.0587],\n",
      "        [    0.0707],\n",
      "        [    0.0599],\n",
      "        [    0.0747],\n",
      "        [    0.0377],\n",
      "        [    0.0569],\n",
      "        [    0.0742],\n",
      "        [    0.0961],\n",
      "        [    0.0756],\n",
      "        [    0.0407],\n",
      "        [    0.0373],\n",
      "        [    0.0536],\n",
      "        [    0.1263],\n",
      "        [    0.1023],\n",
      "        [    0.0849],\n",
      "        [    0.0096],\n",
      "        [    0.0903],\n",
      "        [    0.1273],\n",
      "        [    0.1036],\n",
      "        [    0.0171],\n",
      "        [    0.1039],\n",
      "        [    0.1127],\n",
      "        [    0.0025],\n",
      "        [    0.1203],\n",
      "        [    0.1110],\n",
      "        [    0.1286],\n",
      "        [    0.1215],\n",
      "        [    0.1273],\n",
      "        [    0.1263],\n",
      "        [    0.1401],\n",
      "        [    0.1800],\n",
      "        [    0.1659],\n",
      "        [    0.0964],\n",
      "        [    0.1052],\n",
      "        [    0.1549],\n",
      "        [    0.1561],\n",
      "        [    0.1722],\n",
      "        [    0.3175],\n",
      "        [    0.3315],\n",
      "        [    0.3345],\n",
      "        [    0.3364],\n",
      "        [    0.3374],\n",
      "        [    0.3469],\n",
      "        [    0.3487],\n",
      "        [    0.3498],\n",
      "        [    0.3572],\n",
      "        [    0.3590],\n",
      "        [    0.3611]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0070],\n",
      "        [0.0467],\n",
      "        [0.0170],\n",
      "        [0.0556],\n",
      "        [0.0093],\n",
      "        [0.0091],\n",
      "        [0.0046],\n",
      "        [0.0021],\n",
      "        [0.0091],\n",
      "        [0.0113],\n",
      "        [0.0119],\n",
      "        [0.0966],\n",
      "        [0.0034],\n",
      "        [0.0142],\n",
      "        [0.0111],\n",
      "        [0.0862],\n",
      "        [0.0085],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0206],\n",
      "        [0.0277],\n",
      "        [0.0703],\n",
      "        [0.0079],\n",
      "        [0.0098],\n",
      "        [0.0122],\n",
      "        [0.0157],\n",
      "        [0.0187],\n",
      "        [0.0246],\n",
      "        [0.0022],\n",
      "        [0.0325],\n",
      "        [0.0219],\n",
      "        [0.0501],\n",
      "        [0.0599],\n",
      "        [0.0331],\n",
      "        [0.0171],\n",
      "        [0.0262],\n",
      "        [0.0181],\n",
      "        [0.0120],\n",
      "        [0.0346],\n",
      "        [0.0297],\n",
      "        [0.0010],\n",
      "        [0.0228],\n",
      "        [0.0097],\n",
      "        [0.0381],\n",
      "        [0.0275],\n",
      "        [0.0304],\n",
      "        [0.0360],\n",
      "        [0.0150],\n",
      "        [0.0154],\n",
      "        [0.0442],\n",
      "        [0.0232],\n",
      "        [0.0908],\n",
      "        [0.0379],\n",
      "        [0.0621],\n",
      "        [0.0298],\n",
      "        [0.0345],\n",
      "        [0.0517],\n",
      "        [0.0083],\n",
      "        [0.1265],\n",
      "        [0.0187],\n",
      "        [0.0239],\n",
      "        [0.0434],\n",
      "        [0.0096],\n",
      "        [0.0239],\n",
      "        [0.0528],\n",
      "        [0.0044],\n",
      "        [0.0410],\n",
      "        [0.0357],\n",
      "        [0.0526],\n",
      "        [0.0361],\n",
      "        [0.0357],\n",
      "        [0.0461],\n",
      "        [0.0338],\n",
      "        [0.0287],\n",
      "        [0.0053],\n",
      "        [0.0740],\n",
      "        [0.0337],\n",
      "        [0.0267],\n",
      "        [0.0537],\n",
      "        [0.0757],\n",
      "        [0.0651],\n",
      "        [0.0600],\n",
      "        [0.0783],\n",
      "        [0.0730],\n",
      "        [0.0865],\n",
      "        [0.0620],\n",
      "        [0.0720],\n",
      "        [0.0556],\n",
      "        [0.0705],\n",
      "        [0.0386],\n",
      "        [0.0568],\n",
      "        [0.0708],\n",
      "        [0.0998],\n",
      "        [0.0714],\n",
      "        [0.0403],\n",
      "        [0.0366],\n",
      "        [0.0543],\n",
      "        [0.1301],\n",
      "        [0.1027],\n",
      "        [0.0823],\n",
      "        [0.0103],\n",
      "        [0.0913],\n",
      "        [0.1304],\n",
      "        [0.1043],\n",
      "        [0.0179],\n",
      "        [0.1042],\n",
      "        [0.1084],\n",
      "        [0.0009],\n",
      "        [0.1169],\n",
      "        [0.1108],\n",
      "        [0.1262],\n",
      "        [0.1201],\n",
      "        [0.1213],\n",
      "        [0.1271],\n",
      "        [0.1352],\n",
      "        [0.1761],\n",
      "        [0.1634],\n",
      "        [0.0954],\n",
      "        [0.1027],\n",
      "        [0.1538],\n",
      "        [0.1547],\n",
      "        [0.1705],\n",
      "        [0.3170],\n",
      "        [0.3310],\n",
      "        [0.3340],\n",
      "        [0.3359],\n",
      "        [0.3369],\n",
      "        [0.3463],\n",
      "        [0.3482],\n",
      "        [0.3493],\n",
      "        [0.3567],\n",
      "        [0.3585],\n",
      "        [0.3606]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 82.64153695106506\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 134\n",
      "剩餘X 資料 torch.Size([26, 18])\n",
      "剩餘Y 資料 torch.Size([26, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1307731717824936, 24)\n",
      "The second_loss value of k: (0.1331065148115158, 16)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引24，y= tensor([0.1057])\n",
      "目前模型的Data狀態 torch.Size([134, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6348],\n",
      "        [0.1374],\n",
      "        [0.5196],\n",
      "        [0.7146],\n",
      "        [0.2271],\n",
      "        [0.2245],\n",
      "        [0.6332],\n",
      "        [0.1900],\n",
      "        [0.6397],\n",
      "        [0.6220],\n",
      "        [0.6321],\n",
      "        [0.2722],\n",
      "        [0.6182],\n",
      "        [0.6048],\n",
      "        [0.6180],\n",
      "        [0.2045],\n",
      "        [0.5491],\n",
      "        [0.6247],\n",
      "        [0.6250],\n",
      "        [0.4198],\n",
      "        [0.6349],\n",
      "        [0.1609],\n",
      "        [0.4875],\n",
      "        [0.5790],\n",
      "        [0.6537],\n",
      "        [0.2779],\n",
      "        [0.1615],\n",
      "        [0.1442],\n",
      "        [0.6342],\n",
      "        [0.4384],\n",
      "        [0.5632],\n",
      "        [0.4612],\n",
      "        [0.1530],\n",
      "        [0.6138],\n",
      "        [0.5781],\n",
      "        [0.2179],\n",
      "        [0.4559],\n",
      "        [0.1206],\n",
      "        [0.6015],\n",
      "        [0.6470],\n",
      "        [0.6444],\n",
      "        [0.5786],\n",
      "        [0.3446],\n",
      "        [0.5133],\n",
      "        [0.5865],\n",
      "        [0.6126],\n",
      "        [0.2252],\n",
      "        [0.3430],\n",
      "        [0.6206],\n",
      "        [0.3695],\n",
      "        [0.2072],\n",
      "        [0.1717],\n",
      "        [0.2167],\n",
      "        [0.2288],\n",
      "        [0.5777],\n",
      "        [0.5609],\n",
      "        [0.4823],\n",
      "        [0.6104],\n",
      "        [0.2709],\n",
      "        [0.1570],\n",
      "        [0.3340],\n",
      "        [0.3144],\n",
      "        [0.1422],\n",
      "        [0.2914],\n",
      "        [0.6189],\n",
      "        [0.1375],\n",
      "        [0.1591],\n",
      "        [0.2714],\n",
      "        [0.1450],\n",
      "        [0.3224],\n",
      "        [0.3230],\n",
      "        [0.4258],\n",
      "        [0.3297],\n",
      "        [0.3358],\n",
      "        [0.6848],\n",
      "        [0.3882],\n",
      "        [0.5834],\n",
      "        [0.4836],\n",
      "        [0.4335],\n",
      "        [0.6395],\n",
      "        [0.5454],\n",
      "        [0.4045],\n",
      "        [0.3897],\n",
      "        [0.4501],\n",
      "        [0.4400],\n",
      "        [0.5619],\n",
      "        [0.2358],\n",
      "        [0.2364],\n",
      "        [0.2296],\n",
      "        [0.6335],\n",
      "        [0.3344],\n",
      "        [0.2909],\n",
      "        [0.4331],\n",
      "        [0.2590],\n",
      "        [0.6504],\n",
      "        [0.6600],\n",
      "        [0.6350],\n",
      "        [0.1726],\n",
      "        [0.4290],\n",
      "        [0.3473],\n",
      "        [0.4099],\n",
      "        [0.3076],\n",
      "        [0.1888],\n",
      "        [0.0578],\n",
      "        [0.2919],\n",
      "        [0.3480],\n",
      "        [0.2711],\n",
      "        [0.4133],\n",
      "        [0.3078],\n",
      "        [0.3402],\n",
      "        [0.1262],\n",
      "        [0.4069],\n",
      "        [0.1484],\n",
      "        [0.3094],\n",
      "        [0.5589],\n",
      "        [0.5966],\n",
      "        [0.3908],\n",
      "        [0.3137],\n",
      "        [0.3804],\n",
      "        [0.3134],\n",
      "        [0.3438],\n",
      "        [0.3252],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673],\n",
      "        [0.4673]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0070],\n",
      "        [0.0467],\n",
      "        [0.0170],\n",
      "        [0.0556],\n",
      "        [0.0093],\n",
      "        [0.0091],\n",
      "        [0.0046],\n",
      "        [0.0021],\n",
      "        [0.0091],\n",
      "        [0.0113],\n",
      "        [0.0119],\n",
      "        [0.0966],\n",
      "        [0.0034],\n",
      "        [0.0142],\n",
      "        [0.0111],\n",
      "        [0.0862],\n",
      "        [0.0085],\n",
      "        [0.0081],\n",
      "        [0.0083],\n",
      "        [0.0206],\n",
      "        [0.0277],\n",
      "        [0.0703],\n",
      "        [0.0079],\n",
      "        [0.0098],\n",
      "        [0.0122],\n",
      "        [0.0157],\n",
      "        [0.0187],\n",
      "        [0.0246],\n",
      "        [0.0022],\n",
      "        [0.0325],\n",
      "        [0.0219],\n",
      "        [0.0501],\n",
      "        [0.0599],\n",
      "        [0.0331],\n",
      "        [0.0171],\n",
      "        [0.0262],\n",
      "        [0.0181],\n",
      "        [0.0120],\n",
      "        [0.0346],\n",
      "        [0.0297],\n",
      "        [0.0010],\n",
      "        [0.0228],\n",
      "        [0.0097],\n",
      "        [0.0381],\n",
      "        [0.0275],\n",
      "        [0.0304],\n",
      "        [0.0360],\n",
      "        [0.0150],\n",
      "        [0.0154],\n",
      "        [0.0442],\n",
      "        [0.0232],\n",
      "        [0.0908],\n",
      "        [0.0379],\n",
      "        [0.0621],\n",
      "        [0.0298],\n",
      "        [0.0345],\n",
      "        [0.0517],\n",
      "        [0.0083],\n",
      "        [0.1265],\n",
      "        [0.0187],\n",
      "        [0.0239],\n",
      "        [0.0434],\n",
      "        [0.0096],\n",
      "        [0.0239],\n",
      "        [0.0528],\n",
      "        [0.0044],\n",
      "        [0.0410],\n",
      "        [0.0357],\n",
      "        [0.0526],\n",
      "        [0.0361],\n",
      "        [0.0357],\n",
      "        [0.0461],\n",
      "        [0.0338],\n",
      "        [0.0287],\n",
      "        [0.0053],\n",
      "        [0.0740],\n",
      "        [0.0337],\n",
      "        [0.0267],\n",
      "        [0.0537],\n",
      "        [0.0757],\n",
      "        [0.0651],\n",
      "        [0.0600],\n",
      "        [0.0783],\n",
      "        [0.0730],\n",
      "        [0.0865],\n",
      "        [0.0620],\n",
      "        [0.0720],\n",
      "        [0.0556],\n",
      "        [0.0705],\n",
      "        [0.0386],\n",
      "        [0.0568],\n",
      "        [0.0708],\n",
      "        [0.0998],\n",
      "        [0.0714],\n",
      "        [0.0403],\n",
      "        [0.0366],\n",
      "        [0.0543],\n",
      "        [0.1301],\n",
      "        [0.1027],\n",
      "        [0.0823],\n",
      "        [0.0103],\n",
      "        [0.0913],\n",
      "        [0.1304],\n",
      "        [0.1043],\n",
      "        [0.0179],\n",
      "        [0.1042],\n",
      "        [0.1084],\n",
      "        [0.0009],\n",
      "        [0.1169],\n",
      "        [0.1108],\n",
      "        [0.1262],\n",
      "        [0.1201],\n",
      "        [0.1213],\n",
      "        [0.1271],\n",
      "        [0.1352],\n",
      "        [0.1761],\n",
      "        [0.1634],\n",
      "        [0.0954],\n",
      "        [0.1027],\n",
      "        [0.1538],\n",
      "        [0.1547],\n",
      "        [0.1705],\n",
      "        [0.3170],\n",
      "        [0.3310],\n",
      "        [0.3340],\n",
      "        [0.3359],\n",
      "        [0.3369],\n",
      "        [0.3463],\n",
      "        [0.3482],\n",
      "        [0.3493],\n",
      "        [0.3567],\n",
      "        [0.3585],\n",
      "        [0.3606],\n",
      "        [0.3616]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0075],\n",
      "        [0.0452],\n",
      "        [0.0175],\n",
      "        [0.0591],\n",
      "        [0.0063],\n",
      "        [0.0105],\n",
      "        [0.0034],\n",
      "        [0.0041],\n",
      "        [0.0082],\n",
      "        [0.0127],\n",
      "        [0.0123],\n",
      "        [0.0971],\n",
      "        [0.0029],\n",
      "        [0.0143],\n",
      "        [0.0096],\n",
      "        [0.0862],\n",
      "        [0.0092],\n",
      "        [0.0069],\n",
      "        [0.0084],\n",
      "        [0.0201],\n",
      "        [0.0287],\n",
      "        [0.0693],\n",
      "        [0.0122],\n",
      "        [0.0119],\n",
      "        [0.0125],\n",
      "        [0.0143],\n",
      "        [0.0207],\n",
      "        [0.0225],\n",
      "        [0.0029],\n",
      "        [0.0342],\n",
      "        [0.0235],\n",
      "        [0.0544],\n",
      "        [0.0571],\n",
      "        [0.0329],\n",
      "        [0.0170],\n",
      "        [0.0253],\n",
      "        [0.0177],\n",
      "        [0.0095],\n",
      "        [0.0329],\n",
      "        [0.0304],\n",
      "        [0.0009],\n",
      "        [0.0230],\n",
      "        [0.0085],\n",
      "        [0.0403],\n",
      "        [0.0277],\n",
      "        [0.0305],\n",
      "        [0.0337],\n",
      "        [0.0130],\n",
      "        [0.0164],\n",
      "        [0.0449],\n",
      "        [0.0208],\n",
      "        [0.0903],\n",
      "        [0.0372],\n",
      "        [0.0641],\n",
      "        [0.0304],\n",
      "        [0.0356],\n",
      "        [0.0529],\n",
      "        [0.0070],\n",
      "        [0.1275],\n",
      "        [0.0180],\n",
      "        [0.0218],\n",
      "        [0.0426],\n",
      "        [0.0087],\n",
      "        [0.0223],\n",
      "        [0.0542],\n",
      "        [0.0030],\n",
      "        [0.0410],\n",
      "        [0.0340],\n",
      "        [0.0523],\n",
      "        [0.0350],\n",
      "        [0.0344],\n",
      "        [0.0451],\n",
      "        [0.0321],\n",
      "        [0.0274],\n",
      "        [0.0080],\n",
      "        [0.0758],\n",
      "        [0.0335],\n",
      "        [0.0221],\n",
      "        [0.0544],\n",
      "        [0.0771],\n",
      "        [0.0673],\n",
      "        [0.0597],\n",
      "        [0.0801],\n",
      "        [0.0744],\n",
      "        [0.0899],\n",
      "        [0.0631],\n",
      "        [0.0715],\n",
      "        [0.0533],\n",
      "        [0.0685],\n",
      "        [0.0381],\n",
      "        [0.0548],\n",
      "        [0.0693],\n",
      "        [0.1016],\n",
      "        [0.0691],\n",
      "        [0.0387],\n",
      "        [0.0347],\n",
      "        [0.0538],\n",
      "        [0.1320],\n",
      "        [0.1044],\n",
      "        [0.0814],\n",
      "        [0.0126],\n",
      "        [0.0909],\n",
      "        [0.1316],\n",
      "        [0.1036],\n",
      "        [0.0164],\n",
      "        [0.1029],\n",
      "        [0.1061],\n",
      "        [0.0023],\n",
      "        [0.1155],\n",
      "        [0.1089],\n",
      "        [0.1253],\n",
      "        [0.1203],\n",
      "        [0.1174],\n",
      "        [0.1265],\n",
      "        [0.1320],\n",
      "        [0.1740],\n",
      "        [0.1627],\n",
      "        [0.0925],\n",
      "        [0.0984],\n",
      "        [0.1507],\n",
      "        [0.1512],\n",
      "        [0.1670],\n",
      "        [0.3165],\n",
      "        [0.3305],\n",
      "        [0.3335],\n",
      "        [0.3354],\n",
      "        [0.3364],\n",
      "        [0.3459],\n",
      "        [0.3477],\n",
      "        [0.3488],\n",
      "        [0.3562],\n",
      "        [0.3580],\n",
      "        [0.3601],\n",
      "        [0.3611]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 82.88562440872192\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 135\n",
      "剩餘X 資料 torch.Size([25, 18])\n",
      "剩餘Y 資料 torch.Size([25, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.13274577260017395, 16)\n",
      "The second_loss value of k: (0.1470097452402115, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.1025])\n",
      "目前模型的Data狀態 torch.Size([135, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6353],\n",
      "        [0.1360],\n",
      "        [0.5191],\n",
      "        [0.7181],\n",
      "        [0.2241],\n",
      "        [0.2231],\n",
      "        [0.6320],\n",
      "        [0.1880],\n",
      "        [0.6387],\n",
      "        [0.6235],\n",
      "        [0.6317],\n",
      "        [0.2727],\n",
      "        [0.6187],\n",
      "        [0.6050],\n",
      "        [0.6166],\n",
      "        [0.2045],\n",
      "        [0.5498],\n",
      "        [0.6235],\n",
      "        [0.6249],\n",
      "        [0.4204],\n",
      "        [0.6359],\n",
      "        [0.1599],\n",
      "        [0.4832],\n",
      "        [0.5769],\n",
      "        [0.6541],\n",
      "        [0.2765],\n",
      "        [0.1595],\n",
      "        [0.1421],\n",
      "        [0.6335],\n",
      "        [0.4368],\n",
      "        [0.5616],\n",
      "        [0.4570],\n",
      "        [0.1502],\n",
      "        [0.6136],\n",
      "        [0.5781],\n",
      "        [0.2170],\n",
      "        [0.4562],\n",
      "        [0.1181],\n",
      "        [0.5998],\n",
      "        [0.6463],\n",
      "        [0.6445],\n",
      "        [0.5788],\n",
      "        [0.3458],\n",
      "        [0.5111],\n",
      "        [0.5867],\n",
      "        [0.6124],\n",
      "        [0.2228],\n",
      "        [0.3450],\n",
      "        [0.6196],\n",
      "        [0.3702],\n",
      "        [0.2048],\n",
      "        [0.1712],\n",
      "        [0.2173],\n",
      "        [0.2269],\n",
      "        [0.5784],\n",
      "        [0.5621],\n",
      "        [0.4811],\n",
      "        [0.6117],\n",
      "        [0.2719],\n",
      "        [0.1562],\n",
      "        [0.3361],\n",
      "        [0.3136],\n",
      "        [0.1413],\n",
      "        [0.2898],\n",
      "        [0.6175],\n",
      "        [0.1361],\n",
      "        [0.1591],\n",
      "        [0.2730],\n",
      "        [0.1452],\n",
      "        [0.3236],\n",
      "        [0.3243],\n",
      "        [0.4268],\n",
      "        [0.3313],\n",
      "        [0.3371],\n",
      "        [0.6874],\n",
      "        [0.3899],\n",
      "        [0.5836],\n",
      "        [0.4790],\n",
      "        [0.4329],\n",
      "        [0.6409],\n",
      "        [0.5432],\n",
      "        [0.4049],\n",
      "        [0.3915],\n",
      "        [0.4487],\n",
      "        [0.4366],\n",
      "        [0.5608],\n",
      "        [0.2363],\n",
      "        [0.2341],\n",
      "        [0.2276],\n",
      "        [0.6339],\n",
      "        [0.3363],\n",
      "        [0.2895],\n",
      "        [0.4314],\n",
      "        [0.2567],\n",
      "        [0.6520],\n",
      "        [0.6619],\n",
      "        [0.6355],\n",
      "        [0.1706],\n",
      "        [0.4308],\n",
      "        [0.3464],\n",
      "        [0.4122],\n",
      "        [0.3081],\n",
      "        [0.1876],\n",
      "        [0.0585],\n",
      "        [0.2933],\n",
      "        [0.3493],\n",
      "        [0.2688],\n",
      "        [0.4164],\n",
      "        [0.3064],\n",
      "        [0.3422],\n",
      "        [0.1253],\n",
      "        [0.4070],\n",
      "        [0.1444],\n",
      "        [0.3100],\n",
      "        [0.5557],\n",
      "        [0.5945],\n",
      "        [0.3901],\n",
      "        [0.3166],\n",
      "        [0.3847],\n",
      "        [0.3165],\n",
      "        [0.3473],\n",
      "        [0.3288],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668],\n",
      "        [0.4668]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0075],\n",
      "        [0.0452],\n",
      "        [0.0175],\n",
      "        [0.0591],\n",
      "        [0.0063],\n",
      "        [0.0105],\n",
      "        [0.0034],\n",
      "        [0.0041],\n",
      "        [0.0082],\n",
      "        [0.0127],\n",
      "        [0.0123],\n",
      "        [0.0971],\n",
      "        [0.0029],\n",
      "        [0.0143],\n",
      "        [0.0096],\n",
      "        [0.0862],\n",
      "        [0.0092],\n",
      "        [0.0069],\n",
      "        [0.0084],\n",
      "        [0.0201],\n",
      "        [0.0287],\n",
      "        [0.0693],\n",
      "        [0.0122],\n",
      "        [0.0119],\n",
      "        [0.0125],\n",
      "        [0.0143],\n",
      "        [0.0207],\n",
      "        [0.0225],\n",
      "        [0.0029],\n",
      "        [0.0342],\n",
      "        [0.0235],\n",
      "        [0.0544],\n",
      "        [0.0571],\n",
      "        [0.0329],\n",
      "        [0.0170],\n",
      "        [0.0253],\n",
      "        [0.0177],\n",
      "        [0.0095],\n",
      "        [0.0329],\n",
      "        [0.0304],\n",
      "        [0.0009],\n",
      "        [0.0230],\n",
      "        [0.0085],\n",
      "        [0.0403],\n",
      "        [0.0277],\n",
      "        [0.0305],\n",
      "        [0.0337],\n",
      "        [0.0130],\n",
      "        [0.0164],\n",
      "        [0.0449],\n",
      "        [0.0208],\n",
      "        [0.0903],\n",
      "        [0.0372],\n",
      "        [0.0641],\n",
      "        [0.0304],\n",
      "        [0.0356],\n",
      "        [0.0529],\n",
      "        [0.0070],\n",
      "        [0.1275],\n",
      "        [0.0180],\n",
      "        [0.0218],\n",
      "        [0.0426],\n",
      "        [0.0087],\n",
      "        [0.0223],\n",
      "        [0.0542],\n",
      "        [0.0030],\n",
      "        [0.0410],\n",
      "        [0.0340],\n",
      "        [0.0523],\n",
      "        [0.0350],\n",
      "        [0.0344],\n",
      "        [0.0451],\n",
      "        [0.0321],\n",
      "        [0.0274],\n",
      "        [0.0080],\n",
      "        [0.0758],\n",
      "        [0.0335],\n",
      "        [0.0221],\n",
      "        [0.0544],\n",
      "        [0.0771],\n",
      "        [0.0673],\n",
      "        [0.0597],\n",
      "        [0.0801],\n",
      "        [0.0744],\n",
      "        [0.0899],\n",
      "        [0.0631],\n",
      "        [0.0715],\n",
      "        [0.0533],\n",
      "        [0.0685],\n",
      "        [0.0381],\n",
      "        [0.0548],\n",
      "        [0.0693],\n",
      "        [0.1016],\n",
      "        [0.0691],\n",
      "        [0.0387],\n",
      "        [0.0347],\n",
      "        [0.0538],\n",
      "        [0.1320],\n",
      "        [0.1044],\n",
      "        [0.0814],\n",
      "        [0.0126],\n",
      "        [0.0909],\n",
      "        [0.1316],\n",
      "        [0.1036],\n",
      "        [0.0164],\n",
      "        [0.1029],\n",
      "        [0.1061],\n",
      "        [0.0023],\n",
      "        [0.1155],\n",
      "        [0.1089],\n",
      "        [0.1253],\n",
      "        [0.1203],\n",
      "        [0.1174],\n",
      "        [0.1265],\n",
      "        [0.1320],\n",
      "        [0.1740],\n",
      "        [0.1627],\n",
      "        [0.0925],\n",
      "        [0.0984],\n",
      "        [0.1507],\n",
      "        [0.1512],\n",
      "        [0.1670],\n",
      "        [0.3165],\n",
      "        [0.3305],\n",
      "        [0.3335],\n",
      "        [0.3354],\n",
      "        [0.3364],\n",
      "        [0.3459],\n",
      "        [0.3477],\n",
      "        [0.3488],\n",
      "        [0.3562],\n",
      "        [0.3580],\n",
      "        [0.3601],\n",
      "        [0.3611],\n",
      "        [0.3643]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0115],\n",
      "        [    0.0451],\n",
      "        [    0.0148],\n",
      "        [    0.0659],\n",
      "        [    0.0052],\n",
      "        [    0.0101],\n",
      "        [    0.0058],\n",
      "        [    0.0042],\n",
      "        [    0.0110],\n",
      "        [    0.0175],\n",
      "        [    0.0090],\n",
      "        [    0.0988],\n",
      "        [    0.0010],\n",
      "        [    0.0179],\n",
      "        [    0.0119],\n",
      "        [    0.0874],\n",
      "        [    0.0130],\n",
      "        [    0.0096],\n",
      "        [    0.0051],\n",
      "        [    0.0169],\n",
      "        [    0.0331],\n",
      "        [    0.0695],\n",
      "        [    0.0126],\n",
      "        [    0.0103],\n",
      "        [    0.0164],\n",
      "        [    0.0151],\n",
      "        [    0.0208],\n",
      "        [    0.0218],\n",
      "        [    0.0001],\n",
      "        [    0.0328],\n",
      "        [    0.0216],\n",
      "        [    0.0546],\n",
      "        [    0.0558],\n",
      "        [    0.0364],\n",
      "        [    0.0203],\n",
      "        [    0.0262],\n",
      "        [    0.0148],\n",
      "        [    0.0083],\n",
      "        [    0.0349],\n",
      "        [    0.0273],\n",
      "        [    0.0028],\n",
      "        [    0.0266],\n",
      "        [    0.0054],\n",
      "        [    0.0389],\n",
      "        [    0.0313],\n",
      "        [    0.0271],\n",
      "        [    0.0332],\n",
      "        [    0.0093],\n",
      "        [    0.0138],\n",
      "        [    0.0478],\n",
      "        [    0.0205],\n",
      "        [    0.0911],\n",
      "        [    0.0354],\n",
      "        [    0.0635],\n",
      "        [    0.0343],\n",
      "        [    0.0399],\n",
      "        [    0.0508],\n",
      "        [    0.0024],\n",
      "        [    0.1297],\n",
      "        [    0.0185],\n",
      "        [    0.0179],\n",
      "        [    0.0440],\n",
      "        [    0.0090],\n",
      "        [    0.0233],\n",
      "        [    0.0517],\n",
      "        [    0.0028],\n",
      "        [    0.0414],\n",
      "        [    0.0308],\n",
      "        [    0.0517],\n",
      "        [    0.0319],\n",
      "        [    0.0313],\n",
      "        [    0.0417],\n",
      "        [    0.0287],\n",
      "        [    0.0243],\n",
      "        [    0.0138],\n",
      "        [    0.0794],\n",
      "        [    0.0300],\n",
      "        [    0.0214],\n",
      "        [    0.0523],\n",
      "        [    0.0819],\n",
      "        [    0.0661],\n",
      "        [    0.0568],\n",
      "        [    0.0839],\n",
      "        [    0.0728],\n",
      "        [    0.0895],\n",
      "        [    0.0608],\n",
      "        [    0.0693],\n",
      "        [    0.0532],\n",
      "        [    0.0684],\n",
      "        [    0.0343],\n",
      "        [    0.0512],\n",
      "        [    0.0701],\n",
      "        [    0.1002],\n",
      "        [    0.0691],\n",
      "        [    0.0340],\n",
      "        [    0.0297],\n",
      "        [    0.0499],\n",
      "        [    0.1316],\n",
      "        [    0.1085],\n",
      "        [    0.0831],\n",
      "        [    0.0160],\n",
      "        [    0.0885],\n",
      "        [    0.1307],\n",
      "        [    0.1017],\n",
      "        [    0.0138],\n",
      "        [    0.0996],\n",
      "        [    0.1061],\n",
      "        [    0.0064],\n",
      "        [    0.1164],\n",
      "        [    0.1052],\n",
      "        [    0.1257],\n",
      "        [    0.1230],\n",
      "        [    0.1155],\n",
      "        [    0.1239],\n",
      "        [    0.1326],\n",
      "        [    0.1756],\n",
      "        [    0.1646],\n",
      "        [    0.0885],\n",
      "        [    0.0934],\n",
      "        [    0.1466],\n",
      "        [    0.1469],\n",
      "        [    0.1625],\n",
      "        [    0.3160],\n",
      "        [    0.3300],\n",
      "        [    0.3330],\n",
      "        [    0.3349],\n",
      "        [    0.3359],\n",
      "        [    0.3454],\n",
      "        [    0.3472],\n",
      "        [    0.3484],\n",
      "        [    0.3557],\n",
      "        [    0.3575],\n",
      "        [    0.3596],\n",
      "        [    0.3607],\n",
      "        [    0.3639]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 83.12876772880554\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 136\n",
      "剩餘X 資料 torch.Size([24, 18])\n",
      "剩餘Y 資料 torch.Size([24, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14665956795215607, 14)\n",
      "The second_loss value of k: (0.1474105566740036, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0834])\n",
      "目前模型的Data狀態 torch.Size([136, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6393],\n",
      "        [0.1358],\n",
      "        [0.5218],\n",
      "        [0.7249],\n",
      "        [0.2230],\n",
      "        [0.2235],\n",
      "        [0.6344],\n",
      "        [0.1879],\n",
      "        [0.6415],\n",
      "        [0.6282],\n",
      "        [0.6349],\n",
      "        [0.2744],\n",
      "        [0.6226],\n",
      "        [0.6086],\n",
      "        [0.6188],\n",
      "        [0.2056],\n",
      "        [0.5536],\n",
      "        [0.6261],\n",
      "        [0.6282],\n",
      "        [0.4235],\n",
      "        [0.6403],\n",
      "        [0.1601],\n",
      "        [0.4828],\n",
      "        [0.5785],\n",
      "        [0.6579],\n",
      "        [0.2773],\n",
      "        [0.1594],\n",
      "        [0.1414],\n",
      "        [0.6365],\n",
      "        [0.4381],\n",
      "        [0.5635],\n",
      "        [0.4567],\n",
      "        [0.1489],\n",
      "        [0.6170],\n",
      "        [0.5814],\n",
      "        [0.2179],\n",
      "        [0.4592],\n",
      "        [0.1170],\n",
      "        [0.6018],\n",
      "        [0.6494],\n",
      "        [0.6482],\n",
      "        [0.5824],\n",
      "        [0.3488],\n",
      "        [0.5125],\n",
      "        [0.5903],\n",
      "        [0.6159],\n",
      "        [0.2223],\n",
      "        [0.3487],\n",
      "        [0.6222],\n",
      "        [0.3731],\n",
      "        [0.2046],\n",
      "        [0.1719],\n",
      "        [0.2192],\n",
      "        [0.2274],\n",
      "        [0.5823],\n",
      "        [0.5663],\n",
      "        [0.4832],\n",
      "        [0.6163],\n",
      "        [0.2740],\n",
      "        [0.1568],\n",
      "        [0.3400],\n",
      "        [0.3150],\n",
      "        [0.1416],\n",
      "        [0.2907],\n",
      "        [0.6200],\n",
      "        [0.1360],\n",
      "        [0.1595],\n",
      "        [0.2763],\n",
      "        [0.1459],\n",
      "        [0.3266],\n",
      "        [0.3274],\n",
      "        [0.4302],\n",
      "        [0.3347],\n",
      "        [0.3401],\n",
      "        [0.6932],\n",
      "        [0.3936],\n",
      "        [0.5871],\n",
      "        [0.4783],\n",
      "        [0.4350],\n",
      "        [0.6457],\n",
      "        [0.5444],\n",
      "        [0.4078],\n",
      "        [0.3954],\n",
      "        [0.4503],\n",
      "        [0.4370],\n",
      "        [0.5631],\n",
      "        [0.2385],\n",
      "        [0.2340],\n",
      "        [0.2275],\n",
      "        [0.6377],\n",
      "        [0.3399],\n",
      "        [0.2902],\n",
      "        [0.4328],\n",
      "        [0.2567],\n",
      "        [0.6567],\n",
      "        [0.6669],\n",
      "        [0.6394],\n",
      "        [0.1711],\n",
      "        [0.4349],\n",
      "        [0.3480],\n",
      "        [0.4157],\n",
      "        [0.3104],\n",
      "        [0.1885],\n",
      "        [0.0604],\n",
      "        [0.2959],\n",
      "        [0.3526],\n",
      "        [0.2688],\n",
      "        [0.4205],\n",
      "        [0.3073],\n",
      "        [0.3458],\n",
      "        [0.1257],\n",
      "        [0.4098],\n",
      "        [0.1425],\n",
      "        [0.3126],\n",
      "        [0.5563],\n",
      "        [0.5961],\n",
      "        [0.3919],\n",
      "        [0.3207],\n",
      "        [0.3897],\n",
      "        [0.3206],\n",
      "        [0.3516],\n",
      "        [0.3333],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664],\n",
      "        [0.4664]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0115],\n",
      "        [    0.0451],\n",
      "        [    0.0148],\n",
      "        [    0.0659],\n",
      "        [    0.0052],\n",
      "        [    0.0101],\n",
      "        [    0.0058],\n",
      "        [    0.0042],\n",
      "        [    0.0110],\n",
      "        [    0.0175],\n",
      "        [    0.0090],\n",
      "        [    0.0988],\n",
      "        [    0.0010],\n",
      "        [    0.0179],\n",
      "        [    0.0119],\n",
      "        [    0.0874],\n",
      "        [    0.0130],\n",
      "        [    0.0096],\n",
      "        [    0.0051],\n",
      "        [    0.0169],\n",
      "        [    0.0331],\n",
      "        [    0.0695],\n",
      "        [    0.0126],\n",
      "        [    0.0103],\n",
      "        [    0.0164],\n",
      "        [    0.0151],\n",
      "        [    0.0208],\n",
      "        [    0.0218],\n",
      "        [    0.0001],\n",
      "        [    0.0328],\n",
      "        [    0.0216],\n",
      "        [    0.0546],\n",
      "        [    0.0558],\n",
      "        [    0.0364],\n",
      "        [    0.0203],\n",
      "        [    0.0262],\n",
      "        [    0.0148],\n",
      "        [    0.0083],\n",
      "        [    0.0349],\n",
      "        [    0.0273],\n",
      "        [    0.0028],\n",
      "        [    0.0266],\n",
      "        [    0.0054],\n",
      "        [    0.0389],\n",
      "        [    0.0313],\n",
      "        [    0.0271],\n",
      "        [    0.0332],\n",
      "        [    0.0093],\n",
      "        [    0.0138],\n",
      "        [    0.0478],\n",
      "        [    0.0205],\n",
      "        [    0.0911],\n",
      "        [    0.0354],\n",
      "        [    0.0635],\n",
      "        [    0.0343],\n",
      "        [    0.0399],\n",
      "        [    0.0508],\n",
      "        [    0.0024],\n",
      "        [    0.1297],\n",
      "        [    0.0185],\n",
      "        [    0.0179],\n",
      "        [    0.0440],\n",
      "        [    0.0090],\n",
      "        [    0.0233],\n",
      "        [    0.0517],\n",
      "        [    0.0028],\n",
      "        [    0.0414],\n",
      "        [    0.0308],\n",
      "        [    0.0517],\n",
      "        [    0.0319],\n",
      "        [    0.0313],\n",
      "        [    0.0417],\n",
      "        [    0.0287],\n",
      "        [    0.0243],\n",
      "        [    0.0138],\n",
      "        [    0.0794],\n",
      "        [    0.0300],\n",
      "        [    0.0214],\n",
      "        [    0.0523],\n",
      "        [    0.0819],\n",
      "        [    0.0661],\n",
      "        [    0.0568],\n",
      "        [    0.0839],\n",
      "        [    0.0728],\n",
      "        [    0.0895],\n",
      "        [    0.0608],\n",
      "        [    0.0693],\n",
      "        [    0.0532],\n",
      "        [    0.0684],\n",
      "        [    0.0343],\n",
      "        [    0.0512],\n",
      "        [    0.0701],\n",
      "        [    0.1002],\n",
      "        [    0.0691],\n",
      "        [    0.0340],\n",
      "        [    0.0297],\n",
      "        [    0.0499],\n",
      "        [    0.1316],\n",
      "        [    0.1085],\n",
      "        [    0.0831],\n",
      "        [    0.0160],\n",
      "        [    0.0885],\n",
      "        [    0.1307],\n",
      "        [    0.1017],\n",
      "        [    0.0138],\n",
      "        [    0.0996],\n",
      "        [    0.1061],\n",
      "        [    0.0064],\n",
      "        [    0.1164],\n",
      "        [    0.1052],\n",
      "        [    0.1257],\n",
      "        [    0.1230],\n",
      "        [    0.1155],\n",
      "        [    0.1239],\n",
      "        [    0.1326],\n",
      "        [    0.1756],\n",
      "        [    0.1646],\n",
      "        [    0.0885],\n",
      "        [    0.0934],\n",
      "        [    0.1466],\n",
      "        [    0.1469],\n",
      "        [    0.1625],\n",
      "        [    0.3160],\n",
      "        [    0.3300],\n",
      "        [    0.3330],\n",
      "        [    0.3349],\n",
      "        [    0.3359],\n",
      "        [    0.3454],\n",
      "        [    0.3472],\n",
      "        [    0.3484],\n",
      "        [    0.3557],\n",
      "        [    0.3575],\n",
      "        [    0.3596],\n",
      "        [    0.3607],\n",
      "        [    0.3639],\n",
      "        [    0.3830]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0085],\n",
      "        [    0.0425],\n",
      "        [    0.0182],\n",
      "        [    0.0653],\n",
      "        [    0.0021],\n",
      "        [    0.0121],\n",
      "        [    0.0016],\n",
      "        [    0.0066],\n",
      "        [    0.0074],\n",
      "        [    0.0157],\n",
      "        [    0.0127],\n",
      "        [    0.0980],\n",
      "        [    0.0015],\n",
      "        [    0.0146],\n",
      "        [    0.0073],\n",
      "        [    0.0861],\n",
      "        [    0.0105],\n",
      "        [    0.0052],\n",
      "        [    0.0083],\n",
      "        [    0.0194],\n",
      "        [    0.0304],\n",
      "        [    0.0673],\n",
      "        [    0.0188],\n",
      "        [    0.0153],\n",
      "        [    0.0134],\n",
      "        [    0.0122],\n",
      "        [    0.0231],\n",
      "        [    0.0187],\n",
      "        [    0.0038],\n",
      "        [    0.0371],\n",
      "        [    0.0253],\n",
      "        [    0.0608],\n",
      "        [    0.0522],\n",
      "        [    0.0334],\n",
      "        [    0.0169],\n",
      "        [    0.0243],\n",
      "        [    0.0177],\n",
      "        [    0.0049],\n",
      "        [    0.0303],\n",
      "        [    0.0307],\n",
      "        [    0.0003],\n",
      "        [    0.0233],\n",
      "        [    0.0066],\n",
      "        [    0.0432],\n",
      "        [    0.0281],\n",
      "        [    0.0301],\n",
      "        [    0.0306],\n",
      "        [    0.0094],\n",
      "        [    0.0180],\n",
      "        [    0.0462],\n",
      "        [    0.0174],\n",
      "        [    0.0891],\n",
      "        [    0.0369],\n",
      "        [    0.0670],\n",
      "        [    0.0315],\n",
      "        [    0.0375],\n",
      "        [    0.0545],\n",
      "        [    0.0044],\n",
      "        [    0.1292],\n",
      "        [    0.0167],\n",
      "        [    0.0179],\n",
      "        [    0.0415],\n",
      "        [    0.0068],\n",
      "        [    0.0199],\n",
      "        [    0.0556],\n",
      "        [    0.0003],\n",
      "        [    0.0410],\n",
      "        [    0.0310],\n",
      "        [    0.0518],\n",
      "        [    0.0326],\n",
      "        [    0.0319],\n",
      "        [    0.0440],\n",
      "        [    0.0290],\n",
      "        [    0.0253],\n",
      "        [    0.0126],\n",
      "        [    0.0785],\n",
      "        [    0.0325],\n",
      "        [    0.0150],\n",
      "        [    0.0559],\n",
      "        [    0.0801],\n",
      "        [    0.0704],\n",
      "        [    0.0594],\n",
      "        [    0.0831],\n",
      "        [    0.0768],\n",
      "        [    0.0949],\n",
      "        [    0.0642],\n",
      "        [    0.0705],\n",
      "        [    0.0501],\n",
      "        [    0.0661],\n",
      "        [    0.0371],\n",
      "        [    0.0511],\n",
      "        [    0.0673],\n",
      "        [    0.1044],\n",
      "        [    0.0659],\n",
      "        [    0.0359],\n",
      "        [    0.0314],\n",
      "        [    0.0526],\n",
      "        [    0.1346],\n",
      "        [    0.1071],\n",
      "        [    0.0801],\n",
      "        [    0.0152],\n",
      "        [    0.0900],\n",
      "        [    0.1332],\n",
      "        [    0.1029],\n",
      "        [    0.0141],\n",
      "        [    0.1002],\n",
      "        [    0.1028],\n",
      "        [    0.0064],\n",
      "        [    0.1138],\n",
      "        [    0.1052],\n",
      "        [    0.1232],\n",
      "        [    0.1206],\n",
      "        [    0.1115],\n",
      "        [    0.1252],\n",
      "        [    0.1270],\n",
      "        [    0.1708],\n",
      "        [    0.1615],\n",
      "        [    0.0876],\n",
      "        [    0.0920],\n",
      "        [    0.1457],\n",
      "        [    0.1457],\n",
      "        [    0.1611],\n",
      "        [    0.3156],\n",
      "        [    0.3296],\n",
      "        [    0.3326],\n",
      "        [    0.3345],\n",
      "        [    0.3355],\n",
      "        [    0.3450],\n",
      "        [    0.3468],\n",
      "        [    0.3480],\n",
      "        [    0.3553],\n",
      "        [    0.3571],\n",
      "        [    0.3592],\n",
      "        [    0.3603],\n",
      "        [    0.3635],\n",
      "        [    0.3825]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 83.3662600517273\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 137\n",
      "剩餘X 資料 torch.Size([23, 18])\n",
      "剩餘Y 資料 torch.Size([23, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14708983898162842, 14)\n",
      "The second_loss value of k: (0.15245585143566132, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0825])\n",
      "目前模型的Data狀態 torch.Size([137, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6363],\n",
      "        [0.1332],\n",
      "        [0.5184],\n",
      "        [0.7243],\n",
      "        [0.2199],\n",
      "        [0.2215],\n",
      "        [0.6302],\n",
      "        [0.1855],\n",
      "        [0.6379],\n",
      "        [0.6264],\n",
      "        [0.6312],\n",
      "        [0.2736],\n",
      "        [0.6200],\n",
      "        [0.6053],\n",
      "        [0.6143],\n",
      "        [0.2043],\n",
      "        [0.5510],\n",
      "        [0.6218],\n",
      "        [0.6250],\n",
      "        [0.4211],\n",
      "        [0.6376],\n",
      "        [0.1579],\n",
      "        [0.4765],\n",
      "        [0.5736],\n",
      "        [0.6550],\n",
      "        [0.2744],\n",
      "        [0.1571],\n",
      "        [0.1383],\n",
      "        [0.6326],\n",
      "        [0.4339],\n",
      "        [0.5598],\n",
      "        [0.4506],\n",
      "        [0.1453],\n",
      "        [0.6141],\n",
      "        [0.5780],\n",
      "        [0.2160],\n",
      "        [0.4563],\n",
      "        [0.1136],\n",
      "        [0.5972],\n",
      "        [0.6460],\n",
      "        [0.6450],\n",
      "        [0.5792],\n",
      "        [0.3477],\n",
      "        [0.5082],\n",
      "        [0.5871],\n",
      "        [0.6128],\n",
      "        [0.2197],\n",
      "        [0.3486],\n",
      "        [0.6180],\n",
      "        [0.3715],\n",
      "        [0.2015],\n",
      "        [0.1700],\n",
      "        [0.2176],\n",
      "        [0.2240],\n",
      "        [0.5794],\n",
      "        [0.5640],\n",
      "        [0.4795],\n",
      "        [0.6144],\n",
      "        [0.2735],\n",
      "        [0.1549],\n",
      "        [0.3400],\n",
      "        [0.3125],\n",
      "        [0.1394],\n",
      "        [0.2874],\n",
      "        [0.6162],\n",
      "        [0.1334],\n",
      "        [0.1591],\n",
      "        [0.2760],\n",
      "        [0.1457],\n",
      "        [0.3260],\n",
      "        [0.3269],\n",
      "        [0.4279],\n",
      "        [0.3344],\n",
      "        [0.3391],\n",
      "        [0.6920],\n",
      "        [0.3927],\n",
      "        [0.5846],\n",
      "        [0.4719],\n",
      "        [0.4313],\n",
      "        [0.6439],\n",
      "        [0.5401],\n",
      "        [0.4052],\n",
      "        [0.3945],\n",
      "        [0.4463],\n",
      "        [0.4317],\n",
      "        [0.5596],\n",
      "        [0.2373],\n",
      "        [0.2309],\n",
      "        [0.2252],\n",
      "        [0.6349],\n",
      "        [0.3400],\n",
      "        [0.2875],\n",
      "        [0.4286],\n",
      "        [0.2535],\n",
      "        [0.6548],\n",
      "        [0.6652],\n",
      "        [0.6367],\n",
      "        [0.1680],\n",
      "        [0.4335],\n",
      "        [0.3451],\n",
      "        [0.4149],\n",
      "        [0.3089],\n",
      "        [0.1861],\n",
      "        [0.0592],\n",
      "        [0.2956],\n",
      "        [0.3519],\n",
      "        [0.2656],\n",
      "        [0.4206],\n",
      "        [0.3046],\n",
      "        [0.3458],\n",
      "        [0.1232],\n",
      "        [0.4074],\n",
      "        [0.1386],\n",
      "        [0.3113],\n",
      "        [0.5507],\n",
      "        [0.5913],\n",
      "        [0.3889],\n",
      "        [0.3215],\n",
      "        [0.3911],\n",
      "        [0.3215],\n",
      "        [0.3528],\n",
      "        [0.3346],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660],\n",
      "        [0.4660]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0085],\n",
      "        [    0.0425],\n",
      "        [    0.0182],\n",
      "        [    0.0653],\n",
      "        [    0.0021],\n",
      "        [    0.0121],\n",
      "        [    0.0016],\n",
      "        [    0.0066],\n",
      "        [    0.0074],\n",
      "        [    0.0157],\n",
      "        [    0.0127],\n",
      "        [    0.0980],\n",
      "        [    0.0015],\n",
      "        [    0.0146],\n",
      "        [    0.0073],\n",
      "        [    0.0861],\n",
      "        [    0.0105],\n",
      "        [    0.0052],\n",
      "        [    0.0083],\n",
      "        [    0.0194],\n",
      "        [    0.0304],\n",
      "        [    0.0673],\n",
      "        [    0.0188],\n",
      "        [    0.0153],\n",
      "        [    0.0134],\n",
      "        [    0.0122],\n",
      "        [    0.0231],\n",
      "        [    0.0187],\n",
      "        [    0.0038],\n",
      "        [    0.0371],\n",
      "        [    0.0253],\n",
      "        [    0.0608],\n",
      "        [    0.0522],\n",
      "        [    0.0334],\n",
      "        [    0.0169],\n",
      "        [    0.0243],\n",
      "        [    0.0177],\n",
      "        [    0.0049],\n",
      "        [    0.0303],\n",
      "        [    0.0307],\n",
      "        [    0.0003],\n",
      "        [    0.0233],\n",
      "        [    0.0066],\n",
      "        [    0.0432],\n",
      "        [    0.0281],\n",
      "        [    0.0301],\n",
      "        [    0.0306],\n",
      "        [    0.0094],\n",
      "        [    0.0180],\n",
      "        [    0.0462],\n",
      "        [    0.0174],\n",
      "        [    0.0891],\n",
      "        [    0.0369],\n",
      "        [    0.0670],\n",
      "        [    0.0315],\n",
      "        [    0.0375],\n",
      "        [    0.0545],\n",
      "        [    0.0044],\n",
      "        [    0.1292],\n",
      "        [    0.0167],\n",
      "        [    0.0179],\n",
      "        [    0.0415],\n",
      "        [    0.0068],\n",
      "        [    0.0199],\n",
      "        [    0.0556],\n",
      "        [    0.0003],\n",
      "        [    0.0410],\n",
      "        [    0.0310],\n",
      "        [    0.0518],\n",
      "        [    0.0326],\n",
      "        [    0.0319],\n",
      "        [    0.0440],\n",
      "        [    0.0290],\n",
      "        [    0.0253],\n",
      "        [    0.0126],\n",
      "        [    0.0785],\n",
      "        [    0.0325],\n",
      "        [    0.0150],\n",
      "        [    0.0559],\n",
      "        [    0.0801],\n",
      "        [    0.0704],\n",
      "        [    0.0594],\n",
      "        [    0.0831],\n",
      "        [    0.0768],\n",
      "        [    0.0949],\n",
      "        [    0.0642],\n",
      "        [    0.0705],\n",
      "        [    0.0501],\n",
      "        [    0.0661],\n",
      "        [    0.0371],\n",
      "        [    0.0511],\n",
      "        [    0.0673],\n",
      "        [    0.1044],\n",
      "        [    0.0659],\n",
      "        [    0.0359],\n",
      "        [    0.0314],\n",
      "        [    0.0526],\n",
      "        [    0.1346],\n",
      "        [    0.1071],\n",
      "        [    0.0801],\n",
      "        [    0.0152],\n",
      "        [    0.0900],\n",
      "        [    0.1332],\n",
      "        [    0.1029],\n",
      "        [    0.0141],\n",
      "        [    0.1002],\n",
      "        [    0.1028],\n",
      "        [    0.0064],\n",
      "        [    0.1138],\n",
      "        [    0.1052],\n",
      "        [    0.1232],\n",
      "        [    0.1206],\n",
      "        [    0.1115],\n",
      "        [    0.1252],\n",
      "        [    0.1270],\n",
      "        [    0.1708],\n",
      "        [    0.1615],\n",
      "        [    0.0876],\n",
      "        [    0.0920],\n",
      "        [    0.1457],\n",
      "        [    0.1457],\n",
      "        [    0.1611],\n",
      "        [    0.3156],\n",
      "        [    0.3296],\n",
      "        [    0.3326],\n",
      "        [    0.3345],\n",
      "        [    0.3355],\n",
      "        [    0.3450],\n",
      "        [    0.3468],\n",
      "        [    0.3480],\n",
      "        [    0.3553],\n",
      "        [    0.3571],\n",
      "        [    0.3592],\n",
      "        [    0.3603],\n",
      "        [    0.3635],\n",
      "        [    0.3825],\n",
      "        [    0.3835]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0128],\n",
      "        [    0.0428],\n",
      "        [    0.0150],\n",
      "        [    0.0708],\n",
      "        [    0.0022],\n",
      "        [    0.0108],\n",
      "        [    0.0051],\n",
      "        [    0.0061],\n",
      "        [    0.0115],\n",
      "        [    0.0207],\n",
      "        [    0.0087],\n",
      "        [    0.0997],\n",
      "        [    0.0034],\n",
      "        [    0.0186],\n",
      "        [    0.0106],\n",
      "        [    0.0874],\n",
      "        [    0.0141],\n",
      "        [    0.0088],\n",
      "        [    0.0041],\n",
      "        [    0.0165],\n",
      "        [    0.0349],\n",
      "        [    0.0679],\n",
      "        [    0.0172],\n",
      "        [    0.0125],\n",
      "        [    0.0176],\n",
      "        [    0.0131],\n",
      "        [    0.0225],\n",
      "        [    0.0186],\n",
      "        [    0.0000],\n",
      "        [    0.0349],\n",
      "        [    0.0218],\n",
      "        [    0.0592],\n",
      "        [    0.0517],\n",
      "        [    0.0379],\n",
      "        [    0.0204],\n",
      "        [    0.0258],\n",
      "        [    0.0148],\n",
      "        [    0.0049],\n",
      "        [    0.0334],\n",
      "        [    0.0263],\n",
      "        [    0.0038],\n",
      "        [    0.0270],\n",
      "        [    0.0046],\n",
      "        [    0.0396],\n",
      "        [    0.0320],\n",
      "        [    0.0254],\n",
      "        [    0.0313],\n",
      "        [    0.0067],\n",
      "        [    0.0146],\n",
      "        [    0.0482],\n",
      "        [    0.0181],\n",
      "        [    0.0900],\n",
      "        [    0.0362],\n",
      "        [    0.0657],\n",
      "        [    0.0353],\n",
      "        [    0.0414],\n",
      "        [    0.0517],\n",
      "        [    0.0007],\n",
      "        [    0.1312],\n",
      "        [    0.0176],\n",
      "        [    0.0151],\n",
      "        [    0.0427],\n",
      "        [    0.0076],\n",
      "        [    0.0214],\n",
      "        [    0.0515],\n",
      "        [    0.0006],\n",
      "        [    0.0412],\n",
      "        [    0.0289],\n",
      "        [    0.0515],\n",
      "        [    0.0303],\n",
      "        [    0.0296],\n",
      "        [    0.0410],\n",
      "        [    0.0263],\n",
      "        [    0.0233],\n",
      "        [    0.0176],\n",
      "        [    0.0807],\n",
      "        [    0.0282],\n",
      "        [    0.0163],\n",
      "        [    0.0535],\n",
      "        [    0.0853],\n",
      "        [    0.0671],\n",
      "        [    0.0567],\n",
      "        [    0.0855],\n",
      "        [    0.0743],\n",
      "        [    0.0925],\n",
      "        [    0.0604],\n",
      "        [    0.0688],\n",
      "        [    0.0508],\n",
      "        [    0.0671],\n",
      "        [    0.0329],\n",
      "        [    0.0484],\n",
      "        [    0.0684],\n",
      "        [    0.1020],\n",
      "        [    0.0665],\n",
      "        [    0.0313],\n",
      "        [    0.0267],\n",
      "        [    0.0484],\n",
      "        [    0.1336],\n",
      "        [    0.1107],\n",
      "        [    0.0820],\n",
      "        [    0.0174],\n",
      "        [    0.0883],\n",
      "        [    0.1320],\n",
      "        [    0.1026],\n",
      "        [    0.0118],\n",
      "        [    0.0978],\n",
      "        [    0.1035],\n",
      "        [    0.0090],\n",
      "        [    0.1151],\n",
      "        [    0.1027],\n",
      "        [    0.1223],\n",
      "        [    0.1230],\n",
      "        [    0.1110],\n",
      "        [    0.1234],\n",
      "        [    0.1293],\n",
      "        [    0.1737],\n",
      "        [    0.1634],\n",
      "        [    0.0846],\n",
      "        [    0.0883],\n",
      "        [    0.1424],\n",
      "        [    0.1422],\n",
      "        [    0.1577],\n",
      "        [    0.3152],\n",
      "        [    0.3292],\n",
      "        [    0.3322],\n",
      "        [    0.3341],\n",
      "        [    0.3351],\n",
      "        [    0.3446],\n",
      "        [    0.3464],\n",
      "        [    0.3475],\n",
      "        [    0.3549],\n",
      "        [    0.3567],\n",
      "        [    0.3588],\n",
      "        [    0.3598],\n",
      "        [    0.3631],\n",
      "        [    0.3821],\n",
      "        [    0.3831]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 83.60241460800171\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 138\n",
      "剩餘X 資料 torch.Size([22, 18])\n",
      "剩餘Y 資料 torch.Size([22, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15213343501091003, 4)\n",
      "The second_loss value of k: (0.15557491779327393, 13)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引4，y= tensor([0.0755])\n",
      "目前模型的Data狀態 torch.Size([138, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6406],\n",
      "        [0.1335],\n",
      "        [0.5216],\n",
      "        [0.7298],\n",
      "        [0.2201],\n",
      "        [0.2228],\n",
      "        [0.6337],\n",
      "        [0.1860],\n",
      "        [0.6421],\n",
      "        [0.6315],\n",
      "        [0.6352],\n",
      "        [0.2753],\n",
      "        [0.6250],\n",
      "        [0.6093],\n",
      "        [0.6175],\n",
      "        [0.2056],\n",
      "        [0.5547],\n",
      "        [0.6254],\n",
      "        [0.6293],\n",
      "        [0.4240],\n",
      "        [0.6421],\n",
      "        [0.1585],\n",
      "        [0.4781],\n",
      "        [0.5763],\n",
      "        [0.6592],\n",
      "        [0.2753],\n",
      "        [0.1577],\n",
      "        [0.1382],\n",
      "        [0.6364],\n",
      "        [0.4360],\n",
      "        [0.5633],\n",
      "        [0.4522],\n",
      "        [0.1448],\n",
      "        [0.6186],\n",
      "        [0.5815],\n",
      "        [0.2175],\n",
      "        [0.4592],\n",
      "        [0.1136],\n",
      "        [0.6003],\n",
      "        [0.6504],\n",
      "        [0.6492],\n",
      "        [0.5828],\n",
      "        [0.3497],\n",
      "        [0.5118],\n",
      "        [0.5910],\n",
      "        [0.6176],\n",
      "        [0.2204],\n",
      "        [0.3513],\n",
      "        [0.6214],\n",
      "        [0.3735],\n",
      "        [0.2022],\n",
      "        [0.1709],\n",
      "        [0.2183],\n",
      "        [0.2252],\n",
      "        [0.5832],\n",
      "        [0.5678],\n",
      "        [0.4823],\n",
      "        [0.6194],\n",
      "        [0.2755],\n",
      "        [0.1558],\n",
      "        [0.3427],\n",
      "        [0.3137],\n",
      "        [0.1402],\n",
      "        [0.2888],\n",
      "        [0.6202],\n",
      "        [0.1337],\n",
      "        [0.1593],\n",
      "        [0.2781],\n",
      "        [0.1461],\n",
      "        [0.3283],\n",
      "        [0.3292],\n",
      "        [0.4309],\n",
      "        [0.3371],\n",
      "        [0.3412],\n",
      "        [0.6971],\n",
      "        [0.3949],\n",
      "        [0.5889],\n",
      "        [0.4732],\n",
      "        [0.4338],\n",
      "        [0.6490],\n",
      "        [0.5433],\n",
      "        [0.4078],\n",
      "        [0.3970],\n",
      "        [0.4487],\n",
      "        [0.4340],\n",
      "        [0.5634],\n",
      "        [0.2390],\n",
      "        [0.2316],\n",
      "        [0.2262],\n",
      "        [0.6391],\n",
      "        [0.3427],\n",
      "        [0.2885],\n",
      "        [0.4309],\n",
      "        [0.2541],\n",
      "        [0.6594],\n",
      "        [0.6699],\n",
      "        [0.6409],\n",
      "        [0.1691],\n",
      "        [0.4370],\n",
      "        [0.3470],\n",
      "        [0.4170],\n",
      "        [0.3106],\n",
      "        [0.1872],\n",
      "        [0.0595],\n",
      "        [0.2979],\n",
      "        [0.3544],\n",
      "        [0.2662],\n",
      "        [0.4232],\n",
      "        [0.3060],\n",
      "        [0.3484],\n",
      "        [0.1223],\n",
      "        [0.4098],\n",
      "        [0.1381],\n",
      "        [0.3131],\n",
      "        [0.5530],\n",
      "        [0.5942],\n",
      "        [0.3908],\n",
      "        [0.3246],\n",
      "        [0.3947],\n",
      "        [0.3248],\n",
      "        [0.3563],\n",
      "        [0.3381],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656],\n",
      "        [0.4656]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0128],\n",
      "        [    0.0428],\n",
      "        [    0.0150],\n",
      "        [    0.0708],\n",
      "        [    0.0022],\n",
      "        [    0.0108],\n",
      "        [    0.0051],\n",
      "        [    0.0061],\n",
      "        [    0.0115],\n",
      "        [    0.0207],\n",
      "        [    0.0087],\n",
      "        [    0.0997],\n",
      "        [    0.0034],\n",
      "        [    0.0186],\n",
      "        [    0.0106],\n",
      "        [    0.0874],\n",
      "        [    0.0141],\n",
      "        [    0.0088],\n",
      "        [    0.0041],\n",
      "        [    0.0165],\n",
      "        [    0.0349],\n",
      "        [    0.0679],\n",
      "        [    0.0172],\n",
      "        [    0.0125],\n",
      "        [    0.0176],\n",
      "        [    0.0131],\n",
      "        [    0.0225],\n",
      "        [    0.0186],\n",
      "        [    0.0000],\n",
      "        [    0.0349],\n",
      "        [    0.0218],\n",
      "        [    0.0592],\n",
      "        [    0.0517],\n",
      "        [    0.0379],\n",
      "        [    0.0204],\n",
      "        [    0.0258],\n",
      "        [    0.0148],\n",
      "        [    0.0049],\n",
      "        [    0.0334],\n",
      "        [    0.0263],\n",
      "        [    0.0038],\n",
      "        [    0.0270],\n",
      "        [    0.0046],\n",
      "        [    0.0396],\n",
      "        [    0.0320],\n",
      "        [    0.0254],\n",
      "        [    0.0313],\n",
      "        [    0.0067],\n",
      "        [    0.0146],\n",
      "        [    0.0482],\n",
      "        [    0.0181],\n",
      "        [    0.0900],\n",
      "        [    0.0362],\n",
      "        [    0.0657],\n",
      "        [    0.0353],\n",
      "        [    0.0414],\n",
      "        [    0.0517],\n",
      "        [    0.0007],\n",
      "        [    0.1312],\n",
      "        [    0.0176],\n",
      "        [    0.0151],\n",
      "        [    0.0427],\n",
      "        [    0.0076],\n",
      "        [    0.0214],\n",
      "        [    0.0515],\n",
      "        [    0.0006],\n",
      "        [    0.0412],\n",
      "        [    0.0289],\n",
      "        [    0.0515],\n",
      "        [    0.0303],\n",
      "        [    0.0296],\n",
      "        [    0.0410],\n",
      "        [    0.0263],\n",
      "        [    0.0233],\n",
      "        [    0.0176],\n",
      "        [    0.0807],\n",
      "        [    0.0282],\n",
      "        [    0.0163],\n",
      "        [    0.0535],\n",
      "        [    0.0853],\n",
      "        [    0.0671],\n",
      "        [    0.0567],\n",
      "        [    0.0855],\n",
      "        [    0.0743],\n",
      "        [    0.0925],\n",
      "        [    0.0604],\n",
      "        [    0.0688],\n",
      "        [    0.0508],\n",
      "        [    0.0671],\n",
      "        [    0.0329],\n",
      "        [    0.0484],\n",
      "        [    0.0684],\n",
      "        [    0.1020],\n",
      "        [    0.0665],\n",
      "        [    0.0313],\n",
      "        [    0.0267],\n",
      "        [    0.0484],\n",
      "        [    0.1336],\n",
      "        [    0.1107],\n",
      "        [    0.0820],\n",
      "        [    0.0174],\n",
      "        [    0.0883],\n",
      "        [    0.1320],\n",
      "        [    0.1026],\n",
      "        [    0.0118],\n",
      "        [    0.0978],\n",
      "        [    0.1035],\n",
      "        [    0.0090],\n",
      "        [    0.1151],\n",
      "        [    0.1027],\n",
      "        [    0.1223],\n",
      "        [    0.1230],\n",
      "        [    0.1110],\n",
      "        [    0.1234],\n",
      "        [    0.1293],\n",
      "        [    0.1737],\n",
      "        [    0.1634],\n",
      "        [    0.0846],\n",
      "        [    0.0883],\n",
      "        [    0.1424],\n",
      "        [    0.1422],\n",
      "        [    0.1577],\n",
      "        [    0.3152],\n",
      "        [    0.3292],\n",
      "        [    0.3322],\n",
      "        [    0.3341],\n",
      "        [    0.3351],\n",
      "        [    0.3446],\n",
      "        [    0.3464],\n",
      "        [    0.3475],\n",
      "        [    0.3549],\n",
      "        [    0.3567],\n",
      "        [    0.3588],\n",
      "        [    0.3598],\n",
      "        [    0.3631],\n",
      "        [    0.3821],\n",
      "        [    0.3831],\n",
      "        [    0.3900]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0125],\n",
      "        [0.0417],\n",
      "        [0.0157],\n",
      "        [0.0719],\n",
      "        [0.0012],\n",
      "        [0.0109],\n",
      "        [0.0040],\n",
      "        [0.0067],\n",
      "        [0.0110],\n",
      "        [0.0212],\n",
      "        [0.0094],\n",
      "        [0.0998],\n",
      "        [0.0035],\n",
      "        [0.0181],\n",
      "        [0.0092],\n",
      "        [0.0872],\n",
      "        [0.0139],\n",
      "        [0.0077],\n",
      "        [0.0045],\n",
      "        [0.0167],\n",
      "        [0.0347],\n",
      "        [0.0671],\n",
      "        [0.0196],\n",
      "        [0.0140],\n",
      "        [0.0173],\n",
      "        [0.0122],\n",
      "        [0.0231],\n",
      "        [0.0172],\n",
      "        [0.0008],\n",
      "        [0.0362],\n",
      "        [0.0225],\n",
      "        [0.0615],\n",
      "        [0.0500],\n",
      "        [0.0377],\n",
      "        [0.0197],\n",
      "        [0.0257],\n",
      "        [0.0153],\n",
      "        [0.0036],\n",
      "        [0.0321],\n",
      "        [0.0267],\n",
      "        [0.0034],\n",
      "        [0.0263],\n",
      "        [0.0044],\n",
      "        [0.0404],\n",
      "        [0.0315],\n",
      "        [0.0255],\n",
      "        [0.0306],\n",
      "        [0.0059],\n",
      "        [0.0156],\n",
      "        [0.0481],\n",
      "        [0.0173],\n",
      "        [0.0894],\n",
      "        [0.0362],\n",
      "        [0.0668],\n",
      "        [0.0349],\n",
      "        [0.0412],\n",
      "        [0.0526],\n",
      "        [0.0011],\n",
      "        [0.1316],\n",
      "        [0.0171],\n",
      "        [0.0143],\n",
      "        [0.0420],\n",
      "        [0.0069],\n",
      "        [0.0204],\n",
      "        [0.0522],\n",
      "        [0.0004],\n",
      "        [0.0411],\n",
      "        [0.0284],\n",
      "        [0.0513],\n",
      "        [0.0298],\n",
      "        [0.0291],\n",
      "        [0.0412],\n",
      "        [0.0256],\n",
      "        [0.0230],\n",
      "        [0.0183],\n",
      "        [0.0810],\n",
      "        [0.0282],\n",
      "        [0.0138],\n",
      "        [0.0545],\n",
      "        [0.0857],\n",
      "        [0.0681],\n",
      "        [0.0572],\n",
      "        [0.0859],\n",
      "        [0.0754],\n",
      "        [0.0941],\n",
      "        [0.0609],\n",
      "        [0.0688],\n",
      "        [0.0499],\n",
      "        [0.0668],\n",
      "        [0.0330],\n",
      "        [0.0474],\n",
      "        [0.0677],\n",
      "        [0.1032],\n",
      "        [0.0655],\n",
      "        [0.0310],\n",
      "        [0.0263],\n",
      "        [0.0485],\n",
      "        [0.1344],\n",
      "        [0.1110],\n",
      "        [0.0813],\n",
      "        [0.0173],\n",
      "        [0.0883],\n",
      "        [0.1326],\n",
      "        [0.1026],\n",
      "        [0.0113],\n",
      "        [0.0973],\n",
      "        [0.1025],\n",
      "        [0.0095],\n",
      "        [0.1145],\n",
      "        [0.1018],\n",
      "        [0.1212],\n",
      "        [0.1226],\n",
      "        [0.1094],\n",
      "        [0.1233],\n",
      "        [0.1275],\n",
      "        [0.1723],\n",
      "        [0.1626],\n",
      "        [0.0834],\n",
      "        [0.0868],\n",
      "        [0.1410],\n",
      "        [0.1407],\n",
      "        [0.1561],\n",
      "        [0.3148],\n",
      "        [0.3288],\n",
      "        [0.3318],\n",
      "        [0.3337],\n",
      "        [0.3347],\n",
      "        [0.3442],\n",
      "        [0.3460],\n",
      "        [0.3471],\n",
      "        [0.3545],\n",
      "        [0.3563],\n",
      "        [0.3584],\n",
      "        [0.3594],\n",
      "        [0.3626],\n",
      "        [0.3817],\n",
      "        [0.3827],\n",
      "        [0.3896]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 83.83890843391418\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 139\n",
      "剩餘X 資料 torch.Size([21, 18])\n",
      "剩餘Y 資料 torch.Size([21, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15525032579898834, 12)\n",
      "The second_loss value of k: (0.15682853758335114, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引12，y= tensor([0.0711])\n",
      "目前模型的Data狀態 torch.Size([139, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6403],\n",
      "        [0.1325],\n",
      "        [0.5209],\n",
      "        [0.7308],\n",
      "        [0.2190],\n",
      "        [0.2227],\n",
      "        [0.6326],\n",
      "        [0.1854],\n",
      "        [0.6416],\n",
      "        [0.6320],\n",
      "        [0.6345],\n",
      "        [0.2754],\n",
      "        [0.6251],\n",
      "        [0.6088],\n",
      "        [0.6162],\n",
      "        [0.2055],\n",
      "        [0.5544],\n",
      "        [0.6243],\n",
      "        [0.6289],\n",
      "        [0.4237],\n",
      "        [0.6420],\n",
      "        [0.1577],\n",
      "        [0.4757],\n",
      "        [0.5748],\n",
      "        [0.6589],\n",
      "        [0.2744],\n",
      "        [0.1572],\n",
      "        [0.1368],\n",
      "        [0.6356],\n",
      "        [0.4347],\n",
      "        [0.5626],\n",
      "        [0.4498],\n",
      "        [0.1431],\n",
      "        [0.6184],\n",
      "        [0.5808],\n",
      "        [0.2174],\n",
      "        [0.4586],\n",
      "        [0.1122],\n",
      "        [0.5990],\n",
      "        [0.6500],\n",
      "        [0.6488],\n",
      "        [0.5822],\n",
      "        [0.3498],\n",
      "        [0.5110],\n",
      "        [0.5905],\n",
      "        [0.6175],\n",
      "        [0.2198],\n",
      "        [0.3521],\n",
      "        [0.6204],\n",
      "        [0.3734],\n",
      "        [0.2013],\n",
      "        [0.1703],\n",
      "        [0.2183],\n",
      "        [0.2242],\n",
      "        [0.5828],\n",
      "        [0.5676],\n",
      "        [0.4814],\n",
      "        [0.6198],\n",
      "        [0.2759],\n",
      "        [0.1553],\n",
      "        [0.3436],\n",
      "        [0.3130],\n",
      "        [0.1395],\n",
      "        [0.2878],\n",
      "        [0.6195],\n",
      "        [0.1327],\n",
      "        [0.1592],\n",
      "        [0.2787],\n",
      "        [0.1462],\n",
      "        [0.3288],\n",
      "        [0.3297],\n",
      "        [0.4307],\n",
      "        [0.3379],\n",
      "        [0.3415],\n",
      "        [0.6977],\n",
      "        [0.3952],\n",
      "        [0.5889],\n",
      "        [0.4707],\n",
      "        [0.4327],\n",
      "        [0.6495],\n",
      "        [0.5424],\n",
      "        [0.4073],\n",
      "        [0.3973],\n",
      "        [0.4476],\n",
      "        [0.4324],\n",
      "        [0.5629],\n",
      "        [0.2390],\n",
      "        [0.2307],\n",
      "        [0.2258],\n",
      "        [0.6390],\n",
      "        [0.3437],\n",
      "        [0.2878],\n",
      "        [0.4297],\n",
      "        [0.2531],\n",
      "        [0.6597],\n",
      "        [0.6703],\n",
      "        [0.6407],\n",
      "        [0.1683],\n",
      "        [0.4374],\n",
      "        [0.3462],\n",
      "        [0.4169],\n",
      "        [0.3106],\n",
      "        [0.1867],\n",
      "        [0.0594],\n",
      "        [0.2984],\n",
      "        [0.3549],\n",
      "        [0.2652],\n",
      "        [0.4237],\n",
      "        [0.3054],\n",
      "        [0.3492],\n",
      "        [0.1212],\n",
      "        [0.4094],\n",
      "        [0.1365],\n",
      "        [0.3132],\n",
      "        [0.5512],\n",
      "        [0.5928],\n",
      "        [0.3900],\n",
      "        [0.3258],\n",
      "        [0.3963],\n",
      "        [0.3262],\n",
      "        [0.3578],\n",
      "        [0.3397],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652],\n",
      "        [0.4652]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0125],\n",
      "        [0.0417],\n",
      "        [0.0157],\n",
      "        [0.0719],\n",
      "        [0.0012],\n",
      "        [0.0109],\n",
      "        [0.0040],\n",
      "        [0.0067],\n",
      "        [0.0110],\n",
      "        [0.0212],\n",
      "        [0.0094],\n",
      "        [0.0998],\n",
      "        [0.0035],\n",
      "        [0.0181],\n",
      "        [0.0092],\n",
      "        [0.0872],\n",
      "        [0.0139],\n",
      "        [0.0077],\n",
      "        [0.0045],\n",
      "        [0.0167],\n",
      "        [0.0347],\n",
      "        [0.0671],\n",
      "        [0.0196],\n",
      "        [0.0140],\n",
      "        [0.0173],\n",
      "        [0.0122],\n",
      "        [0.0231],\n",
      "        [0.0172],\n",
      "        [0.0008],\n",
      "        [0.0362],\n",
      "        [0.0225],\n",
      "        [0.0615],\n",
      "        [0.0500],\n",
      "        [0.0377],\n",
      "        [0.0197],\n",
      "        [0.0257],\n",
      "        [0.0153],\n",
      "        [0.0036],\n",
      "        [0.0321],\n",
      "        [0.0267],\n",
      "        [0.0034],\n",
      "        [0.0263],\n",
      "        [0.0044],\n",
      "        [0.0404],\n",
      "        [0.0315],\n",
      "        [0.0255],\n",
      "        [0.0306],\n",
      "        [0.0059],\n",
      "        [0.0156],\n",
      "        [0.0481],\n",
      "        [0.0173],\n",
      "        [0.0894],\n",
      "        [0.0362],\n",
      "        [0.0668],\n",
      "        [0.0349],\n",
      "        [0.0412],\n",
      "        [0.0526],\n",
      "        [0.0011],\n",
      "        [0.1316],\n",
      "        [0.0171],\n",
      "        [0.0143],\n",
      "        [0.0420],\n",
      "        [0.0069],\n",
      "        [0.0204],\n",
      "        [0.0522],\n",
      "        [0.0004],\n",
      "        [0.0411],\n",
      "        [0.0284],\n",
      "        [0.0513],\n",
      "        [0.0298],\n",
      "        [0.0291],\n",
      "        [0.0412],\n",
      "        [0.0256],\n",
      "        [0.0230],\n",
      "        [0.0183],\n",
      "        [0.0810],\n",
      "        [0.0282],\n",
      "        [0.0138],\n",
      "        [0.0545],\n",
      "        [0.0857],\n",
      "        [0.0681],\n",
      "        [0.0572],\n",
      "        [0.0859],\n",
      "        [0.0754],\n",
      "        [0.0941],\n",
      "        [0.0609],\n",
      "        [0.0688],\n",
      "        [0.0499],\n",
      "        [0.0668],\n",
      "        [0.0330],\n",
      "        [0.0474],\n",
      "        [0.0677],\n",
      "        [0.1032],\n",
      "        [0.0655],\n",
      "        [0.0310],\n",
      "        [0.0263],\n",
      "        [0.0485],\n",
      "        [0.1344],\n",
      "        [0.1110],\n",
      "        [0.0813],\n",
      "        [0.0173],\n",
      "        [0.0883],\n",
      "        [0.1326],\n",
      "        [0.1026],\n",
      "        [0.0113],\n",
      "        [0.0973],\n",
      "        [0.1025],\n",
      "        [0.0095],\n",
      "        [0.1145],\n",
      "        [0.1018],\n",
      "        [0.1212],\n",
      "        [0.1226],\n",
      "        [0.1094],\n",
      "        [0.1233],\n",
      "        [0.1275],\n",
      "        [0.1723],\n",
      "        [0.1626],\n",
      "        [0.0834],\n",
      "        [0.0868],\n",
      "        [0.1410],\n",
      "        [0.1407],\n",
      "        [0.1561],\n",
      "        [0.3148],\n",
      "        [0.3288],\n",
      "        [0.3318],\n",
      "        [0.3337],\n",
      "        [0.3347],\n",
      "        [0.3442],\n",
      "        [0.3460],\n",
      "        [0.3471],\n",
      "        [0.3545],\n",
      "        [0.3563],\n",
      "        [0.3584],\n",
      "        [0.3594],\n",
      "        [0.3626],\n",
      "        [0.3817],\n",
      "        [0.3827],\n",
      "        [0.3896],\n",
      "        [0.3940]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0101],\n",
      "        [    0.0396],\n",
      "        [    0.0181],\n",
      "        [    0.0702],\n",
      "        [    0.0008],\n",
      "        [    0.0120],\n",
      "        [    0.0010],\n",
      "        [    0.0085],\n",
      "        [    0.0087],\n",
      "        [    0.0197],\n",
      "        [    0.0121],\n",
      "        [    0.0986],\n",
      "        [    0.0019],\n",
      "        [    0.0156],\n",
      "        [    0.0060],\n",
      "        [    0.0859],\n",
      "        [    0.0116],\n",
      "        [    0.0047],\n",
      "        [    0.0066],\n",
      "        [    0.0188],\n",
      "        [    0.0324],\n",
      "        [    0.0653],\n",
      "        [    0.0234],\n",
      "        [    0.0172],\n",
      "        [    0.0150],\n",
      "        [    0.0099],\n",
      "        [    0.0247],\n",
      "        [    0.0148],\n",
      "        [    0.0034],\n",
      "        [    0.0390],\n",
      "        [    0.0246],\n",
      "        [    0.0653],\n",
      "        [    0.0474],\n",
      "        [    0.0357],\n",
      "        [    0.0170],\n",
      "        [    0.0245],\n",
      "        [    0.0176],\n",
      "        [    0.0014],\n",
      "        [    0.0291],\n",
      "        [    0.0289],\n",
      "        [    0.0011],\n",
      "        [    0.0237],\n",
      "        [    0.0062],\n",
      "        [    0.0425],\n",
      "        [    0.0290],\n",
      "        [    0.0273],\n",
      "        [    0.0291],\n",
      "        [    0.0069],\n",
      "        [    0.0184],\n",
      "        [    0.0461],\n",
      "        [    0.0154],\n",
      "        [    0.0876],\n",
      "        [    0.0374],\n",
      "        [    0.0690],\n",
      "        [    0.0324],\n",
      "        [    0.0388],\n",
      "        [    0.0551],\n",
      "        [    0.0004],\n",
      "        [    0.1305],\n",
      "        [    0.0155],\n",
      "        [    0.0152],\n",
      "        [    0.0397],\n",
      "        [    0.0051],\n",
      "        [    0.0179],\n",
      "        [    0.0546],\n",
      "        [    0.0024],\n",
      "        [    0.0408],\n",
      "        [    0.0296],\n",
      "        [    0.0514],\n",
      "        [    0.0310],\n",
      "        [    0.0303],\n",
      "        [    0.0433],\n",
      "        [    0.0265],\n",
      "        [    0.0245],\n",
      "        [    0.0165],\n",
      "        [    0.0792],\n",
      "        [    0.0299],\n",
      "        [    0.0100],\n",
      "        [    0.0571],\n",
      "        [    0.0842],\n",
      "        [    0.0704],\n",
      "        [    0.0595],\n",
      "        [    0.0841],\n",
      "        [    0.0781],\n",
      "        [    0.0971],\n",
      "        [    0.0629],\n",
      "        [    0.0702],\n",
      "        [    0.0479],\n",
      "        [    0.0654],\n",
      "        [    0.0352],\n",
      "        [    0.0482],\n",
      "        [    0.0656],\n",
      "        [    0.1059],\n",
      "        [    0.0633],\n",
      "        [    0.0329],\n",
      "        [    0.0282],\n",
      "        [    0.0507],\n",
      "        [    0.1365],\n",
      "        [    0.1095],\n",
      "        [    0.0790],\n",
      "        [    0.0152],\n",
      "        [    0.0900],\n",
      "        [    0.1344],\n",
      "        [    0.1038],\n",
      "        [    0.0123],\n",
      "        [    0.0986],\n",
      "        [    0.1002],\n",
      "        [    0.0080],\n",
      "        [    0.1126],\n",
      "        [    0.1028],\n",
      "        [    0.1192],\n",
      "        [    0.1205],\n",
      "        [    0.1070],\n",
      "        [    0.1249],\n",
      "        [    0.1241],\n",
      "        [    0.1692],\n",
      "        [    0.1601],\n",
      "        [    0.0839],\n",
      "        [    0.0872],\n",
      "        [    0.1413],\n",
      "        [    0.1408],\n",
      "        [    0.1562],\n",
      "        [    0.3144],\n",
      "        [    0.3284],\n",
      "        [    0.3314],\n",
      "        [    0.3332],\n",
      "        [    0.3343],\n",
      "        [    0.3437],\n",
      "        [    0.3455],\n",
      "        [    0.3467],\n",
      "        [    0.3541],\n",
      "        [    0.3559],\n",
      "        [    0.3580],\n",
      "        [    0.3590],\n",
      "        [    0.3622],\n",
      "        [    0.3813],\n",
      "        [    0.3823],\n",
      "        [    0.3892],\n",
      "        [    0.3936]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 84.07470440864563\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 140\n",
      "剩餘X 資料 torch.Size([20, 18])\n",
      "剩餘Y 資料 torch.Size([20, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15649457275867462, 3)\n",
      "The second_loss value of k: (0.15717710554599762, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.0691])\n",
      "目前模型的Data狀態 torch.Size([140, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6379],\n",
      "        [0.1304],\n",
      "        [0.5185],\n",
      "        [0.7292],\n",
      "        [0.2170],\n",
      "        [0.2216],\n",
      "        [0.6297],\n",
      "        [0.1836],\n",
      "        [0.6392],\n",
      "        [0.6304],\n",
      "        [0.6318],\n",
      "        [0.2742],\n",
      "        [0.6235],\n",
      "        [0.6063],\n",
      "        [0.6130],\n",
      "        [0.2041],\n",
      "        [0.5522],\n",
      "        [0.6212],\n",
      "        [0.6267],\n",
      "        [0.4216],\n",
      "        [0.6397],\n",
      "        [0.1559],\n",
      "        [0.4720],\n",
      "        [0.5716],\n",
      "        [0.6565],\n",
      "        [0.2721],\n",
      "        [0.1556],\n",
      "        [0.1344],\n",
      "        [0.6330],\n",
      "        [0.4319],\n",
      "        [0.5605],\n",
      "        [0.4461],\n",
      "        [0.1405],\n",
      "        [0.6164],\n",
      "        [0.5780],\n",
      "        [0.2162],\n",
      "        [0.4564],\n",
      "        [0.1100],\n",
      "        [0.5960],\n",
      "        [0.6478],\n",
      "        [0.6465],\n",
      "        [0.5795],\n",
      "        [0.3481],\n",
      "        [0.5090],\n",
      "        [0.5880],\n",
      "        [0.6157],\n",
      "        [0.2182],\n",
      "        [0.3511],\n",
      "        [0.6176],\n",
      "        [0.3714],\n",
      "        [0.1995],\n",
      "        [0.1685],\n",
      "        [0.2171],\n",
      "        [0.2219],\n",
      "        [0.5804],\n",
      "        [0.5653],\n",
      "        [0.4789],\n",
      "        [0.6184],\n",
      "        [0.2749],\n",
      "        [0.1537],\n",
      "        [0.3427],\n",
      "        [0.3107],\n",
      "        [0.1377],\n",
      "        [0.2853],\n",
      "        [0.6171],\n",
      "        [0.1307],\n",
      "        [0.1589],\n",
      "        [0.2775],\n",
      "        [0.1461],\n",
      "        [0.3275],\n",
      "        [0.3284],\n",
      "        [0.4286],\n",
      "        [0.3370],\n",
      "        [0.3400],\n",
      "        [0.6960],\n",
      "        [0.3934],\n",
      "        [0.5872],\n",
      "        [0.4668],\n",
      "        [0.4301],\n",
      "        [0.6480],\n",
      "        [0.5401],\n",
      "        [0.4051],\n",
      "        [0.3955],\n",
      "        [0.4450],\n",
      "        [0.4295],\n",
      "        [0.5609],\n",
      "        [0.2376],\n",
      "        [0.2287],\n",
      "        [0.2245],\n",
      "        [0.6368],\n",
      "        [0.3429],\n",
      "        [0.2857],\n",
      "        [0.4270],\n",
      "        [0.2509],\n",
      "        [0.6578],\n",
      "        [0.6684],\n",
      "        [0.6386],\n",
      "        [0.1662],\n",
      "        [0.4358],\n",
      "        [0.3439],\n",
      "        [0.4148],\n",
      "        [0.3089],\n",
      "        [0.1849],\n",
      "        [0.0583],\n",
      "        [0.2974],\n",
      "        [0.3536],\n",
      "        [0.2629],\n",
      "        [0.4221],\n",
      "        [0.3034],\n",
      "        [0.3482],\n",
      "        [0.1192],\n",
      "        [0.4072],\n",
      "        [0.1341],\n",
      "        [0.3116],\n",
      "        [0.5478],\n",
      "        [0.5897],\n",
      "        [0.3875],\n",
      "        [0.3253],\n",
      "        [0.3959],\n",
      "        [0.3259],\n",
      "        [0.3577],\n",
      "        [0.3396],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647],\n",
      "        [0.4647]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0101],\n",
      "        [    0.0396],\n",
      "        [    0.0181],\n",
      "        [    0.0702],\n",
      "        [    0.0008],\n",
      "        [    0.0120],\n",
      "        [    0.0010],\n",
      "        [    0.0085],\n",
      "        [    0.0087],\n",
      "        [    0.0197],\n",
      "        [    0.0121],\n",
      "        [    0.0986],\n",
      "        [    0.0019],\n",
      "        [    0.0156],\n",
      "        [    0.0060],\n",
      "        [    0.0859],\n",
      "        [    0.0116],\n",
      "        [    0.0047],\n",
      "        [    0.0066],\n",
      "        [    0.0188],\n",
      "        [    0.0324],\n",
      "        [    0.0653],\n",
      "        [    0.0234],\n",
      "        [    0.0172],\n",
      "        [    0.0150],\n",
      "        [    0.0099],\n",
      "        [    0.0247],\n",
      "        [    0.0148],\n",
      "        [    0.0034],\n",
      "        [    0.0390],\n",
      "        [    0.0246],\n",
      "        [    0.0653],\n",
      "        [    0.0474],\n",
      "        [    0.0357],\n",
      "        [    0.0170],\n",
      "        [    0.0245],\n",
      "        [    0.0176],\n",
      "        [    0.0014],\n",
      "        [    0.0291],\n",
      "        [    0.0289],\n",
      "        [    0.0011],\n",
      "        [    0.0237],\n",
      "        [    0.0062],\n",
      "        [    0.0425],\n",
      "        [    0.0290],\n",
      "        [    0.0273],\n",
      "        [    0.0291],\n",
      "        [    0.0069],\n",
      "        [    0.0184],\n",
      "        [    0.0461],\n",
      "        [    0.0154],\n",
      "        [    0.0876],\n",
      "        [    0.0374],\n",
      "        [    0.0690],\n",
      "        [    0.0324],\n",
      "        [    0.0388],\n",
      "        [    0.0551],\n",
      "        [    0.0004],\n",
      "        [    0.1305],\n",
      "        [    0.0155],\n",
      "        [    0.0152],\n",
      "        [    0.0397],\n",
      "        [    0.0051],\n",
      "        [    0.0179],\n",
      "        [    0.0546],\n",
      "        [    0.0024],\n",
      "        [    0.0408],\n",
      "        [    0.0296],\n",
      "        [    0.0514],\n",
      "        [    0.0310],\n",
      "        [    0.0303],\n",
      "        [    0.0433],\n",
      "        [    0.0265],\n",
      "        [    0.0245],\n",
      "        [    0.0165],\n",
      "        [    0.0792],\n",
      "        [    0.0299],\n",
      "        [    0.0100],\n",
      "        [    0.0571],\n",
      "        [    0.0842],\n",
      "        [    0.0704],\n",
      "        [    0.0595],\n",
      "        [    0.0841],\n",
      "        [    0.0781],\n",
      "        [    0.0971],\n",
      "        [    0.0629],\n",
      "        [    0.0702],\n",
      "        [    0.0479],\n",
      "        [    0.0654],\n",
      "        [    0.0352],\n",
      "        [    0.0482],\n",
      "        [    0.0656],\n",
      "        [    0.1059],\n",
      "        [    0.0633],\n",
      "        [    0.0329],\n",
      "        [    0.0282],\n",
      "        [    0.0507],\n",
      "        [    0.1365],\n",
      "        [    0.1095],\n",
      "        [    0.0790],\n",
      "        [    0.0152],\n",
      "        [    0.0900],\n",
      "        [    0.1344],\n",
      "        [    0.1038],\n",
      "        [    0.0123],\n",
      "        [    0.0986],\n",
      "        [    0.1002],\n",
      "        [    0.0080],\n",
      "        [    0.1126],\n",
      "        [    0.1028],\n",
      "        [    0.1192],\n",
      "        [    0.1205],\n",
      "        [    0.1070],\n",
      "        [    0.1249],\n",
      "        [    0.1241],\n",
      "        [    0.1692],\n",
      "        [    0.1601],\n",
      "        [    0.0839],\n",
      "        [    0.0872],\n",
      "        [    0.1413],\n",
      "        [    0.1408],\n",
      "        [    0.1562],\n",
      "        [    0.3144],\n",
      "        [    0.3284],\n",
      "        [    0.3314],\n",
      "        [    0.3332],\n",
      "        [    0.3343],\n",
      "        [    0.3437],\n",
      "        [    0.3455],\n",
      "        [    0.3467],\n",
      "        [    0.3541],\n",
      "        [    0.3559],\n",
      "        [    0.3580],\n",
      "        [    0.3590],\n",
      "        [    0.3622],\n",
      "        [    0.3813],\n",
      "        [    0.3823],\n",
      "        [    0.3892],\n",
      "        [    0.3936],\n",
      "        [    0.3956]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0130],\n",
      "        [    0.0403],\n",
      "        [    0.0157],\n",
      "        [    0.0736],\n",
      "        [    0.0001],\n",
      "        [    0.0104],\n",
      "        [    0.0034],\n",
      "        [    0.0075],\n",
      "        [    0.0115],\n",
      "        [    0.0230],\n",
      "        [    0.0094],\n",
      "        [    0.1002],\n",
      "        [    0.0052],\n",
      "        [    0.0184],\n",
      "        [    0.0082],\n",
      "        [    0.0873],\n",
      "        [    0.0143],\n",
      "        [    0.0071],\n",
      "        [    0.0037],\n",
      "        [    0.0164],\n",
      "        [    0.0355],\n",
      "        [    0.0662],\n",
      "        [    0.0223],\n",
      "        [    0.0154],\n",
      "        [    0.0177],\n",
      "        [    0.0111],\n",
      "        [    0.0236],\n",
      "        [    0.0152],\n",
      "        [    0.0010],\n",
      "        [    0.0372],\n",
      "        [    0.0223],\n",
      "        [    0.0643],\n",
      "        [    0.0476],\n",
      "        [    0.0386],\n",
      "        [    0.0195],\n",
      "        [    0.0262],\n",
      "        [    0.0153],\n",
      "        [    0.0019],\n",
      "        [    0.0311],\n",
      "        [    0.0260],\n",
      "        [    0.0037],\n",
      "        [    0.0262],\n",
      "        [    0.0043],\n",
      "        [    0.0400],\n",
      "        [    0.0317],\n",
      "        [    0.0240],\n",
      "        [    0.0302],\n",
      "        [    0.0046],\n",
      "        [    0.0162],\n",
      "        [    0.0480],\n",
      "        [    0.0166],\n",
      "        [    0.0886],\n",
      "        [    0.0361],\n",
      "        [    0.0677],\n",
      "        [    0.0351],\n",
      "        [    0.0416],\n",
      "        [    0.0529],\n",
      "        [    0.0030],\n",
      "        [    0.1323],\n",
      "        [    0.0166],\n",
      "        [    0.0128],\n",
      "        [    0.0411],\n",
      "        [    0.0061],\n",
      "        [    0.0193],\n",
      "        [    0.0519],\n",
      "        [    0.0017],\n",
      "        [    0.0412],\n",
      "        [    0.0276],\n",
      "        [    0.0509],\n",
      "        [    0.0289],\n",
      "        [    0.0282],\n",
      "        [    0.0409],\n",
      "        [    0.0241],\n",
      "        [    0.0225],\n",
      "        [    0.0197],\n",
      "        [    0.0813],\n",
      "        [    0.0270],\n",
      "        [    0.0109],\n",
      "        [    0.0552],\n",
      "        [    0.0876],\n",
      "        [    0.0682],\n",
      "        [    0.0573],\n",
      "        [    0.0863],\n",
      "        [    0.0761],\n",
      "        [    0.0954],\n",
      "        [    0.0604],\n",
      "        [    0.0686],\n",
      "        [    0.0491],\n",
      "        [    0.0667],\n",
      "        [    0.0325],\n",
      "        [    0.0457],\n",
      "        [    0.0669],\n",
      "        [    0.1040],\n",
      "        [    0.0644],\n",
      "        [    0.0300],\n",
      "        [    0.0252],\n",
      "        [    0.0481],\n",
      "        [    0.1352],\n",
      "        [    0.1122],\n",
      "        [    0.0807],\n",
      "        [    0.0170],\n",
      "        [    0.0883],\n",
      "        [    0.1330],\n",
      "        [    0.1027],\n",
      "        [    0.0103],\n",
      "        [    0.0964],\n",
      "        [    0.1012],\n",
      "        [    0.0101],\n",
      "        [    0.1141],\n",
      "        [    0.1005],\n",
      "        [    0.1195],\n",
      "        [    0.1225],\n",
      "        [    0.1073],\n",
      "        [    0.1231],\n",
      "        [    0.1256],\n",
      "        [    0.1711],\n",
      "        [    0.1618],\n",
      "        [    0.0814],\n",
      "        [    0.0843],\n",
      "        [    0.1386],\n",
      "        [    0.1380],\n",
      "        [    0.1534],\n",
      "        [    0.3139],\n",
      "        [    0.3279],\n",
      "        [    0.3309],\n",
      "        [    0.3328],\n",
      "        [    0.3338],\n",
      "        [    0.3433],\n",
      "        [    0.3451],\n",
      "        [    0.3463],\n",
      "        [    0.3536],\n",
      "        [    0.3554],\n",
      "        [    0.3575],\n",
      "        [    0.3586],\n",
      "        [    0.3618],\n",
      "        [    0.3809],\n",
      "        [    0.3818],\n",
      "        [    0.3888],\n",
      "        [    0.3932],\n",
      "        [    0.3952]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 84.31439065933228\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 141\n",
      "剩餘X 資料 torch.Size([19, 18])\n",
      "剩餘Y 資料 torch.Size([19, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15683239698410034, 3)\n",
      "The second_loss value of k: (0.16038908064365387, 17)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.0683])\n",
      "目前模型的Data狀態 torch.Size([141, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6408],\n",
      "        [0.1310],\n",
      "        [0.5209],\n",
      "        [0.7326],\n",
      "        [0.2177],\n",
      "        [0.2232],\n",
      "        [0.6320],\n",
      "        [0.1846],\n",
      "        [0.6420],\n",
      "        [0.6337],\n",
      "        [0.6345],\n",
      "        [0.2758],\n",
      "        [0.6268],\n",
      "        [0.6091],\n",
      "        [0.6151],\n",
      "        [0.2055],\n",
      "        [0.5549],\n",
      "        [0.6236],\n",
      "        [0.6296],\n",
      "        [0.4240],\n",
      "        [0.6427],\n",
      "        [0.1568],\n",
      "        [0.4731],\n",
      "        [0.5734],\n",
      "        [0.6592],\n",
      "        [0.2733],\n",
      "        [0.1566],\n",
      "        [0.1348],\n",
      "        [0.6354],\n",
      "        [0.4337],\n",
      "        [0.5629],\n",
      "        [0.4471],\n",
      "        [0.1407],\n",
      "        [0.6193],\n",
      "        [0.5805],\n",
      "        [0.2179],\n",
      "        [0.4587],\n",
      "        [0.1106],\n",
      "        [0.5980],\n",
      "        [0.6508],\n",
      "        [0.6491],\n",
      "        [0.5821],\n",
      "        [0.3500],\n",
      "        [0.5114],\n",
      "        [0.5906],\n",
      "        [0.6189],\n",
      "        [0.2193],\n",
      "        [0.3534],\n",
      "        [0.6198],\n",
      "        [0.3732],\n",
      "        [0.2006],\n",
      "        [0.1695],\n",
      "        [0.2184],\n",
      "        [0.2232],\n",
      "        [0.5831],\n",
      "        [0.5681],\n",
      "        [0.4811],\n",
      "        [0.6217],\n",
      "        [0.2767],\n",
      "        [0.1548],\n",
      "        [0.3451],\n",
      "        [0.3121],\n",
      "        [0.1387],\n",
      "        [0.2868],\n",
      "        [0.6198],\n",
      "        [0.1315],\n",
      "        [0.1593],\n",
      "        [0.2795],\n",
      "        [0.1467],\n",
      "        [0.3296],\n",
      "        [0.3305],\n",
      "        [0.4310],\n",
      "        [0.3394],\n",
      "        [0.3420],\n",
      "        [0.6991],\n",
      "        [0.3954],\n",
      "        [0.5901],\n",
      "        [0.4678],\n",
      "        [0.4320],\n",
      "        [0.6514],\n",
      "        [0.5423],\n",
      "        [0.4073],\n",
      "        [0.3977],\n",
      "        [0.4470],\n",
      "        [0.4311],\n",
      "        [0.5635],\n",
      "        [0.2392],\n",
      "        [0.2299],\n",
      "        [0.2258],\n",
      "        [0.6395],\n",
      "        [0.3454],\n",
      "        [0.2871],\n",
      "        [0.4290],\n",
      "        [0.2520],\n",
      "        [0.6608],\n",
      "        [0.6714],\n",
      "        [0.6412],\n",
      "        [0.1675],\n",
      "        [0.4386],\n",
      "        [0.3456],\n",
      "        [0.4166],\n",
      "        [0.3107],\n",
      "        [0.1862],\n",
      "        [0.0593],\n",
      "        [0.2994],\n",
      "        [0.3558],\n",
      "        [0.2640],\n",
      "        [0.4242],\n",
      "        [0.3049],\n",
      "        [0.3506],\n",
      "        [0.1195],\n",
      "        [0.4093],\n",
      "        [0.1344],\n",
      "        [0.3134],\n",
      "        [0.5493],\n",
      "        [0.5916],\n",
      "        [0.3892],\n",
      "        [0.3277],\n",
      "        [0.3988],\n",
      "        [0.3286],\n",
      "        [0.3605],\n",
      "        [0.3423],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643],\n",
      "        [0.4643]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0130],\n",
      "        [    0.0403],\n",
      "        [    0.0157],\n",
      "        [    0.0736],\n",
      "        [    0.0001],\n",
      "        [    0.0104],\n",
      "        [    0.0034],\n",
      "        [    0.0075],\n",
      "        [    0.0115],\n",
      "        [    0.0230],\n",
      "        [    0.0094],\n",
      "        [    0.1002],\n",
      "        [    0.0052],\n",
      "        [    0.0184],\n",
      "        [    0.0082],\n",
      "        [    0.0873],\n",
      "        [    0.0143],\n",
      "        [    0.0071],\n",
      "        [    0.0037],\n",
      "        [    0.0164],\n",
      "        [    0.0355],\n",
      "        [    0.0662],\n",
      "        [    0.0223],\n",
      "        [    0.0154],\n",
      "        [    0.0177],\n",
      "        [    0.0111],\n",
      "        [    0.0236],\n",
      "        [    0.0152],\n",
      "        [    0.0010],\n",
      "        [    0.0372],\n",
      "        [    0.0223],\n",
      "        [    0.0643],\n",
      "        [    0.0476],\n",
      "        [    0.0386],\n",
      "        [    0.0195],\n",
      "        [    0.0262],\n",
      "        [    0.0153],\n",
      "        [    0.0019],\n",
      "        [    0.0311],\n",
      "        [    0.0260],\n",
      "        [    0.0037],\n",
      "        [    0.0262],\n",
      "        [    0.0043],\n",
      "        [    0.0400],\n",
      "        [    0.0317],\n",
      "        [    0.0240],\n",
      "        [    0.0302],\n",
      "        [    0.0046],\n",
      "        [    0.0162],\n",
      "        [    0.0480],\n",
      "        [    0.0166],\n",
      "        [    0.0886],\n",
      "        [    0.0361],\n",
      "        [    0.0677],\n",
      "        [    0.0351],\n",
      "        [    0.0416],\n",
      "        [    0.0529],\n",
      "        [    0.0030],\n",
      "        [    0.1323],\n",
      "        [    0.0166],\n",
      "        [    0.0128],\n",
      "        [    0.0411],\n",
      "        [    0.0061],\n",
      "        [    0.0193],\n",
      "        [    0.0519],\n",
      "        [    0.0017],\n",
      "        [    0.0412],\n",
      "        [    0.0276],\n",
      "        [    0.0509],\n",
      "        [    0.0289],\n",
      "        [    0.0282],\n",
      "        [    0.0409],\n",
      "        [    0.0241],\n",
      "        [    0.0225],\n",
      "        [    0.0197],\n",
      "        [    0.0813],\n",
      "        [    0.0270],\n",
      "        [    0.0109],\n",
      "        [    0.0552],\n",
      "        [    0.0876],\n",
      "        [    0.0682],\n",
      "        [    0.0573],\n",
      "        [    0.0863],\n",
      "        [    0.0761],\n",
      "        [    0.0954],\n",
      "        [    0.0604],\n",
      "        [    0.0686],\n",
      "        [    0.0491],\n",
      "        [    0.0667],\n",
      "        [    0.0325],\n",
      "        [    0.0457],\n",
      "        [    0.0669],\n",
      "        [    0.1040],\n",
      "        [    0.0644],\n",
      "        [    0.0300],\n",
      "        [    0.0252],\n",
      "        [    0.0481],\n",
      "        [    0.1352],\n",
      "        [    0.1122],\n",
      "        [    0.0807],\n",
      "        [    0.0170],\n",
      "        [    0.0883],\n",
      "        [    0.1330],\n",
      "        [    0.1027],\n",
      "        [    0.0103],\n",
      "        [    0.0964],\n",
      "        [    0.1012],\n",
      "        [    0.0101],\n",
      "        [    0.1141],\n",
      "        [    0.1005],\n",
      "        [    0.1195],\n",
      "        [    0.1225],\n",
      "        [    0.1073],\n",
      "        [    0.1231],\n",
      "        [    0.1256],\n",
      "        [    0.1711],\n",
      "        [    0.1618],\n",
      "        [    0.0814],\n",
      "        [    0.0843],\n",
      "        [    0.1386],\n",
      "        [    0.1380],\n",
      "        [    0.1534],\n",
      "        [    0.3139],\n",
      "        [    0.3279],\n",
      "        [    0.3309],\n",
      "        [    0.3328],\n",
      "        [    0.3338],\n",
      "        [    0.3433],\n",
      "        [    0.3451],\n",
      "        [    0.3463],\n",
      "        [    0.3536],\n",
      "        [    0.3554],\n",
      "        [    0.3575],\n",
      "        [    0.3586],\n",
      "        [    0.3618],\n",
      "        [    0.3809],\n",
      "        [    0.3818],\n",
      "        [    0.3888],\n",
      "        [    0.3932],\n",
      "        [    0.3952],\n",
      "        [    0.3960]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0138],\n",
      "        [    0.0400],\n",
      "        [    0.0153],\n",
      "        [    0.0748],\n",
      "        [    0.0003],\n",
      "        [    0.0098],\n",
      "        [    0.0036],\n",
      "        [    0.0076],\n",
      "        [    0.0122],\n",
      "        [    0.0242],\n",
      "        [    0.0089],\n",
      "        [    0.1007],\n",
      "        [    0.0065],\n",
      "        [    0.0190],\n",
      "        [    0.0081],\n",
      "        [    0.0876],\n",
      "        [    0.0150],\n",
      "        [    0.0073],\n",
      "        [    0.0030],\n",
      "        [    0.0159],\n",
      "        [    0.0363],\n",
      "        [    0.0662],\n",
      "        [    0.0230],\n",
      "        [    0.0157],\n",
      "        [    0.0182],\n",
      "        [    0.0109],\n",
      "        [    0.0235],\n",
      "        [    0.0147],\n",
      "        [    0.0007],\n",
      "        [    0.0373],\n",
      "        [    0.0217],\n",
      "        [    0.0650],\n",
      "        [    0.0469],\n",
      "        [    0.0395],\n",
      "        [    0.0198],\n",
      "        [    0.0268],\n",
      "        [    0.0149],\n",
      "        [    0.0016],\n",
      "        [    0.0311],\n",
      "        [    0.0251],\n",
      "        [    0.0042],\n",
      "        [    0.0266],\n",
      "        [    0.0039],\n",
      "        [    0.0392],\n",
      "        [    0.0322],\n",
      "        [    0.0229],\n",
      "        [    0.0303],\n",
      "        [    0.0037],\n",
      "        [    0.0161],\n",
      "        [    0.0482],\n",
      "        [    0.0166],\n",
      "        [    0.0886],\n",
      "        [    0.0359],\n",
      "        [    0.0677],\n",
      "        [    0.0357],\n",
      "        [    0.0423],\n",
      "        [    0.0526],\n",
      "        [    0.0042],\n",
      "        [    0.1330],\n",
      "        [    0.0168],\n",
      "        [    0.0119],\n",
      "        [    0.0410],\n",
      "        [    0.0062],\n",
      "        [    0.0192],\n",
      "        [    0.0512],\n",
      "        [    0.0019],\n",
      "        [    0.0413],\n",
      "        [    0.0270],\n",
      "        [    0.0506],\n",
      "        [    0.0282],\n",
      "        [    0.0275],\n",
      "        [    0.0404],\n",
      "        [    0.0231],\n",
      "        [    0.0220],\n",
      "        [    0.0206],\n",
      "        [    0.0816],\n",
      "        [    0.0260],\n",
      "        [    0.0100],\n",
      "        [    0.0551],\n",
      "        [    0.0889],\n",
      "        [    0.0677],\n",
      "        [    0.0570],\n",
      "        [    0.0868],\n",
      "        [    0.0759],\n",
      "        [    0.0954],\n",
      "        [    0.0597],\n",
      "        [    0.0682],\n",
      "        [    0.0491],\n",
      "        [    0.0671],\n",
      "        [    0.0319],\n",
      "        [    0.0447],\n",
      "        [    0.0670],\n",
      "        [    0.1039],\n",
      "        [    0.0642],\n",
      "        [    0.0291],\n",
      "        [    0.0244],\n",
      "        [    0.0475],\n",
      "        [    0.1351],\n",
      "        [    0.1131],\n",
      "        [    0.0808],\n",
      "        [    0.0170],\n",
      "        [    0.0880],\n",
      "        [    0.1328],\n",
      "        [    0.1025],\n",
      "        [    0.0096],\n",
      "        [    0.0957],\n",
      "        [    0.1010],\n",
      "        [    0.0105],\n",
      "        [    0.1143],\n",
      "        [    0.0996],\n",
      "        [    0.1189],\n",
      "        [    0.1229],\n",
      "        [    0.1067],\n",
      "        [    0.1227],\n",
      "        [    0.1253],\n",
      "        [    0.1710],\n",
      "        [    0.1618],\n",
      "        [    0.0803],\n",
      "        [    0.0830],\n",
      "        [    0.1372],\n",
      "        [    0.1365],\n",
      "        [    0.1519],\n",
      "        [    0.3135],\n",
      "        [    0.3275],\n",
      "        [    0.3305],\n",
      "        [    0.3324],\n",
      "        [    0.3334],\n",
      "        [    0.3429],\n",
      "        [    0.3447],\n",
      "        [    0.3458],\n",
      "        [    0.3532],\n",
      "        [    0.3550],\n",
      "        [    0.3571],\n",
      "        [    0.3581],\n",
      "        [    0.3614],\n",
      "        [    0.3804],\n",
      "        [    0.3814],\n",
      "        [    0.3883],\n",
      "        [    0.3927],\n",
      "        [    0.3947],\n",
      "        [    0.3956]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 84.55356073379517\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 142\n",
      "剩餘X 資料 torch.Size([18, 18])\n",
      "剩餘Y 資料 torch.Size([18, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16004125773906708, 16)\n",
      "The second_loss value of k: (0.16605117917060852, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.0638])\n",
      "目前模型的Data狀態 torch.Size([142, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6415],\n",
      "        [0.1307],\n",
      "        [0.5213],\n",
      "        [0.7338],\n",
      "        [0.2175],\n",
      "        [0.2238],\n",
      "        [0.6322],\n",
      "        [0.1845],\n",
      "        [0.6427],\n",
      "        [0.6350],\n",
      "        [0.6351],\n",
      "        [0.2763],\n",
      "        [0.6281],\n",
      "        [0.6097],\n",
      "        [0.6151],\n",
      "        [0.2059],\n",
      "        [0.5555],\n",
      "        [0.6238],\n",
      "        [0.6304],\n",
      "        [0.4246],\n",
      "        [0.6435],\n",
      "        [0.1568],\n",
      "        [0.4723],\n",
      "        [0.5732],\n",
      "        [0.6597],\n",
      "        [0.2731],\n",
      "        [0.1567],\n",
      "        [0.1343],\n",
      "        [0.6357],\n",
      "        [0.4337],\n",
      "        [0.5635],\n",
      "        [0.4464],\n",
      "        [0.1400],\n",
      "        [0.6202],\n",
      "        [0.5809],\n",
      "        [0.2185],\n",
      "        [0.4591],\n",
      "        [0.1102],\n",
      "        [0.5980],\n",
      "        [0.6516],\n",
      "        [0.6496],\n",
      "        [0.5825],\n",
      "        [0.3503],\n",
      "        [0.5122],\n",
      "        [0.5912],\n",
      "        [0.6201],\n",
      "        [0.2195],\n",
      "        [0.3543],\n",
      "        [0.6199],\n",
      "        [0.3735],\n",
      "        [0.2007],\n",
      "        [0.1695],\n",
      "        [0.2187],\n",
      "        [0.2233],\n",
      "        [0.5837],\n",
      "        [0.5687],\n",
      "        [0.4814],\n",
      "        [0.6229],\n",
      "        [0.2774],\n",
      "        [0.1550],\n",
      "        [0.3460],\n",
      "        [0.3120],\n",
      "        [0.1388],\n",
      "        [0.2867],\n",
      "        [0.6205],\n",
      "        [0.1312],\n",
      "        [0.1594],\n",
      "        [0.2801],\n",
      "        [0.1469],\n",
      "        [0.3303],\n",
      "        [0.3312],\n",
      "        [0.4315],\n",
      "        [0.3404],\n",
      "        [0.3425],\n",
      "        [0.7001],\n",
      "        [0.3958],\n",
      "        [0.5911],\n",
      "        [0.4669],\n",
      "        [0.4321],\n",
      "        [0.6526],\n",
      "        [0.5428],\n",
      "        [0.4076],\n",
      "        [0.3982],\n",
      "        [0.4471],\n",
      "        [0.4311],\n",
      "        [0.5642],\n",
      "        [0.2396],\n",
      "        [0.2299],\n",
      "        [0.2262],\n",
      "        [0.6401],\n",
      "        [0.3464],\n",
      "        [0.2871],\n",
      "        [0.4290],\n",
      "        [0.2518],\n",
      "        [0.6616],\n",
      "        [0.6722],\n",
      "        [0.6418],\n",
      "        [0.1676],\n",
      "        [0.4395],\n",
      "        [0.3457],\n",
      "        [0.4166],\n",
      "        [0.3110],\n",
      "        [0.1864],\n",
      "        [0.0595],\n",
      "        [0.3001],\n",
      "        [0.3565],\n",
      "        [0.2638],\n",
      "        [0.4246],\n",
      "        [0.3051],\n",
      "        [0.3514],\n",
      "        [0.1189],\n",
      "        [0.4096],\n",
      "        [0.1338],\n",
      "        [0.3138],\n",
      "        [0.5490],\n",
      "        [0.5915],\n",
      "        [0.3892],\n",
      "        [0.3289],\n",
      "        [0.4001],\n",
      "        [0.3300],\n",
      "        [0.3620],\n",
      "        [0.3439],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639],\n",
      "        [0.4639]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0138],\n",
      "        [    0.0400],\n",
      "        [    0.0153],\n",
      "        [    0.0748],\n",
      "        [    0.0003],\n",
      "        [    0.0098],\n",
      "        [    0.0036],\n",
      "        [    0.0076],\n",
      "        [    0.0122],\n",
      "        [    0.0242],\n",
      "        [    0.0089],\n",
      "        [    0.1007],\n",
      "        [    0.0065],\n",
      "        [    0.0190],\n",
      "        [    0.0081],\n",
      "        [    0.0876],\n",
      "        [    0.0150],\n",
      "        [    0.0073],\n",
      "        [    0.0030],\n",
      "        [    0.0159],\n",
      "        [    0.0363],\n",
      "        [    0.0662],\n",
      "        [    0.0230],\n",
      "        [    0.0157],\n",
      "        [    0.0182],\n",
      "        [    0.0109],\n",
      "        [    0.0235],\n",
      "        [    0.0147],\n",
      "        [    0.0007],\n",
      "        [    0.0373],\n",
      "        [    0.0217],\n",
      "        [    0.0650],\n",
      "        [    0.0469],\n",
      "        [    0.0395],\n",
      "        [    0.0198],\n",
      "        [    0.0268],\n",
      "        [    0.0149],\n",
      "        [    0.0016],\n",
      "        [    0.0311],\n",
      "        [    0.0251],\n",
      "        [    0.0042],\n",
      "        [    0.0266],\n",
      "        [    0.0039],\n",
      "        [    0.0392],\n",
      "        [    0.0322],\n",
      "        [    0.0229],\n",
      "        [    0.0303],\n",
      "        [    0.0037],\n",
      "        [    0.0161],\n",
      "        [    0.0482],\n",
      "        [    0.0166],\n",
      "        [    0.0886],\n",
      "        [    0.0359],\n",
      "        [    0.0677],\n",
      "        [    0.0357],\n",
      "        [    0.0423],\n",
      "        [    0.0526],\n",
      "        [    0.0042],\n",
      "        [    0.1330],\n",
      "        [    0.0168],\n",
      "        [    0.0119],\n",
      "        [    0.0410],\n",
      "        [    0.0062],\n",
      "        [    0.0192],\n",
      "        [    0.0512],\n",
      "        [    0.0019],\n",
      "        [    0.0413],\n",
      "        [    0.0270],\n",
      "        [    0.0506],\n",
      "        [    0.0282],\n",
      "        [    0.0275],\n",
      "        [    0.0404],\n",
      "        [    0.0231],\n",
      "        [    0.0220],\n",
      "        [    0.0206],\n",
      "        [    0.0816],\n",
      "        [    0.0260],\n",
      "        [    0.0100],\n",
      "        [    0.0551],\n",
      "        [    0.0889],\n",
      "        [    0.0677],\n",
      "        [    0.0570],\n",
      "        [    0.0868],\n",
      "        [    0.0759],\n",
      "        [    0.0954],\n",
      "        [    0.0597],\n",
      "        [    0.0682],\n",
      "        [    0.0491],\n",
      "        [    0.0671],\n",
      "        [    0.0319],\n",
      "        [    0.0447],\n",
      "        [    0.0670],\n",
      "        [    0.1039],\n",
      "        [    0.0642],\n",
      "        [    0.0291],\n",
      "        [    0.0244],\n",
      "        [    0.0475],\n",
      "        [    0.1351],\n",
      "        [    0.1131],\n",
      "        [    0.0808],\n",
      "        [    0.0170],\n",
      "        [    0.0880],\n",
      "        [    0.1328],\n",
      "        [    0.1025],\n",
      "        [    0.0096],\n",
      "        [    0.0957],\n",
      "        [    0.1010],\n",
      "        [    0.0105],\n",
      "        [    0.1143],\n",
      "        [    0.0996],\n",
      "        [    0.1189],\n",
      "        [    0.1229],\n",
      "        [    0.1067],\n",
      "        [    0.1227],\n",
      "        [    0.1253],\n",
      "        [    0.1710],\n",
      "        [    0.1618],\n",
      "        [    0.0803],\n",
      "        [    0.0830],\n",
      "        [    0.1372],\n",
      "        [    0.1365],\n",
      "        [    0.1519],\n",
      "        [    0.3135],\n",
      "        [    0.3275],\n",
      "        [    0.3305],\n",
      "        [    0.3324],\n",
      "        [    0.3334],\n",
      "        [    0.3429],\n",
      "        [    0.3447],\n",
      "        [    0.3458],\n",
      "        [    0.3532],\n",
      "        [    0.3550],\n",
      "        [    0.3571],\n",
      "        [    0.3581],\n",
      "        [    0.3614],\n",
      "        [    0.3804],\n",
      "        [    0.3814],\n",
      "        [    0.3883],\n",
      "        [    0.3927],\n",
      "        [    0.3947],\n",
      "        [    0.3956],\n",
      "        [    0.4001]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0102],\n",
      "        [    0.0373],\n",
      "        [    0.0187],\n",
      "        [    0.0719],\n",
      "        [    0.0027],\n",
      "        [    0.0115],\n",
      "        [    0.0004],\n",
      "        [    0.0097],\n",
      "        [    0.0087],\n",
      "        [    0.0212],\n",
      "        [    0.0127],\n",
      "        [    0.0985],\n",
      "        [    0.0034],\n",
      "        [    0.0154],\n",
      "        [    0.0040],\n",
      "        [    0.0855],\n",
      "        [    0.0117],\n",
      "        [    0.0032],\n",
      "        [    0.0064],\n",
      "        [    0.0187],\n",
      "        [    0.0328],\n",
      "        [    0.0637],\n",
      "        [    0.0271],\n",
      "        [    0.0195],\n",
      "        [    0.0148],\n",
      "        [    0.0082],\n",
      "        [    0.0255],\n",
      "        [    0.0119],\n",
      "        [    0.0043],\n",
      "        [    0.0408],\n",
      "        [    0.0249],\n",
      "        [    0.0691],\n",
      "        [    0.0439],\n",
      "        [    0.0362],\n",
      "        [    0.0161],\n",
      "        [    0.0249],\n",
      "        [    0.0180],\n",
      "        [    0.0011],\n",
      "        [    0.0273],\n",
      "        [    0.0286],\n",
      "        [    0.0009],\n",
      "        [    0.0230],\n",
      "        [    0.0063],\n",
      "        [    0.0424],\n",
      "        [    0.0286],\n",
      "        [    0.0261],\n",
      "        [    0.0282],\n",
      "        [    0.0056],\n",
      "        [    0.0198],\n",
      "        [    0.0456],\n",
      "        [    0.0143],\n",
      "        [    0.0861],\n",
      "        [    0.0374],\n",
      "        [    0.0704],\n",
      "        [    0.0322],\n",
      "        [    0.0389],\n",
      "        [    0.0560],\n",
      "        [    0.0013],\n",
      "        [    0.1310],\n",
      "        [    0.0145],\n",
      "        [    0.0137],\n",
      "        [    0.0383],\n",
      "        [    0.0038],\n",
      "        [    0.0163],\n",
      "        [    0.0548],\n",
      "        [    0.0045],\n",
      "        [    0.0407],\n",
      "        [    0.0289],\n",
      "        [    0.0510],\n",
      "        [    0.0302],\n",
      "        [    0.0295],\n",
      "        [    0.0433],\n",
      "        [    0.0249],\n",
      "        [    0.0242],\n",
      "        [    0.0177],\n",
      "        [    0.0792],\n",
      "        [    0.0290],\n",
      "        [    0.0058],\n",
      "        [    0.0585],\n",
      "        [    0.0859],\n",
      "        [    0.0711],\n",
      "        [    0.0599],\n",
      "        [    0.0843],\n",
      "        [    0.0793],\n",
      "        [    0.0990],\n",
      "        [    0.0628],\n",
      "        [    0.0703],\n",
      "        [    0.0466],\n",
      "        [    0.0651],\n",
      "        [    0.0351],\n",
      "        [    0.0464],\n",
      "        [    0.0644],\n",
      "        [    0.1073],\n",
      "        [    0.0616],\n",
      "        [    0.0321],\n",
      "        [    0.0274],\n",
      "        [    0.0507],\n",
      "        [    0.1375],\n",
      "        [    0.1105],\n",
      "        [    0.0779],\n",
      "        [    0.0140],\n",
      "        [    0.0902],\n",
      "        [    0.1351],\n",
      "        [    0.1039],\n",
      "        [    0.0116],\n",
      "        [    0.0977],\n",
      "        [    0.0984],\n",
      "        [    0.0079],\n",
      "        [    0.1118],\n",
      "        [    0.1014],\n",
      "        [    0.1170],\n",
      "        [    0.1200],\n",
      "        [    0.1041],\n",
      "        [    0.1249],\n",
      "        [    0.1213],\n",
      "        [    0.1672],\n",
      "        [    0.1587],\n",
      "        [    0.0820],\n",
      "        [    0.0848],\n",
      "        [    0.1388],\n",
      "        [    0.1381],\n",
      "        [    0.1534],\n",
      "        [    0.3130],\n",
      "        [    0.3270],\n",
      "        [    0.3300],\n",
      "        [    0.3319],\n",
      "        [    0.3329],\n",
      "        [    0.3424],\n",
      "        [    0.3442],\n",
      "        [    0.3454],\n",
      "        [    0.3527],\n",
      "        [    0.3545],\n",
      "        [    0.3566],\n",
      "        [    0.3577],\n",
      "        [    0.3609],\n",
      "        [    0.3800],\n",
      "        [    0.3809],\n",
      "        [    0.3879],\n",
      "        [    0.3923],\n",
      "        [    0.3943],\n",
      "        [    0.3951],\n",
      "        [    0.3996]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 84.78996729850769\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 143\n",
      "剩餘X 資料 torch.Size([17, 18])\n",
      "剩餘Y 資料 torch.Size([17, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1656728982925415, 14)\n",
      "The second_loss value of k: (0.16701485216617584, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0564])\n",
      "目前模型的Data狀態 torch.Size([143, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6379],\n",
      "        [0.1281],\n",
      "        [0.5179],\n",
      "        [0.7309],\n",
      "        [0.2151],\n",
      "        [0.2221],\n",
      "        [0.6283],\n",
      "        [0.1824],\n",
      "        [0.6392],\n",
      "        [0.6320],\n",
      "        [0.6313],\n",
      "        [0.2741],\n",
      "        [0.6250],\n",
      "        [0.6061],\n",
      "        [0.6110],\n",
      "        [0.2037],\n",
      "        [0.5523],\n",
      "        [0.6198],\n",
      "        [0.6269],\n",
      "        [0.4217],\n",
      "        [0.6400],\n",
      "        [0.1543],\n",
      "        [0.4682],\n",
      "        [0.5693],\n",
      "        [0.6564],\n",
      "        [0.2704],\n",
      "        [0.1547],\n",
      "        [0.1315],\n",
      "        [0.6321],\n",
      "        [0.4301],\n",
      "        [0.5602],\n",
      "        [0.4423],\n",
      "        [0.1370],\n",
      "        [0.6169],\n",
      "        [0.5772],\n",
      "        [0.2166],\n",
      "        [0.4559],\n",
      "        [0.1076],\n",
      "        [0.5942],\n",
      "        [0.6482],\n",
      "        [0.6463],\n",
      "        [0.5788],\n",
      "        [0.3480],\n",
      "        [0.5090],\n",
      "        [0.5876],\n",
      "        [0.6169],\n",
      "        [0.2173],\n",
      "        [0.3524],\n",
      "        [0.6162],\n",
      "        [0.3709],\n",
      "        [0.1984],\n",
      "        [0.1670],\n",
      "        [0.2172],\n",
      "        [0.2206],\n",
      "        [0.5801],\n",
      "        [0.5653],\n",
      "        [0.4780],\n",
      "        [0.6201],\n",
      "        [0.2753],\n",
      "        [0.1527],\n",
      "        [0.3442],\n",
      "        [0.3093],\n",
      "        [0.1364],\n",
      "        [0.2838],\n",
      "        [0.6169],\n",
      "        [0.1287],\n",
      "        [0.1588],\n",
      "        [0.2782],\n",
      "        [0.1465],\n",
      "        [0.3284],\n",
      "        [0.3292],\n",
      "        [0.4286],\n",
      "        [0.3385],\n",
      "        [0.3403],\n",
      "        [0.6971],\n",
      "        [0.3934],\n",
      "        [0.5881],\n",
      "        [0.4627],\n",
      "        [0.4288],\n",
      "        [0.6496],\n",
      "        [0.5394],\n",
      "        [0.4046],\n",
      "        [0.3958],\n",
      "        [0.4437],\n",
      "        [0.4275],\n",
      "        [0.5611],\n",
      "        [0.2375],\n",
      "        [0.2274],\n",
      "        [0.2242],\n",
      "        [0.6369],\n",
      "        [0.3447],\n",
      "        [0.2845],\n",
      "        [0.4256],\n",
      "        [0.2492],\n",
      "        [0.6586],\n",
      "        [0.6692],\n",
      "        [0.6386],\n",
      "        [0.1651],\n",
      "        [0.4369],\n",
      "        [0.3428],\n",
      "        [0.4136],\n",
      "        [0.3087],\n",
      "        [0.1842],\n",
      "        [0.0582],\n",
      "        [0.2981],\n",
      "        [0.3544],\n",
      "        [0.2611],\n",
      "        [0.4221],\n",
      "        [0.3026],\n",
      "        [0.3496],\n",
      "        [0.1170],\n",
      "        [0.4067],\n",
      "        [0.1312],\n",
      "        [0.3116],\n",
      "        [0.5450],\n",
      "        [0.5877],\n",
      "        [0.3861],\n",
      "        [0.3271],\n",
      "        [0.3983],\n",
      "        [0.3284],\n",
      "        [0.3604],\n",
      "        [0.3424],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634],\n",
      "        [0.4634]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0102],\n",
      "        [    0.0373],\n",
      "        [    0.0187],\n",
      "        [    0.0719],\n",
      "        [    0.0027],\n",
      "        [    0.0115],\n",
      "        [    0.0004],\n",
      "        [    0.0097],\n",
      "        [    0.0087],\n",
      "        [    0.0212],\n",
      "        [    0.0127],\n",
      "        [    0.0985],\n",
      "        [    0.0034],\n",
      "        [    0.0154],\n",
      "        [    0.0040],\n",
      "        [    0.0855],\n",
      "        [    0.0117],\n",
      "        [    0.0032],\n",
      "        [    0.0064],\n",
      "        [    0.0187],\n",
      "        [    0.0328],\n",
      "        [    0.0637],\n",
      "        [    0.0271],\n",
      "        [    0.0195],\n",
      "        [    0.0148],\n",
      "        [    0.0082],\n",
      "        [    0.0255],\n",
      "        [    0.0119],\n",
      "        [    0.0043],\n",
      "        [    0.0408],\n",
      "        [    0.0249],\n",
      "        [    0.0691],\n",
      "        [    0.0439],\n",
      "        [    0.0362],\n",
      "        [    0.0161],\n",
      "        [    0.0249],\n",
      "        [    0.0180],\n",
      "        [    0.0011],\n",
      "        [    0.0273],\n",
      "        [    0.0286],\n",
      "        [    0.0009],\n",
      "        [    0.0230],\n",
      "        [    0.0063],\n",
      "        [    0.0424],\n",
      "        [    0.0286],\n",
      "        [    0.0261],\n",
      "        [    0.0282],\n",
      "        [    0.0056],\n",
      "        [    0.0198],\n",
      "        [    0.0456],\n",
      "        [    0.0143],\n",
      "        [    0.0861],\n",
      "        [    0.0374],\n",
      "        [    0.0704],\n",
      "        [    0.0322],\n",
      "        [    0.0389],\n",
      "        [    0.0560],\n",
      "        [    0.0013],\n",
      "        [    0.1310],\n",
      "        [    0.0145],\n",
      "        [    0.0137],\n",
      "        [    0.0383],\n",
      "        [    0.0038],\n",
      "        [    0.0163],\n",
      "        [    0.0548],\n",
      "        [    0.0045],\n",
      "        [    0.0407],\n",
      "        [    0.0289],\n",
      "        [    0.0510],\n",
      "        [    0.0302],\n",
      "        [    0.0295],\n",
      "        [    0.0433],\n",
      "        [    0.0249],\n",
      "        [    0.0242],\n",
      "        [    0.0177],\n",
      "        [    0.0792],\n",
      "        [    0.0290],\n",
      "        [    0.0058],\n",
      "        [    0.0585],\n",
      "        [    0.0859],\n",
      "        [    0.0711],\n",
      "        [    0.0599],\n",
      "        [    0.0843],\n",
      "        [    0.0793],\n",
      "        [    0.0990],\n",
      "        [    0.0628],\n",
      "        [    0.0703],\n",
      "        [    0.0466],\n",
      "        [    0.0651],\n",
      "        [    0.0351],\n",
      "        [    0.0464],\n",
      "        [    0.0644],\n",
      "        [    0.1073],\n",
      "        [    0.0616],\n",
      "        [    0.0321],\n",
      "        [    0.0274],\n",
      "        [    0.0507],\n",
      "        [    0.1375],\n",
      "        [    0.1105],\n",
      "        [    0.0779],\n",
      "        [    0.0140],\n",
      "        [    0.0902],\n",
      "        [    0.1351],\n",
      "        [    0.1039],\n",
      "        [    0.0116],\n",
      "        [    0.0977],\n",
      "        [    0.0984],\n",
      "        [    0.0079],\n",
      "        [    0.1118],\n",
      "        [    0.1014],\n",
      "        [    0.1170],\n",
      "        [    0.1200],\n",
      "        [    0.1041],\n",
      "        [    0.1249],\n",
      "        [    0.1213],\n",
      "        [    0.1672],\n",
      "        [    0.1587],\n",
      "        [    0.0820],\n",
      "        [    0.0848],\n",
      "        [    0.1388],\n",
      "        [    0.1381],\n",
      "        [    0.1534],\n",
      "        [    0.3130],\n",
      "        [    0.3270],\n",
      "        [    0.3300],\n",
      "        [    0.3319],\n",
      "        [    0.3329],\n",
      "        [    0.3424],\n",
      "        [    0.3442],\n",
      "        [    0.3454],\n",
      "        [    0.3527],\n",
      "        [    0.3545],\n",
      "        [    0.3566],\n",
      "        [    0.3577],\n",
      "        [    0.3609],\n",
      "        [    0.3800],\n",
      "        [    0.3809],\n",
      "        [    0.3879],\n",
      "        [    0.3923],\n",
      "        [    0.3943],\n",
      "        [    0.3951],\n",
      "        [    0.3996],\n",
      "        [    0.4070]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0140],\n",
      "        [0.0390],\n",
      "        [0.0154],\n",
      "        [0.0759],\n",
      "        [0.0011],\n",
      "        [0.0093],\n",
      "        [0.0030],\n",
      "        [0.0080],\n",
      "        [0.0124],\n",
      "        [0.0252],\n",
      "        [0.0089],\n",
      "        [0.1009],\n",
      "        [0.0075],\n",
      "        [0.0192],\n",
      "        [0.0073],\n",
      "        [0.0876],\n",
      "        [0.0152],\n",
      "        [0.0068],\n",
      "        [0.0027],\n",
      "        [0.0156],\n",
      "        [0.0367],\n",
      "        [0.0655],\n",
      "        [0.0247],\n",
      "        [0.0166],\n",
      "        [0.0183],\n",
      "        [0.0102],\n",
      "        [0.0237],\n",
      "        [0.0134],\n",
      "        [0.0009],\n",
      "        [0.0380],\n",
      "        [0.0216],\n",
      "        [0.0666],\n",
      "        [0.0453],\n",
      "        [0.0401],\n",
      "        [0.0195],\n",
      "        [0.0272],\n",
      "        [0.0149],\n",
      "        [0.0005],\n",
      "        [0.0304],\n",
      "        [0.0247],\n",
      "        [0.0044],\n",
      "        [0.0265],\n",
      "        [0.0039],\n",
      "        [0.0389],\n",
      "        [0.0323],\n",
      "        [0.0220],\n",
      "        [0.0301],\n",
      "        [0.0029],\n",
      "        [0.0165],\n",
      "        [0.0482],\n",
      "        [0.0163],\n",
      "        [0.0881],\n",
      "        [0.0357],\n",
      "        [0.0681],\n",
      "        [0.0359],\n",
      "        [0.0425],\n",
      "        [0.0528],\n",
      "        [0.0053],\n",
      "        [0.1334],\n",
      "        [0.0165],\n",
      "        [0.0110],\n",
      "        [0.0405],\n",
      "        [0.0057],\n",
      "        [0.0187],\n",
      "        [0.0511],\n",
      "        [0.0028],\n",
      "        [0.0411],\n",
      "        [0.0266],\n",
      "        [0.0505],\n",
      "        [0.0276],\n",
      "        [0.0270],\n",
      "        [0.0402],\n",
      "        [0.0222],\n",
      "        [0.0217],\n",
      "        [0.0215],\n",
      "        [0.0818],\n",
      "        [0.0253],\n",
      "        [0.0081],\n",
      "        [0.0556],\n",
      "        [0.0900],\n",
      "        [0.0678],\n",
      "        [0.0570],\n",
      "        [0.0871],\n",
      "        [0.0764],\n",
      "        [0.0961],\n",
      "        [0.0594],\n",
      "        [0.0680],\n",
      "        [0.0486],\n",
      "        [0.0672],\n",
      "        [0.0316],\n",
      "        [0.0437],\n",
      "        [0.0665],\n",
      "        [0.1044],\n",
      "        [0.0635],\n",
      "        [0.0285],\n",
      "        [0.0238],\n",
      "        [0.0472],\n",
      "        [0.1354],\n",
      "        [0.1139],\n",
      "        [0.0805],\n",
      "        [0.0166],\n",
      "        [0.0879],\n",
      "        [0.1330],\n",
      "        [0.1024],\n",
      "        [0.0091],\n",
      "        [0.0951],\n",
      "        [0.1003],\n",
      "        [0.0106],\n",
      "        [0.1140],\n",
      "        [0.0988],\n",
      "        [0.1179],\n",
      "        [0.1228],\n",
      "        [0.1054],\n",
      "        [0.1226],\n",
      "        [0.1241],\n",
      "        [0.1703],\n",
      "        [0.1613],\n",
      "        [0.0791],\n",
      "        [0.0816],\n",
      "        [0.1357],\n",
      "        [0.1350],\n",
      "        [0.1502],\n",
      "        [0.3126],\n",
      "        [0.3266],\n",
      "        [0.3296],\n",
      "        [0.3314],\n",
      "        [0.3325],\n",
      "        [0.3419],\n",
      "        [0.3437],\n",
      "        [0.3449],\n",
      "        [0.3523],\n",
      "        [0.3541],\n",
      "        [0.3562],\n",
      "        [0.3572],\n",
      "        [0.3604],\n",
      "        [0.3795],\n",
      "        [0.3805],\n",
      "        [0.3874],\n",
      "        [0.3918],\n",
      "        [0.3938],\n",
      "        [0.3947],\n",
      "        [0.3991],\n",
      "        [0.4066]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 85.02691984176636\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 144\n",
      "剩餘X 資料 torch.Size([16, 18])\n",
      "剩餘Y 資料 torch.Size([16, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16664128005504608, 2)\n",
      "The second_loss value of k: (0.16724944114685059, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.0547])\n",
      "目前模型的Data狀態 torch.Size([144, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6418],\n",
      "        [0.1298],\n",
      "        [0.5212],\n",
      "        [0.7349],\n",
      "        [0.2167],\n",
      "        [0.2243],\n",
      "        [0.6316],\n",
      "        [0.1841],\n",
      "        [0.6429],\n",
      "        [0.6360],\n",
      "        [0.6350],\n",
      "        [0.2765],\n",
      "        [0.6291],\n",
      "        [0.6099],\n",
      "        [0.6143],\n",
      "        [0.2059],\n",
      "        [0.5558],\n",
      "        [0.6233],\n",
      "        [0.6307],\n",
      "        [0.4248],\n",
      "        [0.6439],\n",
      "        [0.1561],\n",
      "        [0.4707],\n",
      "        [0.5723],\n",
      "        [0.6599],\n",
      "        [0.2724],\n",
      "        [0.1565],\n",
      "        [0.1330],\n",
      "        [0.6355],\n",
      "        [0.4329],\n",
      "        [0.5635],\n",
      "        [0.4447],\n",
      "        [0.1384],\n",
      "        [0.6208],\n",
      "        [0.5806],\n",
      "        [0.2189],\n",
      "        [0.4590],\n",
      "        [0.1092],\n",
      "        [0.5973],\n",
      "        [0.6520],\n",
      "        [0.6498],\n",
      "        [0.5824],\n",
      "        [0.3504],\n",
      "        [0.5126],\n",
      "        [0.5913],\n",
      "        [0.6210],\n",
      "        [0.2192],\n",
      "        [0.3551],\n",
      "        [0.6195],\n",
      "        [0.3735],\n",
      "        [0.2003],\n",
      "        [0.1689],\n",
      "        [0.2189],\n",
      "        [0.2229],\n",
      "        [0.5838],\n",
      "        [0.5689],\n",
      "        [0.4812],\n",
      "        [0.6240],\n",
      "        [0.2777],\n",
      "        [0.1547],\n",
      "        [0.3469],\n",
      "        [0.3115],\n",
      "        [0.1383],\n",
      "        [0.2861],\n",
      "        [0.6206],\n",
      "        [0.1304],\n",
      "        [0.1592],\n",
      "        [0.2805],\n",
      "        [0.1470],\n",
      "        [0.3309],\n",
      "        [0.3317],\n",
      "        [0.4317],\n",
      "        [0.3413],\n",
      "        [0.3428],\n",
      "        [0.7009],\n",
      "        [0.3960],\n",
      "        [0.5917],\n",
      "        [0.4650],\n",
      "        [0.4316],\n",
      "        [0.6538],\n",
      "        [0.5427],\n",
      "        [0.4076],\n",
      "        [0.3985],\n",
      "        [0.4467],\n",
      "        [0.4304],\n",
      "        [0.5645],\n",
      "        [0.2398],\n",
      "        [0.2294],\n",
      "        [0.2262],\n",
      "        [0.6404],\n",
      "        [0.3474],\n",
      "        [0.2867],\n",
      "        [0.4285],\n",
      "        [0.2511],\n",
      "        [0.6622],\n",
      "        [0.6728],\n",
      "        [0.6421],\n",
      "        [0.1673],\n",
      "        [0.4403],\n",
      "        [0.3454],\n",
      "        [0.4162],\n",
      "        [0.3110],\n",
      "        [0.1863],\n",
      "        [0.0596],\n",
      "        [0.3006],\n",
      "        [0.3571],\n",
      "        [0.2630],\n",
      "        [0.4248],\n",
      "        [0.3049],\n",
      "        [0.3522],\n",
      "        [0.1179],\n",
      "        [0.4096],\n",
      "        [0.1325],\n",
      "        [0.3139],\n",
      "        [0.5478],\n",
      "        [0.5908],\n",
      "        [0.3887],\n",
      "        [0.3300],\n",
      "        [0.4014],\n",
      "        [0.3315],\n",
      "        [0.3636],\n",
      "        [0.3455],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629],\n",
      "        [0.4629]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0140],\n",
      "        [0.0390],\n",
      "        [0.0154],\n",
      "        [0.0759],\n",
      "        [0.0011],\n",
      "        [0.0093],\n",
      "        [0.0030],\n",
      "        [0.0080],\n",
      "        [0.0124],\n",
      "        [0.0252],\n",
      "        [0.0089],\n",
      "        [0.1009],\n",
      "        [0.0075],\n",
      "        [0.0192],\n",
      "        [0.0073],\n",
      "        [0.0876],\n",
      "        [0.0152],\n",
      "        [0.0068],\n",
      "        [0.0027],\n",
      "        [0.0156],\n",
      "        [0.0367],\n",
      "        [0.0655],\n",
      "        [0.0247],\n",
      "        [0.0166],\n",
      "        [0.0183],\n",
      "        [0.0102],\n",
      "        [0.0237],\n",
      "        [0.0134],\n",
      "        [0.0009],\n",
      "        [0.0380],\n",
      "        [0.0216],\n",
      "        [0.0666],\n",
      "        [0.0453],\n",
      "        [0.0401],\n",
      "        [0.0195],\n",
      "        [0.0272],\n",
      "        [0.0149],\n",
      "        [0.0005],\n",
      "        [0.0304],\n",
      "        [0.0247],\n",
      "        [0.0044],\n",
      "        [0.0265],\n",
      "        [0.0039],\n",
      "        [0.0389],\n",
      "        [0.0323],\n",
      "        [0.0220],\n",
      "        [0.0301],\n",
      "        [0.0029],\n",
      "        [0.0165],\n",
      "        [0.0482],\n",
      "        [0.0163],\n",
      "        [0.0881],\n",
      "        [0.0357],\n",
      "        [0.0681],\n",
      "        [0.0359],\n",
      "        [0.0425],\n",
      "        [0.0528],\n",
      "        [0.0053],\n",
      "        [0.1334],\n",
      "        [0.0165],\n",
      "        [0.0110],\n",
      "        [0.0405],\n",
      "        [0.0057],\n",
      "        [0.0187],\n",
      "        [0.0511],\n",
      "        [0.0028],\n",
      "        [0.0411],\n",
      "        [0.0266],\n",
      "        [0.0505],\n",
      "        [0.0276],\n",
      "        [0.0270],\n",
      "        [0.0402],\n",
      "        [0.0222],\n",
      "        [0.0217],\n",
      "        [0.0215],\n",
      "        [0.0818],\n",
      "        [0.0253],\n",
      "        [0.0081],\n",
      "        [0.0556],\n",
      "        [0.0900],\n",
      "        [0.0678],\n",
      "        [0.0570],\n",
      "        [0.0871],\n",
      "        [0.0764],\n",
      "        [0.0961],\n",
      "        [0.0594],\n",
      "        [0.0680],\n",
      "        [0.0486],\n",
      "        [0.0672],\n",
      "        [0.0316],\n",
      "        [0.0437],\n",
      "        [0.0665],\n",
      "        [0.1044],\n",
      "        [0.0635],\n",
      "        [0.0285],\n",
      "        [0.0238],\n",
      "        [0.0472],\n",
      "        [0.1354],\n",
      "        [0.1139],\n",
      "        [0.0805],\n",
      "        [0.0166],\n",
      "        [0.0879],\n",
      "        [0.1330],\n",
      "        [0.1024],\n",
      "        [0.0091],\n",
      "        [0.0951],\n",
      "        [0.1003],\n",
      "        [0.0106],\n",
      "        [0.1140],\n",
      "        [0.0988],\n",
      "        [0.1179],\n",
      "        [0.1228],\n",
      "        [0.1054],\n",
      "        [0.1226],\n",
      "        [0.1241],\n",
      "        [0.1703],\n",
      "        [0.1613],\n",
      "        [0.0791],\n",
      "        [0.0816],\n",
      "        [0.1357],\n",
      "        [0.1350],\n",
      "        [0.1502],\n",
      "        [0.3126],\n",
      "        [0.3266],\n",
      "        [0.3296],\n",
      "        [0.3314],\n",
      "        [0.3325],\n",
      "        [0.3419],\n",
      "        [0.3437],\n",
      "        [0.3449],\n",
      "        [0.3523],\n",
      "        [0.3541],\n",
      "        [0.3562],\n",
      "        [0.3572],\n",
      "        [0.3604],\n",
      "        [0.3795],\n",
      "        [0.3805],\n",
      "        [0.3874],\n",
      "        [0.3918],\n",
      "        [0.3938],\n",
      "        [0.3947],\n",
      "        [0.3991],\n",
      "        [0.4066],\n",
      "        [0.4082]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0110],\n",
      "        [    0.0368],\n",
      "        [    0.0182],\n",
      "        [    0.0733],\n",
      "        [    0.0031],\n",
      "        [    0.0109],\n",
      "        [    0.0002],\n",
      "        [    0.0098],\n",
      "        [    0.0094],\n",
      "        [    0.0227],\n",
      "        [    0.0121],\n",
      "        [    0.0990],\n",
      "        [    0.0050],\n",
      "        [    0.0162],\n",
      "        [    0.0039],\n",
      "        [    0.0857],\n",
      "        [    0.0125],\n",
      "        [    0.0035],\n",
      "        [    0.0055],\n",
      "        [    0.0180],\n",
      "        [    0.0338],\n",
      "        [    0.0635],\n",
      "        [    0.0281],\n",
      "        [    0.0198],\n",
      "        [    0.0154],\n",
      "        [    0.0080],\n",
      "        [    0.0254],\n",
      "        [    0.0111],\n",
      "        [    0.0039],\n",
      "        [    0.0409],\n",
      "        [    0.0243],\n",
      "        [    0.0700],\n",
      "        [    0.0429],\n",
      "        [    0.0373],\n",
      "        [    0.0165],\n",
      "        [    0.0256],\n",
      "        [    0.0175],\n",
      "        [    0.0016],\n",
      "        [    0.0272],\n",
      "        [    0.0276],\n",
      "        [    0.0015],\n",
      "        [    0.0235],\n",
      "        [    0.0059],\n",
      "        [    0.0416],\n",
      "        [    0.0293],\n",
      "        [    0.0246],\n",
      "        [    0.0283],\n",
      "        [    0.0045],\n",
      "        [    0.0196],\n",
      "        [    0.0460],\n",
      "        [    0.0144],\n",
      "        [    0.0860],\n",
      "        [    0.0370],\n",
      "        [    0.0703],\n",
      "        [    0.0330],\n",
      "        [    0.0397],\n",
      "        [    0.0556],\n",
      "        [    0.0028],\n",
      "        [    0.1316],\n",
      "        [    0.0146],\n",
      "        [    0.0126],\n",
      "        [    0.0382],\n",
      "        [    0.0037],\n",
      "        [    0.0163],\n",
      "        [    0.0540],\n",
      "        [    0.0049],\n",
      "        [    0.0405],\n",
      "        [    0.0282],\n",
      "        [    0.0510],\n",
      "        [    0.0293],\n",
      "        [    0.0287],\n",
      "        [    0.0426],\n",
      "        [    0.0237],\n",
      "        [    0.0235],\n",
      "        [    0.0188],\n",
      "        [    0.0797],\n",
      "        [    0.0279],\n",
      "        [    0.0047],\n",
      "        [    0.0584],\n",
      "        [    0.0874],\n",
      "        [    0.0706],\n",
      "        [    0.0594],\n",
      "        [    0.0850],\n",
      "        [    0.0792],\n",
      "        [    0.0991],\n",
      "        [    0.0620],\n",
      "        [    0.0698],\n",
      "        [    0.0466],\n",
      "        [    0.0655],\n",
      "        [    0.0344],\n",
      "        [    0.0452],\n",
      "        [    0.0644],\n",
      "        [    0.1072],\n",
      "        [    0.0614],\n",
      "        [    0.0311],\n",
      "        [    0.0264],\n",
      "        [    0.0500],\n",
      "        [    0.1374],\n",
      "        [    0.1118],\n",
      "        [    0.0781],\n",
      "        [    0.0140],\n",
      "        [    0.0898],\n",
      "        [    0.1348],\n",
      "        [    0.1036],\n",
      "        [    0.0109],\n",
      "        [    0.0969],\n",
      "        [    0.0980],\n",
      "        [    0.0084],\n",
      "        [    0.1120],\n",
      "        [    0.1004],\n",
      "        [    0.1163],\n",
      "        [    0.1205],\n",
      "        [    0.1033],\n",
      "        [    0.1245],\n",
      "        [    0.1208],\n",
      "        [    0.1671],\n",
      "        [    0.1588],\n",
      "        [    0.0807],\n",
      "        [    0.0833],\n",
      "        [    0.1372],\n",
      "        [    0.1364],\n",
      "        [    0.1516],\n",
      "        [    0.3121],\n",
      "        [    0.3261],\n",
      "        [    0.3291],\n",
      "        [    0.3309],\n",
      "        [    0.3320],\n",
      "        [    0.3414],\n",
      "        [    0.3432],\n",
      "        [    0.3444],\n",
      "        [    0.3518],\n",
      "        [    0.3536],\n",
      "        [    0.3557],\n",
      "        [    0.3567],\n",
      "        [    0.3599],\n",
      "        [    0.3790],\n",
      "        [    0.3800],\n",
      "        [    0.3869],\n",
      "        [    0.3913],\n",
      "        [    0.3933],\n",
      "        [    0.3942],\n",
      "        [    0.3986],\n",
      "        [    0.4061],\n",
      "        [    0.4077]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 85.2630066871643\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 145\n",
      "剩餘X 資料 torch.Size([15, 18])\n",
      "剩餘Y 資料 torch.Size([15, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16683515906333923, 8)\n",
      "The second_loss value of k: (0.16686716675758362, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.0540])\n",
      "目前模型的Data狀態 torch.Size([145, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6388],\n",
      "        [0.1276],\n",
      "        [0.5184],\n",
      "        [0.7323],\n",
      "        [0.2147],\n",
      "        [0.2227],\n",
      "        [0.6284],\n",
      "        [0.1822],\n",
      "        [0.6400],\n",
      "        [0.6334],\n",
      "        [0.6319],\n",
      "        [0.2746],\n",
      "        [0.6265],\n",
      "        [0.6069],\n",
      "        [0.6109],\n",
      "        [0.2040],\n",
      "        [0.5531],\n",
      "        [0.6200],\n",
      "        [0.6278],\n",
      "        [0.4224],\n",
      "        [0.6410],\n",
      "        [0.1541],\n",
      "        [0.4673],\n",
      "        [0.5690],\n",
      "        [0.6570],\n",
      "        [0.2702],\n",
      "        [0.1548],\n",
      "        [0.1306],\n",
      "        [0.6325],\n",
      "        [0.4301],\n",
      "        [0.5608],\n",
      "        [0.4413],\n",
      "        [0.1360],\n",
      "        [0.6180],\n",
      "        [0.5776],\n",
      "        [0.2173],\n",
      "        [0.4565],\n",
      "        [0.1070],\n",
      "        [0.5941],\n",
      "        [0.6491],\n",
      "        [0.6469],\n",
      "        [0.5794],\n",
      "        [0.3484],\n",
      "        [0.5098],\n",
      "        [0.5883],\n",
      "        [0.6184],\n",
      "        [0.2174],\n",
      "        [0.3535],\n",
      "        [0.6164],\n",
      "        [0.3713],\n",
      "        [0.1984],\n",
      "        [0.1669],\n",
      "        [0.2176],\n",
      "        [0.2207],\n",
      "        [0.5809],\n",
      "        [0.5661],\n",
      "        [0.4784],\n",
      "        [0.6216],\n",
      "        [0.2759],\n",
      "        [0.1528],\n",
      "        [0.3453],\n",
      "        [0.3092],\n",
      "        [0.1363],\n",
      "        [0.2837],\n",
      "        [0.6177],\n",
      "        [0.1283],\n",
      "        [0.1586],\n",
      "        [0.2789],\n",
      "        [0.1465],\n",
      "        [0.3292],\n",
      "        [0.3300],\n",
      "        [0.4294],\n",
      "        [0.3397],\n",
      "        [0.3409],\n",
      "        [0.6983],\n",
      "        [0.3939],\n",
      "        [0.5892],\n",
      "        [0.4615],\n",
      "        [0.4289],\n",
      "        [0.6512],\n",
      "        [0.5399],\n",
      "        [0.4051],\n",
      "        [0.3965],\n",
      "        [0.4439],\n",
      "        [0.4274],\n",
      "        [0.5618],\n",
      "        [0.2380],\n",
      "        [0.2274],\n",
      "        [0.2245],\n",
      "        [0.6376],\n",
      "        [0.3459],\n",
      "        [0.2845],\n",
      "        [0.4257],\n",
      "        [0.2490],\n",
      "        [0.6596],\n",
      "        [0.6702],\n",
      "        [0.6393],\n",
      "        [0.1653],\n",
      "        [0.4381],\n",
      "        [0.3430],\n",
      "        [0.4137],\n",
      "        [0.3091],\n",
      "        [0.1844],\n",
      "        [0.0585],\n",
      "        [0.2988],\n",
      "        [0.3553],\n",
      "        [0.2608],\n",
      "        [0.4225],\n",
      "        [0.3028],\n",
      "        [0.3507],\n",
      "        [0.1163],\n",
      "        [0.4072],\n",
      "        [0.1304],\n",
      "        [0.3120],\n",
      "        [0.5445],\n",
      "        [0.5875],\n",
      "        [0.3861],\n",
      "        [0.3284],\n",
      "        [0.3998],\n",
      "        [0.3300],\n",
      "        [0.3621],\n",
      "        [0.3441],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624],\n",
      "        [0.4624]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0110],\n",
      "        [    0.0368],\n",
      "        [    0.0182],\n",
      "        [    0.0733],\n",
      "        [    0.0031],\n",
      "        [    0.0109],\n",
      "        [    0.0002],\n",
      "        [    0.0098],\n",
      "        [    0.0094],\n",
      "        [    0.0227],\n",
      "        [    0.0121],\n",
      "        [    0.0990],\n",
      "        [    0.0050],\n",
      "        [    0.0162],\n",
      "        [    0.0039],\n",
      "        [    0.0857],\n",
      "        [    0.0125],\n",
      "        [    0.0035],\n",
      "        [    0.0055],\n",
      "        [    0.0180],\n",
      "        [    0.0338],\n",
      "        [    0.0635],\n",
      "        [    0.0281],\n",
      "        [    0.0198],\n",
      "        [    0.0154],\n",
      "        [    0.0080],\n",
      "        [    0.0254],\n",
      "        [    0.0111],\n",
      "        [    0.0039],\n",
      "        [    0.0409],\n",
      "        [    0.0243],\n",
      "        [    0.0700],\n",
      "        [    0.0429],\n",
      "        [    0.0373],\n",
      "        [    0.0165],\n",
      "        [    0.0256],\n",
      "        [    0.0175],\n",
      "        [    0.0016],\n",
      "        [    0.0272],\n",
      "        [    0.0276],\n",
      "        [    0.0015],\n",
      "        [    0.0235],\n",
      "        [    0.0059],\n",
      "        [    0.0416],\n",
      "        [    0.0293],\n",
      "        [    0.0246],\n",
      "        [    0.0283],\n",
      "        [    0.0045],\n",
      "        [    0.0196],\n",
      "        [    0.0460],\n",
      "        [    0.0144],\n",
      "        [    0.0860],\n",
      "        [    0.0370],\n",
      "        [    0.0703],\n",
      "        [    0.0330],\n",
      "        [    0.0397],\n",
      "        [    0.0556],\n",
      "        [    0.0028],\n",
      "        [    0.1316],\n",
      "        [    0.0146],\n",
      "        [    0.0126],\n",
      "        [    0.0382],\n",
      "        [    0.0037],\n",
      "        [    0.0163],\n",
      "        [    0.0540],\n",
      "        [    0.0049],\n",
      "        [    0.0405],\n",
      "        [    0.0282],\n",
      "        [    0.0510],\n",
      "        [    0.0293],\n",
      "        [    0.0287],\n",
      "        [    0.0426],\n",
      "        [    0.0237],\n",
      "        [    0.0235],\n",
      "        [    0.0188],\n",
      "        [    0.0797],\n",
      "        [    0.0279],\n",
      "        [    0.0047],\n",
      "        [    0.0584],\n",
      "        [    0.0874],\n",
      "        [    0.0706],\n",
      "        [    0.0594],\n",
      "        [    0.0850],\n",
      "        [    0.0792],\n",
      "        [    0.0991],\n",
      "        [    0.0620],\n",
      "        [    0.0698],\n",
      "        [    0.0466],\n",
      "        [    0.0655],\n",
      "        [    0.0344],\n",
      "        [    0.0452],\n",
      "        [    0.0644],\n",
      "        [    0.1072],\n",
      "        [    0.0614],\n",
      "        [    0.0311],\n",
      "        [    0.0264],\n",
      "        [    0.0500],\n",
      "        [    0.1374],\n",
      "        [    0.1118],\n",
      "        [    0.0781],\n",
      "        [    0.0140],\n",
      "        [    0.0898],\n",
      "        [    0.1348],\n",
      "        [    0.1036],\n",
      "        [    0.0109],\n",
      "        [    0.0969],\n",
      "        [    0.0980],\n",
      "        [    0.0084],\n",
      "        [    0.1120],\n",
      "        [    0.1004],\n",
      "        [    0.1163],\n",
      "        [    0.1205],\n",
      "        [    0.1033],\n",
      "        [    0.1245],\n",
      "        [    0.1208],\n",
      "        [    0.1671],\n",
      "        [    0.1588],\n",
      "        [    0.0807],\n",
      "        [    0.0833],\n",
      "        [    0.1372],\n",
      "        [    0.1364],\n",
      "        [    0.1516],\n",
      "        [    0.3121],\n",
      "        [    0.3261],\n",
      "        [    0.3291],\n",
      "        [    0.3309],\n",
      "        [    0.3320],\n",
      "        [    0.3414],\n",
      "        [    0.3432],\n",
      "        [    0.3444],\n",
      "        [    0.3518],\n",
      "        [    0.3536],\n",
      "        [    0.3557],\n",
      "        [    0.3567],\n",
      "        [    0.3599],\n",
      "        [    0.3790],\n",
      "        [    0.3800],\n",
      "        [    0.3869],\n",
      "        [    0.3913],\n",
      "        [    0.3933],\n",
      "        [    0.3942],\n",
      "        [    0.3986],\n",
      "        [    0.4061],\n",
      "        [    0.4077],\n",
      "        [    0.4085]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0111],\n",
      "        [0.0365],\n",
      "        [0.0182],\n",
      "        [0.0736],\n",
      "        [0.0034],\n",
      "        [0.0107],\n",
      "        [0.0004],\n",
      "        [0.0100],\n",
      "        [0.0095],\n",
      "        [0.0230],\n",
      "        [0.0121],\n",
      "        [0.0990],\n",
      "        [0.0053],\n",
      "        [0.0163],\n",
      "        [0.0036],\n",
      "        [0.0857],\n",
      "        [0.0126],\n",
      "        [0.0033],\n",
      "        [0.0054],\n",
      "        [0.0178],\n",
      "        [0.0339],\n",
      "        [0.0633],\n",
      "        [0.0288],\n",
      "        [0.0203],\n",
      "        [0.0154],\n",
      "        [0.0078],\n",
      "        [0.0255],\n",
      "        [0.0106],\n",
      "        [0.0041],\n",
      "        [0.0411],\n",
      "        [0.0244],\n",
      "        [0.0707],\n",
      "        [0.0423],\n",
      "        [0.0374],\n",
      "        [0.0164],\n",
      "        [0.0258],\n",
      "        [0.0175],\n",
      "        [0.0020],\n",
      "        [0.0269],\n",
      "        [0.0275],\n",
      "        [0.0014],\n",
      "        [0.0235],\n",
      "        [0.0058],\n",
      "        [0.0415],\n",
      "        [0.0294],\n",
      "        [0.0243],\n",
      "        [0.0281],\n",
      "        [0.0042],\n",
      "        [0.0199],\n",
      "        [0.0460],\n",
      "        [0.0142],\n",
      "        [0.0858],\n",
      "        [0.0369],\n",
      "        [0.0704],\n",
      "        [0.0330],\n",
      "        [0.0398],\n",
      "        [0.0556],\n",
      "        [0.0032],\n",
      "        [0.1317],\n",
      "        [0.0145],\n",
      "        [0.0122],\n",
      "        [0.0381],\n",
      "        [0.0035],\n",
      "        [0.0161],\n",
      "        [0.0540],\n",
      "        [0.0052],\n",
      "        [0.0403],\n",
      "        [0.0280],\n",
      "        [0.0511],\n",
      "        [0.0291],\n",
      "        [0.0284],\n",
      "        [0.0424],\n",
      "        [0.0233],\n",
      "        [0.0234],\n",
      "        [0.0190],\n",
      "        [0.0798],\n",
      "        [0.0277],\n",
      "        [0.0039],\n",
      "        [0.0585],\n",
      "        [0.0877],\n",
      "        [0.0708],\n",
      "        [0.0594],\n",
      "        [0.0852],\n",
      "        [0.0793],\n",
      "        [0.0995],\n",
      "        [0.0620],\n",
      "        [0.0697],\n",
      "        [0.0464],\n",
      "        [0.0655],\n",
      "        [0.0344],\n",
      "        [0.0447],\n",
      "        [0.0643],\n",
      "        [0.1074],\n",
      "        [0.0612],\n",
      "        [0.0310],\n",
      "        [0.0264],\n",
      "        [0.0500],\n",
      "        [0.1375],\n",
      "        [0.1121],\n",
      "        [0.0780],\n",
      "        [0.0139],\n",
      "        [0.0897],\n",
      "        [0.1348],\n",
      "        [0.1035],\n",
      "        [0.0107],\n",
      "        [0.0966],\n",
      "        [0.0978],\n",
      "        [0.0084],\n",
      "        [0.1119],\n",
      "        [0.1000],\n",
      "        [0.1161],\n",
      "        [0.1205],\n",
      "        [0.1028],\n",
      "        [0.1244],\n",
      "        [0.1203],\n",
      "        [0.1667],\n",
      "        [0.1586],\n",
      "        [0.0803],\n",
      "        [0.0828],\n",
      "        [0.1366],\n",
      "        [0.1359],\n",
      "        [0.1510],\n",
      "        [0.3115],\n",
      "        [0.3256],\n",
      "        [0.3285],\n",
      "        [0.3304],\n",
      "        [0.3314],\n",
      "        [0.3409],\n",
      "        [0.3427],\n",
      "        [0.3439],\n",
      "        [0.3513],\n",
      "        [0.3531],\n",
      "        [0.3551],\n",
      "        [0.3562],\n",
      "        [0.3594],\n",
      "        [0.3785],\n",
      "        [0.3795],\n",
      "        [0.3864],\n",
      "        [0.3908],\n",
      "        [0.3928],\n",
      "        [0.3936],\n",
      "        [0.3981],\n",
      "        [0.4055],\n",
      "        [0.4072],\n",
      "        [0.4079]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 85.49912881851196\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 146\n",
      "剩餘X 資料 torch.Size([14, 18])\n",
      "剩餘Y 資料 torch.Size([14, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16644003987312317, 8)\n",
      "The second_loss value of k: (0.1686686873435974, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.0539])\n",
      "目前模型的Data狀態 torch.Size([146, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6389],\n",
      "        [0.1272],\n",
      "        [0.5184],\n",
      "        [0.7326],\n",
      "        [0.2144],\n",
      "        [0.2229],\n",
      "        [0.6282],\n",
      "        [0.1821],\n",
      "        [0.6400],\n",
      "        [0.6338],\n",
      "        [0.6318],\n",
      "        [0.2746],\n",
      "        [0.6269],\n",
      "        [0.6069],\n",
      "        [0.6106],\n",
      "        [0.2040],\n",
      "        [0.5532],\n",
      "        [0.6198],\n",
      "        [0.6279],\n",
      "        [0.4226],\n",
      "        [0.6412],\n",
      "        [0.1539],\n",
      "        [0.4665],\n",
      "        [0.5686],\n",
      "        [0.6570],\n",
      "        [0.2700],\n",
      "        [0.1547],\n",
      "        [0.1302],\n",
      "        [0.6323],\n",
      "        [0.4299],\n",
      "        [0.5608],\n",
      "        [0.4406],\n",
      "        [0.1354],\n",
      "        [0.6181],\n",
      "        [0.5775],\n",
      "        [0.2175],\n",
      "        [0.4565],\n",
      "        [0.1066],\n",
      "        [0.5938],\n",
      "        [0.6493],\n",
      "        [0.6468],\n",
      "        [0.5793],\n",
      "        [0.3485],\n",
      "        [0.5099],\n",
      "        [0.5883],\n",
      "        [0.6186],\n",
      "        [0.2173],\n",
      "        [0.3538],\n",
      "        [0.6161],\n",
      "        [0.3713],\n",
      "        [0.1983],\n",
      "        [0.1667],\n",
      "        [0.2177],\n",
      "        [0.2205],\n",
      "        [0.5809],\n",
      "        [0.5663],\n",
      "        [0.4784],\n",
      "        [0.6219],\n",
      "        [0.2761],\n",
      "        [0.1527],\n",
      "        [0.3457],\n",
      "        [0.3090],\n",
      "        [0.1361],\n",
      "        [0.2836],\n",
      "        [0.6177],\n",
      "        [0.1280],\n",
      "        [0.1584],\n",
      "        [0.2791],\n",
      "        [0.1464],\n",
      "        [0.3295],\n",
      "        [0.3303],\n",
      "        [0.4295],\n",
      "        [0.3401],\n",
      "        [0.3411],\n",
      "        [0.6984],\n",
      "        [0.3940],\n",
      "        [0.5894],\n",
      "        [0.4608],\n",
      "        [0.4287],\n",
      "        [0.6515],\n",
      "        [0.5397],\n",
      "        [0.4052],\n",
      "        [0.3966],\n",
      "        [0.4438],\n",
      "        [0.4271],\n",
      "        [0.5619],\n",
      "        [0.2381],\n",
      "        [0.2272],\n",
      "        [0.2246],\n",
      "        [0.6376],\n",
      "        [0.3464],\n",
      "        [0.2844],\n",
      "        [0.4256],\n",
      "        [0.2488],\n",
      "        [0.6597],\n",
      "        [0.6702],\n",
      "        [0.6392],\n",
      "        [0.1652],\n",
      "        [0.4385],\n",
      "        [0.3430],\n",
      "        [0.4135],\n",
      "        [0.3092],\n",
      "        [0.1844],\n",
      "        [0.0586],\n",
      "        [0.2990],\n",
      "        [0.3556],\n",
      "        [0.2606],\n",
      "        [0.4226],\n",
      "        [0.3028],\n",
      "        [0.3510],\n",
      "        [0.1161],\n",
      "        [0.4073],\n",
      "        [0.1299],\n",
      "        [0.3121],\n",
      "        [0.5440],\n",
      "        [0.5872],\n",
      "        [0.3860],\n",
      "        [0.3288],\n",
      "        [0.4003],\n",
      "        [0.3306],\n",
      "        [0.3627],\n",
      "        [0.3447],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619],\n",
      "        [0.4619]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0111],\n",
      "        [0.0365],\n",
      "        [0.0182],\n",
      "        [0.0736],\n",
      "        [0.0034],\n",
      "        [0.0107],\n",
      "        [0.0004],\n",
      "        [0.0100],\n",
      "        [0.0095],\n",
      "        [0.0230],\n",
      "        [0.0121],\n",
      "        [0.0990],\n",
      "        [0.0053],\n",
      "        [0.0163],\n",
      "        [0.0036],\n",
      "        [0.0857],\n",
      "        [0.0126],\n",
      "        [0.0033],\n",
      "        [0.0054],\n",
      "        [0.0178],\n",
      "        [0.0339],\n",
      "        [0.0633],\n",
      "        [0.0288],\n",
      "        [0.0203],\n",
      "        [0.0154],\n",
      "        [0.0078],\n",
      "        [0.0255],\n",
      "        [0.0106],\n",
      "        [0.0041],\n",
      "        [0.0411],\n",
      "        [0.0244],\n",
      "        [0.0707],\n",
      "        [0.0423],\n",
      "        [0.0374],\n",
      "        [0.0164],\n",
      "        [0.0258],\n",
      "        [0.0175],\n",
      "        [0.0020],\n",
      "        [0.0269],\n",
      "        [0.0275],\n",
      "        [0.0014],\n",
      "        [0.0235],\n",
      "        [0.0058],\n",
      "        [0.0415],\n",
      "        [0.0294],\n",
      "        [0.0243],\n",
      "        [0.0281],\n",
      "        [0.0042],\n",
      "        [0.0199],\n",
      "        [0.0460],\n",
      "        [0.0142],\n",
      "        [0.0858],\n",
      "        [0.0369],\n",
      "        [0.0704],\n",
      "        [0.0330],\n",
      "        [0.0398],\n",
      "        [0.0556],\n",
      "        [0.0032],\n",
      "        [0.1317],\n",
      "        [0.0145],\n",
      "        [0.0122],\n",
      "        [0.0381],\n",
      "        [0.0035],\n",
      "        [0.0161],\n",
      "        [0.0540],\n",
      "        [0.0052],\n",
      "        [0.0403],\n",
      "        [0.0280],\n",
      "        [0.0511],\n",
      "        [0.0291],\n",
      "        [0.0284],\n",
      "        [0.0424],\n",
      "        [0.0233],\n",
      "        [0.0234],\n",
      "        [0.0190],\n",
      "        [0.0798],\n",
      "        [0.0277],\n",
      "        [0.0039],\n",
      "        [0.0585],\n",
      "        [0.0877],\n",
      "        [0.0708],\n",
      "        [0.0594],\n",
      "        [0.0852],\n",
      "        [0.0793],\n",
      "        [0.0995],\n",
      "        [0.0620],\n",
      "        [0.0697],\n",
      "        [0.0464],\n",
      "        [0.0655],\n",
      "        [0.0344],\n",
      "        [0.0447],\n",
      "        [0.0643],\n",
      "        [0.1074],\n",
      "        [0.0612],\n",
      "        [0.0310],\n",
      "        [0.0264],\n",
      "        [0.0500],\n",
      "        [0.1375],\n",
      "        [0.1121],\n",
      "        [0.0780],\n",
      "        [0.0139],\n",
      "        [0.0897],\n",
      "        [0.1348],\n",
      "        [0.1035],\n",
      "        [0.0107],\n",
      "        [0.0966],\n",
      "        [0.0978],\n",
      "        [0.0084],\n",
      "        [0.1119],\n",
      "        [0.1000],\n",
      "        [0.1161],\n",
      "        [0.1205],\n",
      "        [0.1028],\n",
      "        [0.1244],\n",
      "        [0.1203],\n",
      "        [0.1667],\n",
      "        [0.1586],\n",
      "        [0.0803],\n",
      "        [0.0828],\n",
      "        [0.1366],\n",
      "        [0.1359],\n",
      "        [0.1510],\n",
      "        [0.3115],\n",
      "        [0.3256],\n",
      "        [0.3285],\n",
      "        [0.3304],\n",
      "        [0.3314],\n",
      "        [0.3409],\n",
      "        [0.3427],\n",
      "        [0.3439],\n",
      "        [0.3513],\n",
      "        [0.3531],\n",
      "        [0.3551],\n",
      "        [0.3562],\n",
      "        [0.3594],\n",
      "        [0.3785],\n",
      "        [0.3795],\n",
      "        [0.3864],\n",
      "        [0.3908],\n",
      "        [0.3928],\n",
      "        [0.3936],\n",
      "        [0.3981],\n",
      "        [0.4055],\n",
      "        [0.4072],\n",
      "        [0.4079],\n",
      "        [0.4080]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0107],\n",
      "        [0.0355],\n",
      "        [0.0187],\n",
      "        [0.0733],\n",
      "        [0.0043],\n",
      "        [0.0107],\n",
      "        [0.0012],\n",
      "        [0.0107],\n",
      "        [0.0089],\n",
      "        [0.0229],\n",
      "        [0.0127],\n",
      "        [0.0985],\n",
      "        [0.0053],\n",
      "        [0.0158],\n",
      "        [0.0027],\n",
      "        [0.0852],\n",
      "        [0.0123],\n",
      "        [0.0025],\n",
      "        [0.0058],\n",
      "        [0.0181],\n",
      "        [0.0335],\n",
      "        [0.0625],\n",
      "        [0.0302],\n",
      "        [0.0214],\n",
      "        [0.0147],\n",
      "        [0.0072],\n",
      "        [0.0261],\n",
      "        [0.0096],\n",
      "        [0.0049],\n",
      "        [0.0418],\n",
      "        [0.0249],\n",
      "        [0.0722],\n",
      "        [0.0411],\n",
      "        [0.0370],\n",
      "        [0.0158],\n",
      "        [0.0257],\n",
      "        [0.0178],\n",
      "        [0.0029],\n",
      "        [0.0260],\n",
      "        [0.0278],\n",
      "        [0.0008],\n",
      "        [0.0229],\n",
      "        [0.0060],\n",
      "        [0.0420],\n",
      "        [0.0289],\n",
      "        [0.0244],\n",
      "        [0.0276],\n",
      "        [0.0040],\n",
      "        [0.0208],\n",
      "        [0.0457],\n",
      "        [0.0137],\n",
      "        [0.0851],\n",
      "        [0.0371],\n",
      "        [0.0709],\n",
      "        [0.0325],\n",
      "        [0.0395],\n",
      "        [0.0562],\n",
      "        [0.0031],\n",
      "        [0.1314],\n",
      "        [0.0138],\n",
      "        [0.0120],\n",
      "        [0.0375],\n",
      "        [0.0029],\n",
      "        [0.0155],\n",
      "        [0.0545],\n",
      "        [0.0059],\n",
      "        [0.0401],\n",
      "        [0.0279],\n",
      "        [0.0512],\n",
      "        [0.0291],\n",
      "        [0.0284],\n",
      "        [0.0427],\n",
      "        [0.0231],\n",
      "        [0.0235],\n",
      "        [0.0187],\n",
      "        [0.0795],\n",
      "        [0.0280],\n",
      "        [0.0024],\n",
      "        [0.0591],\n",
      "        [0.0876],\n",
      "        [0.0714],\n",
      "        [0.0598],\n",
      "        [0.0849],\n",
      "        [0.0799],\n",
      "        [0.1004],\n",
      "        [0.0624],\n",
      "        [0.0698],\n",
      "        [0.0459],\n",
      "        [0.0651],\n",
      "        [0.0350],\n",
      "        [0.0444],\n",
      "        [0.0638],\n",
      "        [0.1080],\n",
      "        [0.0605],\n",
      "        [0.0314],\n",
      "        [0.0268],\n",
      "        [0.0506],\n",
      "        [0.1379],\n",
      "        [0.1121],\n",
      "        [0.0775],\n",
      "        [0.0130],\n",
      "        [0.0900],\n",
      "        [0.1351],\n",
      "        [0.1037],\n",
      "        [0.0110],\n",
      "        [0.0966],\n",
      "        [0.0971],\n",
      "        [0.0078],\n",
      "        [0.1115],\n",
      "        [0.0998],\n",
      "        [0.1153],\n",
      "        [0.1201],\n",
      "        [0.1019],\n",
      "        [0.1245],\n",
      "        [0.1191],\n",
      "        [0.1657],\n",
      "        [0.1580],\n",
      "        [0.0803],\n",
      "        [0.0827],\n",
      "        [0.1365],\n",
      "        [0.1357],\n",
      "        [0.1509],\n",
      "        [0.3110],\n",
      "        [0.3250],\n",
      "        [0.3280],\n",
      "        [0.3299],\n",
      "        [0.3309],\n",
      "        [0.3404],\n",
      "        [0.3422],\n",
      "        [0.3434],\n",
      "        [0.3507],\n",
      "        [0.3525],\n",
      "        [0.3546],\n",
      "        [0.3557],\n",
      "        [0.3589],\n",
      "        [0.3780],\n",
      "        [0.3789],\n",
      "        [0.3859],\n",
      "        [0.3903],\n",
      "        [0.3923],\n",
      "        [0.3931],\n",
      "        [0.3976],\n",
      "        [0.4050],\n",
      "        [0.4067],\n",
      "        [0.4074],\n",
      "        [0.4075]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 85.73577618598938\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 147\n",
      "剩餘X 資料 torch.Size([13, 18])\n",
      "剩餘Y 資料 torch.Size([13, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16825151443481445, 6)\n",
      "The second_loss value of k: (0.17097774147987366, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.0512])\n",
      "目前模型的Data狀態 torch.Size([147, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6385],\n",
      "        [0.1263],\n",
      "        [0.5180],\n",
      "        [0.7323],\n",
      "        [0.2135],\n",
      "        [0.2229],\n",
      "        [0.6274],\n",
      "        [0.1814],\n",
      "        [0.6395],\n",
      "        [0.6337],\n",
      "        [0.6313],\n",
      "        [0.2741],\n",
      "        [0.6269],\n",
      "        [0.6065],\n",
      "        [0.6096],\n",
      "        [0.2035],\n",
      "        [0.5529],\n",
      "        [0.6190],\n",
      "        [0.6276],\n",
      "        [0.4223],\n",
      "        [0.6408],\n",
      "        [0.1531],\n",
      "        [0.4651],\n",
      "        [0.5675],\n",
      "        [0.6563],\n",
      "        [0.2694],\n",
      "        [0.1541],\n",
      "        [0.1292],\n",
      "        [0.6315],\n",
      "        [0.4291],\n",
      "        [0.5602],\n",
      "        [0.4392],\n",
      "        [0.1342],\n",
      "        [0.6177],\n",
      "        [0.5768],\n",
      "        [0.2174],\n",
      "        [0.4561],\n",
      "        [0.1057],\n",
      "        [0.5929],\n",
      "        [0.6489],\n",
      "        [0.6462],\n",
      "        [0.5787],\n",
      "        [0.3483],\n",
      "        [0.5094],\n",
      "        [0.5878],\n",
      "        [0.6185],\n",
      "        [0.2167],\n",
      "        [0.3540],\n",
      "        [0.6152],\n",
      "        [0.3709],\n",
      "        [0.1978],\n",
      "        [0.1660],\n",
      "        [0.2175],\n",
      "        [0.2200],\n",
      "        [0.5805],\n",
      "        [0.5659],\n",
      "        [0.4778],\n",
      "        [0.6218],\n",
      "        [0.2757],\n",
      "        [0.1521],\n",
      "        [0.3459],\n",
      "        [0.3085],\n",
      "        [0.1355],\n",
      "        [0.2829],\n",
      "        [0.6172],\n",
      "        [0.1272],\n",
      "        [0.1582],\n",
      "        [0.2792],\n",
      "        [0.1463],\n",
      "        [0.3295],\n",
      "        [0.3303],\n",
      "        [0.4292],\n",
      "        [0.3403],\n",
      "        [0.3410],\n",
      "        [0.6981],\n",
      "        [0.3937],\n",
      "        [0.5891],\n",
      "        [0.4593],\n",
      "        [0.4281],\n",
      "        [0.6514],\n",
      "        [0.5391],\n",
      "        [0.4048],\n",
      "        [0.3964],\n",
      "        [0.4432],\n",
      "        [0.4261],\n",
      "        [0.5614],\n",
      "        [0.2380],\n",
      "        [0.2267],\n",
      "        [0.2242],\n",
      "        [0.6371],\n",
      "        [0.3467],\n",
      "        [0.2839],\n",
      "        [0.4249],\n",
      "        [0.2482],\n",
      "        [0.6593],\n",
      "        [0.6698],\n",
      "        [0.6386],\n",
      "        [0.1648],\n",
      "        [0.4385],\n",
      "        [0.3424],\n",
      "        [0.4127],\n",
      "        [0.3090],\n",
      "        [0.1841],\n",
      "        [0.0583],\n",
      "        [0.2987],\n",
      "        [0.3556],\n",
      "        [0.2599],\n",
      "        [0.4220],\n",
      "        [0.3024],\n",
      "        [0.3512],\n",
      "        [0.1153],\n",
      "        [0.4069],\n",
      "        [0.1289],\n",
      "        [0.3120],\n",
      "        [0.5428],\n",
      "        [0.5862],\n",
      "        [0.3854],\n",
      "        [0.3288],\n",
      "        [0.4004],\n",
      "        [0.3307],\n",
      "        [0.3628],\n",
      "        [0.3449],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614],\n",
      "        [0.4614]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0107],\n",
      "        [0.0355],\n",
      "        [0.0187],\n",
      "        [0.0733],\n",
      "        [0.0043],\n",
      "        [0.0107],\n",
      "        [0.0012],\n",
      "        [0.0107],\n",
      "        [0.0089],\n",
      "        [0.0229],\n",
      "        [0.0127],\n",
      "        [0.0985],\n",
      "        [0.0053],\n",
      "        [0.0158],\n",
      "        [0.0027],\n",
      "        [0.0852],\n",
      "        [0.0123],\n",
      "        [0.0025],\n",
      "        [0.0058],\n",
      "        [0.0181],\n",
      "        [0.0335],\n",
      "        [0.0625],\n",
      "        [0.0302],\n",
      "        [0.0214],\n",
      "        [0.0147],\n",
      "        [0.0072],\n",
      "        [0.0261],\n",
      "        [0.0096],\n",
      "        [0.0049],\n",
      "        [0.0418],\n",
      "        [0.0249],\n",
      "        [0.0722],\n",
      "        [0.0411],\n",
      "        [0.0370],\n",
      "        [0.0158],\n",
      "        [0.0257],\n",
      "        [0.0178],\n",
      "        [0.0029],\n",
      "        [0.0260],\n",
      "        [0.0278],\n",
      "        [0.0008],\n",
      "        [0.0229],\n",
      "        [0.0060],\n",
      "        [0.0420],\n",
      "        [0.0289],\n",
      "        [0.0244],\n",
      "        [0.0276],\n",
      "        [0.0040],\n",
      "        [0.0208],\n",
      "        [0.0457],\n",
      "        [0.0137],\n",
      "        [0.0851],\n",
      "        [0.0371],\n",
      "        [0.0709],\n",
      "        [0.0325],\n",
      "        [0.0395],\n",
      "        [0.0562],\n",
      "        [0.0031],\n",
      "        [0.1314],\n",
      "        [0.0138],\n",
      "        [0.0120],\n",
      "        [0.0375],\n",
      "        [0.0029],\n",
      "        [0.0155],\n",
      "        [0.0545],\n",
      "        [0.0059],\n",
      "        [0.0401],\n",
      "        [0.0279],\n",
      "        [0.0512],\n",
      "        [0.0291],\n",
      "        [0.0284],\n",
      "        [0.0427],\n",
      "        [0.0231],\n",
      "        [0.0235],\n",
      "        [0.0187],\n",
      "        [0.0795],\n",
      "        [0.0280],\n",
      "        [0.0024],\n",
      "        [0.0591],\n",
      "        [0.0876],\n",
      "        [0.0714],\n",
      "        [0.0598],\n",
      "        [0.0849],\n",
      "        [0.0799],\n",
      "        [0.1004],\n",
      "        [0.0624],\n",
      "        [0.0698],\n",
      "        [0.0459],\n",
      "        [0.0651],\n",
      "        [0.0350],\n",
      "        [0.0444],\n",
      "        [0.0638],\n",
      "        [0.1080],\n",
      "        [0.0605],\n",
      "        [0.0314],\n",
      "        [0.0268],\n",
      "        [0.0506],\n",
      "        [0.1379],\n",
      "        [0.1121],\n",
      "        [0.0775],\n",
      "        [0.0130],\n",
      "        [0.0900],\n",
      "        [0.1351],\n",
      "        [0.1037],\n",
      "        [0.0110],\n",
      "        [0.0966],\n",
      "        [0.0971],\n",
      "        [0.0078],\n",
      "        [0.1115],\n",
      "        [0.0998],\n",
      "        [0.1153],\n",
      "        [0.1201],\n",
      "        [0.1019],\n",
      "        [0.1245],\n",
      "        [0.1191],\n",
      "        [0.1657],\n",
      "        [0.1580],\n",
      "        [0.0803],\n",
      "        [0.0827],\n",
      "        [0.1365],\n",
      "        [0.1357],\n",
      "        [0.1509],\n",
      "        [0.3110],\n",
      "        [0.3250],\n",
      "        [0.3280],\n",
      "        [0.3299],\n",
      "        [0.3309],\n",
      "        [0.3404],\n",
      "        [0.3422],\n",
      "        [0.3434],\n",
      "        [0.3507],\n",
      "        [0.3525],\n",
      "        [0.3546],\n",
      "        [0.3557],\n",
      "        [0.3589],\n",
      "        [0.3780],\n",
      "        [0.3789],\n",
      "        [0.3859],\n",
      "        [0.3903],\n",
      "        [0.3923],\n",
      "        [0.3931],\n",
      "        [0.3976],\n",
      "        [0.4050],\n",
      "        [0.4067],\n",
      "        [0.4074],\n",
      "        [0.4075],\n",
      "        [0.4102]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0144],\n",
      "        [0.0374],\n",
      "        [0.0154],\n",
      "        [0.0774],\n",
      "        [0.0025],\n",
      "        [0.0086],\n",
      "        [0.0022],\n",
      "        [0.0088],\n",
      "        [0.0125],\n",
      "        [0.0267],\n",
      "        [0.0090],\n",
      "        [0.1010],\n",
      "        [0.0091],\n",
      "        [0.0194],\n",
      "        [0.0061],\n",
      "        [0.0875],\n",
      "        [0.0157],\n",
      "        [0.0060],\n",
      "        [0.0022],\n",
      "        [0.0151],\n",
      "        [0.0373],\n",
      "        [0.0646],\n",
      "        [0.0274],\n",
      "        [0.0182],\n",
      "        [0.0183],\n",
      "        [0.0094],\n",
      "        [0.0241],\n",
      "        [0.0114],\n",
      "        [0.0014],\n",
      "        [0.0390],\n",
      "        [0.0217],\n",
      "        [0.0694],\n",
      "        [0.0429],\n",
      "        [0.0407],\n",
      "        [0.0192],\n",
      "        [0.0280],\n",
      "        [0.0148],\n",
      "        [0.0011],\n",
      "        [0.0292],\n",
      "        [0.0241],\n",
      "        [0.0044],\n",
      "        [0.0264],\n",
      "        [0.0035],\n",
      "        [0.0386],\n",
      "        [0.0324],\n",
      "        [0.0206],\n",
      "        [0.0296],\n",
      "        [0.0014],\n",
      "        [0.0174],\n",
      "        [0.0483],\n",
      "        [0.0158],\n",
      "        [0.0872],\n",
      "        [0.0352],\n",
      "        [0.0686],\n",
      "        [0.0361],\n",
      "        [0.0429],\n",
      "        [0.0531],\n",
      "        [0.0069],\n",
      "        [0.1339],\n",
      "        [0.0159],\n",
      "        [0.0093],\n",
      "        [0.0399],\n",
      "        [0.0049],\n",
      "        [0.0179],\n",
      "        [0.0509],\n",
      "        [0.0041],\n",
      "        [0.0404],\n",
      "        [0.0256],\n",
      "        [0.0508],\n",
      "        [0.0265],\n",
      "        [0.0259],\n",
      "        [0.0397],\n",
      "        [0.0204],\n",
      "        [0.0209],\n",
      "        [0.0225],\n",
      "        [0.0823],\n",
      "        [0.0245],\n",
      "        [0.0050],\n",
      "        [0.0562],\n",
      "        [0.0915],\n",
      "        [0.0682],\n",
      "        [0.0568],\n",
      "        [0.0878],\n",
      "        [0.0769],\n",
      "        [0.0974],\n",
      "        [0.0591],\n",
      "        [0.0676],\n",
      "        [0.0480],\n",
      "        [0.0672],\n",
      "        [0.0315],\n",
      "        [0.0418],\n",
      "        [0.0660],\n",
      "        [0.1051],\n",
      "        [0.0626],\n",
      "        [0.0278],\n",
      "        [0.0231],\n",
      "        [0.0471],\n",
      "        [0.1357],\n",
      "        [0.1153],\n",
      "        [0.0801],\n",
      "        [0.0159],\n",
      "        [0.0876],\n",
      "        [0.1330],\n",
      "        [0.1022],\n",
      "        [0.0084],\n",
      "        [0.0939],\n",
      "        [0.0992],\n",
      "        [0.0107],\n",
      "        [0.1139],\n",
      "        [0.0972],\n",
      "        [0.1165],\n",
      "        [0.1229],\n",
      "        [0.1034],\n",
      "        [0.1221],\n",
      "        [0.1221],\n",
      "        [0.1689],\n",
      "        [0.1607],\n",
      "        [0.0775],\n",
      "        [0.0796],\n",
      "        [0.1336],\n",
      "        [0.1327],\n",
      "        [0.1478],\n",
      "        [0.3105],\n",
      "        [0.3246],\n",
      "        [0.3275],\n",
      "        [0.3294],\n",
      "        [0.3304],\n",
      "        [0.3399],\n",
      "        [0.3417],\n",
      "        [0.3429],\n",
      "        [0.3503],\n",
      "        [0.3521],\n",
      "        [0.3541],\n",
      "        [0.3552],\n",
      "        [0.3584],\n",
      "        [0.3775],\n",
      "        [0.3785],\n",
      "        [0.3854],\n",
      "        [0.3898],\n",
      "        [0.3918],\n",
      "        [0.3926],\n",
      "        [0.3971],\n",
      "        [0.4045],\n",
      "        [0.4062],\n",
      "        [0.4069],\n",
      "        [0.4070],\n",
      "        [0.4097]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 85.97115087509155\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 148\n",
      "剩餘X 資料 torch.Size([12, 18])\n",
      "剩餘Y 資料 torch.Size([12, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1705724149942398, 9)\n",
      "The second_loss value of k: (0.17147637903690338, 11)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.0479])\n",
      "目前模型的Data狀態 torch.Size([148, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6422],\n",
      "        [0.1282],\n",
      "        [0.5212],\n",
      "        [0.7363],\n",
      "        [0.2153],\n",
      "        [0.2250],\n",
      "        [0.6308],\n",
      "        [0.1833],\n",
      "        [0.6431],\n",
      "        [0.6374],\n",
      "        [0.6349],\n",
      "        [0.2766],\n",
      "        [0.6307],\n",
      "        [0.6101],\n",
      "        [0.6130],\n",
      "        [0.2057],\n",
      "        [0.5563],\n",
      "        [0.6225],\n",
      "        [0.6311],\n",
      "        [0.4254],\n",
      "        [0.6446],\n",
      "        [0.1552],\n",
      "        [0.4679],\n",
      "        [0.5706],\n",
      "        [0.6599],\n",
      "        [0.2716],\n",
      "        [0.1561],\n",
      "        [0.1310],\n",
      "        [0.6350],\n",
      "        [0.4320],\n",
      "        [0.5634],\n",
      "        [0.4419],\n",
      "        [0.1360],\n",
      "        [0.6214],\n",
      "        [0.5802],\n",
      "        [0.2197],\n",
      "        [0.4592],\n",
      "        [0.1075],\n",
      "        [0.5961],\n",
      "        [0.6526],\n",
      "        [0.6498],\n",
      "        [0.5822],\n",
      "        [0.3508],\n",
      "        [0.5128],\n",
      "        [0.5914],\n",
      "        [0.6223],\n",
      "        [0.2187],\n",
      "        [0.3566],\n",
      "        [0.6186],\n",
      "        [0.3736],\n",
      "        [0.1998],\n",
      "        [0.1680],\n",
      "        [0.2194],\n",
      "        [0.2223],\n",
      "        [0.5840],\n",
      "        [0.5694],\n",
      "        [0.4809],\n",
      "        [0.6256],\n",
      "        [0.2782],\n",
      "        [0.1542],\n",
      "        [0.3486],\n",
      "        [0.3108],\n",
      "        [0.1375],\n",
      "        [0.2853],\n",
      "        [0.6208],\n",
      "        [0.1291],\n",
      "        [0.1585],\n",
      "        [0.2815],\n",
      "        [0.1467],\n",
      "        [0.3320],\n",
      "        [0.3328],\n",
      "        [0.4322],\n",
      "        [0.3430],\n",
      "        [0.3436],\n",
      "        [0.7019],\n",
      "        [0.3964],\n",
      "        [0.5926],\n",
      "        [0.4619],\n",
      "        [0.4310],\n",
      "        [0.6553],\n",
      "        [0.5423],\n",
      "        [0.4077],\n",
      "        [0.3992],\n",
      "        [0.4461],\n",
      "        [0.4291],\n",
      "        [0.5648],\n",
      "        [0.2402],\n",
      "        [0.2288],\n",
      "        [0.2263],\n",
      "        [0.6406],\n",
      "        [0.3493],\n",
      "        [0.2862],\n",
      "        [0.4278],\n",
      "        [0.2502],\n",
      "        [0.6629],\n",
      "        [0.6735],\n",
      "        [0.6422],\n",
      "        [0.1670],\n",
      "        [0.4416],\n",
      "        [0.3450],\n",
      "        [0.4155],\n",
      "        [0.3114],\n",
      "        [0.1862],\n",
      "        [0.0598],\n",
      "        [0.3013],\n",
      "        [0.3582],\n",
      "        [0.2620],\n",
      "        [0.4248],\n",
      "        [0.3047],\n",
      "        [0.3538],\n",
      "        [0.1165],\n",
      "        [0.4097],\n",
      "        [0.1305],\n",
      "        [0.3144],\n",
      "        [0.5458],\n",
      "        [0.5894],\n",
      "        [0.3881],\n",
      "        [0.3317],\n",
      "        [0.4035],\n",
      "        [0.3336],\n",
      "        [0.3658],\n",
      "        [0.3479],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609],\n",
      "        [0.4609]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0144],\n",
      "        [0.0374],\n",
      "        [0.0154],\n",
      "        [0.0774],\n",
      "        [0.0025],\n",
      "        [0.0086],\n",
      "        [0.0022],\n",
      "        [0.0088],\n",
      "        [0.0125],\n",
      "        [0.0267],\n",
      "        [0.0090],\n",
      "        [0.1010],\n",
      "        [0.0091],\n",
      "        [0.0194],\n",
      "        [0.0061],\n",
      "        [0.0875],\n",
      "        [0.0157],\n",
      "        [0.0060],\n",
      "        [0.0022],\n",
      "        [0.0151],\n",
      "        [0.0373],\n",
      "        [0.0646],\n",
      "        [0.0274],\n",
      "        [0.0182],\n",
      "        [0.0183],\n",
      "        [0.0094],\n",
      "        [0.0241],\n",
      "        [0.0114],\n",
      "        [0.0014],\n",
      "        [0.0390],\n",
      "        [0.0217],\n",
      "        [0.0694],\n",
      "        [0.0429],\n",
      "        [0.0407],\n",
      "        [0.0192],\n",
      "        [0.0280],\n",
      "        [0.0148],\n",
      "        [0.0011],\n",
      "        [0.0292],\n",
      "        [0.0241],\n",
      "        [0.0044],\n",
      "        [0.0264],\n",
      "        [0.0035],\n",
      "        [0.0386],\n",
      "        [0.0324],\n",
      "        [0.0206],\n",
      "        [0.0296],\n",
      "        [0.0014],\n",
      "        [0.0174],\n",
      "        [0.0483],\n",
      "        [0.0158],\n",
      "        [0.0872],\n",
      "        [0.0352],\n",
      "        [0.0686],\n",
      "        [0.0361],\n",
      "        [0.0429],\n",
      "        [0.0531],\n",
      "        [0.0069],\n",
      "        [0.1339],\n",
      "        [0.0159],\n",
      "        [0.0093],\n",
      "        [0.0399],\n",
      "        [0.0049],\n",
      "        [0.0179],\n",
      "        [0.0509],\n",
      "        [0.0041],\n",
      "        [0.0404],\n",
      "        [0.0256],\n",
      "        [0.0508],\n",
      "        [0.0265],\n",
      "        [0.0259],\n",
      "        [0.0397],\n",
      "        [0.0204],\n",
      "        [0.0209],\n",
      "        [0.0225],\n",
      "        [0.0823],\n",
      "        [0.0245],\n",
      "        [0.0050],\n",
      "        [0.0562],\n",
      "        [0.0915],\n",
      "        [0.0682],\n",
      "        [0.0568],\n",
      "        [0.0878],\n",
      "        [0.0769],\n",
      "        [0.0974],\n",
      "        [0.0591],\n",
      "        [0.0676],\n",
      "        [0.0480],\n",
      "        [0.0672],\n",
      "        [0.0315],\n",
      "        [0.0418],\n",
      "        [0.0660],\n",
      "        [0.1051],\n",
      "        [0.0626],\n",
      "        [0.0278],\n",
      "        [0.0231],\n",
      "        [0.0471],\n",
      "        [0.1357],\n",
      "        [0.1153],\n",
      "        [0.0801],\n",
      "        [0.0159],\n",
      "        [0.0876],\n",
      "        [0.1330],\n",
      "        [0.1022],\n",
      "        [0.0084],\n",
      "        [0.0939],\n",
      "        [0.0992],\n",
      "        [0.0107],\n",
      "        [0.1139],\n",
      "        [0.0972],\n",
      "        [0.1165],\n",
      "        [0.1229],\n",
      "        [0.1034],\n",
      "        [0.1221],\n",
      "        [0.1221],\n",
      "        [0.1689],\n",
      "        [0.1607],\n",
      "        [0.0775],\n",
      "        [0.0796],\n",
      "        [0.1336],\n",
      "        [0.1327],\n",
      "        [0.1478],\n",
      "        [0.3105],\n",
      "        [0.3246],\n",
      "        [0.3275],\n",
      "        [0.3294],\n",
      "        [0.3304],\n",
      "        [0.3399],\n",
      "        [0.3417],\n",
      "        [0.3429],\n",
      "        [0.3503],\n",
      "        [0.3521],\n",
      "        [0.3541],\n",
      "        [0.3552],\n",
      "        [0.3584],\n",
      "        [0.3775],\n",
      "        [0.3785],\n",
      "        [0.3854],\n",
      "        [0.3898],\n",
      "        [0.3918],\n",
      "        [0.3926],\n",
      "        [0.3971],\n",
      "        [0.4045],\n",
      "        [0.4062],\n",
      "        [0.4069],\n",
      "        [0.4070],\n",
      "        [0.4097],\n",
      "        [0.4130]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0141],\n",
      "        [0.0368],\n",
      "        [0.0159],\n",
      "        [0.0774],\n",
      "        [0.0030],\n",
      "        [0.0087],\n",
      "        [0.0015],\n",
      "        [0.0092],\n",
      "        [0.0122],\n",
      "        [0.0266],\n",
      "        [0.0095],\n",
      "        [0.1007],\n",
      "        [0.0090],\n",
      "        [0.0190],\n",
      "        [0.0053],\n",
      "        [0.0872],\n",
      "        [0.0153],\n",
      "        [0.0053],\n",
      "        [0.0026],\n",
      "        [0.0153],\n",
      "        [0.0370],\n",
      "        [0.0641],\n",
      "        [0.0285],\n",
      "        [0.0190],\n",
      "        [0.0180],\n",
      "        [0.0089],\n",
      "        [0.0244],\n",
      "        [0.0107],\n",
      "        [0.0019],\n",
      "        [0.0396],\n",
      "        [0.0220],\n",
      "        [0.0704],\n",
      "        [0.0420],\n",
      "        [0.0404],\n",
      "        [0.0186],\n",
      "        [0.0279],\n",
      "        [0.0152],\n",
      "        [0.0018],\n",
      "        [0.0285],\n",
      "        [0.0244],\n",
      "        [0.0040],\n",
      "        [0.0258],\n",
      "        [0.0038],\n",
      "        [0.0389],\n",
      "        [0.0320],\n",
      "        [0.0208],\n",
      "        [0.0292],\n",
      "        [0.0014],\n",
      "        [0.0180],\n",
      "        [0.0480],\n",
      "        [0.0154],\n",
      "        [0.0867],\n",
      "        [0.0352],\n",
      "        [0.0690],\n",
      "        [0.0357],\n",
      "        [0.0426],\n",
      "        [0.0536],\n",
      "        [0.0068],\n",
      "        [0.1338],\n",
      "        [0.0156],\n",
      "        [0.0093],\n",
      "        [0.0394],\n",
      "        [0.0045],\n",
      "        [0.0174],\n",
      "        [0.0513],\n",
      "        [0.0046],\n",
      "        [0.0402],\n",
      "        [0.0256],\n",
      "        [0.0508],\n",
      "        [0.0266],\n",
      "        [0.0260],\n",
      "        [0.0400],\n",
      "        [0.0204],\n",
      "        [0.0211],\n",
      "        [0.0224],\n",
      "        [0.0820],\n",
      "        [0.0246],\n",
      "        [0.0040],\n",
      "        [0.0568],\n",
      "        [0.0915],\n",
      "        [0.0687],\n",
      "        [0.0572],\n",
      "        [0.0876],\n",
      "        [0.0775],\n",
      "        [0.0980],\n",
      "        [0.0594],\n",
      "        [0.0678],\n",
      "        [0.0476],\n",
      "        [0.0669],\n",
      "        [0.0318],\n",
      "        [0.0416],\n",
      "        [0.0656],\n",
      "        [0.1057],\n",
      "        [0.0621],\n",
      "        [0.0280],\n",
      "        [0.0233],\n",
      "        [0.0474],\n",
      "        [0.1361],\n",
      "        [0.1152],\n",
      "        [0.0796],\n",
      "        [0.0153],\n",
      "        [0.0878],\n",
      "        [0.1333],\n",
      "        [0.1023],\n",
      "        [0.0085],\n",
      "        [0.0940],\n",
      "        [0.0987],\n",
      "        [0.0104],\n",
      "        [0.1135],\n",
      "        [0.0972],\n",
      "        [0.1160],\n",
      "        [0.1225],\n",
      "        [0.1027],\n",
      "        [0.1223],\n",
      "        [0.1212],\n",
      "        [0.1682],\n",
      "        [0.1601],\n",
      "        [0.0773],\n",
      "        [0.0795],\n",
      "        [0.1333],\n",
      "        [0.1324],\n",
      "        [0.1476],\n",
      "        [0.3101],\n",
      "        [0.3241],\n",
      "        [0.3271],\n",
      "        [0.3289],\n",
      "        [0.3300],\n",
      "        [0.3394],\n",
      "        [0.3412],\n",
      "        [0.3424],\n",
      "        [0.3498],\n",
      "        [0.3516],\n",
      "        [0.3537],\n",
      "        [0.3547],\n",
      "        [0.3579],\n",
      "        [0.3770],\n",
      "        [0.3780],\n",
      "        [0.3849],\n",
      "        [0.3893],\n",
      "        [0.3913],\n",
      "        [0.3922],\n",
      "        [0.3966],\n",
      "        [0.4041],\n",
      "        [0.4057],\n",
      "        [0.4065],\n",
      "        [0.4065],\n",
      "        [0.4092],\n",
      "        [0.4125]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 86.2077853679657\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 149\n",
      "剩餘X 資料 torch.Size([11, 18])\n",
      "剩餘Y 資料 torch.Size([11, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1714797168970108, 9)\n",
      "The second_loss value of k: (0.1719464808702469, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.0463])\n",
      "目前模型的Data狀態 torch.Size([149, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.6418],\n",
      "        [0.1276],\n",
      "        [0.5207],\n",
      "        [0.7363],\n",
      "        [0.2148],\n",
      "        [0.2249],\n",
      "        [0.6301],\n",
      "        [0.1829],\n",
      "        [0.6427],\n",
      "        [0.6374],\n",
      "        [0.6344],\n",
      "        [0.2763],\n",
      "        [0.6306],\n",
      "        [0.6097],\n",
      "        [0.6123],\n",
      "        [0.2054],\n",
      "        [0.5559],\n",
      "        [0.6219],\n",
      "        [0.6308],\n",
      "        [0.4251],\n",
      "        [0.6442],\n",
      "        [0.1547],\n",
      "        [0.4669],\n",
      "        [0.5699],\n",
      "        [0.6595],\n",
      "        [0.2711],\n",
      "        [0.1558],\n",
      "        [0.1302],\n",
      "        [0.6345],\n",
      "        [0.4313],\n",
      "        [0.5631],\n",
      "        [0.4410],\n",
      "        [0.1351],\n",
      "        [0.6211],\n",
      "        [0.5797],\n",
      "        [0.2196],\n",
      "        [0.4588],\n",
      "        [0.1068],\n",
      "        [0.5954],\n",
      "        [0.6523],\n",
      "        [0.6494],\n",
      "        [0.5817],\n",
      "        [0.3505],\n",
      "        [0.5125],\n",
      "        [0.5909],\n",
      "        [0.6222],\n",
      "        [0.2184],\n",
      "        [0.3566],\n",
      "        [0.6180],\n",
      "        [0.3733],\n",
      "        [0.1994],\n",
      "        [0.1676],\n",
      "        [0.2193],\n",
      "        [0.2219],\n",
      "        [0.5836],\n",
      "        [0.5690],\n",
      "        [0.4804],\n",
      "        [0.6255],\n",
      "        [0.2781],\n",
      "        [0.1538],\n",
      "        [0.3486],\n",
      "        [0.3104],\n",
      "        [0.1371],\n",
      "        [0.2848],\n",
      "        [0.6204],\n",
      "        [0.1285],\n",
      "        [0.1583],\n",
      "        [0.2815],\n",
      "        [0.1467],\n",
      "        [0.3320],\n",
      "        [0.3327],\n",
      "        [0.4319],\n",
      "        [0.3431],\n",
      "        [0.3434],\n",
      "        [0.7018],\n",
      "        [0.3962],\n",
      "        [0.5925],\n",
      "        [0.4608],\n",
      "        [0.4304],\n",
      "        [0.6552],\n",
      "        [0.5418],\n",
      "        [0.4073],\n",
      "        [0.3990],\n",
      "        [0.4455],\n",
      "        [0.4285],\n",
      "        [0.5645],\n",
      "        [0.2400],\n",
      "        [0.2284],\n",
      "        [0.2260],\n",
      "        [0.6403],\n",
      "        [0.3495],\n",
      "        [0.2857],\n",
      "        [0.4272],\n",
      "        [0.2497],\n",
      "        [0.6628],\n",
      "        [0.6733],\n",
      "        [0.6419],\n",
      "        [0.1666],\n",
      "        [0.4415],\n",
      "        [0.3446],\n",
      "        [0.4150],\n",
      "        [0.3111],\n",
      "        [0.1859],\n",
      "        [0.0598],\n",
      "        [0.3012],\n",
      "        [0.3582],\n",
      "        [0.2614],\n",
      "        [0.4245],\n",
      "        [0.3044],\n",
      "        [0.3539],\n",
      "        [0.1160],\n",
      "        [0.4093],\n",
      "        [0.1298],\n",
      "        [0.3142],\n",
      "        [0.5449],\n",
      "        [0.5887],\n",
      "        [0.3875],\n",
      "        [0.3318],\n",
      "        [0.4036],\n",
      "        [0.3339],\n",
      "        [0.3661],\n",
      "        [0.3482],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604],\n",
      "        [0.4604]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0141],\n",
      "        [0.0368],\n",
      "        [0.0159],\n",
      "        [0.0774],\n",
      "        [0.0030],\n",
      "        [0.0087],\n",
      "        [0.0015],\n",
      "        [0.0092],\n",
      "        [0.0122],\n",
      "        [0.0266],\n",
      "        [0.0095],\n",
      "        [0.1007],\n",
      "        [0.0090],\n",
      "        [0.0190],\n",
      "        [0.0053],\n",
      "        [0.0872],\n",
      "        [0.0153],\n",
      "        [0.0053],\n",
      "        [0.0026],\n",
      "        [0.0153],\n",
      "        [0.0370],\n",
      "        [0.0641],\n",
      "        [0.0285],\n",
      "        [0.0190],\n",
      "        [0.0180],\n",
      "        [0.0089],\n",
      "        [0.0244],\n",
      "        [0.0107],\n",
      "        [0.0019],\n",
      "        [0.0396],\n",
      "        [0.0220],\n",
      "        [0.0704],\n",
      "        [0.0420],\n",
      "        [0.0404],\n",
      "        [0.0186],\n",
      "        [0.0279],\n",
      "        [0.0152],\n",
      "        [0.0018],\n",
      "        [0.0285],\n",
      "        [0.0244],\n",
      "        [0.0040],\n",
      "        [0.0258],\n",
      "        [0.0038],\n",
      "        [0.0389],\n",
      "        [0.0320],\n",
      "        [0.0208],\n",
      "        [0.0292],\n",
      "        [0.0014],\n",
      "        [0.0180],\n",
      "        [0.0480],\n",
      "        [0.0154],\n",
      "        [0.0867],\n",
      "        [0.0352],\n",
      "        [0.0690],\n",
      "        [0.0357],\n",
      "        [0.0426],\n",
      "        [0.0536],\n",
      "        [0.0068],\n",
      "        [0.1338],\n",
      "        [0.0156],\n",
      "        [0.0093],\n",
      "        [0.0394],\n",
      "        [0.0045],\n",
      "        [0.0174],\n",
      "        [0.0513],\n",
      "        [0.0046],\n",
      "        [0.0402],\n",
      "        [0.0256],\n",
      "        [0.0508],\n",
      "        [0.0266],\n",
      "        [0.0260],\n",
      "        [0.0400],\n",
      "        [0.0204],\n",
      "        [0.0211],\n",
      "        [0.0224],\n",
      "        [0.0820],\n",
      "        [0.0246],\n",
      "        [0.0040],\n",
      "        [0.0568],\n",
      "        [0.0915],\n",
      "        [0.0687],\n",
      "        [0.0572],\n",
      "        [0.0876],\n",
      "        [0.0775],\n",
      "        [0.0980],\n",
      "        [0.0594],\n",
      "        [0.0678],\n",
      "        [0.0476],\n",
      "        [0.0669],\n",
      "        [0.0318],\n",
      "        [0.0416],\n",
      "        [0.0656],\n",
      "        [0.1057],\n",
      "        [0.0621],\n",
      "        [0.0280],\n",
      "        [0.0233],\n",
      "        [0.0474],\n",
      "        [0.1361],\n",
      "        [0.1152],\n",
      "        [0.0796],\n",
      "        [0.0153],\n",
      "        [0.0878],\n",
      "        [0.1333],\n",
      "        [0.1023],\n",
      "        [0.0085],\n",
      "        [0.0940],\n",
      "        [0.0987],\n",
      "        [0.0104],\n",
      "        [0.1135],\n",
      "        [0.0972],\n",
      "        [0.1160],\n",
      "        [0.1225],\n",
      "        [0.1027],\n",
      "        [0.1223],\n",
      "        [0.1212],\n",
      "        [0.1682],\n",
      "        [0.1601],\n",
      "        [0.0773],\n",
      "        [0.0795],\n",
      "        [0.1333],\n",
      "        [0.1324],\n",
      "        [0.1476],\n",
      "        [0.3101],\n",
      "        [0.3241],\n",
      "        [0.3271],\n",
      "        [0.3289],\n",
      "        [0.3300],\n",
      "        [0.3394],\n",
      "        [0.3412],\n",
      "        [0.3424],\n",
      "        [0.3498],\n",
      "        [0.3516],\n",
      "        [0.3537],\n",
      "        [0.3547],\n",
      "        [0.3579],\n",
      "        [0.3770],\n",
      "        [0.3780],\n",
      "        [0.3849],\n",
      "        [0.3893],\n",
      "        [0.3913],\n",
      "        [0.3922],\n",
      "        [0.3966],\n",
      "        [0.4041],\n",
      "        [0.4057],\n",
      "        [0.4065],\n",
      "        [0.4065],\n",
      "        [0.4092],\n",
      "        [0.4125],\n",
      "        [0.4141]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0153],\n",
      "        [    0.0373],\n",
      "        [    0.0148],\n",
      "        [    0.0787],\n",
      "        [    0.0026],\n",
      "        [    0.0078],\n",
      "        [    0.0025],\n",
      "        [    0.0087],\n",
      "        [    0.0133],\n",
      "        [    0.0280],\n",
      "        [    0.0083],\n",
      "        [    0.1015],\n",
      "        [    0.0104],\n",
      "        [    0.0202],\n",
      "        [    0.0063],\n",
      "        [    0.0879],\n",
      "        [    0.0166],\n",
      "        [    0.0064],\n",
      "        [    0.0013],\n",
      "        [    0.0142],\n",
      "        [    0.0383],\n",
      "        [    0.0647],\n",
      "        [    0.0279],\n",
      "        [    0.0181],\n",
      "        [    0.0191],\n",
      "        [    0.0096],\n",
      "        [    0.0239],\n",
      "        [    0.0111],\n",
      "        [    0.0008],\n",
      "        [    0.0387],\n",
      "        [    0.0210],\n",
      "        [    0.0698],\n",
      "        [    0.0423],\n",
      "        [    0.0417],\n",
      "        [    0.0197],\n",
      "        [    0.0287],\n",
      "        [    0.0141],\n",
      "        [    0.0014],\n",
      "        [    0.0295],\n",
      "        [    0.0232],\n",
      "        [    0.0051],\n",
      "        [    0.0270],\n",
      "        [    0.0028],\n",
      "        [    0.0378],\n",
      "        [    0.0332],\n",
      "        [    0.0194],\n",
      "        [    0.0298],\n",
      "        [    0.0003],\n",
      "        [    0.0170],\n",
      "        [    0.0490],\n",
      "        [    0.0161],\n",
      "        [    0.0873],\n",
      "        [    0.0347],\n",
      "        [    0.0683],\n",
      "        [    0.0369],\n",
      "        [    0.0438],\n",
      "        [    0.0526],\n",
      "        [    0.0082],\n",
      "        [    0.1346],\n",
      "        [    0.0162],\n",
      "        [    0.0082],\n",
      "        [    0.0402],\n",
      "        [    0.0051],\n",
      "        [    0.0181],\n",
      "        [    0.0501],\n",
      "        [    0.0041],\n",
      "        [    0.0400],\n",
      "        [    0.0247],\n",
      "        [    0.0510],\n",
      "        [    0.0255],\n",
      "        [    0.0250],\n",
      "        [    0.0389],\n",
      "        [    0.0192],\n",
      "        [    0.0201],\n",
      "        [    0.0237],\n",
      "        [    0.0830],\n",
      "        [    0.0234],\n",
      "        [    0.0045],\n",
      "        [    0.0559],\n",
      "        [    0.0929],\n",
      "        [    0.0678],\n",
      "        [    0.0562],\n",
      "        [    0.0887],\n",
      "        [    0.0765],\n",
      "        [    0.0973],\n",
      "        [    0.0583],\n",
      "        [    0.0670],\n",
      "        [    0.0483],\n",
      "        [    0.0676],\n",
      "        [    0.0306],\n",
      "        [    0.0405],\n",
      "        [    0.0664],\n",
      "        [    0.1048],\n",
      "        [    0.0628],\n",
      "        [    0.0267],\n",
      "        [    0.0220],\n",
      "        [    0.0463],\n",
      "        [    0.1354],\n",
      "        [    0.1164],\n",
      "        [    0.0805],\n",
      "        [    0.0162],\n",
      "        [    0.0869],\n",
      "        [    0.1326],\n",
      "        [    0.1019],\n",
      "        [    0.0076],\n",
      "        [    0.0929],\n",
      "        [    0.0993],\n",
      "        [    0.0113],\n",
      "        [    0.1143],\n",
      "        [    0.0961],\n",
      "        [    0.1161],\n",
      "        [    0.1235],\n",
      "        [    0.1030],\n",
      "        [    0.1214],\n",
      "        [    0.1220],\n",
      "        [    0.1691],\n",
      "        [    0.1610],\n",
      "        [    0.0762],\n",
      "        [    0.0782],\n",
      "        [    0.1321],\n",
      "        [    0.1312],\n",
      "        [    0.1463],\n",
      "        [    0.3095],\n",
      "        [    0.3235],\n",
      "        [    0.3265],\n",
      "        [    0.3283],\n",
      "        [    0.3294],\n",
      "        [    0.3388],\n",
      "        [    0.3406],\n",
      "        [    0.3418],\n",
      "        [    0.3492],\n",
      "        [    0.3510],\n",
      "        [    0.3531],\n",
      "        [    0.3541],\n",
      "        [    0.3573],\n",
      "        [    0.3764],\n",
      "        [    0.3774],\n",
      "        [    0.3843],\n",
      "        [    0.3887],\n",
      "        [    0.3907],\n",
      "        [    0.3916],\n",
      "        [    0.3960],\n",
      "        [    0.4035],\n",
      "        [    0.4051],\n",
      "        [    0.4059],\n",
      "        [    0.4059],\n",
      "        [    0.4086],\n",
      "        [    0.4119],\n",
      "        [    0.4135]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 86.44424772262573\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 150\n",
      "剩餘X 資料 torch.Size([10, 18])\n",
      "剩餘Y 資料 torch.Size([10, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1718287467956543, 9)\n",
      "The second_loss value of k: (0.1724129617214203, 8)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.1187])\n",
      "目前模型的Data狀態 torch.Size([150, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6431],\n",
      "        [ 0.1280],\n",
      "        [ 0.5218],\n",
      "        [ 0.7377],\n",
      "        [ 0.2152],\n",
      "        [ 0.2258],\n",
      "        [ 0.6312],\n",
      "        [ 0.1834],\n",
      "        [ 0.6439],\n",
      "        [ 0.6387],\n",
      "        [ 0.6356],\n",
      "        [ 0.2771],\n",
      "        [ 0.6320],\n",
      "        [ 0.6109],\n",
      "        [ 0.6133],\n",
      "        [ 0.2061],\n",
      "        [ 0.5572],\n",
      "        [ 0.6230],\n",
      "        [ 0.6320],\n",
      "        [ 0.4262],\n",
      "        [ 0.6456],\n",
      "        [ 0.1553],\n",
      "        [ 0.4674],\n",
      "        [ 0.5707],\n",
      "        [ 0.6606],\n",
      "        [ 0.2718],\n",
      "        [ 0.1564],\n",
      "        [ 0.1307],\n",
      "        [ 0.6356],\n",
      "        [ 0.4322],\n",
      "        [ 0.5641],\n",
      "        [ 0.4415],\n",
      "        [ 0.1354],\n",
      "        [ 0.6224],\n",
      "        [ 0.5808],\n",
      "        [ 0.2204],\n",
      "        [ 0.4599],\n",
      "        [ 0.1073],\n",
      "        [ 0.5964],\n",
      "        [ 0.6536],\n",
      "        [ 0.6505],\n",
      "        [ 0.5828],\n",
      "        [ 0.3515],\n",
      "        [ 0.5136],\n",
      "        [ 0.5922],\n",
      "        [ 0.6236],\n",
      "        [ 0.2189],\n",
      "        [ 0.3577],\n",
      "        [ 0.6190],\n",
      "        [ 0.3743],\n",
      "        [ 0.2001],\n",
      "        [ 0.1682],\n",
      "        [ 0.2199],\n",
      "        [ 0.2226],\n",
      "        [ 0.5848],\n",
      "        [ 0.5703],\n",
      "        [ 0.4814],\n",
      "        [ 0.6269],\n",
      "        [ 0.2790],\n",
      "        [ 0.1544],\n",
      "        [ 0.3497],\n",
      "        [ 0.3112],\n",
      "        [ 0.1377],\n",
      "        [ 0.2856],\n",
      "        [ 0.6216],\n",
      "        [ 0.1290],\n",
      "        [ 0.1581],\n",
      "        [ 0.2824],\n",
      "        [ 0.1465],\n",
      "        [ 0.3330],\n",
      "        [ 0.3337],\n",
      "        [ 0.4330],\n",
      "        [ 0.3442],\n",
      "        [ 0.3444],\n",
      "        [ 0.7031],\n",
      "        [ 0.3972],\n",
      "        [ 0.5937],\n",
      "        [ 0.4614],\n",
      "        [ 0.4314],\n",
      "        [ 0.6566],\n",
      "        [ 0.5427],\n",
      "        [ 0.4084],\n",
      "        [ 0.4001],\n",
      "        [ 0.4465],\n",
      "        [ 0.4293],\n",
      "        [ 0.5655],\n",
      "        [ 0.2408],\n",
      "        [ 0.2291],\n",
      "        [ 0.2267],\n",
      "        [ 0.6414],\n",
      "        [ 0.3506],\n",
      "        [ 0.2865],\n",
      "        [ 0.4282],\n",
      "        [ 0.2504],\n",
      "        [ 0.6640],\n",
      "        [ 0.6746],\n",
      "        [ 0.6430],\n",
      "        [ 0.1673],\n",
      "        [ 0.4428],\n",
      "        [ 0.3454],\n",
      "        [ 0.4159],\n",
      "        [ 0.3121],\n",
      "        [ 0.1866],\n",
      "        [ 0.0602],\n",
      "        [ 0.3021],\n",
      "        [ 0.3593],\n",
      "        [ 0.2621],\n",
      "        [ 0.4255],\n",
      "        [ 0.3052],\n",
      "        [ 0.3550],\n",
      "        [ 0.1161],\n",
      "        [ 0.4103],\n",
      "        [ 0.1301],\n",
      "        [ 0.3151],\n",
      "        [ 0.5457],\n",
      "        [ 0.5896],\n",
      "        [ 0.3884],\n",
      "        [ 0.3329],\n",
      "        [ 0.4048],\n",
      "        [ 0.3351],\n",
      "        [ 0.3673],\n",
      "        [ 0.3495],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [ 0.4598],\n",
      "        [-0.2958]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0153],\n",
      "        [    0.0373],\n",
      "        [    0.0148],\n",
      "        [    0.0787],\n",
      "        [    0.0026],\n",
      "        [    0.0078],\n",
      "        [    0.0025],\n",
      "        [    0.0087],\n",
      "        [    0.0133],\n",
      "        [    0.0280],\n",
      "        [    0.0083],\n",
      "        [    0.1015],\n",
      "        [    0.0104],\n",
      "        [    0.0202],\n",
      "        [    0.0063],\n",
      "        [    0.0879],\n",
      "        [    0.0166],\n",
      "        [    0.0064],\n",
      "        [    0.0013],\n",
      "        [    0.0142],\n",
      "        [    0.0383],\n",
      "        [    0.0647],\n",
      "        [    0.0279],\n",
      "        [    0.0181],\n",
      "        [    0.0191],\n",
      "        [    0.0096],\n",
      "        [    0.0239],\n",
      "        [    0.0111],\n",
      "        [    0.0008],\n",
      "        [    0.0387],\n",
      "        [    0.0210],\n",
      "        [    0.0698],\n",
      "        [    0.0423],\n",
      "        [    0.0417],\n",
      "        [    0.0197],\n",
      "        [    0.0287],\n",
      "        [    0.0141],\n",
      "        [    0.0014],\n",
      "        [    0.0295],\n",
      "        [    0.0232],\n",
      "        [    0.0051],\n",
      "        [    0.0270],\n",
      "        [    0.0028],\n",
      "        [    0.0378],\n",
      "        [    0.0332],\n",
      "        [    0.0194],\n",
      "        [    0.0298],\n",
      "        [    0.0003],\n",
      "        [    0.0170],\n",
      "        [    0.0490],\n",
      "        [    0.0161],\n",
      "        [    0.0873],\n",
      "        [    0.0347],\n",
      "        [    0.0683],\n",
      "        [    0.0369],\n",
      "        [    0.0438],\n",
      "        [    0.0526],\n",
      "        [    0.0082],\n",
      "        [    0.1346],\n",
      "        [    0.0162],\n",
      "        [    0.0082],\n",
      "        [    0.0402],\n",
      "        [    0.0051],\n",
      "        [    0.0181],\n",
      "        [    0.0501],\n",
      "        [    0.0041],\n",
      "        [    0.0400],\n",
      "        [    0.0247],\n",
      "        [    0.0510],\n",
      "        [    0.0255],\n",
      "        [    0.0250],\n",
      "        [    0.0389],\n",
      "        [    0.0192],\n",
      "        [    0.0201],\n",
      "        [    0.0237],\n",
      "        [    0.0830],\n",
      "        [    0.0234],\n",
      "        [    0.0045],\n",
      "        [    0.0559],\n",
      "        [    0.0929],\n",
      "        [    0.0678],\n",
      "        [    0.0562],\n",
      "        [    0.0887],\n",
      "        [    0.0765],\n",
      "        [    0.0973],\n",
      "        [    0.0583],\n",
      "        [    0.0670],\n",
      "        [    0.0483],\n",
      "        [    0.0676],\n",
      "        [    0.0306],\n",
      "        [    0.0405],\n",
      "        [    0.0664],\n",
      "        [    0.1048],\n",
      "        [    0.0628],\n",
      "        [    0.0267],\n",
      "        [    0.0220],\n",
      "        [    0.0463],\n",
      "        [    0.1354],\n",
      "        [    0.1164],\n",
      "        [    0.0805],\n",
      "        [    0.0162],\n",
      "        [    0.0869],\n",
      "        [    0.1326],\n",
      "        [    0.1019],\n",
      "        [    0.0076],\n",
      "        [    0.0929],\n",
      "        [    0.0993],\n",
      "        [    0.0113],\n",
      "        [    0.1143],\n",
      "        [    0.0961],\n",
      "        [    0.1161],\n",
      "        [    0.1235],\n",
      "        [    0.1030],\n",
      "        [    0.1214],\n",
      "        [    0.1220],\n",
      "        [    0.1691],\n",
      "        [    0.1610],\n",
      "        [    0.0762],\n",
      "        [    0.0782],\n",
      "        [    0.1321],\n",
      "        [    0.1312],\n",
      "        [    0.1463],\n",
      "        [    0.3095],\n",
      "        [    0.3235],\n",
      "        [    0.3265],\n",
      "        [    0.3283],\n",
      "        [    0.3294],\n",
      "        [    0.3388],\n",
      "        [    0.3406],\n",
      "        [    0.3418],\n",
      "        [    0.3492],\n",
      "        [    0.3510],\n",
      "        [    0.3531],\n",
      "        [    0.3541],\n",
      "        [    0.3573],\n",
      "        [    0.3764],\n",
      "        [    0.3774],\n",
      "        [    0.3843],\n",
      "        [    0.3887],\n",
      "        [    0.3907],\n",
      "        [    0.3916],\n",
      "        [    0.3960],\n",
      "        [    0.4035],\n",
      "        [    0.4051],\n",
      "        [    0.4059],\n",
      "        [    0.4059],\n",
      "        [    0.4086],\n",
      "        [    0.4119],\n",
      "        [    0.4135],\n",
      "        [    0.4145]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0189],\n",
      "        [    0.0454],\n",
      "        [    0.0129],\n",
      "        [    0.0787],\n",
      "        [    0.0004],\n",
      "        [    0.0065],\n",
      "        [    0.0066],\n",
      "        [    0.0064],\n",
      "        [    0.0172],\n",
      "        [    0.0330],\n",
      "        [    0.0051],\n",
      "        [    0.1093],\n",
      "        [    0.0154],\n",
      "        [    0.0233],\n",
      "        [    0.0100],\n",
      "        [    0.0962],\n",
      "        [    0.0183],\n",
      "        [    0.0097],\n",
      "        [    0.0033],\n",
      "        [    0.0128],\n",
      "        [    0.0419],\n",
      "        [    0.0723],\n",
      "        [    0.0283],\n",
      "        [    0.0188],\n",
      "        [    0.0193],\n",
      "        [    0.0088],\n",
      "        [    0.0215],\n",
      "        [    0.0182],\n",
      "        [    0.0009],\n",
      "        [    0.0374],\n",
      "        [    0.0166],\n",
      "        [    0.0695],\n",
      "        [    0.0493],\n",
      "        [    0.0463],\n",
      "        [    0.0223],\n",
      "        [    0.0299],\n",
      "        [    0.0125],\n",
      "        [    0.0068],\n",
      "        [    0.0289],\n",
      "        [    0.0195],\n",
      "        [    0.0052],\n",
      "        [    0.0293],\n",
      "        [    0.0036],\n",
      "        [    0.0344],\n",
      "        [    0.0361],\n",
      "        [    0.0149],\n",
      "        [    0.0330],\n",
      "        [    0.0008],\n",
      "        [    0.0174],\n",
      "        [    0.0482],\n",
      "        [    0.0171],\n",
      "        [    0.0953],\n",
      "        [    0.0331],\n",
      "        [    0.0678],\n",
      "        [    0.0393],\n",
      "        [    0.0458],\n",
      "        [    0.0504],\n",
      "        [    0.0134],\n",
      "        [    0.1425],\n",
      "        [    0.0247],\n",
      "        [    0.0065],\n",
      "        [    0.0392],\n",
      "        [    0.0135],\n",
      "        [    0.0189],\n",
      "        [    0.0463],\n",
      "        [    0.0035],\n",
      "        [    0.0495],\n",
      "        [    0.0232],\n",
      "        [    0.0424],\n",
      "        [    0.0243],\n",
      "        [    0.0238],\n",
      "        [    0.0373],\n",
      "        [    0.0177],\n",
      "        [    0.0208],\n",
      "        [    0.0234],\n",
      "        [    0.0823],\n",
      "        [    0.0185],\n",
      "        [    0.0035],\n",
      "        [    0.0542],\n",
      "        [    0.0979],\n",
      "        [    0.0636],\n",
      "        [    0.0548],\n",
      "        [    0.0880],\n",
      "        [    0.0750],\n",
      "        [    0.0954],\n",
      "        [    0.0538],\n",
      "        [    0.0658],\n",
      "        [    0.0488],\n",
      "        [    0.0704],\n",
      "        [    0.0302],\n",
      "        [    0.0401],\n",
      "        [    0.0661],\n",
      "        [    0.1032],\n",
      "        [    0.0633],\n",
      "        [    0.0264],\n",
      "        [    0.0220],\n",
      "        [    0.0461],\n",
      "        [    0.1352],\n",
      "        [    0.1184],\n",
      "        [    0.0813],\n",
      "        [    0.0229],\n",
      "        [    0.0877],\n",
      "        [    0.1318],\n",
      "        [    0.0718],\n",
      "        [    0.0001],\n",
      "        [    0.0934],\n",
      "        [    0.0997],\n",
      "        [    0.0193],\n",
      "        [    0.1148],\n",
      "        [    0.0962],\n",
      "        [    0.1423],\n",
      "        [    0.1245],\n",
      "        [    0.1052],\n",
      "        [    0.1221],\n",
      "        [    0.1217],\n",
      "        [    0.1686],\n",
      "        [    0.1617],\n",
      "        [    0.0682],\n",
      "        [    0.0691],\n",
      "        [    0.1236],\n",
      "        [    0.1218],\n",
      "        [    0.1371],\n",
      "        [    0.3086],\n",
      "        [    0.3226],\n",
      "        [    0.3256],\n",
      "        [    0.3275],\n",
      "        [    0.3285],\n",
      "        [    0.3380],\n",
      "        [    0.3398],\n",
      "        [    0.3409],\n",
      "        [    0.3483],\n",
      "        [    0.3501],\n",
      "        [    0.3522],\n",
      "        [    0.3532],\n",
      "        [    0.3565],\n",
      "        [    0.3755],\n",
      "        [    0.3765],\n",
      "        [    0.3834],\n",
      "        [    0.3878],\n",
      "        [    0.3898],\n",
      "        [    0.3907],\n",
      "        [    0.3952],\n",
      "        [    0.4026],\n",
      "        [    0.4042],\n",
      "        [    0.4050],\n",
      "        [    0.4050],\n",
      "        [    0.4078],\n",
      "        [    0.4111],\n",
      "        [    0.4126],\n",
      "        [    0.3735]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 86.67918968200684\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 151\n",
      "剩餘X 資料 torch.Size([9, 18])\n",
      "剩餘Y 資料 torch.Size([9, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.17168782651424408, 8)\n",
      "The second_loss value of k: (0.17220757901668549, 7)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.0446])\n",
      "目前模型的Data狀態 torch.Size([151, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6466],\n",
      "        [ 0.1361],\n",
      "        [ 0.5237],\n",
      "        [ 0.7377],\n",
      "        [ 0.2182],\n",
      "        [ 0.2271],\n",
      "        [ 0.6352],\n",
      "        [ 0.1857],\n",
      "        [ 0.6478],\n",
      "        [ 0.6437],\n",
      "        [ 0.6388],\n",
      "        [ 0.2849],\n",
      "        [ 0.6369],\n",
      "        [ 0.6140],\n",
      "        [ 0.6170],\n",
      "        [ 0.2144],\n",
      "        [ 0.5589],\n",
      "        [ 0.6262],\n",
      "        [ 0.6366],\n",
      "        [ 0.4276],\n",
      "        [ 0.6492],\n",
      "        [ 0.1629],\n",
      "        [ 0.4671],\n",
      "        [ 0.5701],\n",
      "        [ 0.6609],\n",
      "        [ 0.2710],\n",
      "        [ 0.1587],\n",
      "        [ 0.1378],\n",
      "        [ 0.6355],\n",
      "        [ 0.4335],\n",
      "        [ 0.5685],\n",
      "        [ 0.4418],\n",
      "        [ 0.1424],\n",
      "        [ 0.6270],\n",
      "        [ 0.5834],\n",
      "        [ 0.2216],\n",
      "        [ 0.4614],\n",
      "        [ 0.1154],\n",
      "        [ 0.5958],\n",
      "        [ 0.6572],\n",
      "        [ 0.6505],\n",
      "        [ 0.5851],\n",
      "        [ 0.3507],\n",
      "        [ 0.5171],\n",
      "        [ 0.5950],\n",
      "        [ 0.6281],\n",
      "        [ 0.2221],\n",
      "        [ 0.3589],\n",
      "        [ 0.6186],\n",
      "        [ 0.3735],\n",
      "        [ 0.2012],\n",
      "        [ 0.1762],\n",
      "        [ 0.2215],\n",
      "        [ 0.2231],\n",
      "        [ 0.5872],\n",
      "        [ 0.5722],\n",
      "        [ 0.4836],\n",
      "        [ 0.6321],\n",
      "        [ 0.2869],\n",
      "        [ 0.1629],\n",
      "        [ 0.3514],\n",
      "        [ 0.3102],\n",
      "        [ 0.1461],\n",
      "        [ 0.2864],\n",
      "        [ 0.6255],\n",
      "        [ 0.1367],\n",
      "        [ 0.1676],\n",
      "        [ 0.2839],\n",
      "        [ 0.1552],\n",
      "        [ 0.3343],\n",
      "        [ 0.3349],\n",
      "        [ 0.4346],\n",
      "        [ 0.3458],\n",
      "        [ 0.3437],\n",
      "        [ 0.7029],\n",
      "        [ 0.3965],\n",
      "        [ 0.5985],\n",
      "        [ 0.4604],\n",
      "        [ 0.4330],\n",
      "        [ 0.6617],\n",
      "        [ 0.5469],\n",
      "        [ 0.4097],\n",
      "        [ 0.3994],\n",
      "        [ 0.4480],\n",
      "        [ 0.4312],\n",
      "        [ 0.5700],\n",
      "        [ 0.2420],\n",
      "        [ 0.2296],\n",
      "        [ 0.2295],\n",
      "        [ 0.6419],\n",
      "        [ 0.3510],\n",
      "        [ 0.2863],\n",
      "        [ 0.4298],\n",
      "        [ 0.2509],\n",
      "        [ 0.6643],\n",
      "        [ 0.6746],\n",
      "        [ 0.6432],\n",
      "        [ 0.1674],\n",
      "        [ 0.4448],\n",
      "        [ 0.3462],\n",
      "        [ 0.4226],\n",
      "        [ 0.3112],\n",
      "        [ 0.1874],\n",
      "        [ 0.0903],\n",
      "        [ 0.3099],\n",
      "        [ 0.3587],\n",
      "        [ 0.2625],\n",
      "        [ 0.4334],\n",
      "        [ 0.3057],\n",
      "        [ 0.3548],\n",
      "        [ 0.1423],\n",
      "        [ 0.4112],\n",
      "        [ 0.1323],\n",
      "        [ 0.3144],\n",
      "        [ 0.5454],\n",
      "        [ 0.5891],\n",
      "        [ 0.3890],\n",
      "        [ 0.3409],\n",
      "        [ 0.4140],\n",
      "        [ 0.3436],\n",
      "        [ 0.3767],\n",
      "        [ 0.3587],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [ 0.4590],\n",
      "        [-0.2547],\n",
      "        [ 0.4590]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0189],\n",
      "        [    0.0454],\n",
      "        [    0.0129],\n",
      "        [    0.0787],\n",
      "        [    0.0004],\n",
      "        [    0.0065],\n",
      "        [    0.0066],\n",
      "        [    0.0064],\n",
      "        [    0.0172],\n",
      "        [    0.0330],\n",
      "        [    0.0051],\n",
      "        [    0.1093],\n",
      "        [    0.0154],\n",
      "        [    0.0233],\n",
      "        [    0.0100],\n",
      "        [    0.0962],\n",
      "        [    0.0183],\n",
      "        [    0.0097],\n",
      "        [    0.0033],\n",
      "        [    0.0128],\n",
      "        [    0.0419],\n",
      "        [    0.0723],\n",
      "        [    0.0283],\n",
      "        [    0.0188],\n",
      "        [    0.0193],\n",
      "        [    0.0088],\n",
      "        [    0.0215],\n",
      "        [    0.0182],\n",
      "        [    0.0009],\n",
      "        [    0.0374],\n",
      "        [    0.0166],\n",
      "        [    0.0695],\n",
      "        [    0.0493],\n",
      "        [    0.0463],\n",
      "        [    0.0223],\n",
      "        [    0.0299],\n",
      "        [    0.0125],\n",
      "        [    0.0068],\n",
      "        [    0.0289],\n",
      "        [    0.0195],\n",
      "        [    0.0052],\n",
      "        [    0.0293],\n",
      "        [    0.0036],\n",
      "        [    0.0344],\n",
      "        [    0.0361],\n",
      "        [    0.0149],\n",
      "        [    0.0330],\n",
      "        [    0.0008],\n",
      "        [    0.0174],\n",
      "        [    0.0482],\n",
      "        [    0.0171],\n",
      "        [    0.0953],\n",
      "        [    0.0331],\n",
      "        [    0.0678],\n",
      "        [    0.0393],\n",
      "        [    0.0458],\n",
      "        [    0.0504],\n",
      "        [    0.0134],\n",
      "        [    0.1425],\n",
      "        [    0.0247],\n",
      "        [    0.0065],\n",
      "        [    0.0392],\n",
      "        [    0.0135],\n",
      "        [    0.0189],\n",
      "        [    0.0463],\n",
      "        [    0.0035],\n",
      "        [    0.0495],\n",
      "        [    0.0232],\n",
      "        [    0.0424],\n",
      "        [    0.0243],\n",
      "        [    0.0238],\n",
      "        [    0.0373],\n",
      "        [    0.0177],\n",
      "        [    0.0208],\n",
      "        [    0.0234],\n",
      "        [    0.0823],\n",
      "        [    0.0185],\n",
      "        [    0.0035],\n",
      "        [    0.0542],\n",
      "        [    0.0979],\n",
      "        [    0.0636],\n",
      "        [    0.0548],\n",
      "        [    0.0880],\n",
      "        [    0.0750],\n",
      "        [    0.0954],\n",
      "        [    0.0538],\n",
      "        [    0.0658],\n",
      "        [    0.0488],\n",
      "        [    0.0704],\n",
      "        [    0.0302],\n",
      "        [    0.0401],\n",
      "        [    0.0661],\n",
      "        [    0.1032],\n",
      "        [    0.0633],\n",
      "        [    0.0264],\n",
      "        [    0.0220],\n",
      "        [    0.0461],\n",
      "        [    0.1352],\n",
      "        [    0.1184],\n",
      "        [    0.0813],\n",
      "        [    0.0229],\n",
      "        [    0.0877],\n",
      "        [    0.1318],\n",
      "        [    0.0718],\n",
      "        [    0.0001],\n",
      "        [    0.0934],\n",
      "        [    0.0997],\n",
      "        [    0.0193],\n",
      "        [    0.1148],\n",
      "        [    0.0962],\n",
      "        [    0.1423],\n",
      "        [    0.1245],\n",
      "        [    0.1052],\n",
      "        [    0.1221],\n",
      "        [    0.1217],\n",
      "        [    0.1686],\n",
      "        [    0.1617],\n",
      "        [    0.0682],\n",
      "        [    0.0691],\n",
      "        [    0.1236],\n",
      "        [    0.1218],\n",
      "        [    0.1371],\n",
      "        [    0.3086],\n",
      "        [    0.3226],\n",
      "        [    0.3256],\n",
      "        [    0.3275],\n",
      "        [    0.3285],\n",
      "        [    0.3380],\n",
      "        [    0.3398],\n",
      "        [    0.3409],\n",
      "        [    0.3483],\n",
      "        [    0.3501],\n",
      "        [    0.3522],\n",
      "        [    0.3532],\n",
      "        [    0.3565],\n",
      "        [    0.3755],\n",
      "        [    0.3765],\n",
      "        [    0.3834],\n",
      "        [    0.3878],\n",
      "        [    0.3898],\n",
      "        [    0.3907],\n",
      "        [    0.3952],\n",
      "        [    0.4026],\n",
      "        [    0.4042],\n",
      "        [    0.4050],\n",
      "        [    0.4050],\n",
      "        [    0.4078],\n",
      "        [    0.4111],\n",
      "        [    0.4126],\n",
      "        [    0.3735],\n",
      "        [    0.4144]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0187],\n",
      "        [    0.0443],\n",
      "        [    0.0140],\n",
      "        [    0.0763],\n",
      "        [    0.0028],\n",
      "        [    0.0103],\n",
      "        [    0.0064],\n",
      "        [    0.0094],\n",
      "        [    0.0167],\n",
      "        [    0.0331],\n",
      "        [    0.0055],\n",
      "        [    0.1069],\n",
      "        [    0.0162],\n",
      "        [    0.0227],\n",
      "        [    0.0097],\n",
      "        [    0.0943],\n",
      "        [    0.0173],\n",
      "        [    0.0094],\n",
      "        [    0.0035],\n",
      "        [    0.0142],\n",
      "        [    0.0417],\n",
      "        [    0.0714],\n",
      "        [    0.0314],\n",
      "        [    0.0220],\n",
      "        [    0.0168],\n",
      "        [    0.0059],\n",
      "        [    0.0240],\n",
      "        [    0.0162],\n",
      "        [    0.0032],\n",
      "        [    0.0397],\n",
      "        [    0.0184],\n",
      "        [    0.0727],\n",
      "        [    0.0474],\n",
      "        [    0.0466],\n",
      "        [    0.0216],\n",
      "        [    0.0263],\n",
      "        [    0.0143],\n",
      "        [    0.0054],\n",
      "        [    0.0257],\n",
      "        [    0.0197],\n",
      "        [    0.0029],\n",
      "        [    0.0285],\n",
      "        [    0.0058],\n",
      "        [    0.0368],\n",
      "        [    0.0357],\n",
      "        [    0.0144],\n",
      "        [    0.0300],\n",
      "        [    0.0013],\n",
      "        [    0.0202],\n",
      "        [    0.0461],\n",
      "        [    0.0134],\n",
      "        [    0.0945],\n",
      "        [    0.0372],\n",
      "        [    0.0696],\n",
      "        [    0.0385],\n",
      "        [    0.0450],\n",
      "        [    0.0516],\n",
      "        [    0.0133],\n",
      "        [    0.1401],\n",
      "        [    0.0232],\n",
      "        [    0.0084],\n",
      "        [    0.0366],\n",
      "        [    0.0122],\n",
      "        [    0.0173],\n",
      "        [    0.0463],\n",
      "        [    0.0019],\n",
      "        [    0.0567],\n",
      "        [    0.0252],\n",
      "        [    0.0360],\n",
      "        [    0.0260],\n",
      "        [    0.0258],\n",
      "        [    0.0382],\n",
      "        [    0.0188],\n",
      "        [    0.0230],\n",
      "        [    0.0206],\n",
      "        [    0.0805],\n",
      "        [    0.0192],\n",
      "        [    0.0003],\n",
      "        [    0.0562],\n",
      "        [    0.0985],\n",
      "        [    0.0662],\n",
      "        [    0.0561],\n",
      "        [    0.0863],\n",
      "        [    0.0767],\n",
      "        [    0.0979],\n",
      "        [    0.0549],\n",
      "        [    0.0682],\n",
      "        [    0.0453],\n",
      "        [    0.0672],\n",
      "        [    0.0325],\n",
      "        [    0.0416],\n",
      "        [    0.0631],\n",
      "        [    0.1048],\n",
      "        [    0.0601],\n",
      "        [    0.0289],\n",
      "        [    0.0247],\n",
      "        [    0.0486],\n",
      "        [    0.1368],\n",
      "        [    0.1174],\n",
      "        [    0.0798],\n",
      "        [    0.0236],\n",
      "        [    0.0903],\n",
      "        [    0.1341],\n",
      "        [    0.0465],\n",
      "        [    0.0020],\n",
      "        [    0.0957],\n",
      "        [    0.0965],\n",
      "        [    0.0193],\n",
      "        [    0.1121],\n",
      "        [    0.0982],\n",
      "        [    0.1390],\n",
      "        [    0.1230],\n",
      "        [    0.1017],\n",
      "        [    0.1247],\n",
      "        [    0.1183],\n",
      "        [    0.1653],\n",
      "        [    0.1599],\n",
      "        [    0.0700],\n",
      "        [    0.0685],\n",
      "        [    0.1251],\n",
      "        [    0.1220],\n",
      "        [    0.1377],\n",
      "        [    0.3077],\n",
      "        [    0.3217],\n",
      "        [    0.3247],\n",
      "        [    0.3266],\n",
      "        [    0.3276],\n",
      "        [    0.3371],\n",
      "        [    0.3389],\n",
      "        [    0.3401],\n",
      "        [    0.3474],\n",
      "        [    0.3492],\n",
      "        [    0.3513],\n",
      "        [    0.3524],\n",
      "        [    0.3556],\n",
      "        [    0.3746],\n",
      "        [    0.3756],\n",
      "        [    0.3826],\n",
      "        [    0.3869],\n",
      "        [    0.3889],\n",
      "        [    0.3898],\n",
      "        [    0.3943],\n",
      "        [    0.4017],\n",
      "        [    0.4034],\n",
      "        [    0.4041],\n",
      "        [    0.4041],\n",
      "        [    0.4069],\n",
      "        [    0.4102],\n",
      "        [    0.4117],\n",
      "        [    0.3410],\n",
      "        [    0.4135]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 86.91349148750305\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 152\n",
      "剩餘X 資料 torch.Size([8, 18])\n",
      "剩餘Y 資料 torch.Size([8, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.17147280275821686, 7)\n",
      "The second_loss value of k: (0.1727728396654129, 5)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引7，y= tensor([0.0440])\n",
      "目前模型的Data狀態 torch.Size([152, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6465],\n",
      "        [ 0.1351],\n",
      "        [ 0.5226],\n",
      "        [ 0.7353],\n",
      "        [ 0.2150],\n",
      "        [ 0.2233],\n",
      "        [ 0.6350],\n",
      "        [ 0.1827],\n",
      "        [ 0.6473],\n",
      "        [ 0.6439],\n",
      "        [ 0.6384],\n",
      "        [ 0.2825],\n",
      "        [ 0.6378],\n",
      "        [ 0.6134],\n",
      "        [ 0.6166],\n",
      "        [ 0.2126],\n",
      "        [ 0.5578],\n",
      "        [ 0.6259],\n",
      "        [ 0.6368],\n",
      "        [ 0.4262],\n",
      "        [ 0.6489],\n",
      "        [ 0.1620],\n",
      "        [ 0.4639],\n",
      "        [ 0.5668],\n",
      "        [ 0.6583],\n",
      "        [ 0.2681],\n",
      "        [ 0.1562],\n",
      "        [ 0.1358],\n",
      "        [ 0.6331],\n",
      "        [ 0.4313],\n",
      "        [ 0.5667],\n",
      "        [ 0.4386],\n",
      "        [ 0.1405],\n",
      "        [ 0.6273],\n",
      "        [ 0.5826],\n",
      "        [ 0.2180],\n",
      "        [ 0.4597],\n",
      "        [ 0.1141],\n",
      "        [ 0.5926],\n",
      "        [ 0.6570],\n",
      "        [ 0.6482],\n",
      "        [ 0.5843],\n",
      "        [ 0.3485],\n",
      "        [ 0.5146],\n",
      "        [ 0.5946],\n",
      "        [ 0.6286],\n",
      "        [ 0.2191],\n",
      "        [ 0.3567],\n",
      "        [ 0.6158],\n",
      "        [ 0.3714],\n",
      "        [ 0.1974],\n",
      "        [ 0.1754],\n",
      "        [ 0.2174],\n",
      "        [ 0.2214],\n",
      "        [ 0.5865],\n",
      "        [ 0.5714],\n",
      "        [ 0.4824],\n",
      "        [ 0.6320],\n",
      "        [ 0.2844],\n",
      "        [ 0.1614],\n",
      "        [ 0.3495],\n",
      "        [ 0.3076],\n",
      "        [ 0.1448],\n",
      "        [ 0.2848],\n",
      "        [ 0.6254],\n",
      "        [ 0.1350],\n",
      "        [ 0.1748],\n",
      "        [ 0.2818],\n",
      "        [ 0.1615],\n",
      "        [ 0.3325],\n",
      "        [ 0.3329],\n",
      "        [ 0.4337],\n",
      "        [ 0.3447],\n",
      "        [ 0.3415],\n",
      "        [ 0.7001],\n",
      "        [ 0.3947],\n",
      "        [ 0.5979],\n",
      "        [ 0.4572],\n",
      "        [ 0.4310],\n",
      "        [ 0.6623],\n",
      "        [ 0.5443],\n",
      "        [ 0.4084],\n",
      "        [ 0.3977],\n",
      "        [ 0.4463],\n",
      "        [ 0.4287],\n",
      "        [ 0.5689],\n",
      "        [ 0.2396],\n",
      "        [ 0.2261],\n",
      "        [ 0.2263],\n",
      "        [ 0.6395],\n",
      "        [ 0.3495],\n",
      "        [ 0.2832],\n",
      "        [ 0.4282],\n",
      "        [ 0.2477],\n",
      "        [ 0.6619],\n",
      "        [ 0.6719],\n",
      "        [ 0.6407],\n",
      "        [ 0.1658],\n",
      "        [ 0.4438],\n",
      "        [ 0.3448],\n",
      "        [ 0.4232],\n",
      "        [ 0.3086],\n",
      "        [ 0.1852],\n",
      "        [ 0.1155],\n",
      "        [ 0.3077],\n",
      "        [ 0.3565],\n",
      "        [ 0.2593],\n",
      "        [ 0.4334],\n",
      "        [ 0.3030],\n",
      "        [ 0.3528],\n",
      "        [ 0.1390],\n",
      "        [ 0.4098],\n",
      "        [ 0.1287],\n",
      "        [ 0.3118],\n",
      "        [ 0.5420],\n",
      "        [ 0.5858],\n",
      "        [ 0.3873],\n",
      "        [ 0.3391],\n",
      "        [ 0.4146],\n",
      "        [ 0.3421],\n",
      "        [ 0.3765],\n",
      "        [ 0.3580],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581],\n",
      "        [-0.2223],\n",
      "        [ 0.4581],\n",
      "        [ 0.4581]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0187],\n",
      "        [    0.0443],\n",
      "        [    0.0140],\n",
      "        [    0.0763],\n",
      "        [    0.0028],\n",
      "        [    0.0103],\n",
      "        [    0.0064],\n",
      "        [    0.0094],\n",
      "        [    0.0167],\n",
      "        [    0.0331],\n",
      "        [    0.0055],\n",
      "        [    0.1069],\n",
      "        [    0.0162],\n",
      "        [    0.0227],\n",
      "        [    0.0097],\n",
      "        [    0.0943],\n",
      "        [    0.0173],\n",
      "        [    0.0094],\n",
      "        [    0.0035],\n",
      "        [    0.0142],\n",
      "        [    0.0417],\n",
      "        [    0.0714],\n",
      "        [    0.0314],\n",
      "        [    0.0220],\n",
      "        [    0.0168],\n",
      "        [    0.0059],\n",
      "        [    0.0240],\n",
      "        [    0.0162],\n",
      "        [    0.0032],\n",
      "        [    0.0397],\n",
      "        [    0.0184],\n",
      "        [    0.0727],\n",
      "        [    0.0474],\n",
      "        [    0.0466],\n",
      "        [    0.0216],\n",
      "        [    0.0263],\n",
      "        [    0.0143],\n",
      "        [    0.0054],\n",
      "        [    0.0257],\n",
      "        [    0.0197],\n",
      "        [    0.0029],\n",
      "        [    0.0285],\n",
      "        [    0.0058],\n",
      "        [    0.0368],\n",
      "        [    0.0357],\n",
      "        [    0.0144],\n",
      "        [    0.0300],\n",
      "        [    0.0013],\n",
      "        [    0.0202],\n",
      "        [    0.0461],\n",
      "        [    0.0134],\n",
      "        [    0.0945],\n",
      "        [    0.0372],\n",
      "        [    0.0696],\n",
      "        [    0.0385],\n",
      "        [    0.0450],\n",
      "        [    0.0516],\n",
      "        [    0.0133],\n",
      "        [    0.1401],\n",
      "        [    0.0232],\n",
      "        [    0.0084],\n",
      "        [    0.0366],\n",
      "        [    0.0122],\n",
      "        [    0.0173],\n",
      "        [    0.0463],\n",
      "        [    0.0019],\n",
      "        [    0.0567],\n",
      "        [    0.0252],\n",
      "        [    0.0360],\n",
      "        [    0.0260],\n",
      "        [    0.0258],\n",
      "        [    0.0382],\n",
      "        [    0.0188],\n",
      "        [    0.0230],\n",
      "        [    0.0206],\n",
      "        [    0.0805],\n",
      "        [    0.0192],\n",
      "        [    0.0003],\n",
      "        [    0.0562],\n",
      "        [    0.0985],\n",
      "        [    0.0662],\n",
      "        [    0.0561],\n",
      "        [    0.0863],\n",
      "        [    0.0767],\n",
      "        [    0.0979],\n",
      "        [    0.0549],\n",
      "        [    0.0682],\n",
      "        [    0.0453],\n",
      "        [    0.0672],\n",
      "        [    0.0325],\n",
      "        [    0.0416],\n",
      "        [    0.0631],\n",
      "        [    0.1048],\n",
      "        [    0.0601],\n",
      "        [    0.0289],\n",
      "        [    0.0247],\n",
      "        [    0.0486],\n",
      "        [    0.1368],\n",
      "        [    0.1174],\n",
      "        [    0.0798],\n",
      "        [    0.0236],\n",
      "        [    0.0903],\n",
      "        [    0.1341],\n",
      "        [    0.0465],\n",
      "        [    0.0020],\n",
      "        [    0.0957],\n",
      "        [    0.0965],\n",
      "        [    0.0193],\n",
      "        [    0.1121],\n",
      "        [    0.0982],\n",
      "        [    0.1390],\n",
      "        [    0.1230],\n",
      "        [    0.1017],\n",
      "        [    0.1247],\n",
      "        [    0.1183],\n",
      "        [    0.1653],\n",
      "        [    0.1599],\n",
      "        [    0.0700],\n",
      "        [    0.0685],\n",
      "        [    0.1251],\n",
      "        [    0.1220],\n",
      "        [    0.1377],\n",
      "        [    0.3077],\n",
      "        [    0.3217],\n",
      "        [    0.3247],\n",
      "        [    0.3266],\n",
      "        [    0.3276],\n",
      "        [    0.3371],\n",
      "        [    0.3389],\n",
      "        [    0.3401],\n",
      "        [    0.3474],\n",
      "        [    0.3492],\n",
      "        [    0.3513],\n",
      "        [    0.3524],\n",
      "        [    0.3556],\n",
      "        [    0.3746],\n",
      "        [    0.3756],\n",
      "        [    0.3826],\n",
      "        [    0.3869],\n",
      "        [    0.3889],\n",
      "        [    0.3898],\n",
      "        [    0.3943],\n",
      "        [    0.4017],\n",
      "        [    0.4034],\n",
      "        [    0.4041],\n",
      "        [    0.4041],\n",
      "        [    0.4069],\n",
      "        [    0.4102],\n",
      "        [    0.4117],\n",
      "        [    0.3410],\n",
      "        [    0.4135],\n",
      "        [    0.4141]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0145],\n",
      "        [    0.0401],\n",
      "        [    0.0184],\n",
      "        [    0.0721],\n",
      "        [    0.0074],\n",
      "        [    0.0142],\n",
      "        [    0.0020],\n",
      "        [    0.0136],\n",
      "        [    0.0122],\n",
      "        [    0.0297],\n",
      "        [    0.0099],\n",
      "        [    0.1016],\n",
      "        [    0.0130],\n",
      "        [    0.0185],\n",
      "        [    0.0051],\n",
      "        [    0.0896],\n",
      "        [    0.0130],\n",
      "        [    0.0048],\n",
      "        [    0.0002],\n",
      "        [    0.0180],\n",
      "        [    0.0375],\n",
      "        [    0.0674],\n",
      "        [    0.0365],\n",
      "        [    0.0272],\n",
      "        [    0.0123],\n",
      "        [    0.0020],\n",
      "        [    0.0278],\n",
      "        [    0.0112],\n",
      "        [    0.0076],\n",
      "        [    0.0445],\n",
      "        [    0.0227],\n",
      "        [    0.0779],\n",
      "        [    0.0424],\n",
      "        [    0.0428],\n",
      "        [    0.0172],\n",
      "        [    0.0225],\n",
      "        [    0.0184],\n",
      "        [    0.0013],\n",
      "        [    0.0205],\n",
      "        [    0.0239],\n",
      "        [    0.0015],\n",
      "        [    0.0241],\n",
      "        [    0.0089],\n",
      "        [    0.0411],\n",
      "        [    0.0316],\n",
      "        [    0.0178],\n",
      "        [    0.0257],\n",
      "        [    0.0046],\n",
      "        [    0.0249],\n",
      "        [    0.0427],\n",
      "        [    0.0092],\n",
      "        [    0.0905],\n",
      "        [    0.0414],\n",
      "        [    0.0732],\n",
      "        [    0.0343],\n",
      "        [    0.0407],\n",
      "        [    0.0559],\n",
      "        [    0.0099],\n",
      "        [    0.1349],\n",
      "        [    0.0188],\n",
      "        [    0.0117],\n",
      "        [    0.0329],\n",
      "        [    0.0079],\n",
      "        [    0.0133],\n",
      "        [    0.0505],\n",
      "        [    0.0027],\n",
      "        [    0.0618],\n",
      "        [    0.0285],\n",
      "        [    0.0313],\n",
      "        [    0.0291],\n",
      "        [    0.0291],\n",
      "        [    0.0416],\n",
      "        [    0.0212],\n",
      "        [    0.0260],\n",
      "        [    0.0162],\n",
      "        [    0.0774],\n",
      "        [    0.0229],\n",
      "        [    0.0049],\n",
      "        [    0.0607],\n",
      "        [    0.0952],\n",
      "        [    0.0709],\n",
      "        [    0.0598],\n",
      "        [    0.0832],\n",
      "        [    0.0813],\n",
      "        [    0.1022],\n",
      "        [    0.0587],\n",
      "        [    0.0718],\n",
      "        [    0.0412],\n",
      "        [    0.0631],\n",
      "        [    0.0368],\n",
      "        [    0.0442],\n",
      "        [    0.0592],\n",
      "        [    0.1092],\n",
      "        [    0.0560],\n",
      "        [    0.0329],\n",
      "        [    0.0290],\n",
      "        [    0.0528],\n",
      "        [    0.1401],\n",
      "        [    0.1139],\n",
      "        [    0.0759],\n",
      "        [    0.0191],\n",
      "        [    0.0936],\n",
      "        [    0.1375],\n",
      "        [    0.0261],\n",
      "        [    0.0071],\n",
      "        [    0.0988],\n",
      "        [    0.0924],\n",
      "        [    0.0149],\n",
      "        [    0.1085],\n",
      "        [    0.1012],\n",
      "        [    0.1345],\n",
      "        [    0.1192],\n",
      "        [    0.0969],\n",
      "        [    0.1280],\n",
      "        [    0.1129],\n",
      "        [    0.1600],\n",
      "        [    0.1557],\n",
      "        [    0.0746],\n",
      "        [    0.0719],\n",
      "        [    0.1295],\n",
      "        [    0.1257],\n",
      "        [    0.1416],\n",
      "        [    0.3069],\n",
      "        [    0.3209],\n",
      "        [    0.3239],\n",
      "        [    0.3258],\n",
      "        [    0.3268],\n",
      "        [    0.3363],\n",
      "        [    0.3381],\n",
      "        [    0.3392],\n",
      "        [    0.3466],\n",
      "        [    0.3484],\n",
      "        [    0.3505],\n",
      "        [    0.3515],\n",
      "        [    0.3547],\n",
      "        [    0.3738],\n",
      "        [    0.3748],\n",
      "        [    0.3817],\n",
      "        [    0.3861],\n",
      "        [    0.3881],\n",
      "        [    0.3890],\n",
      "        [    0.3934],\n",
      "        [    0.4009],\n",
      "        [    0.4025],\n",
      "        [    0.4033],\n",
      "        [    0.4033],\n",
      "        [    0.4060],\n",
      "        [    0.4093],\n",
      "        [    0.4109],\n",
      "        [    0.3151],\n",
      "        [    0.4126],\n",
      "        [    0.4133]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 87.14959478378296\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 153\n",
      "剩餘X 資料 torch.Size([7, 18])\n",
      "剩餘Y 資料 torch.Size([7, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.17208534479141235, 5)\n",
      "The second_loss value of k: (0.1744331270456314, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.0424])\n",
      "目前模型的Data狀態 torch.Size([153, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.6423],\n",
      "        [ 0.1309],\n",
      "        [ 0.5183],\n",
      "        [ 0.7311],\n",
      "        [ 0.2104],\n",
      "        [ 0.2194],\n",
      "        [ 0.6306],\n",
      "        [ 0.1785],\n",
      "        [ 0.6428],\n",
      "        [ 0.6404],\n",
      "        [ 0.6341],\n",
      "        [ 0.2772],\n",
      "        [ 0.6346],\n",
      "        [ 0.6091],\n",
      "        [ 0.6121],\n",
      "        [ 0.2079],\n",
      "        [ 0.5535],\n",
      "        [ 0.6214],\n",
      "        [ 0.6331],\n",
      "        [ 0.4225],\n",
      "        [ 0.6448],\n",
      "        [ 0.1580],\n",
      "        [ 0.4588],\n",
      "        [ 0.5616],\n",
      "        [ 0.6538],\n",
      "        [ 0.2642],\n",
      "        [ 0.1524],\n",
      "        [ 0.1308],\n",
      "        [ 0.6287],\n",
      "        [ 0.4265],\n",
      "        [ 0.5625],\n",
      "        [ 0.4335],\n",
      "        [ 0.1355],\n",
      "        [ 0.6234],\n",
      "        [ 0.5782],\n",
      "        [ 0.2142],\n",
      "        [ 0.4556],\n",
      "        [ 0.1099],\n",
      "        [ 0.5874],\n",
      "        [ 0.6529],\n",
      "        [ 0.6439],\n",
      "        [ 0.5799],\n",
      "        [ 0.3454],\n",
      "        [ 0.5103],\n",
      "        [ 0.5906],\n",
      "        [ 0.6252],\n",
      "        [ 0.2149],\n",
      "        [ 0.3534],\n",
      "        [ 0.6111],\n",
      "        [ 0.3680],\n",
      "        [ 0.1933],\n",
      "        [ 0.1714],\n",
      "        [ 0.2132],\n",
      "        [ 0.2177],\n",
      "        [ 0.5822],\n",
      "        [ 0.5671],\n",
      "        [ 0.4781],\n",
      "        [ 0.6287],\n",
      "        [ 0.2792],\n",
      "        [ 0.1571],\n",
      "        [ 0.3462],\n",
      "        [ 0.3039],\n",
      "        [ 0.1405],\n",
      "        [ 0.2807],\n",
      "        [ 0.6212],\n",
      "        [ 0.1304],\n",
      "        [ 0.1799],\n",
      "        [ 0.2786],\n",
      "        [ 0.1662],\n",
      "        [ 0.3295],\n",
      "        [ 0.3297],\n",
      "        [ 0.4303],\n",
      "        [ 0.3423],\n",
      "        [ 0.3385],\n",
      "        [ 0.6956],\n",
      "        [ 0.3916],\n",
      "        [ 0.5942],\n",
      "        [ 0.4519],\n",
      "        [ 0.4265],\n",
      "        [ 0.6590],\n",
      "        [ 0.5396],\n",
      "        [ 0.4048],\n",
      "        [ 0.3946],\n",
      "        [ 0.4417],\n",
      "        [ 0.4243],\n",
      "        [ 0.5652],\n",
      "        [ 0.2360],\n",
      "        [ 0.2220],\n",
      "        [ 0.2222],\n",
      "        [ 0.6352],\n",
      "        [ 0.3469],\n",
      "        [ 0.2793],\n",
      "        [ 0.4237],\n",
      "        [ 0.2437],\n",
      "        [ 0.6578],\n",
      "        [ 0.6675],\n",
      "        [ 0.6365],\n",
      "        [ 0.1626],\n",
      "        [ 0.4403],\n",
      "        [ 0.3408],\n",
      "        [ 0.4187],\n",
      "        [ 0.3054],\n",
      "        [ 0.1817],\n",
      "        [ 0.1360],\n",
      "        [ 0.3026],\n",
      "        [ 0.3533],\n",
      "        [ 0.2552],\n",
      "        [ 0.4290],\n",
      "        [ 0.2994],\n",
      "        [ 0.3498],\n",
      "        [ 0.1345],\n",
      "        [ 0.4059],\n",
      "        [ 0.1240],\n",
      "        [ 0.3085],\n",
      "        [ 0.5366],\n",
      "        [ 0.5805],\n",
      "        [ 0.3831],\n",
      "        [ 0.3345],\n",
      "        [ 0.4112],\n",
      "        [ 0.3377],\n",
      "        [ 0.3728],\n",
      "        [ 0.3542],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [-0.1963],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573],\n",
      "        [ 0.4573]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0145],\n",
      "        [    0.0401],\n",
      "        [    0.0184],\n",
      "        [    0.0721],\n",
      "        [    0.0074],\n",
      "        [    0.0142],\n",
      "        [    0.0020],\n",
      "        [    0.0136],\n",
      "        [    0.0122],\n",
      "        [    0.0297],\n",
      "        [    0.0099],\n",
      "        [    0.1016],\n",
      "        [    0.0130],\n",
      "        [    0.0185],\n",
      "        [    0.0051],\n",
      "        [    0.0896],\n",
      "        [    0.0130],\n",
      "        [    0.0048],\n",
      "        [    0.0002],\n",
      "        [    0.0180],\n",
      "        [    0.0375],\n",
      "        [    0.0674],\n",
      "        [    0.0365],\n",
      "        [    0.0272],\n",
      "        [    0.0123],\n",
      "        [    0.0020],\n",
      "        [    0.0278],\n",
      "        [    0.0112],\n",
      "        [    0.0076],\n",
      "        [    0.0445],\n",
      "        [    0.0227],\n",
      "        [    0.0779],\n",
      "        [    0.0424],\n",
      "        [    0.0428],\n",
      "        [    0.0172],\n",
      "        [    0.0225],\n",
      "        [    0.0184],\n",
      "        [    0.0013],\n",
      "        [    0.0205],\n",
      "        [    0.0239],\n",
      "        [    0.0015],\n",
      "        [    0.0241],\n",
      "        [    0.0089],\n",
      "        [    0.0411],\n",
      "        [    0.0316],\n",
      "        [    0.0178],\n",
      "        [    0.0257],\n",
      "        [    0.0046],\n",
      "        [    0.0249],\n",
      "        [    0.0427],\n",
      "        [    0.0092],\n",
      "        [    0.0905],\n",
      "        [    0.0414],\n",
      "        [    0.0732],\n",
      "        [    0.0343],\n",
      "        [    0.0407],\n",
      "        [    0.0559],\n",
      "        [    0.0099],\n",
      "        [    0.1349],\n",
      "        [    0.0188],\n",
      "        [    0.0117],\n",
      "        [    0.0329],\n",
      "        [    0.0079],\n",
      "        [    0.0133],\n",
      "        [    0.0505],\n",
      "        [    0.0027],\n",
      "        [    0.0618],\n",
      "        [    0.0285],\n",
      "        [    0.0313],\n",
      "        [    0.0291],\n",
      "        [    0.0291],\n",
      "        [    0.0416],\n",
      "        [    0.0212],\n",
      "        [    0.0260],\n",
      "        [    0.0162],\n",
      "        [    0.0774],\n",
      "        [    0.0229],\n",
      "        [    0.0049],\n",
      "        [    0.0607],\n",
      "        [    0.0952],\n",
      "        [    0.0709],\n",
      "        [    0.0598],\n",
      "        [    0.0832],\n",
      "        [    0.0813],\n",
      "        [    0.1022],\n",
      "        [    0.0587],\n",
      "        [    0.0718],\n",
      "        [    0.0412],\n",
      "        [    0.0631],\n",
      "        [    0.0368],\n",
      "        [    0.0442],\n",
      "        [    0.0592],\n",
      "        [    0.1092],\n",
      "        [    0.0560],\n",
      "        [    0.0329],\n",
      "        [    0.0290],\n",
      "        [    0.0528],\n",
      "        [    0.1401],\n",
      "        [    0.1139],\n",
      "        [    0.0759],\n",
      "        [    0.0191],\n",
      "        [    0.0936],\n",
      "        [    0.1375],\n",
      "        [    0.0261],\n",
      "        [    0.0071],\n",
      "        [    0.0988],\n",
      "        [    0.0924],\n",
      "        [    0.0149],\n",
      "        [    0.1085],\n",
      "        [    0.1012],\n",
      "        [    0.1345],\n",
      "        [    0.1192],\n",
      "        [    0.0969],\n",
      "        [    0.1280],\n",
      "        [    0.1129],\n",
      "        [    0.1600],\n",
      "        [    0.1557],\n",
      "        [    0.0746],\n",
      "        [    0.0719],\n",
      "        [    0.1295],\n",
      "        [    0.1257],\n",
      "        [    0.1416],\n",
      "        [    0.3069],\n",
      "        [    0.3209],\n",
      "        [    0.3239],\n",
      "        [    0.3258],\n",
      "        [    0.3268],\n",
      "        [    0.3363],\n",
      "        [    0.3381],\n",
      "        [    0.3392],\n",
      "        [    0.3466],\n",
      "        [    0.3484],\n",
      "        [    0.3505],\n",
      "        [    0.3515],\n",
      "        [    0.3547],\n",
      "        [    0.3738],\n",
      "        [    0.3748],\n",
      "        [    0.3817],\n",
      "        [    0.3861],\n",
      "        [    0.3881],\n",
      "        [    0.3890],\n",
      "        [    0.3934],\n",
      "        [    0.4009],\n",
      "        [    0.4025],\n",
      "        [    0.4033],\n",
      "        [    0.4033],\n",
      "        [    0.4060],\n",
      "        [    0.4093],\n",
      "        [    0.4109],\n",
      "        [    0.3151],\n",
      "        [    0.4126],\n",
      "        [    0.4133],\n",
      "        [    0.4148]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0193],\n",
      "        [0.0426],\n",
      "        [0.0145],\n",
      "        [0.0768],\n",
      "        [0.0061],\n",
      "        [0.0119],\n",
      "        [0.0062],\n",
      "        [0.0120],\n",
      "        [0.0163],\n",
      "        [0.0345],\n",
      "        [0.0053],\n",
      "        [0.1039],\n",
      "        [0.0180],\n",
      "        [0.0231],\n",
      "        [0.0093],\n",
      "        [0.0920],\n",
      "        [0.0169],\n",
      "        [0.0092],\n",
      "        [0.0044],\n",
      "        [0.0143],\n",
      "        [0.0424],\n",
      "        [0.0700],\n",
      "        [0.0333],\n",
      "        [0.0237],\n",
      "        [0.0163],\n",
      "        [0.0046],\n",
      "        [0.0261],\n",
      "        [0.0131],\n",
      "        [0.0036],\n",
      "        [0.0412],\n",
      "        [0.0185],\n",
      "        [0.0746],\n",
      "        [0.0444],\n",
      "        [0.0472],\n",
      "        [0.0214],\n",
      "        [0.0250],\n",
      "        [0.0146],\n",
      "        [0.0037],\n",
      "        [0.0241],\n",
      "        [0.0195],\n",
      "        [0.0026],\n",
      "        [0.0284],\n",
      "        [0.0056],\n",
      "        [0.0365],\n",
      "        [0.0362],\n",
      "        [0.0129],\n",
      "        [0.0275],\n",
      "        [0.0016],\n",
      "        [0.0211],\n",
      "        [0.0460],\n",
      "        [0.0114],\n",
      "        [0.0933],\n",
      "        [0.0394],\n",
      "        [0.0707],\n",
      "        [0.0387],\n",
      "        [0.0449],\n",
      "        [0.0521],\n",
      "        [0.0149],\n",
      "        [0.1373],\n",
      "        [0.0214],\n",
      "        [0.0085],\n",
      "        [0.0358],\n",
      "        [0.0106],\n",
      "        [0.0158],\n",
      "        [0.0462],\n",
      "        [0.0005],\n",
      "        [0.0663],\n",
      "        [0.0257],\n",
      "        [0.0269],\n",
      "        [0.0260],\n",
      "        [0.0261],\n",
      "        [0.0377],\n",
      "        [0.0176],\n",
      "        [0.0226],\n",
      "        [0.0205],\n",
      "        [0.0809],\n",
      "        [0.0183],\n",
      "        [0.0021],\n",
      "        [0.0573],\n",
      "        [0.1001],\n",
      "        [0.0668],\n",
      "        [0.0562],\n",
      "        [0.0867],\n",
      "        [0.0779],\n",
      "        [0.0983],\n",
      "        [0.0543],\n",
      "        [0.0691],\n",
      "        [0.0435],\n",
      "        [0.0651],\n",
      "        [0.0328],\n",
      "        [0.0409],\n",
      "        [0.0618],\n",
      "        [0.1058],\n",
      "        [0.0583],\n",
      "        [0.0286],\n",
      "        [0.0248],\n",
      "        [0.0487],\n",
      "        [0.1375],\n",
      "        [0.1178],\n",
      "        [0.0786],\n",
      "        [0.0220],\n",
      "        [0.0905],\n",
      "        [0.1350],\n",
      "        [0.0080],\n",
      "        [0.0045],\n",
      "        [0.0956],\n",
      "        [0.0947],\n",
      "        [0.0180],\n",
      "        [0.1113],\n",
      "        [0.0982],\n",
      "        [0.1357],\n",
      "        [0.1224],\n",
      "        [0.0981],\n",
      "        [0.1250],\n",
      "        [0.1161],\n",
      "        [0.1635],\n",
      "        [0.1585],\n",
      "        [0.0717],\n",
      "        [0.0681],\n",
      "        [0.1263],\n",
      "        [0.1222],\n",
      "        [0.1383],\n",
      "        [0.3061],\n",
      "        [0.3201],\n",
      "        [0.3231],\n",
      "        [0.3250],\n",
      "        [0.3260],\n",
      "        [0.3355],\n",
      "        [0.3373],\n",
      "        [0.3385],\n",
      "        [0.3458],\n",
      "        [0.3476],\n",
      "        [0.3497],\n",
      "        [0.3508],\n",
      "        [0.3540],\n",
      "        [0.3730],\n",
      "        [0.3740],\n",
      "        [0.3810],\n",
      "        [0.3853],\n",
      "        [0.3873],\n",
      "        [0.3882],\n",
      "        [0.3927],\n",
      "        [0.4001],\n",
      "        [0.4018],\n",
      "        [0.4025],\n",
      "        [0.4025],\n",
      "        [0.4053],\n",
      "        [0.4086],\n",
      "        [0.4101],\n",
      "        [0.2901],\n",
      "        [0.4119],\n",
      "        [0.4125],\n",
      "        [0.4141]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 87.38469743728638\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 5 個區塊累積花費時間(s) 6.258111476898193\n",
      "<<The performance of 5 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 6.258111476898193\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 2898.29\n",
      "The MAPE for l = 1: 0.07%\n",
      "The RMSE for l = 1: 4488.16\n",
      "The accuracy(2000) for l = 1: 61.44%\n",
      "The accuracy(3000) for l = 1: 71.90%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in toutlier>>\n",
      "The MAE for l = 1: 10887.89\n",
      "The MAPE for l = 1: 0.31%\n",
      "The RMSE for l = 1: 10889.00\n",
      "The accuracy(2000) for l = 1: 0.00%\n",
      "The accuracy(3000) for l = 1: 0.00%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 2780.8\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 2965.6\n",
      "The accuracy(2000) for l = 1: 19.2%\n",
      "The accuracy(3000) for l = 1: 57.7%\n",
      "------------------------------------------------------------\n",
      "0.6143790849673203\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "0.19230769230769232\n",
      "<class 'float'>\n",
      "The <<6>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.069499771550909e-07, 119)\n",
      "The second_loss value of k: (2.4242258405138273e-06, 41)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [119, 41, 113, 6, 125, 56, 10, 51, 69, 7, 72, 40, 9, 8, 120, 62, 64, 70, 4, 118, 29, 20, 26, 34, 1, 19, 44, 5, 131, 11, 2, 151, 121, 15, 132, 50, 13, 63, 39, 43, 71, 42, 66, 68, 16, 55, 14, 27, 17, 65, 45, 24, 152, 12, 114, 61, 117, 18, 138, 54, 3, 0, 150, 21, 28, 25, 60, 57, 67, 73, 130, 38, 116, 35, 126, 133, 23, 33, 52, 143, 53, 49, 153, 149, 122, 115, 142, 59, 47, 76, 46, 154, 124, 141, 22, 144, 157, 58, 137, 158, 30, 140, 155, 129, 31, 134, 48, 127, 148, 145, 37, 75, 156, 123, 36, 128, 147, 139, 32, 146, 135, 136, 112, 109, 110, 111, 108]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0016],\n",
      "        [0.0037],\n",
      "        [0.0044],\n",
      "        [0.0045],\n",
      "        [0.0046],\n",
      "        [0.0053],\n",
      "        [0.0056],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0080],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0106],\n",
      "        [0.0114],\n",
      "        [0.0119],\n",
      "        [0.0120],\n",
      "        [0.0129],\n",
      "        [0.0131],\n",
      "        [0.0143],\n",
      "        [0.0145],\n",
      "        [0.0146],\n",
      "        [0.0158],\n",
      "        [0.0163],\n",
      "        [0.0169],\n",
      "        [0.0176],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0193],\n",
      "        [0.0195],\n",
      "        [0.0197],\n",
      "        [0.0214],\n",
      "        [0.0214],\n",
      "        [0.0220],\n",
      "        [0.0226],\n",
      "        [0.0231],\n",
      "        [0.0250],\n",
      "        [0.0257],\n",
      "        [0.0260],\n",
      "        [0.0261],\n",
      "        [0.0261],\n",
      "        [0.0269],\n",
      "        [0.0275],\n",
      "        [0.0284],\n",
      "        [0.0358],\n",
      "        [0.0362],\n",
      "        [0.0377],\n",
      "        [0.0387],\n",
      "        [0.0394],\n",
      "        [0.0409],\n",
      "        [0.0412],\n",
      "        [0.0413],\n",
      "        [0.0424],\n",
      "        [0.0426],\n",
      "        [0.0435],\n",
      "        [0.0444],\n",
      "        [0.0449],\n",
      "        [0.0457],\n",
      "        [0.0460],\n",
      "        [0.0462],\n",
      "        [0.0472],\n",
      "        [0.0488],\n",
      "        [0.0521],\n",
      "        [0.0562],\n",
      "        [0.0573],\n",
      "        [0.0583],\n",
      "        [0.0618],\n",
      "        [0.0651],\n",
      "        [0.0663],\n",
      "        [0.0681],\n",
      "        [0.0691],\n",
      "        [0.0700],\n",
      "        [0.0707],\n",
      "        [0.0717],\n",
      "        [0.0728],\n",
      "        [0.0779],\n",
      "        [0.0786],\n",
      "        [0.0809],\n",
      "        [0.0860],\n",
      "        [0.0867],\n",
      "        [0.0905],\n",
      "        [0.0911],\n",
      "        [0.0914],\n",
      "        [0.0920],\n",
      "        [0.0933],\n",
      "        [0.0933],\n",
      "        [0.0947],\n",
      "        [0.0956],\n",
      "        [0.0981],\n",
      "        [0.0982],\n",
      "        [0.1032],\n",
      "        [0.1039],\n",
      "        [0.1041],\n",
      "        [0.1058],\n",
      "        [0.1062],\n",
      "        [0.1086],\n",
      "        [0.1113],\n",
      "        [0.1122],\n",
      "        [0.1151],\n",
      "        [0.1178],\n",
      "        [0.1191],\n",
      "        [0.1218],\n",
      "        [0.1222],\n",
      "        [0.1224],\n",
      "        [0.1228],\n",
      "        [0.1250],\n",
      "        [0.1263],\n",
      "        [0.1311],\n",
      "        [0.1341],\n",
      "        [0.1350],\n",
      "        [0.1357],\n",
      "        [0.1372],\n",
      "        [0.1373],\n",
      "        [0.1375],\n",
      "        [0.1383],\n",
      "        [0.1449],\n",
      "        [0.1510],\n",
      "        [0.1585],\n",
      "        [0.1690],\n",
      "        [0.1790],\n",
      "        [0.1826],\n",
      "        [0.2901],\n",
      "        [0.3061],\n",
      "        [0.3201],\n",
      "        [0.3231],\n",
      "        [0.3250]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 128\n",
      "剩餘X 資料 torch.Size([32, 18])\n",
      "剩餘Y 資料 torch.Size([32, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10627450793981552, 16)\n",
      "The second_loss value of k: (0.11254465579986572, 21)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.1305])\n",
      "目前模型的Data狀態 torch.Size([128, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1327],\n",
      "        [ 0.3565],\n",
      "        [ 0.1123],\n",
      "        [ 0.6377],\n",
      "        [ 0.3052],\n",
      "        [ 0.2668],\n",
      "        [ 0.6387],\n",
      "        [ 0.3486],\n",
      "        [ 0.2118],\n",
      "        [ 0.6348],\n",
      "        [ 0.1540],\n",
      "        [ 0.3494],\n",
      "        [ 0.6257],\n",
      "        [ 0.6163],\n",
      "        [ 0.1432],\n",
      "        [ 0.1955],\n",
      "        [ 0.2218],\n",
      "        [ 0.1801],\n",
      "        [ 0.6301],\n",
      "        [ 0.1327],\n",
      "        [ 0.4262],\n",
      "        [ 0.5221],\n",
      "        [ 0.4593],\n",
      "        [ 0.2832],\n",
      "        [ 0.6468],\n",
      "        [ 0.5575],\n",
      "        [ 0.3459],\n",
      "        [ 0.6395],\n",
      "        [ 0.4322],\n",
      "        [ 0.6470],\n",
      "        [ 0.6572],\n",
      "        [ 0.3925],\n",
      "        [ 0.1596],\n",
      "        [ 0.5825],\n",
      "        [ 0.4216],\n",
      "        [ 0.3418],\n",
      "        [ 0.6138],\n",
      "        [ 0.2167],\n",
      "        [ 0.2814],\n",
      "        [ 0.3326],\n",
      "        [ 0.1541],\n",
      "        [ 0.3326],\n",
      "        [ 0.1706],\n",
      "        [ 0.2166],\n",
      "        [ 0.5842],\n",
      "        [ 0.3067],\n",
      "        [ 0.5952],\n",
      "        [ 0.4342],\n",
      "        [ 0.5866],\n",
      "        [ 0.2152],\n",
      "        [ 0.3502],\n",
      "        [ 0.4297],\n",
      "        [ 0.3782],\n",
      "        [ 0.6496],\n",
      "        [ 0.1334],\n",
      "        [ 0.2244],\n",
      "        [ 0.1375],\n",
      "        [ 0.5713],\n",
      "        [ 0.4418],\n",
      "        [ 0.3712],\n",
      "        [ 0.6255],\n",
      "        [ 0.6279],\n",
      "        [ 0.4105],\n",
      "        [ 0.4819],\n",
      "        [ 0.4084],\n",
      "        [ 0.4299],\n",
      "        [ 0.2460],\n",
      "        [ 0.2819],\n",
      "        [ 0.2242],\n",
      "        [ 0.1844],\n",
      "        [ 0.4149],\n",
      "        [ 0.2387],\n",
      "        [ 0.1606],\n",
      "        [ 0.2203],\n",
      "        [ 0.3375],\n",
      "        [ 0.3536],\n",
      "        [ 0.4452],\n",
      "        [ 0.3436],\n",
      "        [ 0.3951],\n",
      "        [ 0.4135],\n",
      "        [ 0.3981],\n",
      "        [ 0.3085],\n",
      "        [ 0.3544],\n",
      "        [ 0.3653],\n",
      "        [ 0.2103],\n",
      "        [ 0.1742],\n",
      "        [ 0.3993],\n",
      "        [ 0.2575],\n",
      "        [ 0.3566],\n",
      "        [ 0.1251],\n",
      "        [ 0.3529],\n",
      "        [ 0.3293],\n",
      "        [ 0.2795],\n",
      "        [ 0.4210],\n",
      "        [ 0.4272],\n",
      "        [ 0.3820],\n",
      "        [ 0.3297],\n",
      "        [ 0.3022],\n",
      "        [ 0.3795],\n",
      "        [ 0.3658],\n",
      "        [ 0.4442],\n",
      "        [ 0.4221],\n",
      "        [ 0.3014],\n",
      "        [ 0.3763],\n",
      "        [ 0.4092],\n",
      "        [ 0.3454],\n",
      "        [ 0.3115],\n",
      "        [ 0.3409],\n",
      "        [ 0.3228],\n",
      "        [ 0.3512],\n",
      "        [ 0.1842],\n",
      "        [ 0.1357],\n",
      "        [ 0.3064],\n",
      "        [ 0.2816],\n",
      "        [ 0.1651],\n",
      "        [ 0.3575],\n",
      "        [ 0.3250],\n",
      "        [ 0.4186],\n",
      "        [ 0.3859],\n",
      "        [ 0.3371],\n",
      "        [ 0.3046],\n",
      "        [ 0.3178],\n",
      "        [-0.1714],\n",
      "        [ 0.4565],\n",
      "        [ 0.4565],\n",
      "        [ 0.4565],\n",
      "        [ 0.4565],\n",
      "        [ 0.4565]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0005],\n",
      "        [0.0016],\n",
      "        [0.0037],\n",
      "        [0.0044],\n",
      "        [0.0045],\n",
      "        [0.0046],\n",
      "        [0.0053],\n",
      "        [0.0056],\n",
      "        [0.0061],\n",
      "        [0.0062],\n",
      "        [0.0080],\n",
      "        [0.0085],\n",
      "        [0.0092],\n",
      "        [0.0093],\n",
      "        [0.0106],\n",
      "        [0.0114],\n",
      "        [0.0119],\n",
      "        [0.0120],\n",
      "        [0.0129],\n",
      "        [0.0131],\n",
      "        [0.0143],\n",
      "        [0.0145],\n",
      "        [0.0146],\n",
      "        [0.0158],\n",
      "        [0.0163],\n",
      "        [0.0169],\n",
      "        [0.0176],\n",
      "        [0.0180],\n",
      "        [0.0180],\n",
      "        [0.0193],\n",
      "        [0.0195],\n",
      "        [0.0197],\n",
      "        [0.0214],\n",
      "        [0.0214],\n",
      "        [0.0220],\n",
      "        [0.0226],\n",
      "        [0.0231],\n",
      "        [0.0250],\n",
      "        [0.0257],\n",
      "        [0.0260],\n",
      "        [0.0261],\n",
      "        [0.0261],\n",
      "        [0.0269],\n",
      "        [0.0275],\n",
      "        [0.0284],\n",
      "        [0.0358],\n",
      "        [0.0362],\n",
      "        [0.0377],\n",
      "        [0.0387],\n",
      "        [0.0394],\n",
      "        [0.0409],\n",
      "        [0.0412],\n",
      "        [0.0413],\n",
      "        [0.0424],\n",
      "        [0.0426],\n",
      "        [0.0435],\n",
      "        [0.0444],\n",
      "        [0.0449],\n",
      "        [0.0457],\n",
      "        [0.0460],\n",
      "        [0.0462],\n",
      "        [0.0472],\n",
      "        [0.0488],\n",
      "        [0.0521],\n",
      "        [0.0562],\n",
      "        [0.0573],\n",
      "        [0.0583],\n",
      "        [0.0618],\n",
      "        [0.0651],\n",
      "        [0.0663],\n",
      "        [0.0681],\n",
      "        [0.0691],\n",
      "        [0.0700],\n",
      "        [0.0707],\n",
      "        [0.0717],\n",
      "        [0.0728],\n",
      "        [0.0779],\n",
      "        [0.0786],\n",
      "        [0.0809],\n",
      "        [0.0860],\n",
      "        [0.0867],\n",
      "        [0.0905],\n",
      "        [0.0911],\n",
      "        [0.0914],\n",
      "        [0.0920],\n",
      "        [0.0933],\n",
      "        [0.0933],\n",
      "        [0.0947],\n",
      "        [0.0956],\n",
      "        [0.0981],\n",
      "        [0.0982],\n",
      "        [0.1032],\n",
      "        [0.1039],\n",
      "        [0.1041],\n",
      "        [0.1058],\n",
      "        [0.1062],\n",
      "        [0.1086],\n",
      "        [0.1113],\n",
      "        [0.1122],\n",
      "        [0.1151],\n",
      "        [0.1178],\n",
      "        [0.1191],\n",
      "        [0.1218],\n",
      "        [0.1222],\n",
      "        [0.1224],\n",
      "        [0.1228],\n",
      "        [0.1250],\n",
      "        [0.1263],\n",
      "        [0.1311],\n",
      "        [0.1341],\n",
      "        [0.1350],\n",
      "        [0.1357],\n",
      "        [0.1372],\n",
      "        [0.1373],\n",
      "        [0.1375],\n",
      "        [0.1383],\n",
      "        [0.1449],\n",
      "        [0.1510],\n",
      "        [0.1585],\n",
      "        [0.1690],\n",
      "        [0.1790],\n",
      "        [0.1826],\n",
      "        [0.2901],\n",
      "        [0.3061],\n",
      "        [0.3201],\n",
      "        [0.3231],\n",
      "        [0.3250],\n",
      "        [0.3260]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0077],\n",
      "        [0.0111],\n",
      "        [0.0134],\n",
      "        [0.0074],\n",
      "        [0.0047],\n",
      "        [0.0051],\n",
      "        [0.0215],\n",
      "        [0.0050],\n",
      "        [0.0097],\n",
      "        [0.0070],\n",
      "        [0.0139],\n",
      "        [0.0207],\n",
      "        [0.0079],\n",
      "        [0.0033],\n",
      "        [0.0153],\n",
      "        [0.0026],\n",
      "        [0.0192],\n",
      "        [0.0143],\n",
      "        [0.0326],\n",
      "        [0.0215],\n",
      "        [0.0054],\n",
      "        [0.0148],\n",
      "        [0.0023],\n",
      "        [0.0209],\n",
      "        [0.0092],\n",
      "        [0.0166],\n",
      "        [0.0229],\n",
      "        [0.0031],\n",
      "        [0.0666],\n",
      "        [0.0075],\n",
      "        [0.0428],\n",
      "        [0.0745],\n",
      "        [0.0277],\n",
      "        [0.0119],\n",
      "        [0.0856],\n",
      "        [0.0137],\n",
      "        [0.0149],\n",
      "        [0.0178],\n",
      "        [0.0372],\n",
      "        [0.0336],\n",
      "        [0.0280],\n",
      "        [0.0351],\n",
      "        [0.0244],\n",
      "        [0.0213],\n",
      "        [0.0227],\n",
      "        [0.0358],\n",
      "        [0.0281],\n",
      "        [0.0228],\n",
      "        [0.0348],\n",
      "        [0.0486],\n",
      "        [0.0362],\n",
      "        [0.0359],\n",
      "        [0.0512],\n",
      "        [0.0318],\n",
      "        [0.0488],\n",
      "        [0.0358],\n",
      "        [0.0507],\n",
      "        [0.0450],\n",
      "        [0.0451],\n",
      "        [0.0471],\n",
      "        [0.0695],\n",
      "        [0.0243],\n",
      "        [0.0484],\n",
      "        [0.0497],\n",
      "        [0.0432],\n",
      "        [0.0488],\n",
      "        [0.0504],\n",
      "        [0.0592],\n",
      "        [0.0566],\n",
      "        [0.0695],\n",
      "        [0.0266],\n",
      "        [0.0855],\n",
      "        [0.0792],\n",
      "        [0.0725],\n",
      "        [0.0615],\n",
      "        [0.0093],\n",
      "        [0.0702],\n",
      "        [0.0844],\n",
      "        [0.0887],\n",
      "        [0.0129],\n",
      "        [0.0960],\n",
      "        [0.0872],\n",
      "        [0.0015],\n",
      "        [0.0076],\n",
      "        [0.1029],\n",
      "        [0.1000],\n",
      "        [0.0020],\n",
      "        [0.0850],\n",
      "        [0.0978],\n",
      "        [0.0888],\n",
      "        [0.0946],\n",
      "        [0.0158],\n",
      "        [0.1136],\n",
      "        [0.0069],\n",
      "        [0.0969],\n",
      "        [0.0052],\n",
      "        [0.0261],\n",
      "        [0.1047],\n",
      "        [0.0217],\n",
      "        [0.0346],\n",
      "        [0.1263],\n",
      "        [0.0236],\n",
      "        [0.0370],\n",
      "        [0.0931],\n",
      "        [0.1247],\n",
      "        [0.0329],\n",
      "        [0.1230],\n",
      "        [0.1093],\n",
      "        [0.0285],\n",
      "        [0.0287],\n",
      "        [0.1509],\n",
      "        [0.1286],\n",
      "        [0.0524],\n",
      "        [0.1454],\n",
      "        [0.1486],\n",
      "        [0.1159],\n",
      "        [0.0393],\n",
      "        [0.0608],\n",
      "        [0.1604],\n",
      "        [0.0620],\n",
      "        [0.0888],\n",
      "        [0.0945],\n",
      "        [0.2585],\n",
      "        [0.3046],\n",
      "        [0.3187],\n",
      "        [0.3216],\n",
      "        [0.3235],\n",
      "        [0.3245]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 87.86388349533081\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 129\n",
      "剩餘X 資料 torch.Size([31, 18])\n",
      "剩餘Y 資料 torch.Size([31, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11155901849269867, 20)\n",
      "The second_loss value of k: (0.11276587843894958, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引20，y= tensor([0.1210])\n",
      "目前模型的Data狀態 torch.Size([129, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1409],\n",
      "        [ 0.3469],\n",
      "        [ 0.1220],\n",
      "        [ 0.6259],\n",
      "        [ 0.3144],\n",
      "        [ 0.2673],\n",
      "        [ 0.6224],\n",
      "        [ 0.3593],\n",
      "        [ 0.2082],\n",
      "        [ 0.6216],\n",
      "        [ 0.1482],\n",
      "        [ 0.3372],\n",
      "        [ 0.6086],\n",
      "        [ 0.6036],\n",
      "        [ 0.1479],\n",
      "        [ 0.1867],\n",
      "        [ 0.2144],\n",
      "        [ 0.1778],\n",
      "        [ 0.6104],\n",
      "        [ 0.1411],\n",
      "        [ 0.4351],\n",
      "        [ 0.5218],\n",
      "        [ 0.4716],\n",
      "        [ 0.2884],\n",
      "        [ 0.6213],\n",
      "        [ 0.5572],\n",
      "        [ 0.3406],\n",
      "        [ 0.6246],\n",
      "        [ 0.4807],\n",
      "        [ 0.6353],\n",
      "        [ 0.6339],\n",
      "        [ 0.4866],\n",
      "        [ 0.1660],\n",
      "        [ 0.5730],\n",
      "        [ 0.4853],\n",
      "        [ 0.3507],\n",
      "        [ 0.6056],\n",
      "        [ 0.2095],\n",
      "        [ 0.2699],\n",
      "        [ 0.3249],\n",
      "        [ 0.1522],\n",
      "        [ 0.3236],\n",
      "        [ 0.1731],\n",
      "        [ 0.2104],\n",
      "        [ 0.5785],\n",
      "        [ 0.3067],\n",
      "        [ 0.5871],\n",
      "        [ 0.4491],\n",
      "        [ 0.5827],\n",
      "        [ 0.2059],\n",
      "        [ 0.3550],\n",
      "        [ 0.4351],\n",
      "        [ 0.4708],\n",
      "        [ 0.6390],\n",
      "        [ 0.1395],\n",
      "        [ 0.2166],\n",
      "        [ 0.1438],\n",
      "        [ 0.5714],\n",
      "        [ 0.5325],\n",
      "        [ 0.3724],\n",
      "        [ 0.6022],\n",
      "        [ 0.6049],\n",
      "        [ 0.5076],\n",
      "        [ 0.4843],\n",
      "        [ 0.4214],\n",
      "        [ 0.4384],\n",
      "        [ 0.2380],\n",
      "        [ 0.2793],\n",
      "        [ 0.2157],\n",
      "        [ 0.1876],\n",
      "        [ 0.4565],\n",
      "        [ 0.2223],\n",
      "        [ 0.1698],\n",
      "        [ 0.2185],\n",
      "        [ 0.3476],\n",
      "        [ 0.4357],\n",
      "        [ 0.4528],\n",
      "        [ 0.3493],\n",
      "        [ 0.4029],\n",
      "        [ 0.5124],\n",
      "        [ 0.4075],\n",
      "        [ 0.3117],\n",
      "        [ 0.4440],\n",
      "        [ 0.4642],\n",
      "        [ 0.2212],\n",
      "        [ 0.1809],\n",
      "        [ 0.4946],\n",
      "        [ 0.2477],\n",
      "        [ 0.3543],\n",
      "        [ 0.1159],\n",
      "        [ 0.3564],\n",
      "        [ 0.4168],\n",
      "        [ 0.2892],\n",
      "        [ 0.5182],\n",
      "        [ 0.4361],\n",
      "        [ 0.4831],\n",
      "        [ 0.4122],\n",
      "        [ 0.2956],\n",
      "        [ 0.4700],\n",
      "        [ 0.4463],\n",
      "        [ 0.4527],\n",
      "        [ 0.5177],\n",
      "        [ 0.3862],\n",
      "        [ 0.4054],\n",
      "        [ 0.4114],\n",
      "        [ 0.4353],\n",
      "        [ 0.3135],\n",
      "        [ 0.3579],\n",
      "        [ 0.4254],\n",
      "        [ 0.4567],\n",
      "        [ 0.1684],\n",
      "        [ 0.1286],\n",
      "        [ 0.3912],\n",
      "        [ 0.2898],\n",
      "        [ 0.1541],\n",
      "        [ 0.3799],\n",
      "        [ 0.4305],\n",
      "        [ 0.5087],\n",
      "        [ 0.3878],\n",
      "        [ 0.4441],\n",
      "        [ 0.3948],\n",
      "        [ 0.4059],\n",
      "        [-0.1398],\n",
      "        [ 0.4550],\n",
      "        [ 0.4550],\n",
      "        [ 0.4550],\n",
      "        [ 0.4550],\n",
      "        [ 0.4550],\n",
      "        [ 0.4550]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0077],\n",
      "        [0.0111],\n",
      "        [0.0134],\n",
      "        [0.0074],\n",
      "        [0.0047],\n",
      "        [0.0051],\n",
      "        [0.0215],\n",
      "        [0.0050],\n",
      "        [0.0097],\n",
      "        [0.0070],\n",
      "        [0.0139],\n",
      "        [0.0207],\n",
      "        [0.0079],\n",
      "        [0.0033],\n",
      "        [0.0153],\n",
      "        [0.0026],\n",
      "        [0.0192],\n",
      "        [0.0143],\n",
      "        [0.0326],\n",
      "        [0.0215],\n",
      "        [0.0054],\n",
      "        [0.0148],\n",
      "        [0.0023],\n",
      "        [0.0209],\n",
      "        [0.0092],\n",
      "        [0.0166],\n",
      "        [0.0229],\n",
      "        [0.0031],\n",
      "        [0.0666],\n",
      "        [0.0075],\n",
      "        [0.0428],\n",
      "        [0.0745],\n",
      "        [0.0277],\n",
      "        [0.0119],\n",
      "        [0.0856],\n",
      "        [0.0137],\n",
      "        [0.0149],\n",
      "        [0.0178],\n",
      "        [0.0372],\n",
      "        [0.0336],\n",
      "        [0.0280],\n",
      "        [0.0351],\n",
      "        [0.0244],\n",
      "        [0.0213],\n",
      "        [0.0227],\n",
      "        [0.0358],\n",
      "        [0.0281],\n",
      "        [0.0228],\n",
      "        [0.0348],\n",
      "        [0.0486],\n",
      "        [0.0362],\n",
      "        [0.0359],\n",
      "        [0.0512],\n",
      "        [0.0318],\n",
      "        [0.0488],\n",
      "        [0.0358],\n",
      "        [0.0507],\n",
      "        [0.0450],\n",
      "        [0.0451],\n",
      "        [0.0471],\n",
      "        [0.0695],\n",
      "        [0.0243],\n",
      "        [0.0484],\n",
      "        [0.0497],\n",
      "        [0.0432],\n",
      "        [0.0488],\n",
      "        [0.0504],\n",
      "        [0.0592],\n",
      "        [0.0566],\n",
      "        [0.0695],\n",
      "        [0.0266],\n",
      "        [0.0855],\n",
      "        [0.0792],\n",
      "        [0.0725],\n",
      "        [0.0615],\n",
      "        [0.0093],\n",
      "        [0.0702],\n",
      "        [0.0844],\n",
      "        [0.0887],\n",
      "        [0.0129],\n",
      "        [0.0960],\n",
      "        [0.0872],\n",
      "        [0.0015],\n",
      "        [0.0076],\n",
      "        [0.1029],\n",
      "        [0.1000],\n",
      "        [0.0020],\n",
      "        [0.0850],\n",
      "        [0.0978],\n",
      "        [0.0888],\n",
      "        [0.0946],\n",
      "        [0.0158],\n",
      "        [0.1136],\n",
      "        [0.0069],\n",
      "        [0.0969],\n",
      "        [0.0052],\n",
      "        [0.0261],\n",
      "        [0.1047],\n",
      "        [0.0217],\n",
      "        [0.0346],\n",
      "        [0.1263],\n",
      "        [0.0236],\n",
      "        [0.0370],\n",
      "        [0.0931],\n",
      "        [0.1247],\n",
      "        [0.0329],\n",
      "        [0.1230],\n",
      "        [0.1093],\n",
      "        [0.0285],\n",
      "        [0.0287],\n",
      "        [0.1509],\n",
      "        [0.1286],\n",
      "        [0.0524],\n",
      "        [0.1454],\n",
      "        [0.1486],\n",
      "        [0.1159],\n",
      "        [0.0393],\n",
      "        [0.0608],\n",
      "        [0.1604],\n",
      "        [0.0620],\n",
      "        [0.0888],\n",
      "        [0.0945],\n",
      "        [0.2585],\n",
      "        [0.3046],\n",
      "        [0.3187],\n",
      "        [0.3216],\n",
      "        [0.3235],\n",
      "        [0.3245],\n",
      "        [0.3340]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0013],\n",
      "        [    0.0113],\n",
      "        [    0.0075],\n",
      "        [    0.0023],\n",
      "        [    0.0026],\n",
      "        [    0.0066],\n",
      "        [    0.0165],\n",
      "        [    0.0090],\n",
      "        [    0.0156],\n",
      "        [    0.0025],\n",
      "        [    0.0181],\n",
      "        [    0.0215],\n",
      "        [    0.0032],\n",
      "        [    0.0014],\n",
      "        [    0.0077],\n",
      "        [    0.0002],\n",
      "        [    0.0211],\n",
      "        [    0.0188],\n",
      "        [    0.0272],\n",
      "        [    0.0146],\n",
      "        [    0.0012],\n",
      "        [    0.0085],\n",
      "        [    0.0047],\n",
      "        [    0.0254],\n",
      "        [    0.0068],\n",
      "        [    0.0233],\n",
      "        [    0.0214],\n",
      "        [    0.0086],\n",
      "        [    0.0670],\n",
      "        [    0.0130],\n",
      "        [    0.0384],\n",
      "        [    0.0851],\n",
      "        [    0.0196],\n",
      "        [    0.0169],\n",
      "        [    0.0905],\n",
      "        [    0.0100],\n",
      "        [    0.0208],\n",
      "        [    0.0163],\n",
      "        [    0.0387],\n",
      "        [    0.0332],\n",
      "        [    0.0317],\n",
      "        [    0.0354],\n",
      "        [    0.0232],\n",
      "        [    0.0161],\n",
      "        [    0.0286],\n",
      "        [    0.0377],\n",
      "        [    0.0335],\n",
      "        [    0.0151],\n",
      "        [    0.0410],\n",
      "        [    0.0518],\n",
      "        [    0.0324],\n",
      "        [    0.0305],\n",
      "        [    0.0615],\n",
      "        [    0.0373],\n",
      "        [    0.0420],\n",
      "        [    0.0346],\n",
      "        [    0.0438],\n",
      "        [    0.0517],\n",
      "        [    0.0558],\n",
      "        [    0.0499],\n",
      "        [    0.0652],\n",
      "        [    0.0263],\n",
      "        [    0.0599],\n",
      "        [    0.0434],\n",
      "        [    0.0361],\n",
      "        [    0.0432],\n",
      "        [    0.0494],\n",
      "        [    0.0600],\n",
      "        [    0.0523],\n",
      "        [    0.0703],\n",
      "        [    0.0271],\n",
      "        [    0.0877],\n",
      "        [    0.0739],\n",
      "        [    0.0694],\n",
      "        [    0.0684],\n",
      "        [    0.0177],\n",
      "        [    0.0637],\n",
      "        [    0.0892],\n",
      "        [    0.0926],\n",
      "        [    0.0255],\n",
      "        [    0.1003],\n",
      "        [    0.0846],\n",
      "        [    0.0089],\n",
      "        [    0.0193],\n",
      "        [    0.0949],\n",
      "        [    0.0937],\n",
      "        [    0.0135],\n",
      "        [    0.0836],\n",
      "        [    0.0954],\n",
      "        [    0.0830],\n",
      "        [    0.0915],\n",
      "        [    0.0060],\n",
      "        [    0.1057],\n",
      "        [    0.0048],\n",
      "        [    0.0900],\n",
      "        [    0.0073],\n",
      "        [    0.0177],\n",
      "        [    0.1049],\n",
      "        [    0.0103],\n",
      "        [    0.0271],\n",
      "        [    0.1327],\n",
      "        [    0.0122],\n",
      "        [    0.0282],\n",
      "        [    0.0963],\n",
      "        [    0.1295],\n",
      "        [    0.0222],\n",
      "        [    0.1206],\n",
      "        [    0.1148],\n",
      "        [    0.0155],\n",
      "        [    0.0157],\n",
      "        [    0.1521],\n",
      "        [    0.1232],\n",
      "        [    0.0438],\n",
      "        [    0.1374],\n",
      "        [    0.1473],\n",
      "        [    0.1202],\n",
      "        [    0.0255],\n",
      "        [    0.0503],\n",
      "        [    0.1646],\n",
      "        [    0.0477],\n",
      "        [    0.0778],\n",
      "        [    0.0836],\n",
      "        [    0.2463],\n",
      "        [    0.3039],\n",
      "        [    0.3180],\n",
      "        [    0.3209],\n",
      "        [    0.3228],\n",
      "        [    0.3238],\n",
      "        [    0.3333]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 88.11307740211487\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 130\n",
      "剩餘X 資料 torch.Size([30, 18])\n",
      "剩餘Y 資料 torch.Size([30, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11230736970901489, 15)\n",
      "The second_loss value of k: (0.11309633404016495, 29)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.1192])\n",
      "目前模型的Data狀態 torch.Size([130, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1344],\n",
      "        [ 0.3467],\n",
      "        [ 0.1161],\n",
      "        [ 0.6310],\n",
      "        [ 0.3071],\n",
      "        [ 0.2688],\n",
      "        [ 0.6275],\n",
      "        [ 0.3632],\n",
      "        [ 0.2022],\n",
      "        [ 0.6261],\n",
      "        [ 0.1440],\n",
      "        [ 0.3364],\n",
      "        [ 0.6134],\n",
      "        [ 0.6083],\n",
      "        [ 0.1403],\n",
      "        [ 0.1843],\n",
      "        [ 0.2125],\n",
      "        [ 0.1733],\n",
      "        [ 0.6158],\n",
      "        [ 0.1342],\n",
      "        [ 0.4416],\n",
      "        [ 0.5281],\n",
      "        [ 0.4787],\n",
      "        [ 0.2929],\n",
      "        [ 0.6238],\n",
      "        [ 0.5639],\n",
      "        [ 0.3420],\n",
      "        [ 0.6302],\n",
      "        [ 0.4811],\n",
      "        [ 0.6407],\n",
      "        [ 0.6384],\n",
      "        [ 0.4972],\n",
      "        [ 0.1578],\n",
      "        [ 0.5780],\n",
      "        [ 0.4901],\n",
      "        [ 0.3544],\n",
      "        [ 0.6114],\n",
      "        [ 0.2080],\n",
      "        [ 0.2684],\n",
      "        [ 0.3254],\n",
      "        [ 0.1485],\n",
      "        [ 0.3233],\n",
      "        [ 0.1744],\n",
      "        [ 0.2053],\n",
      "        [ 0.5844],\n",
      "        [ 0.3087],\n",
      "        [ 0.5925],\n",
      "        [ 0.4569],\n",
      "        [ 0.5890],\n",
      "        [ 0.2027],\n",
      "        [ 0.3587],\n",
      "        [ 0.4404],\n",
      "        [ 0.4810],\n",
      "        [ 0.6445],\n",
      "        [ 0.1327],\n",
      "        [ 0.2154],\n",
      "        [ 0.1369],\n",
      "        [ 0.5781],\n",
      "        [ 0.5433],\n",
      "        [ 0.3752],\n",
      "        [ 0.6065],\n",
      "        [ 0.6070],\n",
      "        [ 0.5191],\n",
      "        [ 0.4906],\n",
      "        [ 0.4284],\n",
      "        [ 0.4440],\n",
      "        [ 0.2370],\n",
      "        [ 0.2802],\n",
      "        [ 0.2114],\n",
      "        [ 0.1884],\n",
      "        [ 0.4560],\n",
      "        [ 0.2201],\n",
      "        [ 0.1645],\n",
      "        [ 0.2215],\n",
      "        [ 0.3407],\n",
      "        [ 0.4441],\n",
      "        [ 0.4593],\n",
      "        [ 0.3541],\n",
      "        [ 0.4068],\n",
      "        [ 0.5250],\n",
      "        [ 0.4118],\n",
      "        [ 0.3143],\n",
      "        [ 0.4544],\n",
      "        [ 0.4760],\n",
      "        [ 0.2132],\n",
      "        [ 0.1746],\n",
      "        [ 0.5060],\n",
      "        [ 0.2464],\n",
      "        [ 0.3568],\n",
      "        [ 0.1101],\n",
      "        [ 0.3596],\n",
      "        [ 0.4265],\n",
      "        [ 0.2813],\n",
      "        [ 0.5299],\n",
      "        [ 0.4429],\n",
      "        [ 0.4956],\n",
      "        [ 0.4206],\n",
      "        [ 0.2958],\n",
      "        [ 0.4814],\n",
      "        [ 0.4537],\n",
      "        [ 0.4590],\n",
      "        [ 0.5291],\n",
      "        [ 0.3949],\n",
      "        [ 0.4022],\n",
      "        [ 0.4163],\n",
      "        [ 0.4460],\n",
      "        [ 0.3159],\n",
      "        [ 0.3524],\n",
      "        [ 0.4385],\n",
      "        [ 0.4696],\n",
      "        [ 0.1671],\n",
      "        [ 0.1232],\n",
      "        [ 0.3998],\n",
      "        [ 0.2817],\n",
      "        [ 0.1554],\n",
      "        [ 0.3756],\n",
      "        [ 0.4443],\n",
      "        [ 0.5192],\n",
      "        [ 0.3920],\n",
      "        [ 0.4584],\n",
      "        [ 0.4058],\n",
      "        [ 0.4167],\n",
      "        [-0.1276],\n",
      "        [ 0.4543],\n",
      "        [ 0.4543],\n",
      "        [ 0.4543],\n",
      "        [ 0.4543],\n",
      "        [ 0.4543],\n",
      "        [ 0.4543],\n",
      "        [ 0.4543]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0013],\n",
      "        [    0.0113],\n",
      "        [    0.0075],\n",
      "        [    0.0023],\n",
      "        [    0.0026],\n",
      "        [    0.0066],\n",
      "        [    0.0165],\n",
      "        [    0.0090],\n",
      "        [    0.0156],\n",
      "        [    0.0025],\n",
      "        [    0.0181],\n",
      "        [    0.0215],\n",
      "        [    0.0032],\n",
      "        [    0.0014],\n",
      "        [    0.0077],\n",
      "        [    0.0002],\n",
      "        [    0.0211],\n",
      "        [    0.0188],\n",
      "        [    0.0272],\n",
      "        [    0.0146],\n",
      "        [    0.0012],\n",
      "        [    0.0085],\n",
      "        [    0.0047],\n",
      "        [    0.0254],\n",
      "        [    0.0068],\n",
      "        [    0.0233],\n",
      "        [    0.0214],\n",
      "        [    0.0086],\n",
      "        [    0.0670],\n",
      "        [    0.0130],\n",
      "        [    0.0384],\n",
      "        [    0.0851],\n",
      "        [    0.0196],\n",
      "        [    0.0169],\n",
      "        [    0.0905],\n",
      "        [    0.0100],\n",
      "        [    0.0208],\n",
      "        [    0.0163],\n",
      "        [    0.0387],\n",
      "        [    0.0332],\n",
      "        [    0.0317],\n",
      "        [    0.0354],\n",
      "        [    0.0232],\n",
      "        [    0.0161],\n",
      "        [    0.0286],\n",
      "        [    0.0377],\n",
      "        [    0.0335],\n",
      "        [    0.0151],\n",
      "        [    0.0410],\n",
      "        [    0.0518],\n",
      "        [    0.0324],\n",
      "        [    0.0305],\n",
      "        [    0.0615],\n",
      "        [    0.0373],\n",
      "        [    0.0420],\n",
      "        [    0.0346],\n",
      "        [    0.0438],\n",
      "        [    0.0517],\n",
      "        [    0.0558],\n",
      "        [    0.0499],\n",
      "        [    0.0652],\n",
      "        [    0.0263],\n",
      "        [    0.0599],\n",
      "        [    0.0434],\n",
      "        [    0.0361],\n",
      "        [    0.0432],\n",
      "        [    0.0494],\n",
      "        [    0.0600],\n",
      "        [    0.0523],\n",
      "        [    0.0703],\n",
      "        [    0.0271],\n",
      "        [    0.0877],\n",
      "        [    0.0739],\n",
      "        [    0.0694],\n",
      "        [    0.0684],\n",
      "        [    0.0177],\n",
      "        [    0.0637],\n",
      "        [    0.0892],\n",
      "        [    0.0926],\n",
      "        [    0.0255],\n",
      "        [    0.1003],\n",
      "        [    0.0846],\n",
      "        [    0.0089],\n",
      "        [    0.0193],\n",
      "        [    0.0949],\n",
      "        [    0.0937],\n",
      "        [    0.0135],\n",
      "        [    0.0836],\n",
      "        [    0.0954],\n",
      "        [    0.0830],\n",
      "        [    0.0915],\n",
      "        [    0.0060],\n",
      "        [    0.1057],\n",
      "        [    0.0048],\n",
      "        [    0.0900],\n",
      "        [    0.0073],\n",
      "        [    0.0177],\n",
      "        [    0.1049],\n",
      "        [    0.0103],\n",
      "        [    0.0271],\n",
      "        [    0.1327],\n",
      "        [    0.0122],\n",
      "        [    0.0282],\n",
      "        [    0.0963],\n",
      "        [    0.1295],\n",
      "        [    0.0222],\n",
      "        [    0.1206],\n",
      "        [    0.1148],\n",
      "        [    0.0155],\n",
      "        [    0.0157],\n",
      "        [    0.1521],\n",
      "        [    0.1232],\n",
      "        [    0.0438],\n",
      "        [    0.1374],\n",
      "        [    0.1473],\n",
      "        [    0.1202],\n",
      "        [    0.0255],\n",
      "        [    0.0503],\n",
      "        [    0.1646],\n",
      "        [    0.0477],\n",
      "        [    0.0778],\n",
      "        [    0.0836],\n",
      "        [    0.2463],\n",
      "        [    0.3039],\n",
      "        [    0.3180],\n",
      "        [    0.3209],\n",
      "        [    0.3228],\n",
      "        [    0.3238],\n",
      "        [    0.3333],\n",
      "        [    0.3351]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0044],\n",
      "        [0.0137],\n",
      "        [0.0026],\n",
      "        [0.0059],\n",
      "        [0.0086],\n",
      "        [0.0048],\n",
      "        [0.0210],\n",
      "        [0.0079],\n",
      "        [0.0193],\n",
      "        [0.0069],\n",
      "        [0.0207],\n",
      "        [0.0242],\n",
      "        [0.0079],\n",
      "        [0.0034],\n",
      "        [0.0014],\n",
      "        [0.0020],\n",
      "        [0.0223],\n",
      "        [0.0219],\n",
      "        [0.0300],\n",
      "        [0.0086],\n",
      "        [0.0011],\n",
      "        [0.0115],\n",
      "        [0.0021],\n",
      "        [0.0236],\n",
      "        [0.0114],\n",
      "        [0.0203],\n",
      "        [0.0226],\n",
      "        [0.0056],\n",
      "        [0.0624],\n",
      "        [0.0087],\n",
      "        [0.0419],\n",
      "        [0.0831],\n",
      "        [0.0132],\n",
      "        [0.0123],\n",
      "        [0.0876],\n",
      "        [0.0110],\n",
      "        [0.0165],\n",
      "        [0.0148],\n",
      "        [0.0419],\n",
      "        [0.0351],\n",
      "        [0.0342],\n",
      "        [0.0379],\n",
      "        [0.0218],\n",
      "        [0.0132],\n",
      "        [0.0244],\n",
      "        [0.0356],\n",
      "        [0.0290],\n",
      "        [0.0170],\n",
      "        [0.0371],\n",
      "        [0.0537],\n",
      "        [0.0320],\n",
      "        [0.0339],\n",
      "        [0.0593],\n",
      "        [0.0328],\n",
      "        [0.0362],\n",
      "        [0.0326],\n",
      "        [0.0381],\n",
      "        [0.0481],\n",
      "        [0.0565],\n",
      "        [0.0476],\n",
      "        [0.0687],\n",
      "        [0.0212],\n",
      "        [0.0587],\n",
      "        [0.0462],\n",
      "        [0.0383],\n",
      "        [0.0466],\n",
      "        [0.0471],\n",
      "        [0.0582],\n",
      "        [0.0500],\n",
      "        [0.0715],\n",
      "        [0.0307],\n",
      "        [0.0915],\n",
      "        [0.0691],\n",
      "        [0.0714],\n",
      "        [0.0744],\n",
      "        [0.0165],\n",
      "        [0.0664],\n",
      "        [0.0871],\n",
      "        [0.0908],\n",
      "        [0.0253],\n",
      "        [0.0985],\n",
      "        [0.0859],\n",
      "        [0.0074],\n",
      "        [0.0185],\n",
      "        [0.0886],\n",
      "        [0.0882],\n",
      "        [0.0129],\n",
      "        [0.0810],\n",
      "        [0.0965],\n",
      "        [0.0791],\n",
      "        [0.0917],\n",
      "        [0.0076],\n",
      "        [0.0995],\n",
      "        [0.0048],\n",
      "        [0.0923],\n",
      "        [0.0063],\n",
      "        [0.0195],\n",
      "        [0.1028],\n",
      "        [0.0096],\n",
      "        [0.0292],\n",
      "        [0.1305],\n",
      "        [0.0118],\n",
      "        [0.0302],\n",
      "        [0.1004],\n",
      "        [0.1270],\n",
      "        [0.0224],\n",
      "        [0.1218],\n",
      "        [0.1199],\n",
      "        [0.0163],\n",
      "        [0.0169],\n",
      "        [0.1552],\n",
      "        [0.1197],\n",
      "        [0.0460],\n",
      "        [0.1310],\n",
      "        [0.1493],\n",
      "        [0.1245],\n",
      "        [0.0261],\n",
      "        [0.0500],\n",
      "        [0.1617],\n",
      "        [0.0482],\n",
      "        [0.0778],\n",
      "        [0.0837],\n",
      "        [0.2344],\n",
      "        [0.3033],\n",
      "        [0.3173],\n",
      "        [0.3203],\n",
      "        [0.3222],\n",
      "        [0.3232],\n",
      "        [0.3327],\n",
      "        [0.3345]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 88.35826277732849\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 131\n",
      "剩餘X 資料 torch.Size([29, 18])\n",
      "剩餘Y 資料 torch.Size([29, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11267734318971634, 28)\n",
      "The second_loss value of k: (0.11767532676458359, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引28，y= tensor([0.1180])\n",
      "目前模型的Data狀態 torch.Size([131, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1288],\n",
      "        [ 0.3443],\n",
      "        [ 0.1112],\n",
      "        [ 0.6274],\n",
      "        [ 0.3011],\n",
      "        [ 0.2670],\n",
      "        [ 0.6229],\n",
      "        [ 0.3622],\n",
      "        [ 0.1985],\n",
      "        [ 0.6217],\n",
      "        [ 0.1413],\n",
      "        [ 0.3336],\n",
      "        [ 0.6087],\n",
      "        [ 0.6036],\n",
      "        [ 0.1340],\n",
      "        [ 0.1820],\n",
      "        [ 0.2113],\n",
      "        [ 0.1702],\n",
      "        [ 0.6130],\n",
      "        [ 0.1282],\n",
      "        [ 0.4393],\n",
      "        [ 0.5251],\n",
      "        [ 0.4761],\n",
      "        [ 0.2911],\n",
      "        [ 0.6191],\n",
      "        [ 0.5609],\n",
      "        [ 0.3408],\n",
      "        [ 0.6272],\n",
      "        [ 0.4766],\n",
      "        [ 0.6364],\n",
      "        [ 0.6349],\n",
      "        [ 0.4953],\n",
      "        [ 0.1514],\n",
      "        [ 0.5734],\n",
      "        [ 0.4873],\n",
      "        [ 0.3535],\n",
      "        [ 0.6072],\n",
      "        [ 0.2065],\n",
      "        [ 0.2652],\n",
      "        [ 0.3234],\n",
      "        [ 0.1460],\n",
      "        [ 0.3208],\n",
      "        [ 0.1757],\n",
      "        [ 0.2024],\n",
      "        [ 0.5802],\n",
      "        [ 0.3066],\n",
      "        [ 0.5880],\n",
      "        [ 0.4549],\n",
      "        [ 0.5851],\n",
      "        [ 0.2009],\n",
      "        [ 0.3591],\n",
      "        [ 0.4370],\n",
      "        [ 0.4788],\n",
      "        [ 0.6400],\n",
      "        [ 0.1270],\n",
      "        [ 0.2134],\n",
      "        [ 0.1312],\n",
      "        [ 0.5746],\n",
      "        [ 0.5440],\n",
      "        [ 0.3729],\n",
      "        [ 0.6030],\n",
      "        [ 0.6019],\n",
      "        [ 0.5179],\n",
      "        [ 0.4878],\n",
      "        [ 0.4263],\n",
      "        [ 0.4406],\n",
      "        [ 0.2347],\n",
      "        [ 0.2783],\n",
      "        [ 0.2091],\n",
      "        [ 0.1896],\n",
      "        [ 0.4523],\n",
      "        [ 0.2163],\n",
      "        [ 0.1597],\n",
      "        [ 0.2196],\n",
      "        [ 0.3347],\n",
      "        [ 0.4429],\n",
      "        [ 0.4566],\n",
      "        [ 0.3520],\n",
      "        [ 0.4049],\n",
      "        [ 0.5248],\n",
      "        [ 0.4099],\n",
      "        [ 0.3130],\n",
      "        [ 0.4529],\n",
      "        [ 0.4751],\n",
      "        [ 0.2069],\n",
      "        [ 0.1691],\n",
      "        [ 0.5055],\n",
      "        [ 0.2437],\n",
      "        [ 0.3557],\n",
      "        [ 0.1061],\n",
      "        [ 0.3593],\n",
      "        [ 0.4249],\n",
      "        [ 0.2751],\n",
      "        [ 0.5298],\n",
      "        [ 0.4407],\n",
      "        [ 0.4946],\n",
      "        [ 0.4188],\n",
      "        [ 0.2937],\n",
      "        [ 0.4821],\n",
      "        [ 0.4517],\n",
      "        [ 0.4568],\n",
      "        [ 0.5295],\n",
      "        [ 0.3929],\n",
      "        [ 0.3981],\n",
      "        [ 0.4138],\n",
      "        [ 0.4458],\n",
      "        [ 0.3147],\n",
      "        [ 0.3473],\n",
      "        [ 0.4376],\n",
      "        [ 0.4684],\n",
      "        [ 0.1641],\n",
      "        [ 0.1197],\n",
      "        [ 0.3976],\n",
      "        [ 0.2753],\n",
      "        [ 0.1533],\n",
      "        [ 0.3712],\n",
      "        [ 0.4438],\n",
      "        [ 0.5196],\n",
      "        [ 0.3891],\n",
      "        [ 0.4579],\n",
      "        [ 0.4058],\n",
      "        [ 0.4167],\n",
      "        [-0.1157],\n",
      "        [ 0.4537],\n",
      "        [ 0.4537],\n",
      "        [ 0.4537],\n",
      "        [ 0.4537],\n",
      "        [ 0.4537],\n",
      "        [ 0.4537],\n",
      "        [ 0.4537],\n",
      "        [ 0.4537]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0044],\n",
      "        [0.0137],\n",
      "        [0.0026],\n",
      "        [0.0059],\n",
      "        [0.0086],\n",
      "        [0.0048],\n",
      "        [0.0210],\n",
      "        [0.0079],\n",
      "        [0.0193],\n",
      "        [0.0069],\n",
      "        [0.0207],\n",
      "        [0.0242],\n",
      "        [0.0079],\n",
      "        [0.0034],\n",
      "        [0.0014],\n",
      "        [0.0020],\n",
      "        [0.0223],\n",
      "        [0.0219],\n",
      "        [0.0300],\n",
      "        [0.0086],\n",
      "        [0.0011],\n",
      "        [0.0115],\n",
      "        [0.0021],\n",
      "        [0.0236],\n",
      "        [0.0114],\n",
      "        [0.0203],\n",
      "        [0.0226],\n",
      "        [0.0056],\n",
      "        [0.0624],\n",
      "        [0.0087],\n",
      "        [0.0419],\n",
      "        [0.0831],\n",
      "        [0.0132],\n",
      "        [0.0123],\n",
      "        [0.0876],\n",
      "        [0.0110],\n",
      "        [0.0165],\n",
      "        [0.0148],\n",
      "        [0.0419],\n",
      "        [0.0351],\n",
      "        [0.0342],\n",
      "        [0.0379],\n",
      "        [0.0218],\n",
      "        [0.0132],\n",
      "        [0.0244],\n",
      "        [0.0356],\n",
      "        [0.0290],\n",
      "        [0.0170],\n",
      "        [0.0371],\n",
      "        [0.0537],\n",
      "        [0.0320],\n",
      "        [0.0339],\n",
      "        [0.0593],\n",
      "        [0.0328],\n",
      "        [0.0362],\n",
      "        [0.0326],\n",
      "        [0.0381],\n",
      "        [0.0481],\n",
      "        [0.0565],\n",
      "        [0.0476],\n",
      "        [0.0687],\n",
      "        [0.0212],\n",
      "        [0.0587],\n",
      "        [0.0462],\n",
      "        [0.0383],\n",
      "        [0.0466],\n",
      "        [0.0471],\n",
      "        [0.0582],\n",
      "        [0.0500],\n",
      "        [0.0715],\n",
      "        [0.0307],\n",
      "        [0.0915],\n",
      "        [0.0691],\n",
      "        [0.0714],\n",
      "        [0.0744],\n",
      "        [0.0165],\n",
      "        [0.0664],\n",
      "        [0.0871],\n",
      "        [0.0908],\n",
      "        [0.0253],\n",
      "        [0.0985],\n",
      "        [0.0859],\n",
      "        [0.0074],\n",
      "        [0.0185],\n",
      "        [0.0886],\n",
      "        [0.0882],\n",
      "        [0.0129],\n",
      "        [0.0810],\n",
      "        [0.0965],\n",
      "        [0.0791],\n",
      "        [0.0917],\n",
      "        [0.0076],\n",
      "        [0.0995],\n",
      "        [0.0048],\n",
      "        [0.0923],\n",
      "        [0.0063],\n",
      "        [0.0195],\n",
      "        [0.1028],\n",
      "        [0.0096],\n",
      "        [0.0292],\n",
      "        [0.1305],\n",
      "        [0.0118],\n",
      "        [0.0302],\n",
      "        [0.1004],\n",
      "        [0.1270],\n",
      "        [0.0224],\n",
      "        [0.1218],\n",
      "        [0.1199],\n",
      "        [0.0163],\n",
      "        [0.0169],\n",
      "        [0.1552],\n",
      "        [0.1197],\n",
      "        [0.0460],\n",
      "        [0.1310],\n",
      "        [0.1493],\n",
      "        [0.1245],\n",
      "        [0.0261],\n",
      "        [0.0500],\n",
      "        [0.1617],\n",
      "        [0.0482],\n",
      "        [0.0778],\n",
      "        [0.0837],\n",
      "        [0.2344],\n",
      "        [0.3033],\n",
      "        [0.3173],\n",
      "        [0.3203],\n",
      "        [0.3222],\n",
      "        [0.3232],\n",
      "        [0.3327],\n",
      "        [0.3345],\n",
      "        [0.3357]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0049],\n",
      "        [    0.0114],\n",
      "        [    0.0025],\n",
      "        [    0.0019],\n",
      "        [    0.0094],\n",
      "        [    0.0075],\n",
      "        [    0.0174],\n",
      "        [    0.0108],\n",
      "        [    0.0186],\n",
      "        [    0.0035],\n",
      "        [    0.0191],\n",
      "        [    0.0222],\n",
      "        [    0.0043],\n",
      "        [    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0193],\n",
      "        [    0.0210],\n",
      "        [    0.0248],\n",
      "        [    0.0078],\n",
      "        [    0.0023],\n",
      "        [    0.0077],\n",
      "        [    0.0053],\n",
      "        [    0.0270],\n",
      "        [    0.0079],\n",
      "        [    0.0243],\n",
      "        [    0.0192],\n",
      "        [    0.0105],\n",
      "        [    0.0625],\n",
      "        [    0.0122],\n",
      "        [    0.0371],\n",
      "        [    0.0856],\n",
      "        [    0.0118],\n",
      "        [    0.0151],\n",
      "        [    0.0896],\n",
      "        [    0.0079],\n",
      "        [    0.0199],\n",
      "        [    0.0176],\n",
      "        [    0.0406],\n",
      "        [    0.0324],\n",
      "        [    0.0327],\n",
      "        [    0.0357],\n",
      "        [    0.0189],\n",
      "        [    0.0147],\n",
      "        [    0.0275],\n",
      "        [    0.0382],\n",
      "        [    0.0319],\n",
      "        [    0.0136],\n",
      "        [    0.0405],\n",
      "        [    0.0514],\n",
      "        [    0.0274],\n",
      "        [    0.0311],\n",
      "        [    0.0617],\n",
      "        [    0.0360],\n",
      "        [    0.0355],\n",
      "        [    0.0353],\n",
      "        [    0.0376],\n",
      "        [    0.0517],\n",
      "        [    0.0616],\n",
      "        [    0.0502],\n",
      "        [    0.0640],\n",
      "        [    0.0242],\n",
      "        [    0.0619],\n",
      "        [    0.0424],\n",
      "        [    0.0350],\n",
      "        [    0.0441],\n",
      "        [    0.0496],\n",
      "        [    0.0611],\n",
      "        [    0.0521],\n",
      "        [    0.0742],\n",
      "        [    0.0298],\n",
      "        [    0.0905],\n",
      "        [    0.0693],\n",
      "        [    0.0683],\n",
      "        [    0.0751],\n",
      "        [    0.0200],\n",
      "        [    0.0628],\n",
      "        [    0.0904],\n",
      "        [    0.0933],\n",
      "        [    0.0295],\n",
      "        [    0.1012],\n",
      "        [    0.0830],\n",
      "        [    0.0105],\n",
      "        [    0.0221],\n",
      "        [    0.0873],\n",
      "        [    0.0877],\n",
      "        [    0.0170],\n",
      "        [    0.0831],\n",
      "        [    0.0931],\n",
      "        [    0.0797],\n",
      "        [    0.0878],\n",
      "        [    0.0046],\n",
      "        [    0.0985],\n",
      "        [    0.0094],\n",
      "        [    0.0883],\n",
      "        [    0.0097],\n",
      "        [    0.0169],\n",
      "        [    0.1056],\n",
      "        [    0.0043],\n",
      "        [    0.0270],\n",
      "        [    0.1339],\n",
      "        [    0.0068],\n",
      "        [    0.0277],\n",
      "        [    0.0996],\n",
      "        [    0.1301],\n",
      "        [    0.0180],\n",
      "        [    0.1188],\n",
      "        [    0.1200],\n",
      "        [    0.0125],\n",
      "        [    0.0137],\n",
      "        [    0.1534],\n",
      "        [    0.1205],\n",
      "        [    0.0436],\n",
      "        [    0.1299],\n",
      "        [    0.1464],\n",
      "        [    0.1239],\n",
      "        [    0.0221],\n",
      "        [    0.0453],\n",
      "        [    0.1645],\n",
      "        [    0.0442],\n",
      "        [    0.0731],\n",
      "        [    0.0788],\n",
      "        [    0.2177],\n",
      "        [    0.3028],\n",
      "        [    0.3168],\n",
      "        [    0.3198],\n",
      "        [    0.3216],\n",
      "        [    0.3227],\n",
      "        [    0.3321],\n",
      "        [    0.3339],\n",
      "        [    0.3351]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 88.59568452835083\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 132\n",
      "剩餘X 資料 torch.Size([28, 18])\n",
      "剩餘Y 資料 torch.Size([28, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11728830635547638, 15)\n",
      "The second_loss value of k: (0.1185256838798523, 18)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.1107])\n",
      "目前模型的Data狀態 torch.Size([132, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1282],\n",
      "        [ 0.3466],\n",
      "        [ 0.1111],\n",
      "        [ 0.6314],\n",
      "        [ 0.3003],\n",
      "        [ 0.2697],\n",
      "        [ 0.6265],\n",
      "        [ 0.3651],\n",
      "        [ 0.1992],\n",
      "        [ 0.6251],\n",
      "        [ 0.1430],\n",
      "        [ 0.3357],\n",
      "        [ 0.6122],\n",
      "        [ 0.6069],\n",
      "        [ 0.1329],\n",
      "        [ 0.1842],\n",
      "        [ 0.2143],\n",
      "        [ 0.1711],\n",
      "        [ 0.6182],\n",
      "        [ 0.1274],\n",
      "        [ 0.4428],\n",
      "        [ 0.5289],\n",
      "        [ 0.4793],\n",
      "        [ 0.2945],\n",
      "        [ 0.6226],\n",
      "        [ 0.5649],\n",
      "        [ 0.3443],\n",
      "        [ 0.6321],\n",
      "        [ 0.4767],\n",
      "        [ 0.6400],\n",
      "        [ 0.6396],\n",
      "        [ 0.4977],\n",
      "        [ 0.1500],\n",
      "        [ 0.5761],\n",
      "        [ 0.4892],\n",
      "        [ 0.3565],\n",
      "        [ 0.6105],\n",
      "        [ 0.2093],\n",
      "        [ 0.2665],\n",
      "        [ 0.3262],\n",
      "        [ 0.1476],\n",
      "        [ 0.3230],\n",
      "        [ 0.1786],\n",
      "        [ 0.2039],\n",
      "        [ 0.5833],\n",
      "        [ 0.3092],\n",
      "        [ 0.5909],\n",
      "        [ 0.4583],\n",
      "        [ 0.5885],\n",
      "        [ 0.2032],\n",
      "        [ 0.3637],\n",
      "        [ 0.4398],\n",
      "        [ 0.4812],\n",
      "        [ 0.6432],\n",
      "        [ 0.1263],\n",
      "        [ 0.2161],\n",
      "        [ 0.1307],\n",
      "        [ 0.5781],\n",
      "        [ 0.5491],\n",
      "        [ 0.3754],\n",
      "        [ 0.6077],\n",
      "        [ 0.6049],\n",
      "        [ 0.5211],\n",
      "        [ 0.4916],\n",
      "        [ 0.4296],\n",
      "        [ 0.4431],\n",
      "        [ 0.2372],\n",
      "        [ 0.2812],\n",
      "        [ 0.2112],\n",
      "        [ 0.1923],\n",
      "        [ 0.4533],\n",
      "        [ 0.2173],\n",
      "        [ 0.1599],\n",
      "        [ 0.2227],\n",
      "        [ 0.3340],\n",
      "        [ 0.4463],\n",
      "        [ 0.4603],\n",
      "        [ 0.3553],\n",
      "        [ 0.4074],\n",
      "        [ 0.5290],\n",
      "        [ 0.4127],\n",
      "        [ 0.3159],\n",
      "        [ 0.4559],\n",
      "        [ 0.4787],\n",
      "        [ 0.2056],\n",
      "        [ 0.1686],\n",
      "        [ 0.5095],\n",
      "        [ 0.2459],\n",
      "        [ 0.3591],\n",
      "        [ 0.1067],\n",
      "        [ 0.3632],\n",
      "        [ 0.4280],\n",
      "        [ 0.2741],\n",
      "        [ 0.5345],\n",
      "        [ 0.4446],\n",
      "        [ 0.4980],\n",
      "        [ 0.4214],\n",
      "        [ 0.2965],\n",
      "        [ 0.4874],\n",
      "        [ 0.4538],\n",
      "        [ 0.4602],\n",
      "        [ 0.5345],\n",
      "        [ 0.3954],\n",
      "        [ 0.3989],\n",
      "        [ 0.4168],\n",
      "        [ 0.4502],\n",
      "        [ 0.3177],\n",
      "        [ 0.3472],\n",
      "        [ 0.4414],\n",
      "        [ 0.4716],\n",
      "        [ 0.1658],\n",
      "        [ 0.1205],\n",
      "        [ 0.3999],\n",
      "        [ 0.2742],\n",
      "        [ 0.1562],\n",
      "        [ 0.3719],\n",
      "        [ 0.4478],\n",
      "        [ 0.5242],\n",
      "        [ 0.3919],\n",
      "        [ 0.4619],\n",
      "        [ 0.4104],\n",
      "        [ 0.4215],\n",
      "        [-0.0989],\n",
      "        [ 0.4531],\n",
      "        [ 0.4531],\n",
      "        [ 0.4531],\n",
      "        [ 0.4531],\n",
      "        [ 0.4531],\n",
      "        [ 0.4531],\n",
      "        [ 0.4531],\n",
      "        [ 0.4531],\n",
      "        [ 0.4531]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0049],\n",
      "        [    0.0114],\n",
      "        [    0.0025],\n",
      "        [    0.0019],\n",
      "        [    0.0094],\n",
      "        [    0.0075],\n",
      "        [    0.0174],\n",
      "        [    0.0108],\n",
      "        [    0.0186],\n",
      "        [    0.0035],\n",
      "        [    0.0191],\n",
      "        [    0.0222],\n",
      "        [    0.0043],\n",
      "        [    0.0001],\n",
      "        [    0.0003],\n",
      "        [    0.0002],\n",
      "        [    0.0193],\n",
      "        [    0.0210],\n",
      "        [    0.0248],\n",
      "        [    0.0078],\n",
      "        [    0.0023],\n",
      "        [    0.0077],\n",
      "        [    0.0053],\n",
      "        [    0.0270],\n",
      "        [    0.0079],\n",
      "        [    0.0243],\n",
      "        [    0.0192],\n",
      "        [    0.0105],\n",
      "        [    0.0625],\n",
      "        [    0.0122],\n",
      "        [    0.0371],\n",
      "        [    0.0856],\n",
      "        [    0.0118],\n",
      "        [    0.0151],\n",
      "        [    0.0896],\n",
      "        [    0.0079],\n",
      "        [    0.0199],\n",
      "        [    0.0176],\n",
      "        [    0.0406],\n",
      "        [    0.0324],\n",
      "        [    0.0327],\n",
      "        [    0.0357],\n",
      "        [    0.0189],\n",
      "        [    0.0147],\n",
      "        [    0.0275],\n",
      "        [    0.0382],\n",
      "        [    0.0319],\n",
      "        [    0.0136],\n",
      "        [    0.0405],\n",
      "        [    0.0514],\n",
      "        [    0.0274],\n",
      "        [    0.0311],\n",
      "        [    0.0617],\n",
      "        [    0.0360],\n",
      "        [    0.0355],\n",
      "        [    0.0353],\n",
      "        [    0.0376],\n",
      "        [    0.0517],\n",
      "        [    0.0616],\n",
      "        [    0.0502],\n",
      "        [    0.0640],\n",
      "        [    0.0242],\n",
      "        [    0.0619],\n",
      "        [    0.0424],\n",
      "        [    0.0350],\n",
      "        [    0.0441],\n",
      "        [    0.0496],\n",
      "        [    0.0611],\n",
      "        [    0.0521],\n",
      "        [    0.0742],\n",
      "        [    0.0298],\n",
      "        [    0.0905],\n",
      "        [    0.0693],\n",
      "        [    0.0683],\n",
      "        [    0.0751],\n",
      "        [    0.0200],\n",
      "        [    0.0628],\n",
      "        [    0.0904],\n",
      "        [    0.0933],\n",
      "        [    0.0295],\n",
      "        [    0.1012],\n",
      "        [    0.0830],\n",
      "        [    0.0105],\n",
      "        [    0.0221],\n",
      "        [    0.0873],\n",
      "        [    0.0877],\n",
      "        [    0.0170],\n",
      "        [    0.0831],\n",
      "        [    0.0931],\n",
      "        [    0.0797],\n",
      "        [    0.0878],\n",
      "        [    0.0046],\n",
      "        [    0.0985],\n",
      "        [    0.0094],\n",
      "        [    0.0883],\n",
      "        [    0.0097],\n",
      "        [    0.0169],\n",
      "        [    0.1056],\n",
      "        [    0.0043],\n",
      "        [    0.0270],\n",
      "        [    0.1339],\n",
      "        [    0.0068],\n",
      "        [    0.0277],\n",
      "        [    0.0996],\n",
      "        [    0.1301],\n",
      "        [    0.0180],\n",
      "        [    0.1188],\n",
      "        [    0.1200],\n",
      "        [    0.0125],\n",
      "        [    0.0137],\n",
      "        [    0.1534],\n",
      "        [    0.1205],\n",
      "        [    0.0436],\n",
      "        [    0.1299],\n",
      "        [    0.1464],\n",
      "        [    0.1239],\n",
      "        [    0.0221],\n",
      "        [    0.0453],\n",
      "        [    0.1645],\n",
      "        [    0.0442],\n",
      "        [    0.0731],\n",
      "        [    0.0788],\n",
      "        [    0.2177],\n",
      "        [    0.3028],\n",
      "        [    0.3168],\n",
      "        [    0.3198],\n",
      "        [    0.3216],\n",
      "        [    0.3227],\n",
      "        [    0.3321],\n",
      "        [    0.3339],\n",
      "        [    0.3351],\n",
      "        [    0.3425]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0069],\n",
      "        [0.0130],\n",
      "        [0.0007],\n",
      "        [0.0057],\n",
      "        [0.0124],\n",
      "        [0.0047],\n",
      "        [0.0221],\n",
      "        [0.0077],\n",
      "        [0.0211],\n",
      "        [0.0081],\n",
      "        [0.0204],\n",
      "        [0.0236],\n",
      "        [0.0091],\n",
      "        [0.0052],\n",
      "        [0.0024],\n",
      "        [0.0016],\n",
      "        [0.0197],\n",
      "        [0.0234],\n",
      "        [0.0269],\n",
      "        [0.0054],\n",
      "        [0.0013],\n",
      "        [0.0115],\n",
      "        [0.0008],\n",
      "        [0.0252],\n",
      "        [0.0119],\n",
      "        [0.0205],\n",
      "        [0.0201],\n",
      "        [0.0079],\n",
      "        [0.0579],\n",
      "        [0.0075],\n",
      "        [0.0398],\n",
      "        [0.0805],\n",
      "        [0.0087],\n",
      "        [0.0097],\n",
      "        [0.0860],\n",
      "        [0.0106],\n",
      "        [0.0149],\n",
      "        [0.0168],\n",
      "        [0.0422],\n",
      "        [0.0337],\n",
      "        [0.0343],\n",
      "        [0.0375],\n",
      "        [0.0187],\n",
      "        [0.0134],\n",
      "        [0.0223],\n",
      "        [0.0350],\n",
      "        [0.0266],\n",
      "        [0.0177],\n",
      "        [0.0356],\n",
      "        [0.0523],\n",
      "        [0.0273],\n",
      "        [0.0357],\n",
      "        [0.0565],\n",
      "        [0.0310],\n",
      "        [0.0332],\n",
      "        [0.0336],\n",
      "        [0.0355],\n",
      "        [0.0472],\n",
      "        [0.0606],\n",
      "        [0.0464],\n",
      "        [0.0665],\n",
      "        [0.0194],\n",
      "        [0.0575],\n",
      "        [0.0462],\n",
      "        [0.0390],\n",
      "        [0.0491],\n",
      "        [0.0476],\n",
      "        [0.0589],\n",
      "        [0.0513],\n",
      "        [0.0743],\n",
      "        [0.0326],\n",
      "        [0.0926],\n",
      "        [0.0678],\n",
      "        [0.0699],\n",
      "        [0.0784],\n",
      "        [0.0174],\n",
      "        [0.0665],\n",
      "        [0.0880],\n",
      "        [0.0893],\n",
      "        [0.0262],\n",
      "        [0.0973],\n",
      "        [0.0856],\n",
      "        [0.0064],\n",
      "        [0.0184],\n",
      "        [0.0842],\n",
      "        [0.0854],\n",
      "        [0.0139],\n",
      "        [0.0807],\n",
      "        [0.0948],\n",
      "        [0.0775],\n",
      "        [0.0886],\n",
      "        [0.0083],\n",
      "        [0.0953],\n",
      "        [0.0070],\n",
      "        [0.0919],\n",
      "        [0.0056],\n",
      "        [0.0208],\n",
      "        [0.1035],\n",
      "        [0.0050],\n",
      "        [0.0313],\n",
      "        [0.1306],\n",
      "        [0.0084],\n",
      "        [0.0318],\n",
      "        [0.1017],\n",
      "        [0.1269],\n",
      "        [0.0199],\n",
      "        [0.1210],\n",
      "        [0.1227],\n",
      "        [0.0159],\n",
      "        [0.0182],\n",
      "        [0.1549],\n",
      "        [0.1185],\n",
      "        [0.0480],\n",
      "        [0.1268],\n",
      "        [0.1472],\n",
      "        [0.1257],\n",
      "        [0.0255],\n",
      "        [0.0470],\n",
      "        [0.1614],\n",
      "        [0.0479],\n",
      "        [0.0748],\n",
      "        [0.0802],\n",
      "        [0.2027],\n",
      "        [0.3022],\n",
      "        [0.3162],\n",
      "        [0.3192],\n",
      "        [0.3211],\n",
      "        [0.3221],\n",
      "        [0.3316],\n",
      "        [0.3334],\n",
      "        [0.3345],\n",
      "        [0.3419]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 88.84822297096252\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 133\n",
      "剩餘X 資料 torch.Size([27, 18])\n",
      "剩餘Y 資料 torch.Size([27, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11813756823539734, 17)\n",
      "The second_loss value of k: (0.11956897377967834, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引17，y= tensor([0.1089])\n",
      "目前模型的Data狀態 torch.Size([133, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1263],\n",
      "        [ 0.3450],\n",
      "        [ 0.1093],\n",
      "        [ 0.6276],\n",
      "        [ 0.2973],\n",
      "        [ 0.2669],\n",
      "        [ 0.6219],\n",
      "        [ 0.3619],\n",
      "        [ 0.1967],\n",
      "        [ 0.6205],\n",
      "        [ 0.1416],\n",
      "        [ 0.3343],\n",
      "        [ 0.6075],\n",
      "        [ 0.6017],\n",
      "        [ 0.1302],\n",
      "        [ 0.1825],\n",
      "        [ 0.2139],\n",
      "        [ 0.1687],\n",
      "        [ 0.6161],\n",
      "        [ 0.1250],\n",
      "        [ 0.4392],\n",
      "        [ 0.5251],\n",
      "        [ 0.4747],\n",
      "        [ 0.2927],\n",
      "        [ 0.6186],\n",
      "        [ 0.5611],\n",
      "        [ 0.3434],\n",
      "        [ 0.6295],\n",
      "        [ 0.4720],\n",
      "        [ 0.6353],\n",
      "        [ 0.6369],\n",
      "        [ 0.4927],\n",
      "        [ 0.1470],\n",
      "        [ 0.5708],\n",
      "        [ 0.4856],\n",
      "        [ 0.3538],\n",
      "        [ 0.6055],\n",
      "        [ 0.2085],\n",
      "        [ 0.2648],\n",
      "        [ 0.3249],\n",
      "        [ 0.1459],\n",
      "        [ 0.3212],\n",
      "        [ 0.1789],\n",
      "        [ 0.2025],\n",
      "        [ 0.5781],\n",
      "        [ 0.3059],\n",
      "        [ 0.5856],\n",
      "        [ 0.4542],\n",
      "        [ 0.5835],\n",
      "        [ 0.2023],\n",
      "        [ 0.3638],\n",
      "        [ 0.4353],\n",
      "        [ 0.4761],\n",
      "        [ 0.6382],\n",
      "        [ 0.1239],\n",
      "        [ 0.2144],\n",
      "        [ 0.1286],\n",
      "        [ 0.5736],\n",
      "        [ 0.5481],\n",
      "        [ 0.3717],\n",
      "        [ 0.6052],\n",
      "        [ 0.6001],\n",
      "        [ 0.5168],\n",
      "        [ 0.4878],\n",
      "        [ 0.4255],\n",
      "        [ 0.4381],\n",
      "        [ 0.2353],\n",
      "        [ 0.2790],\n",
      "        [ 0.2104],\n",
      "        [ 0.1924],\n",
      "        [ 0.4505],\n",
      "        [ 0.2152],\n",
      "        [ 0.1584],\n",
      "        [ 0.2210],\n",
      "        [ 0.3308],\n",
      "        [ 0.4438],\n",
      "        [ 0.4565],\n",
      "        [ 0.3529],\n",
      "        [ 0.4035],\n",
      "        [ 0.5257],\n",
      "        [ 0.4088],\n",
      "        [ 0.3134],\n",
      "        [ 0.4519],\n",
      "        [ 0.4751],\n",
      "        [ 0.2024],\n",
      "        [ 0.1663],\n",
      "        [ 0.5064],\n",
      "        [ 0.2434],\n",
      "        [ 0.3573],\n",
      "        [ 0.1046],\n",
      "        [ 0.3624],\n",
      "        [ 0.4242],\n",
      "        [ 0.2709],\n",
      "        [ 0.5321],\n",
      "        [ 0.4411],\n",
      "        [ 0.4938],\n",
      "        [ 0.4175],\n",
      "        [ 0.2943],\n",
      "        [ 0.4866],\n",
      "        [ 0.4496],\n",
      "        [ 0.4569],\n",
      "        [ 0.5328],\n",
      "        [ 0.3913],\n",
      "        [ 0.3968],\n",
      "        [ 0.4137],\n",
      "        [ 0.4483],\n",
      "        [ 0.3155],\n",
      "        [ 0.3445],\n",
      "        [ 0.4380],\n",
      "        [ 0.4671],\n",
      "        [ 0.1644],\n",
      "        [ 0.1185],\n",
      "        [ 0.3956],\n",
      "        [ 0.2711],\n",
      "        [ 0.1555],\n",
      "        [ 0.3700],\n",
      "        [ 0.4443],\n",
      "        [ 0.5226],\n",
      "        [ 0.3888],\n",
      "        [ 0.4582],\n",
      "        [ 0.4088],\n",
      "        [ 0.4202],\n",
      "        [-0.0839],\n",
      "        [ 0.4526],\n",
      "        [ 0.4526],\n",
      "        [ 0.4526],\n",
      "        [ 0.4526],\n",
      "        [ 0.4526],\n",
      "        [ 0.4526],\n",
      "        [ 0.4526],\n",
      "        [ 0.4526],\n",
      "        [ 0.4526],\n",
      "        [ 0.4526]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0069],\n",
      "        [0.0130],\n",
      "        [0.0007],\n",
      "        [0.0057],\n",
      "        [0.0124],\n",
      "        [0.0047],\n",
      "        [0.0221],\n",
      "        [0.0077],\n",
      "        [0.0211],\n",
      "        [0.0081],\n",
      "        [0.0204],\n",
      "        [0.0236],\n",
      "        [0.0091],\n",
      "        [0.0052],\n",
      "        [0.0024],\n",
      "        [0.0016],\n",
      "        [0.0197],\n",
      "        [0.0234],\n",
      "        [0.0269],\n",
      "        [0.0054],\n",
      "        [0.0013],\n",
      "        [0.0115],\n",
      "        [0.0008],\n",
      "        [0.0252],\n",
      "        [0.0119],\n",
      "        [0.0205],\n",
      "        [0.0201],\n",
      "        [0.0079],\n",
      "        [0.0579],\n",
      "        [0.0075],\n",
      "        [0.0398],\n",
      "        [0.0805],\n",
      "        [0.0087],\n",
      "        [0.0097],\n",
      "        [0.0860],\n",
      "        [0.0106],\n",
      "        [0.0149],\n",
      "        [0.0168],\n",
      "        [0.0422],\n",
      "        [0.0337],\n",
      "        [0.0343],\n",
      "        [0.0375],\n",
      "        [0.0187],\n",
      "        [0.0134],\n",
      "        [0.0223],\n",
      "        [0.0350],\n",
      "        [0.0266],\n",
      "        [0.0177],\n",
      "        [0.0356],\n",
      "        [0.0523],\n",
      "        [0.0273],\n",
      "        [0.0357],\n",
      "        [0.0565],\n",
      "        [0.0310],\n",
      "        [0.0332],\n",
      "        [0.0336],\n",
      "        [0.0355],\n",
      "        [0.0472],\n",
      "        [0.0606],\n",
      "        [0.0464],\n",
      "        [0.0665],\n",
      "        [0.0194],\n",
      "        [0.0575],\n",
      "        [0.0462],\n",
      "        [0.0390],\n",
      "        [0.0491],\n",
      "        [0.0476],\n",
      "        [0.0589],\n",
      "        [0.0513],\n",
      "        [0.0743],\n",
      "        [0.0326],\n",
      "        [0.0926],\n",
      "        [0.0678],\n",
      "        [0.0699],\n",
      "        [0.0784],\n",
      "        [0.0174],\n",
      "        [0.0665],\n",
      "        [0.0880],\n",
      "        [0.0893],\n",
      "        [0.0262],\n",
      "        [0.0973],\n",
      "        [0.0856],\n",
      "        [0.0064],\n",
      "        [0.0184],\n",
      "        [0.0842],\n",
      "        [0.0854],\n",
      "        [0.0139],\n",
      "        [0.0807],\n",
      "        [0.0948],\n",
      "        [0.0775],\n",
      "        [0.0886],\n",
      "        [0.0083],\n",
      "        [0.0953],\n",
      "        [0.0070],\n",
      "        [0.0919],\n",
      "        [0.0056],\n",
      "        [0.0208],\n",
      "        [0.1035],\n",
      "        [0.0050],\n",
      "        [0.0313],\n",
      "        [0.1306],\n",
      "        [0.0084],\n",
      "        [0.0318],\n",
      "        [0.1017],\n",
      "        [0.1269],\n",
      "        [0.0199],\n",
      "        [0.1210],\n",
      "        [0.1227],\n",
      "        [0.0159],\n",
      "        [0.0182],\n",
      "        [0.1549],\n",
      "        [0.1185],\n",
      "        [0.0480],\n",
      "        [0.1268],\n",
      "        [0.1472],\n",
      "        [0.1257],\n",
      "        [0.0255],\n",
      "        [0.0470],\n",
      "        [0.1614],\n",
      "        [0.0479],\n",
      "        [0.0748],\n",
      "        [0.0802],\n",
      "        [0.2027],\n",
      "        [0.3022],\n",
      "        [0.3162],\n",
      "        [0.3192],\n",
      "        [0.3211],\n",
      "        [0.3221],\n",
      "        [0.3316],\n",
      "        [0.3334],\n",
      "        [0.3345],\n",
      "        [0.3419],\n",
      "        [0.3437]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0062],\n",
      "        [    0.0109],\n",
      "        [    0.0016],\n",
      "        [    0.0030],\n",
      "        [    0.0121],\n",
      "        [    0.0064],\n",
      "        [    0.0197],\n",
      "        [    0.0094],\n",
      "        [    0.0207],\n",
      "        [    0.0060],\n",
      "        [    0.0192],\n",
      "        [    0.0215],\n",
      "        [    0.0069],\n",
      "        [    0.0033],\n",
      "        [    0.0022],\n",
      "        [    0.0001],\n",
      "        [    0.0172],\n",
      "        [    0.0228],\n",
      "        [    0.0231],\n",
      "        [    0.0058],\n",
      "        [    0.0010],\n",
      "        [    0.0090],\n",
      "        [    0.0027],\n",
      "        [    0.0277],\n",
      "        [    0.0096],\n",
      "        [    0.0232],\n",
      "        [    0.0173],\n",
      "        [    0.0114],\n",
      "        [    0.0582],\n",
      "        [    0.0099],\n",
      "        [    0.0364],\n",
      "        [    0.0823],\n",
      "        [    0.0087],\n",
      "        [    0.0113],\n",
      "        [    0.0876],\n",
      "        [    0.0086],\n",
      "        [    0.0170],\n",
      "        [    0.0191],\n",
      "        [    0.0407],\n",
      "        [    0.0314],\n",
      "        [    0.0332],\n",
      "        [    0.0357],\n",
      "        [    0.0172],\n",
      "        [    0.0146],\n",
      "        [    0.0241],\n",
      "        [    0.0367],\n",
      "        [    0.0284],\n",
      "        [    0.0157],\n",
      "        [    0.0377],\n",
      "        [    0.0504],\n",
      "        [    0.0237],\n",
      "        [    0.0340],\n",
      "        [    0.0583],\n",
      "        [    0.0331],\n",
      "        [    0.0336],\n",
      "        [    0.0355],\n",
      "        [    0.0360],\n",
      "        [    0.0495],\n",
      "        [    0.0647],\n",
      "        [    0.0480],\n",
      "        [    0.0631],\n",
      "        [    0.0211],\n",
      "        [    0.0599],\n",
      "        [    0.0437],\n",
      "        [    0.0370],\n",
      "        [    0.0477],\n",
      "        [    0.0495],\n",
      "        [    0.0609],\n",
      "        [    0.0530],\n",
      "        [    0.0756],\n",
      "        [    0.0313],\n",
      "        [    0.0913],\n",
      "        [    0.0689],\n",
      "        [    0.0675],\n",
      "        [    0.0779],\n",
      "        [    0.0200],\n",
      "        [    0.0641],\n",
      "        [    0.0903],\n",
      "        [    0.0907],\n",
      "        [    0.0293],\n",
      "        [    0.0990],\n",
      "        [    0.0836],\n",
      "        [    0.0088],\n",
      "        [    0.0211],\n",
      "        [    0.0841],\n",
      "        [    0.0860],\n",
      "        [    0.0170],\n",
      "        [    0.0822],\n",
      "        [    0.0923],\n",
      "        [    0.0781],\n",
      "        [    0.0856],\n",
      "        [    0.0059],\n",
      "        [    0.0954],\n",
      "        [    0.0106],\n",
      "        [    0.0893],\n",
      "        [    0.0080],\n",
      "        [    0.0187],\n",
      "        [    0.1055],\n",
      "        [    0.0007],\n",
      "        [    0.0297],\n",
      "        [    0.1330],\n",
      "        [    0.0046],\n",
      "        [    0.0298],\n",
      "        [    0.1002],\n",
      "        [    0.1290],\n",
      "        [    0.0166],\n",
      "        [    0.1189],\n",
      "        [    0.1219],\n",
      "        [    0.0130],\n",
      "        [    0.0159],\n",
      "        [    0.1531],\n",
      "        [    0.1192],\n",
      "        [    0.0461],\n",
      "        [    0.1269],\n",
      "        [    0.1447],\n",
      "        [    0.1243],\n",
      "        [    0.0224],\n",
      "        [    0.0434],\n",
      "        [    0.1633],\n",
      "        [    0.0449],\n",
      "        [    0.0712],\n",
      "        [    0.0763],\n",
      "        [    0.1879],\n",
      "        [    0.3017],\n",
      "        [    0.3157],\n",
      "        [    0.3187],\n",
      "        [    0.3205],\n",
      "        [    0.3216],\n",
      "        [    0.3310],\n",
      "        [    0.3328],\n",
      "        [    0.3340],\n",
      "        [    0.3414],\n",
      "        [    0.3432]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 89.09510827064514\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 134\n",
      "剩餘X 資料 torch.Size([26, 18])\n",
      "剩餘Y 資料 torch.Size([26, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11920011788606644, 14)\n",
      "The second_loss value of k: (0.11993151903152466, 25)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.1068])\n",
      "目前模型的Data狀態 torch.Size([134, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1270],\n",
      "        [ 0.3471],\n",
      "        [ 0.1102],\n",
      "        [ 0.6303],\n",
      "        [ 0.2976],\n",
      "        [ 0.2686],\n",
      "        [ 0.6242],\n",
      "        [ 0.3637],\n",
      "        [ 0.1971],\n",
      "        [ 0.6226],\n",
      "        [ 0.1429],\n",
      "        [ 0.3364],\n",
      "        [ 0.6097],\n",
      "        [ 0.6037],\n",
      "        [ 0.1304],\n",
      "        [ 0.1842],\n",
      "        [ 0.2164],\n",
      "        [ 0.1693],\n",
      "        [ 0.6199],\n",
      "        [ 0.1254],\n",
      "        [ 0.4415],\n",
      "        [ 0.5277],\n",
      "        [ 0.4767],\n",
      "        [ 0.2952],\n",
      "        [ 0.6209],\n",
      "        [ 0.5638],\n",
      "        [ 0.3461],\n",
      "        [ 0.6330],\n",
      "        [ 0.4724],\n",
      "        [ 0.6377],\n",
      "        [ 0.6403],\n",
      "        [ 0.4944],\n",
      "        [ 0.1469],\n",
      "        [ 0.5724],\n",
      "        [ 0.4872],\n",
      "        [ 0.3559],\n",
      "        [ 0.6077],\n",
      "        [ 0.2108],\n",
      "        [ 0.2664],\n",
      "        [ 0.3272],\n",
      "        [ 0.1471],\n",
      "        [ 0.3231],\n",
      "        [ 0.1803],\n",
      "        [ 0.2038],\n",
      "        [ 0.5800],\n",
      "        [ 0.3076],\n",
      "        [ 0.5874],\n",
      "        [ 0.4562],\n",
      "        [ 0.5856],\n",
      "        [ 0.2041],\n",
      "        [ 0.3674],\n",
      "        [ 0.4370],\n",
      "        [ 0.4778],\n",
      "        [ 0.6403],\n",
      "        [ 0.1243],\n",
      "        [ 0.2164],\n",
      "        [ 0.1291],\n",
      "        [ 0.5760],\n",
      "        [ 0.5522],\n",
      "        [ 0.3733],\n",
      "        [ 0.6087],\n",
      "        [ 0.6018],\n",
      "        [ 0.5191],\n",
      "        [ 0.4903],\n",
      "        [ 0.4275],\n",
      "        [ 0.4396],\n",
      "        [ 0.2371],\n",
      "        [ 0.2810],\n",
      "        [ 0.2121],\n",
      "        [ 0.1937],\n",
      "        [ 0.4518],\n",
      "        [ 0.2165],\n",
      "        [ 0.1595],\n",
      "        [ 0.2235],\n",
      "        [ 0.3312],\n",
      "        [ 0.4464],\n",
      "        [ 0.4589],\n",
      "        [ 0.3552],\n",
      "        [ 0.4049],\n",
      "        [ 0.5288],\n",
      "        [ 0.4104],\n",
      "        [ 0.3154],\n",
      "        [ 0.4543],\n",
      "        [ 0.4777],\n",
      "        [ 0.2024],\n",
      "        [ 0.1669],\n",
      "        [ 0.5095],\n",
      "        [ 0.2449],\n",
      "        [ 0.3598],\n",
      "        [ 0.1052],\n",
      "        [ 0.3654],\n",
      "        [ 0.4267],\n",
      "        [ 0.2710],\n",
      "        [ 0.5356],\n",
      "        [ 0.4437],\n",
      "        [ 0.4962],\n",
      "        [ 0.4195],\n",
      "        [ 0.2964],\n",
      "        [ 0.4909],\n",
      "        [ 0.4512],\n",
      "        [ 0.4593],\n",
      "        [ 0.5367],\n",
      "        [ 0.3933],\n",
      "        [ 0.3983],\n",
      "        [ 0.4158],\n",
      "        [ 0.4516],\n",
      "        [ 0.3176],\n",
      "        [ 0.3453],\n",
      "        [ 0.4410],\n",
      "        [ 0.4694],\n",
      "        [ 0.1661],\n",
      "        [ 0.1192],\n",
      "        [ 0.3975],\n",
      "        [ 0.2713],\n",
      "        [ 0.1580],\n",
      "        [ 0.3715],\n",
      "        [ 0.4474],\n",
      "        [ 0.5262],\n",
      "        [ 0.3907],\n",
      "        [ 0.4613],\n",
      "        [ 0.4123],\n",
      "        [ 0.4240],\n",
      "        [-0.0692],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520],\n",
      "        [ 0.4520]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0062],\n",
      "        [    0.0109],\n",
      "        [    0.0016],\n",
      "        [    0.0030],\n",
      "        [    0.0121],\n",
      "        [    0.0064],\n",
      "        [    0.0197],\n",
      "        [    0.0094],\n",
      "        [    0.0207],\n",
      "        [    0.0060],\n",
      "        [    0.0192],\n",
      "        [    0.0215],\n",
      "        [    0.0069],\n",
      "        [    0.0033],\n",
      "        [    0.0022],\n",
      "        [    0.0001],\n",
      "        [    0.0172],\n",
      "        [    0.0228],\n",
      "        [    0.0231],\n",
      "        [    0.0058],\n",
      "        [    0.0010],\n",
      "        [    0.0090],\n",
      "        [    0.0027],\n",
      "        [    0.0277],\n",
      "        [    0.0096],\n",
      "        [    0.0232],\n",
      "        [    0.0173],\n",
      "        [    0.0114],\n",
      "        [    0.0582],\n",
      "        [    0.0099],\n",
      "        [    0.0364],\n",
      "        [    0.0823],\n",
      "        [    0.0087],\n",
      "        [    0.0113],\n",
      "        [    0.0876],\n",
      "        [    0.0086],\n",
      "        [    0.0170],\n",
      "        [    0.0191],\n",
      "        [    0.0407],\n",
      "        [    0.0314],\n",
      "        [    0.0332],\n",
      "        [    0.0357],\n",
      "        [    0.0172],\n",
      "        [    0.0146],\n",
      "        [    0.0241],\n",
      "        [    0.0367],\n",
      "        [    0.0284],\n",
      "        [    0.0157],\n",
      "        [    0.0377],\n",
      "        [    0.0504],\n",
      "        [    0.0237],\n",
      "        [    0.0340],\n",
      "        [    0.0583],\n",
      "        [    0.0331],\n",
      "        [    0.0336],\n",
      "        [    0.0355],\n",
      "        [    0.0360],\n",
      "        [    0.0495],\n",
      "        [    0.0647],\n",
      "        [    0.0480],\n",
      "        [    0.0631],\n",
      "        [    0.0211],\n",
      "        [    0.0599],\n",
      "        [    0.0437],\n",
      "        [    0.0370],\n",
      "        [    0.0477],\n",
      "        [    0.0495],\n",
      "        [    0.0609],\n",
      "        [    0.0530],\n",
      "        [    0.0756],\n",
      "        [    0.0313],\n",
      "        [    0.0913],\n",
      "        [    0.0689],\n",
      "        [    0.0675],\n",
      "        [    0.0779],\n",
      "        [    0.0200],\n",
      "        [    0.0641],\n",
      "        [    0.0903],\n",
      "        [    0.0907],\n",
      "        [    0.0293],\n",
      "        [    0.0990],\n",
      "        [    0.0836],\n",
      "        [    0.0088],\n",
      "        [    0.0211],\n",
      "        [    0.0841],\n",
      "        [    0.0860],\n",
      "        [    0.0170],\n",
      "        [    0.0822],\n",
      "        [    0.0923],\n",
      "        [    0.0781],\n",
      "        [    0.0856],\n",
      "        [    0.0059],\n",
      "        [    0.0954],\n",
      "        [    0.0106],\n",
      "        [    0.0893],\n",
      "        [    0.0080],\n",
      "        [    0.0187],\n",
      "        [    0.1055],\n",
      "        [    0.0007],\n",
      "        [    0.0297],\n",
      "        [    0.1330],\n",
      "        [    0.0046],\n",
      "        [    0.0298],\n",
      "        [    0.1002],\n",
      "        [    0.1290],\n",
      "        [    0.0166],\n",
      "        [    0.1189],\n",
      "        [    0.1219],\n",
      "        [    0.0130],\n",
      "        [    0.0159],\n",
      "        [    0.1531],\n",
      "        [    0.1192],\n",
      "        [    0.0461],\n",
      "        [    0.1269],\n",
      "        [    0.1447],\n",
      "        [    0.1243],\n",
      "        [    0.0224],\n",
      "        [    0.0434],\n",
      "        [    0.1633],\n",
      "        [    0.0449],\n",
      "        [    0.0712],\n",
      "        [    0.0763],\n",
      "        [    0.1879],\n",
      "        [    0.3017],\n",
      "        [    0.3157],\n",
      "        [    0.3187],\n",
      "        [    0.3205],\n",
      "        [    0.3216],\n",
      "        [    0.3310],\n",
      "        [    0.3328],\n",
      "        [    0.3340],\n",
      "        [    0.3414],\n",
      "        [    0.3432],\n",
      "        [    0.3453]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0082],\n",
      "        [    0.0124],\n",
      "        [    0.0005],\n",
      "        [    0.0052],\n",
      "        [    0.0149],\n",
      "        [    0.0043],\n",
      "        [    0.0222],\n",
      "        [    0.0069],\n",
      "        [    0.0236],\n",
      "        [    0.0088],\n",
      "        [    0.0209],\n",
      "        [    0.0229],\n",
      "        [    0.0095],\n",
      "        [    0.0063],\n",
      "        [    0.0047],\n",
      "        [    0.0016],\n",
      "        [    0.0180],\n",
      "        [    0.0256],\n",
      "        [    0.0238],\n",
      "        [    0.0035],\n",
      "        [    0.0010],\n",
      "        [    0.0110],\n",
      "        [    0.0001],\n",
      "        [    0.0264],\n",
      "        [    0.0120],\n",
      "        [    0.0213],\n",
      "        [    0.0182],\n",
      "        [    0.0103],\n",
      "        [    0.0542],\n",
      "        [    0.0073],\n",
      "        [    0.0376],\n",
      "        [    0.0791],\n",
      "        [    0.0058],\n",
      "        [    0.0081],\n",
      "        [    0.0845],\n",
      "        [    0.0107],\n",
      "        [    0.0143],\n",
      "        [    0.0182],\n",
      "        [    0.0423],\n",
      "        [    0.0327],\n",
      "        [    0.0352],\n",
      "        [    0.0374],\n",
      "        [    0.0169],\n",
      "        [    0.0126],\n",
      "        [    0.0211],\n",
      "        [    0.0344],\n",
      "        [    0.0253],\n",
      "        [    0.0182],\n",
      "        [    0.0350],\n",
      "        [    0.0518],\n",
      "        [    0.0238],\n",
      "        [    0.0367],\n",
      "        [    0.0550],\n",
      "        [    0.0302],\n",
      "        [    0.0310],\n",
      "        [    0.0340],\n",
      "        [    0.0337],\n",
      "        [    0.0471],\n",
      "        [    0.0639],\n",
      "        [    0.0454],\n",
      "        [    0.0642],\n",
      "        [    0.0183],\n",
      "        [    0.0572],\n",
      "        [    0.0458],\n",
      "        [    0.0395],\n",
      "        [    0.0507],\n",
      "        [    0.0478],\n",
      "        [    0.0592],\n",
      "        [    0.0515],\n",
      "        [    0.0758],\n",
      "        [    0.0340],\n",
      "        [    0.0931],\n",
      "        [    0.0671],\n",
      "        [    0.0686],\n",
      "        [    0.0809],\n",
      "        [    0.0178],\n",
      "        [    0.0662],\n",
      "        [    0.0886],\n",
      "        [    0.0877],\n",
      "        [    0.0272],\n",
      "        [    0.0963],\n",
      "        [    0.0856],\n",
      "        [    0.0063],\n",
      "        [    0.0188],\n",
      "        [    0.0811],\n",
      "        [    0.0836],\n",
      "        [    0.0149],\n",
      "        [    0.0800],\n",
      "        [    0.0937],\n",
      "        [    0.0758],\n",
      "        [    0.0865],\n",
      "        [    0.0082],\n",
      "        [    0.0924],\n",
      "        [    0.0089],\n",
      "        [    0.0912],\n",
      "        [    0.0052],\n",
      "        [    0.0214],\n",
      "        [    0.1038],\n",
      "        [    0.0012],\n",
      "        [    0.0328],\n",
      "        [    0.1310],\n",
      "        [    0.0059],\n",
      "        [    0.0325],\n",
      "        [    0.1024],\n",
      "        [    0.1269],\n",
      "        [    0.0182],\n",
      "        [    0.1207],\n",
      "        [    0.1244],\n",
      "        [    0.0149],\n",
      "        [    0.0187],\n",
      "        [    0.1544],\n",
      "        [    0.1168],\n",
      "        [    0.0489],\n",
      "        [    0.1239],\n",
      "        [    0.1453],\n",
      "        [    0.1263],\n",
      "        [    0.0242],\n",
      "        [    0.0447],\n",
      "        [    0.1612],\n",
      "        [    0.0469],\n",
      "        [    0.0726],\n",
      "        [    0.0772],\n",
      "        [    0.1757],\n",
      "        [    0.3012],\n",
      "        [    0.3152],\n",
      "        [    0.3182],\n",
      "        [    0.3200],\n",
      "        [    0.3211],\n",
      "        [    0.3305],\n",
      "        [    0.3323],\n",
      "        [    0.3335],\n",
      "        [    0.3409],\n",
      "        [    0.3427],\n",
      "        [    0.3448]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 89.33439564704895\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 135\n",
      "剩餘X 資料 torch.Size([25, 18])\n",
      "剩餘Y 資料 torch.Size([25, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11958477646112442, 24)\n",
      "The second_loss value of k: (0.12181652337312698, 16)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引24，y= tensor([0.1057])\n",
      "目前模型的Data狀態 torch.Size([135, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1250],\n",
      "        [ 0.3456],\n",
      "        [ 0.1081],\n",
      "        [ 0.6282],\n",
      "        [ 0.2948],\n",
      "        [ 0.2665],\n",
      "        [ 0.6218],\n",
      "        [ 0.3612],\n",
      "        [ 0.1942],\n",
      "        [ 0.6198],\n",
      "        [ 0.1411],\n",
      "        [ 0.3350],\n",
      "        [ 0.6071],\n",
      "        [ 0.6007],\n",
      "        [ 0.1279],\n",
      "        [ 0.1825],\n",
      "        [ 0.2156],\n",
      "        [ 0.1665],\n",
      "        [ 0.6192],\n",
      "        [ 0.1230],\n",
      "        [ 0.4394],\n",
      "        [ 0.5256],\n",
      "        [ 0.4741],\n",
      "        [ 0.2938],\n",
      "        [ 0.6186],\n",
      "        [ 0.5619],\n",
      "        [ 0.3452],\n",
      "        [ 0.6319],\n",
      "        [ 0.4683],\n",
      "        [ 0.6351],\n",
      "        [ 0.6391],\n",
      "        [ 0.4912],\n",
      "        [ 0.1440],\n",
      "        [ 0.5692],\n",
      "        [ 0.4841],\n",
      "        [ 0.3538],\n",
      "        [ 0.6049],\n",
      "        [ 0.2099],\n",
      "        [ 0.2648],\n",
      "        [ 0.3259],\n",
      "        [ 0.1450],\n",
      "        [ 0.3213],\n",
      "        [ 0.1807],\n",
      "        [ 0.2018],\n",
      "        [ 0.5770],\n",
      "        [ 0.3053],\n",
      "        [ 0.5843],\n",
      "        [ 0.4537],\n",
      "        [ 0.5829],\n",
      "        [ 0.2027],\n",
      "        [ 0.3673],\n",
      "        [ 0.4342],\n",
      "        [ 0.4746],\n",
      "        [ 0.6375],\n",
      "        [ 0.1218],\n",
      "        [ 0.2148],\n",
      "        [ 0.1268],\n",
      "        [ 0.5736],\n",
      "        [ 0.5513],\n",
      "        [ 0.3707],\n",
      "        [ 0.6075],\n",
      "        [ 0.5990],\n",
      "        [ 0.5164],\n",
      "        [ 0.4882],\n",
      "        [ 0.4251],\n",
      "        [ 0.4365],\n",
      "        [ 0.2354],\n",
      "        [ 0.2793],\n",
      "        [ 0.2106],\n",
      "        [ 0.1939],\n",
      "        [ 0.4491],\n",
      "        [ 0.2147],\n",
      "        [ 0.1577],\n",
      "        [ 0.2224],\n",
      "        [ 0.3283],\n",
      "        [ 0.4442],\n",
      "        [ 0.4568],\n",
      "        [ 0.3535],\n",
      "        [ 0.4019],\n",
      "        [ 0.5267],\n",
      "        [ 0.4077],\n",
      "        [ 0.3134],\n",
      "        [ 0.4518],\n",
      "        [ 0.4755],\n",
      "        [ 0.1993],\n",
      "        [ 0.1645],\n",
      "        [ 0.5074],\n",
      "        [ 0.2428],\n",
      "        [ 0.3584],\n",
      "        [ 0.1028],\n",
      "        [ 0.3645],\n",
      "        [ 0.4243],\n",
      "        [ 0.2679],\n",
      "        [ 0.5340],\n",
      "        [ 0.4417],\n",
      "        [ 0.4935],\n",
      "        [ 0.4168],\n",
      "        [ 0.2946],\n",
      "        [ 0.4905],\n",
      "        [ 0.4480],\n",
      "        [ 0.4574],\n",
      "        [ 0.5354],\n",
      "        [ 0.3906],\n",
      "        [ 0.3961],\n",
      "        [ 0.4137],\n",
      "        [ 0.4500],\n",
      "        [ 0.3158],\n",
      "        [ 0.3427],\n",
      "        [ 0.4391],\n",
      "        [ 0.4666],\n",
      "        [ 0.1649],\n",
      "        [ 0.1168],\n",
      "        [ 0.3947],\n",
      "        [ 0.2683],\n",
      "        [ 0.1574],\n",
      "        [ 0.3695],\n",
      "        [ 0.4456],\n",
      "        [ 0.5248],\n",
      "        [ 0.3886],\n",
      "        [ 0.4593],\n",
      "        [ 0.4110],\n",
      "        [ 0.4231],\n",
      "        [-0.0570],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515],\n",
      "        [ 0.4515]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0082],\n",
      "        [    0.0124],\n",
      "        [    0.0005],\n",
      "        [    0.0052],\n",
      "        [    0.0149],\n",
      "        [    0.0043],\n",
      "        [    0.0222],\n",
      "        [    0.0069],\n",
      "        [    0.0236],\n",
      "        [    0.0088],\n",
      "        [    0.0209],\n",
      "        [    0.0229],\n",
      "        [    0.0095],\n",
      "        [    0.0063],\n",
      "        [    0.0047],\n",
      "        [    0.0016],\n",
      "        [    0.0180],\n",
      "        [    0.0256],\n",
      "        [    0.0238],\n",
      "        [    0.0035],\n",
      "        [    0.0010],\n",
      "        [    0.0110],\n",
      "        [    0.0001],\n",
      "        [    0.0264],\n",
      "        [    0.0120],\n",
      "        [    0.0213],\n",
      "        [    0.0182],\n",
      "        [    0.0103],\n",
      "        [    0.0542],\n",
      "        [    0.0073],\n",
      "        [    0.0376],\n",
      "        [    0.0791],\n",
      "        [    0.0058],\n",
      "        [    0.0081],\n",
      "        [    0.0845],\n",
      "        [    0.0107],\n",
      "        [    0.0143],\n",
      "        [    0.0182],\n",
      "        [    0.0423],\n",
      "        [    0.0327],\n",
      "        [    0.0352],\n",
      "        [    0.0374],\n",
      "        [    0.0169],\n",
      "        [    0.0126],\n",
      "        [    0.0211],\n",
      "        [    0.0344],\n",
      "        [    0.0253],\n",
      "        [    0.0182],\n",
      "        [    0.0350],\n",
      "        [    0.0518],\n",
      "        [    0.0238],\n",
      "        [    0.0367],\n",
      "        [    0.0550],\n",
      "        [    0.0302],\n",
      "        [    0.0310],\n",
      "        [    0.0340],\n",
      "        [    0.0337],\n",
      "        [    0.0471],\n",
      "        [    0.0639],\n",
      "        [    0.0454],\n",
      "        [    0.0642],\n",
      "        [    0.0183],\n",
      "        [    0.0572],\n",
      "        [    0.0458],\n",
      "        [    0.0395],\n",
      "        [    0.0507],\n",
      "        [    0.0478],\n",
      "        [    0.0592],\n",
      "        [    0.0515],\n",
      "        [    0.0758],\n",
      "        [    0.0340],\n",
      "        [    0.0931],\n",
      "        [    0.0671],\n",
      "        [    0.0686],\n",
      "        [    0.0809],\n",
      "        [    0.0178],\n",
      "        [    0.0662],\n",
      "        [    0.0886],\n",
      "        [    0.0877],\n",
      "        [    0.0272],\n",
      "        [    0.0963],\n",
      "        [    0.0856],\n",
      "        [    0.0063],\n",
      "        [    0.0188],\n",
      "        [    0.0811],\n",
      "        [    0.0836],\n",
      "        [    0.0149],\n",
      "        [    0.0800],\n",
      "        [    0.0937],\n",
      "        [    0.0758],\n",
      "        [    0.0865],\n",
      "        [    0.0082],\n",
      "        [    0.0924],\n",
      "        [    0.0089],\n",
      "        [    0.0912],\n",
      "        [    0.0052],\n",
      "        [    0.0214],\n",
      "        [    0.1038],\n",
      "        [    0.0012],\n",
      "        [    0.0328],\n",
      "        [    0.1310],\n",
      "        [    0.0059],\n",
      "        [    0.0325],\n",
      "        [    0.1024],\n",
      "        [    0.1269],\n",
      "        [    0.0182],\n",
      "        [    0.1207],\n",
      "        [    0.1244],\n",
      "        [    0.0149],\n",
      "        [    0.0187],\n",
      "        [    0.1544],\n",
      "        [    0.1168],\n",
      "        [    0.0489],\n",
      "        [    0.1239],\n",
      "        [    0.1453],\n",
      "        [    0.1263],\n",
      "        [    0.0242],\n",
      "        [    0.0447],\n",
      "        [    0.1612],\n",
      "        [    0.0469],\n",
      "        [    0.0726],\n",
      "        [    0.0772],\n",
      "        [    0.1757],\n",
      "        [    0.3012],\n",
      "        [    0.3152],\n",
      "        [    0.3182],\n",
      "        [    0.3200],\n",
      "        [    0.3211],\n",
      "        [    0.3305],\n",
      "        [    0.3323],\n",
      "        [    0.3335],\n",
      "        [    0.3409],\n",
      "        [    0.3427],\n",
      "        [    0.3448],\n",
      "        [    0.3458]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0056],\n",
      "        [    0.0094],\n",
      "        [    0.0018],\n",
      "        [    0.0026],\n",
      "        [    0.0129],\n",
      "        [    0.0059],\n",
      "        [    0.0198],\n",
      "        [    0.0083],\n",
      "        [    0.0226],\n",
      "        [    0.0067],\n",
      "        [    0.0190],\n",
      "        [    0.0197],\n",
      "        [    0.0072],\n",
      "        [    0.0045],\n",
      "        [    0.0026],\n",
      "        [    0.0003],\n",
      "        [    0.0152],\n",
      "        [    0.0244],\n",
      "        [    0.0198],\n",
      "        [    0.0058],\n",
      "        [    0.0010],\n",
      "        [    0.0087],\n",
      "        [    0.0014],\n",
      "        [    0.0292],\n",
      "        [    0.0093],\n",
      "        [    0.0239],\n",
      "        [    0.0150],\n",
      "        [    0.0139],\n",
      "        [    0.0550],\n",
      "        [    0.0097],\n",
      "        [    0.0338],\n",
      "        [    0.0802],\n",
      "        [    0.0075],\n",
      "        [    0.0097],\n",
      "        [    0.0862],\n",
      "        [    0.0091],\n",
      "        [    0.0162],\n",
      "        [    0.0208],\n",
      "        [    0.0393],\n",
      "        [    0.0297],\n",
      "        [    0.0335],\n",
      "        [    0.0348],\n",
      "        [    0.0169],\n",
      "        [    0.0146],\n",
      "        [    0.0228],\n",
      "        [    0.0358],\n",
      "        [    0.0270],\n",
      "        [    0.0169],\n",
      "        [    0.0368],\n",
      "        [    0.0496],\n",
      "        [    0.0200],\n",
      "        [    0.0354],\n",
      "        [    0.0562],\n",
      "        [    0.0323],\n",
      "        [    0.0332],\n",
      "        [    0.0361],\n",
      "        [    0.0360],\n",
      "        [    0.0493],\n",
      "        [    0.0675],\n",
      "        [    0.0467],\n",
      "        [    0.0604],\n",
      "        [    0.0203],\n",
      "        [    0.0589],\n",
      "        [    0.0437],\n",
      "        [    0.0380],\n",
      "        [    0.0498],\n",
      "        [    0.0498],\n",
      "        [    0.0612],\n",
      "        [    0.0539],\n",
      "        [    0.0756],\n",
      "        [    0.0318],\n",
      "        [    0.0904],\n",
      "        [    0.0700],\n",
      "        [    0.0657],\n",
      "        [    0.0789],\n",
      "        [    0.0202],\n",
      "        [    0.0642],\n",
      "        [    0.0911],\n",
      "        [    0.0887],\n",
      "        [    0.0292],\n",
      "        [    0.0975],\n",
      "        [    0.0838],\n",
      "        [    0.0081],\n",
      "        [    0.0208],\n",
      "        [    0.0828],\n",
      "        [    0.0859],\n",
      "        [    0.0171],\n",
      "        [    0.0818],\n",
      "        [    0.0912],\n",
      "        [    0.0773],\n",
      "        [    0.0834],\n",
      "        [    0.0063],\n",
      "        [    0.0941],\n",
      "        [    0.0117],\n",
      "        [    0.0892],\n",
      "        [    0.0068],\n",
      "        [    0.0197],\n",
      "        [    0.1059],\n",
      "        [    0.0028],\n",
      "        [    0.0315],\n",
      "        [    0.1333],\n",
      "        [    0.0027],\n",
      "        [    0.0309],\n",
      "        [    0.0998],\n",
      "        [    0.1292],\n",
      "        [    0.0152],\n",
      "        [    0.1188],\n",
      "        [    0.1222],\n",
      "        [    0.0126],\n",
      "        [    0.0173],\n",
      "        [    0.1514],\n",
      "        [    0.1183],\n",
      "        [    0.0474],\n",
      "        [    0.1259],\n",
      "        [    0.1417],\n",
      "        [    0.1235],\n",
      "        [    0.0218],\n",
      "        [    0.0417],\n",
      "        [    0.1634],\n",
      "        [    0.0447],\n",
      "        [    0.0694],\n",
      "        [    0.0738],\n",
      "        [    0.1616],\n",
      "        [    0.3007],\n",
      "        [    0.3147],\n",
      "        [    0.3177],\n",
      "        [    0.3195],\n",
      "        [    0.3206],\n",
      "        [    0.3300],\n",
      "        [    0.3318],\n",
      "        [    0.3330],\n",
      "        [    0.3404],\n",
      "        [    0.3422],\n",
      "        [    0.3442],\n",
      "        [    0.3453]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 89.57280230522156\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 136\n",
      "剩餘X 資料 torch.Size([24, 18])\n",
      "剩餘Y 資料 torch.Size([24, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12146517634391785, 16)\n",
      "The second_loss value of k: (0.13512544333934784, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.1025])\n",
      "目前模型的Data狀態 torch.Size([136, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1276],\n",
      "        [ 0.3486],\n",
      "        [ 0.1104],\n",
      "        [ 0.6308],\n",
      "        [ 0.2968],\n",
      "        [ 0.2681],\n",
      "        [ 0.6242],\n",
      "        [ 0.3626],\n",
      "        [ 0.1952],\n",
      "        [ 0.6219],\n",
      "        [ 0.1430],\n",
      "        [ 0.3382],\n",
      "        [ 0.6094],\n",
      "        [ 0.6025],\n",
      "        [ 0.1300],\n",
      "        [ 0.1844],\n",
      "        [ 0.2184],\n",
      "        [ 0.1676],\n",
      "        [ 0.6232],\n",
      "        [ 0.1253],\n",
      "        [ 0.4414],\n",
      "        [ 0.5279],\n",
      "        [ 0.4754],\n",
      "        [ 0.2967],\n",
      "        [ 0.6212],\n",
      "        [ 0.5645],\n",
      "        [ 0.3484],\n",
      "        [ 0.6355],\n",
      "        [ 0.4692],\n",
      "        [ 0.6374],\n",
      "        [ 0.6429],\n",
      "        [ 0.4923],\n",
      "        [ 0.1457],\n",
      "        [ 0.5708],\n",
      "        [ 0.4859],\n",
      "        [ 0.3554],\n",
      "        [ 0.6069],\n",
      "        [ 0.2125],\n",
      "        [ 0.2678],\n",
      "        [ 0.3289],\n",
      "        [ 0.1467],\n",
      "        [ 0.3240],\n",
      "        [ 0.1806],\n",
      "        [ 0.2037],\n",
      "        [ 0.5786],\n",
      "        [ 0.3068],\n",
      "        [ 0.5859],\n",
      "        [ 0.4551],\n",
      "        [ 0.5848],\n",
      "        [ 0.2050],\n",
      "        [ 0.3711],\n",
      "        [ 0.4355],\n",
      "        [ 0.4757],\n",
      "        [ 0.6395],\n",
      "        [ 0.1239],\n",
      "        [ 0.2169],\n",
      "        [ 0.1291],\n",
      "        [ 0.5758],\n",
      "        [ 0.5550],\n",
      "        [ 0.3720],\n",
      "        [ 0.6113],\n",
      "        [ 0.6010],\n",
      "        [ 0.5181],\n",
      "        [ 0.4903],\n",
      "        [ 0.4265],\n",
      "        [ 0.4374],\n",
      "        [ 0.2374],\n",
      "        [ 0.2813],\n",
      "        [ 0.2130],\n",
      "        [ 0.1937],\n",
      "        [ 0.4513],\n",
      "        [ 0.2174],\n",
      "        [ 0.1606],\n",
      "        [ 0.2253],\n",
      "        [ 0.3303],\n",
      "        [ 0.4466],\n",
      "        [ 0.4588],\n",
      "        [ 0.3561],\n",
      "        [ 0.4029],\n",
      "        [ 0.5288],\n",
      "        [ 0.4090],\n",
      "        [ 0.3151],\n",
      "        [ 0.4536],\n",
      "        [ 0.4775],\n",
      "        [ 0.2010],\n",
      "        [ 0.1668],\n",
      "        [ 0.5096],\n",
      "        [ 0.2445],\n",
      "        [ 0.3610],\n",
      "        [ 0.1043],\n",
      "        [ 0.3676],\n",
      "        [ 0.4263],\n",
      "        [ 0.2697],\n",
      "        [ 0.5368],\n",
      "        [ 0.4438],\n",
      "        [ 0.4950],\n",
      "        [ 0.4185],\n",
      "        [ 0.2968],\n",
      "        [ 0.4944],\n",
      "        [ 0.4493],\n",
      "        [ 0.4597],\n",
      "        [ 0.5386],\n",
      "        [ 0.3922],\n",
      "        [ 0.3987],\n",
      "        [ 0.4159],\n",
      "        [ 0.4530],\n",
      "        [ 0.3177],\n",
      "        [ 0.3449],\n",
      "        [ 0.4414],\n",
      "        [ 0.4681],\n",
      "        [ 0.1678],\n",
      "        [ 0.1183],\n",
      "        [ 0.3962],\n",
      "        [ 0.2702],\n",
      "        [ 0.1610],\n",
      "        [ 0.3723],\n",
      "        [ 0.4480],\n",
      "        [ 0.5279],\n",
      "        [ 0.3908],\n",
      "        [ 0.4614],\n",
      "        [ 0.4141],\n",
      "        [ 0.4266],\n",
      "        [-0.0428],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510],\n",
      "        [ 0.4510]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0056],\n",
      "        [    0.0094],\n",
      "        [    0.0018],\n",
      "        [    0.0026],\n",
      "        [    0.0129],\n",
      "        [    0.0059],\n",
      "        [    0.0198],\n",
      "        [    0.0083],\n",
      "        [    0.0226],\n",
      "        [    0.0067],\n",
      "        [    0.0190],\n",
      "        [    0.0197],\n",
      "        [    0.0072],\n",
      "        [    0.0045],\n",
      "        [    0.0026],\n",
      "        [    0.0003],\n",
      "        [    0.0152],\n",
      "        [    0.0244],\n",
      "        [    0.0198],\n",
      "        [    0.0058],\n",
      "        [    0.0010],\n",
      "        [    0.0087],\n",
      "        [    0.0014],\n",
      "        [    0.0292],\n",
      "        [    0.0093],\n",
      "        [    0.0239],\n",
      "        [    0.0150],\n",
      "        [    0.0139],\n",
      "        [    0.0550],\n",
      "        [    0.0097],\n",
      "        [    0.0338],\n",
      "        [    0.0802],\n",
      "        [    0.0075],\n",
      "        [    0.0097],\n",
      "        [    0.0862],\n",
      "        [    0.0091],\n",
      "        [    0.0162],\n",
      "        [    0.0208],\n",
      "        [    0.0393],\n",
      "        [    0.0297],\n",
      "        [    0.0335],\n",
      "        [    0.0348],\n",
      "        [    0.0169],\n",
      "        [    0.0146],\n",
      "        [    0.0228],\n",
      "        [    0.0358],\n",
      "        [    0.0270],\n",
      "        [    0.0169],\n",
      "        [    0.0368],\n",
      "        [    0.0496],\n",
      "        [    0.0200],\n",
      "        [    0.0354],\n",
      "        [    0.0562],\n",
      "        [    0.0323],\n",
      "        [    0.0332],\n",
      "        [    0.0361],\n",
      "        [    0.0360],\n",
      "        [    0.0493],\n",
      "        [    0.0675],\n",
      "        [    0.0467],\n",
      "        [    0.0604],\n",
      "        [    0.0203],\n",
      "        [    0.0589],\n",
      "        [    0.0437],\n",
      "        [    0.0380],\n",
      "        [    0.0498],\n",
      "        [    0.0498],\n",
      "        [    0.0612],\n",
      "        [    0.0539],\n",
      "        [    0.0756],\n",
      "        [    0.0318],\n",
      "        [    0.0904],\n",
      "        [    0.0700],\n",
      "        [    0.0657],\n",
      "        [    0.0789],\n",
      "        [    0.0202],\n",
      "        [    0.0642],\n",
      "        [    0.0911],\n",
      "        [    0.0887],\n",
      "        [    0.0292],\n",
      "        [    0.0975],\n",
      "        [    0.0838],\n",
      "        [    0.0081],\n",
      "        [    0.0208],\n",
      "        [    0.0828],\n",
      "        [    0.0859],\n",
      "        [    0.0171],\n",
      "        [    0.0818],\n",
      "        [    0.0912],\n",
      "        [    0.0773],\n",
      "        [    0.0834],\n",
      "        [    0.0063],\n",
      "        [    0.0941],\n",
      "        [    0.0117],\n",
      "        [    0.0892],\n",
      "        [    0.0068],\n",
      "        [    0.0197],\n",
      "        [    0.1059],\n",
      "        [    0.0028],\n",
      "        [    0.0315],\n",
      "        [    0.1333],\n",
      "        [    0.0027],\n",
      "        [    0.0309],\n",
      "        [    0.0998],\n",
      "        [    0.1292],\n",
      "        [    0.0152],\n",
      "        [    0.1188],\n",
      "        [    0.1222],\n",
      "        [    0.0126],\n",
      "        [    0.0173],\n",
      "        [    0.1514],\n",
      "        [    0.1183],\n",
      "        [    0.0474],\n",
      "        [    0.1259],\n",
      "        [    0.1417],\n",
      "        [    0.1235],\n",
      "        [    0.0218],\n",
      "        [    0.0417],\n",
      "        [    0.1634],\n",
      "        [    0.0447],\n",
      "        [    0.0694],\n",
      "        [    0.0738],\n",
      "        [    0.1616],\n",
      "        [    0.3007],\n",
      "        [    0.3147],\n",
      "        [    0.3177],\n",
      "        [    0.3195],\n",
      "        [    0.3206],\n",
      "        [    0.3300],\n",
      "        [    0.3318],\n",
      "        [    0.3330],\n",
      "        [    0.3404],\n",
      "        [    0.3422],\n",
      "        [    0.3442],\n",
      "        [    0.3453],\n",
      "        [    0.3485]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0060],\n",
      "        [    0.0091],\n",
      "        [    0.0013],\n",
      "        [    0.0022],\n",
      "        [    0.0139],\n",
      "        [    0.0055],\n",
      "        [    0.0195],\n",
      "        [    0.0077],\n",
      "        [    0.0242],\n",
      "        [    0.0069],\n",
      "        [    0.0195],\n",
      "        [    0.0192],\n",
      "        [    0.0071],\n",
      "        [    0.0048],\n",
      "        [    0.0035],\n",
      "        [    0.0000],\n",
      "        [    0.0147],\n",
      "        [    0.0258],\n",
      "        [    0.0182],\n",
      "        [    0.0050],\n",
      "        [    0.0012],\n",
      "        [    0.0084],\n",
      "        [    0.0012],\n",
      "        [    0.0298],\n",
      "        [    0.0092],\n",
      "        [    0.0245],\n",
      "        [    0.0143],\n",
      "        [    0.0152],\n",
      "        [    0.0533],\n",
      "        [    0.0098],\n",
      "        [    0.0326],\n",
      "        [    0.0797],\n",
      "        [    0.0063],\n",
      "        [    0.0092],\n",
      "        [    0.0855],\n",
      "        [    0.0093],\n",
      "        [    0.0161],\n",
      "        [    0.0212],\n",
      "        [    0.0391],\n",
      "        [    0.0293],\n",
      "        [    0.0343],\n",
      "        [    0.0347],\n",
      "        [    0.0169],\n",
      "        [    0.0139],\n",
      "        [    0.0224],\n",
      "        [    0.0353],\n",
      "        [    0.0266],\n",
      "        [    0.0170],\n",
      "        [    0.0367],\n",
      "        [    0.0497],\n",
      "        [    0.0187],\n",
      "        [    0.0358],\n",
      "        [    0.0556],\n",
      "        [    0.0322],\n",
      "        [    0.0323],\n",
      "        [    0.0361],\n",
      "        [    0.0352],\n",
      "        [    0.0496],\n",
      "        [    0.0688],\n",
      "        [    0.0461],\n",
      "        [    0.0592],\n",
      "        [    0.0200],\n",
      "        [    0.0588],\n",
      "        [    0.0434],\n",
      "        [    0.0382],\n",
      "        [    0.0504],\n",
      "        [    0.0496],\n",
      "        [    0.0611],\n",
      "        [    0.0536],\n",
      "        [    0.0755],\n",
      "        [    0.0324],\n",
      "        [    0.0904],\n",
      "        [    0.0698],\n",
      "        [    0.0650],\n",
      "        [    0.0799],\n",
      "        [    0.0202],\n",
      "        [    0.0640],\n",
      "        [    0.0915],\n",
      "        [    0.0878],\n",
      "        [    0.0296],\n",
      "        [    0.0969],\n",
      "        [    0.0840],\n",
      "        [    0.0081],\n",
      "        [    0.0210],\n",
      "        [    0.0815],\n",
      "        [    0.0851],\n",
      "        [    0.0174],\n",
      "        [    0.0812],\n",
      "        [    0.0908],\n",
      "        [    0.0762],\n",
      "        [    0.0827],\n",
      "        [    0.0061],\n",
      "        [    0.0928],\n",
      "        [    0.0124],\n",
      "        [    0.0889],\n",
      "        [    0.0066],\n",
      "        [    0.0200],\n",
      "        [    0.1058],\n",
      "        [    0.0044],\n",
      "        [    0.0322],\n",
      "        [    0.1337],\n",
      "        [    0.0017],\n",
      "        [    0.0311],\n",
      "        [    0.1001],\n",
      "        [    0.1293],\n",
      "        [    0.0147],\n",
      "        [    0.1189],\n",
      "        [    0.1230],\n",
      "        [    0.0119],\n",
      "        [    0.0174],\n",
      "        [    0.1510],\n",
      "        [    0.1172],\n",
      "        [    0.0476],\n",
      "        [    0.1247],\n",
      "        [    0.1408],\n",
      "        [    0.1237],\n",
      "        [    0.0211],\n",
      "        [    0.0408],\n",
      "        [    0.1634],\n",
      "        [    0.0440],\n",
      "        [    0.0686],\n",
      "        [    0.0726],\n",
      "        [    0.1503],\n",
      "        [    0.3002],\n",
      "        [    0.3142],\n",
      "        [    0.3172],\n",
      "        [    0.3191],\n",
      "        [    0.3201],\n",
      "        [    0.3296],\n",
      "        [    0.3314],\n",
      "        [    0.3325],\n",
      "        [    0.3399],\n",
      "        [    0.3417],\n",
      "        [    0.3438],\n",
      "        [    0.3448],\n",
      "        [    0.3480]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 89.8121702671051\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 137\n",
      "剩餘X 資料 torch.Size([23, 18])\n",
      "剩餘Y 資料 torch.Size([23, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.13477851450443268, 14)\n",
      "The second_loss value of k: (0.13549847900867462, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0834])\n",
      "目前模型的Data狀態 torch.Size([137, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1271],\n",
      "        [ 0.3489],\n",
      "        [ 0.1099],\n",
      "        [ 0.6312],\n",
      "        [ 0.2958],\n",
      "        [ 0.2677],\n",
      "        [ 0.6244],\n",
      "        [ 0.3620],\n",
      "        [ 0.1936],\n",
      "        [ 0.6217],\n",
      "        [ 0.1425],\n",
      "        [ 0.3387],\n",
      "        [ 0.6094],\n",
      "        [ 0.6022],\n",
      "        [ 0.1291],\n",
      "        [ 0.1841],\n",
      "        [ 0.2189],\n",
      "        [ 0.1663],\n",
      "        [ 0.6248],\n",
      "        [ 0.1246],\n",
      "        [ 0.4416],\n",
      "        [ 0.5283],\n",
      "        [ 0.4752],\n",
      "        [ 0.2972],\n",
      "        [ 0.6214],\n",
      "        [ 0.5650],\n",
      "        [ 0.3492],\n",
      "        [ 0.6368],\n",
      "        [ 0.4674],\n",
      "        [ 0.6376],\n",
      "        [ 0.6441],\n",
      "        [ 0.4918],\n",
      "        [ 0.1445],\n",
      "        [ 0.5703],\n",
      "        [ 0.4851],\n",
      "        [ 0.3551],\n",
      "        [ 0.6068],\n",
      "        [ 0.2129],\n",
      "        [ 0.2680],\n",
      "        [ 0.3293],\n",
      "        [ 0.1459],\n",
      "        [ 0.3240],\n",
      "        [ 0.1806],\n",
      "        [ 0.2030],\n",
      "        [ 0.5783],\n",
      "        [ 0.3063],\n",
      "        [ 0.5856],\n",
      "        [ 0.4549],\n",
      "        [ 0.5847],\n",
      "        [ 0.2049],\n",
      "        [ 0.3724],\n",
      "        [ 0.4351],\n",
      "        [ 0.4752],\n",
      "        [ 0.6394],\n",
      "        [ 0.1230],\n",
      "        [ 0.2169],\n",
      "        [ 0.1283],\n",
      "        [ 0.5760],\n",
      "        [ 0.5563],\n",
      "        [ 0.3714],\n",
      "        [ 0.6125],\n",
      "        [ 0.6006],\n",
      "        [ 0.5180],\n",
      "        [ 0.4906],\n",
      "        [ 0.4263],\n",
      "        [ 0.4368],\n",
      "        [ 0.2372],\n",
      "        [ 0.2812],\n",
      "        [ 0.2127],\n",
      "        [ 0.1936],\n",
      "        [ 0.4507],\n",
      "        [ 0.2174],\n",
      "        [ 0.1604],\n",
      "        [ 0.2259],\n",
      "        [ 0.3293],\n",
      "        [ 0.4466],\n",
      "        [ 0.4591],\n",
      "        [ 0.3564],\n",
      "        [ 0.4020],\n",
      "        [ 0.5291],\n",
      "        [ 0.4083],\n",
      "        [ 0.3149],\n",
      "        [ 0.4536],\n",
      "        [ 0.4777],\n",
      "        [ 0.1998],\n",
      "        [ 0.1660],\n",
      "        [ 0.5100],\n",
      "        [ 0.2440],\n",
      "        [ 0.3614],\n",
      "        [ 0.1033],\n",
      "        [ 0.3684],\n",
      "        [ 0.4264],\n",
      "        [ 0.2684],\n",
      "        [ 0.5375],\n",
      "        [ 0.4441],\n",
      "        [ 0.4949],\n",
      "        [ 0.4183],\n",
      "        [ 0.2967],\n",
      "        [ 0.4961],\n",
      "        [ 0.4487],\n",
      "        [ 0.4600],\n",
      "        [ 0.5395],\n",
      "        [ 0.3920],\n",
      "        [ 0.3984],\n",
      "        [ 0.4160],\n",
      "        [ 0.4535],\n",
      "        [ 0.3176],\n",
      "        [ 0.3442],\n",
      "        [ 0.4420],\n",
      "        [ 0.4680],\n",
      "        [ 0.1682],\n",
      "        [ 0.1172],\n",
      "        [ 0.3960],\n",
      "        [ 0.2691],\n",
      "        [ 0.1619],\n",
      "        [ 0.3721],\n",
      "        [ 0.4488],\n",
      "        [ 0.5287],\n",
      "        [ 0.3908],\n",
      "        [ 0.4621],\n",
      "        [ 0.4149],\n",
      "        [ 0.4277],\n",
      "        [-0.0316],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506],\n",
      "        [ 0.4506]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0060],\n",
      "        [    0.0091],\n",
      "        [    0.0013],\n",
      "        [    0.0022],\n",
      "        [    0.0139],\n",
      "        [    0.0055],\n",
      "        [    0.0195],\n",
      "        [    0.0077],\n",
      "        [    0.0242],\n",
      "        [    0.0069],\n",
      "        [    0.0195],\n",
      "        [    0.0192],\n",
      "        [    0.0071],\n",
      "        [    0.0048],\n",
      "        [    0.0035],\n",
      "        [    0.0000],\n",
      "        [    0.0147],\n",
      "        [    0.0258],\n",
      "        [    0.0182],\n",
      "        [    0.0050],\n",
      "        [    0.0012],\n",
      "        [    0.0084],\n",
      "        [    0.0012],\n",
      "        [    0.0298],\n",
      "        [    0.0092],\n",
      "        [    0.0245],\n",
      "        [    0.0143],\n",
      "        [    0.0152],\n",
      "        [    0.0533],\n",
      "        [    0.0098],\n",
      "        [    0.0326],\n",
      "        [    0.0797],\n",
      "        [    0.0063],\n",
      "        [    0.0092],\n",
      "        [    0.0855],\n",
      "        [    0.0093],\n",
      "        [    0.0161],\n",
      "        [    0.0212],\n",
      "        [    0.0391],\n",
      "        [    0.0293],\n",
      "        [    0.0343],\n",
      "        [    0.0347],\n",
      "        [    0.0169],\n",
      "        [    0.0139],\n",
      "        [    0.0224],\n",
      "        [    0.0353],\n",
      "        [    0.0266],\n",
      "        [    0.0170],\n",
      "        [    0.0367],\n",
      "        [    0.0497],\n",
      "        [    0.0187],\n",
      "        [    0.0358],\n",
      "        [    0.0556],\n",
      "        [    0.0322],\n",
      "        [    0.0323],\n",
      "        [    0.0361],\n",
      "        [    0.0352],\n",
      "        [    0.0496],\n",
      "        [    0.0688],\n",
      "        [    0.0461],\n",
      "        [    0.0592],\n",
      "        [    0.0200],\n",
      "        [    0.0588],\n",
      "        [    0.0434],\n",
      "        [    0.0382],\n",
      "        [    0.0504],\n",
      "        [    0.0496],\n",
      "        [    0.0611],\n",
      "        [    0.0536],\n",
      "        [    0.0755],\n",
      "        [    0.0324],\n",
      "        [    0.0904],\n",
      "        [    0.0698],\n",
      "        [    0.0650],\n",
      "        [    0.0799],\n",
      "        [    0.0202],\n",
      "        [    0.0640],\n",
      "        [    0.0915],\n",
      "        [    0.0878],\n",
      "        [    0.0296],\n",
      "        [    0.0969],\n",
      "        [    0.0840],\n",
      "        [    0.0081],\n",
      "        [    0.0210],\n",
      "        [    0.0815],\n",
      "        [    0.0851],\n",
      "        [    0.0174],\n",
      "        [    0.0812],\n",
      "        [    0.0908],\n",
      "        [    0.0762],\n",
      "        [    0.0827],\n",
      "        [    0.0061],\n",
      "        [    0.0928],\n",
      "        [    0.0124],\n",
      "        [    0.0889],\n",
      "        [    0.0066],\n",
      "        [    0.0200],\n",
      "        [    0.1058],\n",
      "        [    0.0044],\n",
      "        [    0.0322],\n",
      "        [    0.1337],\n",
      "        [    0.0017],\n",
      "        [    0.0311],\n",
      "        [    0.1001],\n",
      "        [    0.1293],\n",
      "        [    0.0147],\n",
      "        [    0.1189],\n",
      "        [    0.1230],\n",
      "        [    0.0119],\n",
      "        [    0.0174],\n",
      "        [    0.1510],\n",
      "        [    0.1172],\n",
      "        [    0.0476],\n",
      "        [    0.1247],\n",
      "        [    0.1408],\n",
      "        [    0.1237],\n",
      "        [    0.0211],\n",
      "        [    0.0408],\n",
      "        [    0.1634],\n",
      "        [    0.0440],\n",
      "        [    0.0686],\n",
      "        [    0.0726],\n",
      "        [    0.1503],\n",
      "        [    0.3002],\n",
      "        [    0.3142],\n",
      "        [    0.3172],\n",
      "        [    0.3191],\n",
      "        [    0.3201],\n",
      "        [    0.3296],\n",
      "        [    0.3314],\n",
      "        [    0.3325],\n",
      "        [    0.3399],\n",
      "        [    0.3417],\n",
      "        [    0.3438],\n",
      "        [    0.3448],\n",
      "        [    0.3480],\n",
      "        [    0.3671]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0081],\n",
      "        [0.0107],\n",
      "        [0.0009],\n",
      "        [0.0048],\n",
      "        [0.0165],\n",
      "        [0.0036],\n",
      "        [0.0224],\n",
      "        [0.0055],\n",
      "        [0.0270],\n",
      "        [0.0101],\n",
      "        [0.0213],\n",
      "        [0.0207],\n",
      "        [0.0102],\n",
      "        [0.0081],\n",
      "        [0.0060],\n",
      "        [0.0015],\n",
      "        [0.0155],\n",
      "        [0.0284],\n",
      "        [0.0197],\n",
      "        [0.0026],\n",
      "        [0.0009],\n",
      "        [0.0107],\n",
      "        [0.0013],\n",
      "        [0.0280],\n",
      "        [0.0120],\n",
      "        [0.0222],\n",
      "        [0.0154],\n",
      "        [0.0134],\n",
      "        [0.0495],\n",
      "        [0.0069],\n",
      "        [0.0346],\n",
      "        [0.0769],\n",
      "        [0.0035],\n",
      "        [0.0058],\n",
      "        [0.0823],\n",
      "        [0.0112],\n",
      "        [0.0132],\n",
      "        [0.0203],\n",
      "        [0.0408],\n",
      "        [0.0307],\n",
      "        [0.0363],\n",
      "        [0.0365],\n",
      "        [0.0170],\n",
      "        [0.0119],\n",
      "        [0.0192],\n",
      "        [0.0332],\n",
      "        [0.0233],\n",
      "        [0.0195],\n",
      "        [0.0338],\n",
      "        [0.0510],\n",
      "        [0.0191],\n",
      "        [0.0386],\n",
      "        [0.0529],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0346],\n",
      "        [0.0327],\n",
      "        [0.0469],\n",
      "        [0.0677],\n",
      "        [0.0438],\n",
      "        [0.0611],\n",
      "        [0.0168],\n",
      "        [0.0564],\n",
      "        [0.0457],\n",
      "        [0.0407],\n",
      "        [0.0533],\n",
      "        [0.0479],\n",
      "        [0.0594],\n",
      "        [0.0520],\n",
      "        [0.0752],\n",
      "        [0.0350],\n",
      "        [0.0923],\n",
      "        [0.0677],\n",
      "        [0.0665],\n",
      "        [0.0825],\n",
      "        [0.0177],\n",
      "        [0.0662],\n",
      "        [0.0894],\n",
      "        [0.0852],\n",
      "        [0.0275],\n",
      "        [0.0944],\n",
      "        [0.0858],\n",
      "        [0.0059],\n",
      "        [0.0189],\n",
      "        [0.0787],\n",
      "        [0.0826],\n",
      "        [0.0153],\n",
      "        [0.0792],\n",
      "        [0.0921],\n",
      "        [0.0739],\n",
      "        [0.0836],\n",
      "        [0.0083],\n",
      "        [0.0900],\n",
      "        [0.0106],\n",
      "        [0.0910],\n",
      "        [0.0040],\n",
      "        [0.0224],\n",
      "        [0.1041],\n",
      "        [0.0034],\n",
      "        [0.0350],\n",
      "        [0.1316],\n",
      "        [0.0032],\n",
      "        [0.0336],\n",
      "        [0.1022],\n",
      "        [0.1270],\n",
      "        [0.0167],\n",
      "        [0.1205],\n",
      "        [0.1253],\n",
      "        [0.0139],\n",
      "        [0.0200],\n",
      "        [0.1525],\n",
      "        [0.1149],\n",
      "        [0.0501],\n",
      "        [0.1220],\n",
      "        [0.1418],\n",
      "        [0.1256],\n",
      "        [0.0228],\n",
      "        [0.0425],\n",
      "        [0.1611],\n",
      "        [0.0460],\n",
      "        [0.0704],\n",
      "        [0.0740],\n",
      "        [0.1414],\n",
      "        [0.2997],\n",
      "        [0.3137],\n",
      "        [0.3167],\n",
      "        [0.3186],\n",
      "        [0.3196],\n",
      "        [0.3291],\n",
      "        [0.3309],\n",
      "        [0.3321],\n",
      "        [0.3394],\n",
      "        [0.3412],\n",
      "        [0.3433],\n",
      "        [0.3444],\n",
      "        [0.3476],\n",
      "        [0.3667]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 90.05132913589478\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 138\n",
      "剩餘X 資料 torch.Size([22, 18])\n",
      "剩餘Y 資料 torch.Size([22, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1351577788591385, 14)\n",
      "The second_loss value of k: (0.14030353724956512, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0825])\n",
      "目前模型的Data狀態 torch.Size([138, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1250],\n",
      "        [ 0.3473],\n",
      "        [ 0.1077],\n",
      "        [ 0.6286],\n",
      "        [ 0.2932],\n",
      "        [ 0.2658],\n",
      "        [ 0.6215],\n",
      "        [ 0.3598],\n",
      "        [ 0.1908],\n",
      "        [ 0.6185],\n",
      "        [ 0.1407],\n",
      "        [ 0.3371],\n",
      "        [ 0.6064],\n",
      "        [ 0.5989],\n",
      "        [ 0.1266],\n",
      "        [ 0.1825],\n",
      "        [ 0.2181],\n",
      "        [ 0.1637],\n",
      "        [ 0.6233],\n",
      "        [ 0.1222],\n",
      "        [ 0.4395],\n",
      "        [ 0.5259],\n",
      "        [ 0.4726],\n",
      "        [ 0.2955],\n",
      "        [ 0.6185],\n",
      "        [ 0.5628],\n",
      "        [ 0.3481],\n",
      "        [ 0.6350],\n",
      "        [ 0.4636],\n",
      "        [ 0.6346],\n",
      "        [ 0.6421],\n",
      "        [ 0.4890],\n",
      "        [ 0.1417],\n",
      "        [ 0.5669],\n",
      "        [ 0.4820],\n",
      "        [ 0.3533],\n",
      "        [ 0.6038],\n",
      "        [ 0.2120],\n",
      "        [ 0.2663],\n",
      "        [ 0.3279],\n",
      "        [ 0.1439],\n",
      "        [ 0.3222],\n",
      "        [ 0.1806],\n",
      "        [ 0.2010],\n",
      "        [ 0.5751],\n",
      "        [ 0.3042],\n",
      "        [ 0.5823],\n",
      "        [ 0.4524],\n",
      "        [ 0.5817],\n",
      "        [ 0.2036],\n",
      "        [ 0.3720],\n",
      "        [ 0.4324],\n",
      "        [ 0.4724],\n",
      "        [ 0.6363],\n",
      "        [ 0.1204],\n",
      "        [ 0.2154],\n",
      "        [ 0.1258],\n",
      "        [ 0.5734],\n",
      "        [ 0.5552],\n",
      "        [ 0.3690],\n",
      "        [ 0.6106],\n",
      "        [ 0.5975],\n",
      "        [ 0.5157],\n",
      "        [ 0.4883],\n",
      "        [ 0.4238],\n",
      "        [ 0.4339],\n",
      "        [ 0.2355],\n",
      "        [ 0.2795],\n",
      "        [ 0.2111],\n",
      "        [ 0.1933],\n",
      "        [ 0.4481],\n",
      "        [ 0.2155],\n",
      "        [ 0.1583],\n",
      "        [ 0.2245],\n",
      "        [ 0.3267],\n",
      "        [ 0.4441],\n",
      "        [ 0.4568],\n",
      "        [ 0.3544],\n",
      "        [ 0.3993],\n",
      "        [ 0.5270],\n",
      "        [ 0.4059],\n",
      "        [ 0.3131],\n",
      "        [ 0.4513],\n",
      "        [ 0.4756],\n",
      "        [ 0.1969],\n",
      "        [ 0.1635],\n",
      "        [ 0.5079],\n",
      "        [ 0.2420],\n",
      "        [ 0.3600],\n",
      "        [ 0.1010],\n",
      "        [ 0.3674],\n",
      "        [ 0.4243],\n",
      "        [ 0.2656],\n",
      "        [ 0.5357],\n",
      "        [ 0.4419],\n",
      "        [ 0.4922],\n",
      "        [ 0.4158],\n",
      "        [ 0.2950],\n",
      "        [ 0.4951],\n",
      "        [ 0.4458],\n",
      "        [ 0.4580],\n",
      "        [ 0.5380],\n",
      "        [ 0.3896],\n",
      "        [ 0.3963],\n",
      "        [ 0.4137],\n",
      "        [ 0.4515],\n",
      "        [ 0.3160],\n",
      "        [ 0.3419],\n",
      "        [ 0.4400],\n",
      "        [ 0.4654],\n",
      "        [ 0.1668],\n",
      "        [ 0.1149],\n",
      "        [ 0.3935],\n",
      "        [ 0.2664],\n",
      "        [ 0.1608],\n",
      "        [ 0.3702],\n",
      "        [ 0.4470],\n",
      "        [ 0.5271],\n",
      "        [ 0.3884],\n",
      "        [ 0.4602],\n",
      "        [ 0.4131],\n",
      "        [ 0.4263],\n",
      "        [-0.0227],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501],\n",
      "        [ 0.4501]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0081],\n",
      "        [0.0107],\n",
      "        [0.0009],\n",
      "        [0.0048],\n",
      "        [0.0165],\n",
      "        [0.0036],\n",
      "        [0.0224],\n",
      "        [0.0055],\n",
      "        [0.0270],\n",
      "        [0.0101],\n",
      "        [0.0213],\n",
      "        [0.0207],\n",
      "        [0.0102],\n",
      "        [0.0081],\n",
      "        [0.0060],\n",
      "        [0.0015],\n",
      "        [0.0155],\n",
      "        [0.0284],\n",
      "        [0.0197],\n",
      "        [0.0026],\n",
      "        [0.0009],\n",
      "        [0.0107],\n",
      "        [0.0013],\n",
      "        [0.0280],\n",
      "        [0.0120],\n",
      "        [0.0222],\n",
      "        [0.0154],\n",
      "        [0.0134],\n",
      "        [0.0495],\n",
      "        [0.0069],\n",
      "        [0.0346],\n",
      "        [0.0769],\n",
      "        [0.0035],\n",
      "        [0.0058],\n",
      "        [0.0823],\n",
      "        [0.0112],\n",
      "        [0.0132],\n",
      "        [0.0203],\n",
      "        [0.0408],\n",
      "        [0.0307],\n",
      "        [0.0363],\n",
      "        [0.0365],\n",
      "        [0.0170],\n",
      "        [0.0119],\n",
      "        [0.0192],\n",
      "        [0.0332],\n",
      "        [0.0233],\n",
      "        [0.0195],\n",
      "        [0.0338],\n",
      "        [0.0510],\n",
      "        [0.0191],\n",
      "        [0.0386],\n",
      "        [0.0529],\n",
      "        [0.0291],\n",
      "        [0.0297],\n",
      "        [0.0346],\n",
      "        [0.0327],\n",
      "        [0.0469],\n",
      "        [0.0677],\n",
      "        [0.0438],\n",
      "        [0.0611],\n",
      "        [0.0168],\n",
      "        [0.0564],\n",
      "        [0.0457],\n",
      "        [0.0407],\n",
      "        [0.0533],\n",
      "        [0.0479],\n",
      "        [0.0594],\n",
      "        [0.0520],\n",
      "        [0.0752],\n",
      "        [0.0350],\n",
      "        [0.0923],\n",
      "        [0.0677],\n",
      "        [0.0665],\n",
      "        [0.0825],\n",
      "        [0.0177],\n",
      "        [0.0662],\n",
      "        [0.0894],\n",
      "        [0.0852],\n",
      "        [0.0275],\n",
      "        [0.0944],\n",
      "        [0.0858],\n",
      "        [0.0059],\n",
      "        [0.0189],\n",
      "        [0.0787],\n",
      "        [0.0826],\n",
      "        [0.0153],\n",
      "        [0.0792],\n",
      "        [0.0921],\n",
      "        [0.0739],\n",
      "        [0.0836],\n",
      "        [0.0083],\n",
      "        [0.0900],\n",
      "        [0.0106],\n",
      "        [0.0910],\n",
      "        [0.0040],\n",
      "        [0.0224],\n",
      "        [0.1041],\n",
      "        [0.0034],\n",
      "        [0.0350],\n",
      "        [0.1316],\n",
      "        [0.0032],\n",
      "        [0.0336],\n",
      "        [0.1022],\n",
      "        [0.1270],\n",
      "        [0.0167],\n",
      "        [0.1205],\n",
      "        [0.1253],\n",
      "        [0.0139],\n",
      "        [0.0200],\n",
      "        [0.1525],\n",
      "        [0.1149],\n",
      "        [0.0501],\n",
      "        [0.1220],\n",
      "        [0.1418],\n",
      "        [0.1256],\n",
      "        [0.0228],\n",
      "        [0.0425],\n",
      "        [0.1611],\n",
      "        [0.0460],\n",
      "        [0.0704],\n",
      "        [0.0740],\n",
      "        [0.1414],\n",
      "        [0.2997],\n",
      "        [0.3137],\n",
      "        [0.3167],\n",
      "        [0.3186],\n",
      "        [0.3196],\n",
      "        [0.3291],\n",
      "        [0.3309],\n",
      "        [0.3321],\n",
      "        [0.3394],\n",
      "        [0.3412],\n",
      "        [0.3433],\n",
      "        [0.3444],\n",
      "        [0.3476],\n",
      "        [0.3667],\n",
      "        [0.3676]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0078],\n",
      "        [0.0102],\n",
      "        [0.0009],\n",
      "        [0.0052],\n",
      "        [0.0167],\n",
      "        [0.0033],\n",
      "        [0.0229],\n",
      "        [0.0047],\n",
      "        [0.0280],\n",
      "        [0.0111],\n",
      "        [0.0213],\n",
      "        [0.0201],\n",
      "        [0.0109],\n",
      "        [0.0093],\n",
      "        [0.0061],\n",
      "        [0.0014],\n",
      "        [0.0146],\n",
      "        [0.0293],\n",
      "        [0.0188],\n",
      "        [0.0026],\n",
      "        [0.0013],\n",
      "        [0.0110],\n",
      "        [0.0022],\n",
      "        [0.0282],\n",
      "        [0.0125],\n",
      "        [0.0222],\n",
      "        [0.0145],\n",
      "        [0.0139],\n",
      "        [0.0477],\n",
      "        [0.0062],\n",
      "        [0.0341],\n",
      "        [0.0759],\n",
      "        [0.0031],\n",
      "        [0.0046],\n",
      "        [0.0813],\n",
      "        [0.0115],\n",
      "        [0.0123],\n",
      "        [0.0211],\n",
      "        [0.0403],\n",
      "        [0.0301],\n",
      "        [0.0366],\n",
      "        [0.0363],\n",
      "        [0.0175],\n",
      "        [0.0117],\n",
      "        [0.0181],\n",
      "        [0.0328],\n",
      "        [0.0222],\n",
      "        [0.0205],\n",
      "        [0.0330],\n",
      "        [0.0506],\n",
      "        [0.0177],\n",
      "        [0.0395],\n",
      "        [0.0518],\n",
      "        [0.0282],\n",
      "        [0.0294],\n",
      "        [0.0349],\n",
      "        [0.0326],\n",
      "        [0.0464],\n",
      "        [0.0685],\n",
      "        [0.0431],\n",
      "        [0.0605],\n",
      "        [0.0159],\n",
      "        [0.0559],\n",
      "        [0.0461],\n",
      "        [0.0415],\n",
      "        [0.0546],\n",
      "        [0.0479],\n",
      "        [0.0594],\n",
      "        [0.0523],\n",
      "        [0.0746],\n",
      "        [0.0354],\n",
      "        [0.0920],\n",
      "        [0.0681],\n",
      "        [0.0660],\n",
      "        [0.0826],\n",
      "        [0.0172],\n",
      "        [0.0666],\n",
      "        [0.0893],\n",
      "        [0.0840],\n",
      "        [0.0271],\n",
      "        [0.0936],\n",
      "        [0.0860],\n",
      "        [0.0054],\n",
      "        [0.0186],\n",
      "        [0.0782],\n",
      "        [0.0825],\n",
      "        [0.0150],\n",
      "        [0.0789],\n",
      "        [0.0917],\n",
      "        [0.0735],\n",
      "        [0.0828],\n",
      "        [0.0086],\n",
      "        [0.0895],\n",
      "        [0.0107],\n",
      "        [0.0913],\n",
      "        [0.0031],\n",
      "        [0.0230],\n",
      "        [0.1042],\n",
      "        [0.0045],\n",
      "        [0.0360],\n",
      "        [0.1315],\n",
      "        [0.0029],\n",
      "        [0.0342],\n",
      "        [0.1020],\n",
      "        [0.1266],\n",
      "        [0.0168],\n",
      "        [0.1206],\n",
      "        [0.1253],\n",
      "        [0.0140],\n",
      "        [0.0209],\n",
      "        [0.1519],\n",
      "        [0.1144],\n",
      "        [0.0508],\n",
      "        [0.1217],\n",
      "        [0.1408],\n",
      "        [0.1252],\n",
      "        [0.0228],\n",
      "        [0.0422],\n",
      "        [0.1606],\n",
      "        [0.0461],\n",
      "        [0.0702],\n",
      "        [0.0734],\n",
      "        [0.1306],\n",
      "        [0.2993],\n",
      "        [0.3133],\n",
      "        [0.3163],\n",
      "        [0.3181],\n",
      "        [0.3192],\n",
      "        [0.3286],\n",
      "        [0.3304],\n",
      "        [0.3316],\n",
      "        [0.3390],\n",
      "        [0.3408],\n",
      "        [0.3429],\n",
      "        [0.3439],\n",
      "        [0.3471],\n",
      "        [0.3662],\n",
      "        [0.3672]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 90.2891116142273\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 139\n",
      "剩餘X 資料 torch.Size([21, 18])\n",
      "剩餘Y 資料 torch.Size([21, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.13996082544326782, 4)\n",
      "The second_loss value of k: (0.14326253533363342, 13)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引4，y= tensor([0.0755])\n",
      "目前模型的Data狀態 torch.Size([139, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1253],\n",
      "        [ 0.3478],\n",
      "        [ 0.1077],\n",
      "        [ 0.6282],\n",
      "        [ 0.2930],\n",
      "        [ 0.2655],\n",
      "        [ 0.6210],\n",
      "        [ 0.3590],\n",
      "        [ 0.1898],\n",
      "        [ 0.6175],\n",
      "        [ 0.1407],\n",
      "        [ 0.3378],\n",
      "        [ 0.6057],\n",
      "        [ 0.5977],\n",
      "        [ 0.1265],\n",
      "        [ 0.1826],\n",
      "        [ 0.2190],\n",
      "        [ 0.1628],\n",
      "        [ 0.6242],\n",
      "        [ 0.1222],\n",
      "        [ 0.4391],\n",
      "        [ 0.5256],\n",
      "        [ 0.4717],\n",
      "        [ 0.2957],\n",
      "        [ 0.6181],\n",
      "        [ 0.5627],\n",
      "        [ 0.3489],\n",
      "        [ 0.6355],\n",
      "        [ 0.4619],\n",
      "        [ 0.6340],\n",
      "        [ 0.6427],\n",
      "        [ 0.4880],\n",
      "        [ 0.1413],\n",
      "        [ 0.5657],\n",
      "        [ 0.4809],\n",
      "        [ 0.3530],\n",
      "        [ 0.6030],\n",
      "        [ 0.2128],\n",
      "        [ 0.2668],\n",
      "        [ 0.3284],\n",
      "        [ 0.1436],\n",
      "        [ 0.3224],\n",
      "        [ 0.1801],\n",
      "        [ 0.2009],\n",
      "        [ 0.5740],\n",
      "        [ 0.3038],\n",
      "        [ 0.5812],\n",
      "        [ 0.4515],\n",
      "        [ 0.5809],\n",
      "        [ 0.2039],\n",
      "        [ 0.3734],\n",
      "        [ 0.4314],\n",
      "        [ 0.4714],\n",
      "        [ 0.6355],\n",
      "        [ 0.1202],\n",
      "        [ 0.2157],\n",
      "        [ 0.1257],\n",
      "        [ 0.5729],\n",
      "        [ 0.5560],\n",
      "        [ 0.3683],\n",
      "        [ 0.6112],\n",
      "        [ 0.5966],\n",
      "        [ 0.5151],\n",
      "        [ 0.4879],\n",
      "        [ 0.4230],\n",
      "        [ 0.4327],\n",
      "        [ 0.2356],\n",
      "        [ 0.2796],\n",
      "        [ 0.2114],\n",
      "        [ 0.1927],\n",
      "        [ 0.4477],\n",
      "        [ 0.2158],\n",
      "        [ 0.1587],\n",
      "        [ 0.2249],\n",
      "        [ 0.3265],\n",
      "        [ 0.4435],\n",
      "        [ 0.4564],\n",
      "        [ 0.3543],\n",
      "        [ 0.3982],\n",
      "        [ 0.5266],\n",
      "        [ 0.4050],\n",
      "        [ 0.3129],\n",
      "        [ 0.4509],\n",
      "        [ 0.4752],\n",
      "        [ 0.1964],\n",
      "        [ 0.1634],\n",
      "        [ 0.5076],\n",
      "        [ 0.2416],\n",
      "        [ 0.3604],\n",
      "        [ 0.1005],\n",
      "        [ 0.3682],\n",
      "        [ 0.4239],\n",
      "        [ 0.2651],\n",
      "        [ 0.5357],\n",
      "        [ 0.4416],\n",
      "        [ 0.4913],\n",
      "        [ 0.4152],\n",
      "        [ 0.2951],\n",
      "        [ 0.4962],\n",
      "        [ 0.4448],\n",
      "        [ 0.4578],\n",
      "        [ 0.5384],\n",
      "        [ 0.3890],\n",
      "        [ 0.3965],\n",
      "        [ 0.4133],\n",
      "        [ 0.4514],\n",
      "        [ 0.3159],\n",
      "        [ 0.3419],\n",
      "        [ 0.4400],\n",
      "        [ 0.4645],\n",
      "        [ 0.1674],\n",
      "        [ 0.1144],\n",
      "        [ 0.3928],\n",
      "        [ 0.2661],\n",
      "        [ 0.1619],\n",
      "        [ 0.3706],\n",
      "        [ 0.4471],\n",
      "        [ 0.5274],\n",
      "        [ 0.3880],\n",
      "        [ 0.4600],\n",
      "        [ 0.4133],\n",
      "        [ 0.4269],\n",
      "        [-0.0119],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496],\n",
      "        [ 0.4496]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0078],\n",
      "        [0.0102],\n",
      "        [0.0009],\n",
      "        [0.0052],\n",
      "        [0.0167],\n",
      "        [0.0033],\n",
      "        [0.0229],\n",
      "        [0.0047],\n",
      "        [0.0280],\n",
      "        [0.0111],\n",
      "        [0.0213],\n",
      "        [0.0201],\n",
      "        [0.0109],\n",
      "        [0.0093],\n",
      "        [0.0061],\n",
      "        [0.0014],\n",
      "        [0.0146],\n",
      "        [0.0293],\n",
      "        [0.0188],\n",
      "        [0.0026],\n",
      "        [0.0013],\n",
      "        [0.0110],\n",
      "        [0.0022],\n",
      "        [0.0282],\n",
      "        [0.0125],\n",
      "        [0.0222],\n",
      "        [0.0145],\n",
      "        [0.0139],\n",
      "        [0.0477],\n",
      "        [0.0062],\n",
      "        [0.0341],\n",
      "        [0.0759],\n",
      "        [0.0031],\n",
      "        [0.0046],\n",
      "        [0.0813],\n",
      "        [0.0115],\n",
      "        [0.0123],\n",
      "        [0.0211],\n",
      "        [0.0403],\n",
      "        [0.0301],\n",
      "        [0.0366],\n",
      "        [0.0363],\n",
      "        [0.0175],\n",
      "        [0.0117],\n",
      "        [0.0181],\n",
      "        [0.0328],\n",
      "        [0.0222],\n",
      "        [0.0205],\n",
      "        [0.0330],\n",
      "        [0.0506],\n",
      "        [0.0177],\n",
      "        [0.0395],\n",
      "        [0.0518],\n",
      "        [0.0282],\n",
      "        [0.0294],\n",
      "        [0.0349],\n",
      "        [0.0326],\n",
      "        [0.0464],\n",
      "        [0.0685],\n",
      "        [0.0431],\n",
      "        [0.0605],\n",
      "        [0.0159],\n",
      "        [0.0559],\n",
      "        [0.0461],\n",
      "        [0.0415],\n",
      "        [0.0546],\n",
      "        [0.0479],\n",
      "        [0.0594],\n",
      "        [0.0523],\n",
      "        [0.0746],\n",
      "        [0.0354],\n",
      "        [0.0920],\n",
      "        [0.0681],\n",
      "        [0.0660],\n",
      "        [0.0826],\n",
      "        [0.0172],\n",
      "        [0.0666],\n",
      "        [0.0893],\n",
      "        [0.0840],\n",
      "        [0.0271],\n",
      "        [0.0936],\n",
      "        [0.0860],\n",
      "        [0.0054],\n",
      "        [0.0186],\n",
      "        [0.0782],\n",
      "        [0.0825],\n",
      "        [0.0150],\n",
      "        [0.0789],\n",
      "        [0.0917],\n",
      "        [0.0735],\n",
      "        [0.0828],\n",
      "        [0.0086],\n",
      "        [0.0895],\n",
      "        [0.0107],\n",
      "        [0.0913],\n",
      "        [0.0031],\n",
      "        [0.0230],\n",
      "        [0.1042],\n",
      "        [0.0045],\n",
      "        [0.0360],\n",
      "        [0.1315],\n",
      "        [0.0029],\n",
      "        [0.0342],\n",
      "        [0.1020],\n",
      "        [0.1266],\n",
      "        [0.0168],\n",
      "        [0.1206],\n",
      "        [0.1253],\n",
      "        [0.0140],\n",
      "        [0.0209],\n",
      "        [0.1519],\n",
      "        [0.1144],\n",
      "        [0.0508],\n",
      "        [0.1217],\n",
      "        [0.1408],\n",
      "        [0.1252],\n",
      "        [0.0228],\n",
      "        [0.0422],\n",
      "        [0.1606],\n",
      "        [0.0461],\n",
      "        [0.0702],\n",
      "        [0.0734],\n",
      "        [0.1306],\n",
      "        [0.2993],\n",
      "        [0.3133],\n",
      "        [0.3163],\n",
      "        [0.3181],\n",
      "        [0.3192],\n",
      "        [0.3286],\n",
      "        [0.3304],\n",
      "        [0.3316],\n",
      "        [0.3390],\n",
      "        [0.3408],\n",
      "        [0.3429],\n",
      "        [0.3439],\n",
      "        [0.3471],\n",
      "        [0.3662],\n",
      "        [0.3672],\n",
      "        [0.3741]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0063],\n",
      "        [    0.0079],\n",
      "        [    0.0004],\n",
      "        [    0.0019],\n",
      "        [    0.0153],\n",
      "        [    0.0051],\n",
      "        [    0.0195],\n",
      "        [    0.0064],\n",
      "        [    0.0275],\n",
      "        [    0.0081],\n",
      "        [    0.0201],\n",
      "        [    0.0177],\n",
      "        [    0.0076],\n",
      "        [    0.0063],\n",
      "        [    0.0049],\n",
      "        [    0.0002],\n",
      "        [    0.0126],\n",
      "        [    0.0286],\n",
      "        [    0.0146],\n",
      "        [    0.0040],\n",
      "        [    0.0015],\n",
      "        [    0.0079],\n",
      "        [    0.0003],\n",
      "        [    0.0308],\n",
      "        [    0.0092],\n",
      "        [    0.0255],\n",
      "        [    0.0120],\n",
      "        [    0.0178],\n",
      "        [    0.0489],\n",
      "        [    0.0096],\n",
      "        [    0.0301],\n",
      "        [    0.0784],\n",
      "        [    0.0041],\n",
      "        [    0.0073],\n",
      "        [    0.0832],\n",
      "        [    0.0096],\n",
      "        [    0.0155],\n",
      "        [    0.0232],\n",
      "        [    0.0382],\n",
      "        [    0.0278],\n",
      "        [    0.0356],\n",
      "        [    0.0342],\n",
      "        [    0.0172],\n",
      "        [    0.0129],\n",
      "        [    0.0210],\n",
      "        [    0.0346],\n",
      "        [    0.0251],\n",
      "        [    0.0180],\n",
      "        [    0.0361],\n",
      "        [    0.0491],\n",
      "        [    0.0148],\n",
      "        [    0.0371],\n",
      "        [    0.0543],\n",
      "        [    0.0314],\n",
      "        [    0.0306],\n",
      "        [    0.0367],\n",
      "        [    0.0338],\n",
      "        [    0.0497],\n",
      "        [    0.0719],\n",
      "        [    0.0450],\n",
      "        [    0.0566],\n",
      "        [    0.0188],\n",
      "        [    0.0587],\n",
      "        [    0.0432],\n",
      "        [    0.0391],\n",
      "        [    0.0523],\n",
      "        [    0.0497],\n",
      "        [    0.0614],\n",
      "        [    0.0537],\n",
      "        [    0.0747],\n",
      "        [    0.0336],\n",
      "        [    0.0900],\n",
      "        [    0.0697],\n",
      "        [    0.0635],\n",
      "        [    0.0811],\n",
      "        [    0.0195],\n",
      "        [    0.0638],\n",
      "        [    0.0919],\n",
      "        [    0.0857],\n",
      "        [    0.0301],\n",
      "        [    0.0955],\n",
      "        [    0.0841],\n",
      "        [    0.0082],\n",
      "        [    0.0214],\n",
      "        [    0.0793],\n",
      "        [    0.0838],\n",
      "        [    0.0179],\n",
      "        [    0.0804],\n",
      "        [    0.0894],\n",
      "        [    0.0743],\n",
      "        [    0.0803],\n",
      "        [    0.0059],\n",
      "        [    0.0907],\n",
      "        [    0.0138],\n",
      "        [    0.0885],\n",
      "        [    0.0056],\n",
      "        [    0.0206],\n",
      "        [    0.1062],\n",
      "        [    0.0082],\n",
      "        [    0.0339],\n",
      "        [    0.1343],\n",
      "        [    0.0004],\n",
      "        [    0.0317],\n",
      "        [    0.1001],\n",
      "        [    0.1291],\n",
      "        [    0.0139],\n",
      "        [    0.1186],\n",
      "        [    0.1235],\n",
      "        [    0.0109],\n",
      "        [    0.0182],\n",
      "        [    0.1497],\n",
      "        [    0.1151],\n",
      "        [    0.0483],\n",
      "        [    0.1230],\n",
      "        [    0.1382],\n",
      "        [    0.1232],\n",
      "        [    0.0195],\n",
      "        [    0.0391],\n",
      "        [    0.1631],\n",
      "        [    0.0429],\n",
      "        [    0.0673],\n",
      "        [    0.0701],\n",
      "        [    0.1217],\n",
      "        [    0.2988],\n",
      "        [    0.3129],\n",
      "        [    0.3158],\n",
      "        [    0.3177],\n",
      "        [    0.3187],\n",
      "        [    0.3282],\n",
      "        [    0.3300],\n",
      "        [    0.3312],\n",
      "        [    0.3386],\n",
      "        [    0.3404],\n",
      "        [    0.3424],\n",
      "        [    0.3435],\n",
      "        [    0.3467],\n",
      "        [    0.3658],\n",
      "        [    0.3668],\n",
      "        [    0.3737]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 90.52705669403076\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 140\n",
      "剩餘X 資料 torch.Size([20, 18])\n",
      "剩餘Y 資料 torch.Size([20, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14294618368148804, 12)\n",
      "The second_loss value of k: (0.14446072280406952, 3)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引12，y= tensor([0.0711])\n",
      "目前模型的Data狀態 torch.Size([140, 1])\n",
      "<<預測值>>\n",
      "tensor([[ 0.1268],\n",
      "        [ 0.3501],\n",
      "        [ 0.1090],\n",
      "        [ 0.6315],\n",
      "        [ 0.2944],\n",
      "        [ 0.2673],\n",
      "        [ 0.6245],\n",
      "        [ 0.3607],\n",
      "        [ 0.1903],\n",
      "        [ 0.6205],\n",
      "        [ 0.1419],\n",
      "        [ 0.3402],\n",
      "        [ 0.6090],\n",
      "        [ 0.6007],\n",
      "        [ 0.1277],\n",
      "        [ 0.1842],\n",
      "        [ 0.2210],\n",
      "        [ 0.1634],\n",
      "        [ 0.6284],\n",
      "        [ 0.1236],\n",
      "        [ 0.4419],\n",
      "        [ 0.5288],\n",
      "        [ 0.4743],\n",
      "        [ 0.2982],\n",
      "        [ 0.6213],\n",
      "        [ 0.5661],\n",
      "        [ 0.3515],\n",
      "        [ 0.6394],\n",
      "        [ 0.4631],\n",
      "        [ 0.6373],\n",
      "        [ 0.6467],\n",
      "        [ 0.4905],\n",
      "        [ 0.1423],\n",
      "        [ 0.5684],\n",
      "        [ 0.4828],\n",
      "        [ 0.3549],\n",
      "        [ 0.6062],\n",
      "        [ 0.2149],\n",
      "        [ 0.2689],\n",
      "        [ 0.3308],\n",
      "        [ 0.1447],\n",
      "        [ 0.3245],\n",
      "        [ 0.1803],\n",
      "        [ 0.2020],\n",
      "        [ 0.5769],\n",
      "        [ 0.3056],\n",
      "        [ 0.5841],\n",
      "        [ 0.4539],\n",
      "        [ 0.5840],\n",
      "        [ 0.2054],\n",
      "        [ 0.3763],\n",
      "        [ 0.4338],\n",
      "        [ 0.4739],\n",
      "        [ 0.6386],\n",
      "        [ 0.1213],\n",
      "        [ 0.2175],\n",
      "        [ 0.1269],\n",
      "        [ 0.5761],\n",
      "        [ 0.5594],\n",
      "        [ 0.3703],\n",
      "        [ 0.6151],\n",
      "        [ 0.5995],\n",
      "        [ 0.5179],\n",
      "        [ 0.4908],\n",
      "        [ 0.4254],\n",
      "        [ 0.4349],\n",
      "        [ 0.2373],\n",
      "        [ 0.2815],\n",
      "        [ 0.2128],\n",
      "        [ 0.1928],\n",
      "        [ 0.4495],\n",
      "        [ 0.2178],\n",
      "        [ 0.1603],\n",
      "        [ 0.2275],\n",
      "        [ 0.3280],\n",
      "        [ 0.4459],\n",
      "        [ 0.4593],\n",
      "        [ 0.3568],\n",
      "        [ 0.3999],\n",
      "        [ 0.5296],\n",
      "        [ 0.4070],\n",
      "        [ 0.3148],\n",
      "        [ 0.4537],\n",
      "        [ 0.4781],\n",
      "        [ 0.1975],\n",
      "        [ 0.1647],\n",
      "        [ 0.5105],\n",
      "        [ 0.2432],\n",
      "        [ 0.3628],\n",
      "        [ 0.1014],\n",
      "        [ 0.3708],\n",
      "        [ 0.4267],\n",
      "        [ 0.2663],\n",
      "        [ 0.5388],\n",
      "        [ 0.4445],\n",
      "        [ 0.4939],\n",
      "        [ 0.4177],\n",
      "        [ 0.2970],\n",
      "        [ 0.4998],\n",
      "        [ 0.4470],\n",
      "        [ 0.4607],\n",
      "        [ 0.5416],\n",
      "        [ 0.3914],\n",
      "        [ 0.3984],\n",
      "        [ 0.4158],\n",
      "        [ 0.4542],\n",
      "        [ 0.3179],\n",
      "        [ 0.3436],\n",
      "        [ 0.4430],\n",
      "        [ 0.4671],\n",
      "        [ 0.1695],\n",
      "        [ 0.1151],\n",
      "        [ 0.3953],\n",
      "        [ 0.2674],\n",
      "        [ 0.1645],\n",
      "        [ 0.3726],\n",
      "        [ 0.4503],\n",
      "        [ 0.5304],\n",
      "        [ 0.3905],\n",
      "        [ 0.4632],\n",
      "        [ 0.4163],\n",
      "        [ 0.4302],\n",
      "        [-0.0030],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492],\n",
      "        [ 0.4492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0063],\n",
      "        [    0.0079],\n",
      "        [    0.0004],\n",
      "        [    0.0019],\n",
      "        [    0.0153],\n",
      "        [    0.0051],\n",
      "        [    0.0195],\n",
      "        [    0.0064],\n",
      "        [    0.0275],\n",
      "        [    0.0081],\n",
      "        [    0.0201],\n",
      "        [    0.0177],\n",
      "        [    0.0076],\n",
      "        [    0.0063],\n",
      "        [    0.0049],\n",
      "        [    0.0002],\n",
      "        [    0.0126],\n",
      "        [    0.0286],\n",
      "        [    0.0146],\n",
      "        [    0.0040],\n",
      "        [    0.0015],\n",
      "        [    0.0079],\n",
      "        [    0.0003],\n",
      "        [    0.0308],\n",
      "        [    0.0092],\n",
      "        [    0.0255],\n",
      "        [    0.0120],\n",
      "        [    0.0178],\n",
      "        [    0.0489],\n",
      "        [    0.0096],\n",
      "        [    0.0301],\n",
      "        [    0.0784],\n",
      "        [    0.0041],\n",
      "        [    0.0073],\n",
      "        [    0.0832],\n",
      "        [    0.0096],\n",
      "        [    0.0155],\n",
      "        [    0.0232],\n",
      "        [    0.0382],\n",
      "        [    0.0278],\n",
      "        [    0.0356],\n",
      "        [    0.0342],\n",
      "        [    0.0172],\n",
      "        [    0.0129],\n",
      "        [    0.0210],\n",
      "        [    0.0346],\n",
      "        [    0.0251],\n",
      "        [    0.0180],\n",
      "        [    0.0361],\n",
      "        [    0.0491],\n",
      "        [    0.0148],\n",
      "        [    0.0371],\n",
      "        [    0.0543],\n",
      "        [    0.0314],\n",
      "        [    0.0306],\n",
      "        [    0.0367],\n",
      "        [    0.0338],\n",
      "        [    0.0497],\n",
      "        [    0.0719],\n",
      "        [    0.0450],\n",
      "        [    0.0566],\n",
      "        [    0.0188],\n",
      "        [    0.0587],\n",
      "        [    0.0432],\n",
      "        [    0.0391],\n",
      "        [    0.0523],\n",
      "        [    0.0497],\n",
      "        [    0.0614],\n",
      "        [    0.0537],\n",
      "        [    0.0747],\n",
      "        [    0.0336],\n",
      "        [    0.0900],\n",
      "        [    0.0697],\n",
      "        [    0.0635],\n",
      "        [    0.0811],\n",
      "        [    0.0195],\n",
      "        [    0.0638],\n",
      "        [    0.0919],\n",
      "        [    0.0857],\n",
      "        [    0.0301],\n",
      "        [    0.0955],\n",
      "        [    0.0841],\n",
      "        [    0.0082],\n",
      "        [    0.0214],\n",
      "        [    0.0793],\n",
      "        [    0.0838],\n",
      "        [    0.0179],\n",
      "        [    0.0804],\n",
      "        [    0.0894],\n",
      "        [    0.0743],\n",
      "        [    0.0803],\n",
      "        [    0.0059],\n",
      "        [    0.0907],\n",
      "        [    0.0138],\n",
      "        [    0.0885],\n",
      "        [    0.0056],\n",
      "        [    0.0206],\n",
      "        [    0.1062],\n",
      "        [    0.0082],\n",
      "        [    0.0339],\n",
      "        [    0.1343],\n",
      "        [    0.0004],\n",
      "        [    0.0317],\n",
      "        [    0.1001],\n",
      "        [    0.1291],\n",
      "        [    0.0139],\n",
      "        [    0.1186],\n",
      "        [    0.1235],\n",
      "        [    0.0109],\n",
      "        [    0.0182],\n",
      "        [    0.1497],\n",
      "        [    0.1151],\n",
      "        [    0.0483],\n",
      "        [    0.1230],\n",
      "        [    0.1382],\n",
      "        [    0.1232],\n",
      "        [    0.0195],\n",
      "        [    0.0391],\n",
      "        [    0.1631],\n",
      "        [    0.0429],\n",
      "        [    0.0673],\n",
      "        [    0.0701],\n",
      "        [    0.1217],\n",
      "        [    0.2988],\n",
      "        [    0.3129],\n",
      "        [    0.3158],\n",
      "        [    0.3177],\n",
      "        [    0.3187],\n",
      "        [    0.3282],\n",
      "        [    0.3300],\n",
      "        [    0.3312],\n",
      "        [    0.3386],\n",
      "        [    0.3404],\n",
      "        [    0.3424],\n",
      "        [    0.3435],\n",
      "        [    0.3467],\n",
      "        [    0.3658],\n",
      "        [    0.3668],\n",
      "        [    0.3737],\n",
      "        [    0.3781]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0066],\n",
      "        [    0.0078],\n",
      "        [    0.0000],\n",
      "        [    0.0023],\n",
      "        [    0.0158],\n",
      "        [    0.0046],\n",
      "        [    0.0200],\n",
      "        [    0.0058],\n",
      "        [    0.0285],\n",
      "        [    0.0090],\n",
      "        [    0.0204],\n",
      "        [    0.0175],\n",
      "        [    0.0083],\n",
      "        [    0.0073],\n",
      "        [    0.0054],\n",
      "        [    0.0000],\n",
      "        [    0.0123],\n",
      "        [    0.0295],\n",
      "        [    0.0142],\n",
      "        [    0.0035],\n",
      "        [    0.0011],\n",
      "        [    0.0083],\n",
      "        [    0.0004],\n",
      "        [    0.0307],\n",
      "        [    0.0097],\n",
      "        [    0.0253],\n",
      "        [    0.0116],\n",
      "        [    0.0180],\n",
      "        [    0.0475],\n",
      "        [    0.0090],\n",
      "        [    0.0299],\n",
      "        [    0.0776],\n",
      "        [    0.0034],\n",
      "        [    0.0063],\n",
      "        [    0.0822],\n",
      "        [    0.0099],\n",
      "        [    0.0148],\n",
      "        [    0.0235],\n",
      "        [    0.0381],\n",
      "        [    0.0277],\n",
      "        [    0.0360],\n",
      "        [    0.0343],\n",
      "        [    0.0176],\n",
      "        [    0.0125],\n",
      "        [    0.0201],\n",
      "        [    0.0341],\n",
      "        [    0.0242],\n",
      "        [    0.0188],\n",
      "        [    0.0354],\n",
      "        [    0.0491],\n",
      "        [    0.0141],\n",
      "        [    0.0379],\n",
      "        [    0.0536],\n",
      "        [    0.0307],\n",
      "        [    0.0300],\n",
      "        [    0.0366],\n",
      "        [    0.0333],\n",
      "        [    0.0492],\n",
      "        [    0.0723],\n",
      "        [    0.0443],\n",
      "        [    0.0564],\n",
      "        [    0.0180],\n",
      "        [    0.0583],\n",
      "        [    0.0436],\n",
      "        [    0.0398],\n",
      "        [    0.0533],\n",
      "        [    0.0495],\n",
      "        [    0.0612],\n",
      "        [    0.0536],\n",
      "        [    0.0742],\n",
      "        [    0.0341],\n",
      "        [    0.0901],\n",
      "        [    0.0696],\n",
      "        [    0.0634],\n",
      "        [    0.0815],\n",
      "        [    0.0189],\n",
      "        [    0.0642],\n",
      "        [    0.0916],\n",
      "        [    0.0848],\n",
      "        [    0.0298],\n",
      "        [    0.0948],\n",
      "        [    0.0844],\n",
      "        [    0.0078],\n",
      "        [    0.0211],\n",
      "        [    0.0786],\n",
      "        [    0.0833],\n",
      "        [    0.0176],\n",
      "        [    0.0800],\n",
      "        [    0.0893],\n",
      "        [    0.0736],\n",
      "        [    0.0799],\n",
      "        [    0.0062],\n",
      "        [    0.0901],\n",
      "        [    0.0137],\n",
      "        [    0.0889],\n",
      "        [    0.0049],\n",
      "        [    0.0211],\n",
      "        [    0.1059],\n",
      "        [    0.0087],\n",
      "        [    0.0346],\n",
      "        [    0.1340],\n",
      "        [    0.0005],\n",
      "        [    0.0323],\n",
      "        [    0.1002],\n",
      "        [    0.1287],\n",
      "        [    0.0142],\n",
      "        [    0.1188],\n",
      "        [    0.1238],\n",
      "        [    0.0111],\n",
      "        [    0.0189],\n",
      "        [    0.1495],\n",
      "        [    0.1145],\n",
      "        [    0.0489],\n",
      "        [    0.1225],\n",
      "        [    0.1378],\n",
      "        [    0.1232],\n",
      "        [    0.0196],\n",
      "        [    0.0391],\n",
      "        [    0.1626],\n",
      "        [    0.0431],\n",
      "        [    0.0674],\n",
      "        [    0.0700],\n",
      "        [    0.1147],\n",
      "        [    0.2984],\n",
      "        [    0.3124],\n",
      "        [    0.3154],\n",
      "        [    0.3173],\n",
      "        [    0.3183],\n",
      "        [    0.3278],\n",
      "        [    0.3296],\n",
      "        [    0.3308],\n",
      "        [    0.3381],\n",
      "        [    0.3399],\n",
      "        [    0.3420],\n",
      "        [    0.3431],\n",
      "        [    0.3463],\n",
      "        [    0.3654],\n",
      "        [    0.3663],\n",
      "        [    0.3733],\n",
      "        [    0.3777]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 90.76448273658752\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 141\n",
      "剩餘X 資料 torch.Size([19, 18])\n",
      "剩餘Y 資料 torch.Size([19, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14413334429264069, 3)\n",
      "The second_loss value of k: (0.14478839933872223, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.0691])\n",
      "目前模型的Data狀態 torch.Size([141, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1266],\n",
      "        [0.3502],\n",
      "        [0.1087],\n",
      "        [0.6310],\n",
      "        [0.2940],\n",
      "        [0.2668],\n",
      "        [0.6239],\n",
      "        [0.3601],\n",
      "        [0.1893],\n",
      "        [0.6196],\n",
      "        [0.1416],\n",
      "        [0.3404],\n",
      "        [0.6083],\n",
      "        [0.5997],\n",
      "        [0.1272],\n",
      "        [0.1840],\n",
      "        [0.2213],\n",
      "        [0.1626],\n",
      "        [0.6288],\n",
      "        [0.1231],\n",
      "        [0.4415],\n",
      "        [0.5284],\n",
      "        [0.4736],\n",
      "        [0.2981],\n",
      "        [0.6209],\n",
      "        [0.5659],\n",
      "        [0.3518],\n",
      "        [0.6396],\n",
      "        [0.4617],\n",
      "        [0.6367],\n",
      "        [0.6468],\n",
      "        [0.4897],\n",
      "        [0.1417],\n",
      "        [0.5674],\n",
      "        [0.4819],\n",
      "        [0.3545],\n",
      "        [0.6054],\n",
      "        [0.2152],\n",
      "        [0.2690],\n",
      "        [0.3309],\n",
      "        [0.1442],\n",
      "        [0.3244],\n",
      "        [0.1800],\n",
      "        [0.2016],\n",
      "        [0.5760],\n",
      "        [0.3051],\n",
      "        [0.5832],\n",
      "        [0.4532],\n",
      "        [0.5833],\n",
      "        [0.2054],\n",
      "        [0.3770],\n",
      "        [0.4330],\n",
      "        [0.4732],\n",
      "        [0.6379],\n",
      "        [0.1208],\n",
      "        [0.2174],\n",
      "        [0.1264],\n",
      "        [0.5756],\n",
      "        [0.5598],\n",
      "        [0.3696],\n",
      "        [0.6153],\n",
      "        [0.5987],\n",
      "        [0.5175],\n",
      "        [0.4904],\n",
      "        [0.4247],\n",
      "        [0.4339],\n",
      "        [0.2371],\n",
      "        [0.2813],\n",
      "        [0.2127],\n",
      "        [0.1923],\n",
      "        [0.4490],\n",
      "        [0.2177],\n",
      "        [0.1602],\n",
      "        [0.2276],\n",
      "        [0.3276],\n",
      "        [0.4453],\n",
      "        [0.4588],\n",
      "        [0.3566],\n",
      "        [0.3990],\n",
      "        [0.5293],\n",
      "        [0.4062],\n",
      "        [0.3145],\n",
      "        [0.4533],\n",
      "        [0.4778],\n",
      "        [0.1968],\n",
      "        [0.1642],\n",
      "        [0.5101],\n",
      "        [0.2427],\n",
      "        [0.3629],\n",
      "        [0.1007],\n",
      "        [0.3711],\n",
      "        [0.4263],\n",
      "        [0.2657],\n",
      "        [0.5388],\n",
      "        [0.4441],\n",
      "        [0.4932],\n",
      "        [0.4172],\n",
      "        [0.2968],\n",
      "        [0.5003],\n",
      "        [0.4462],\n",
      "        [0.4604],\n",
      "        [0.5417],\n",
      "        [0.3909],\n",
      "        [0.3983],\n",
      "        [0.4154],\n",
      "        [0.4540],\n",
      "        [0.3177],\n",
      "        [0.3434],\n",
      "        [0.4428],\n",
      "        [0.4665],\n",
      "        [0.1697],\n",
      "        [0.1145],\n",
      "        [0.3947],\n",
      "        [0.2669],\n",
      "        [0.1649],\n",
      "        [0.3726],\n",
      "        [0.4503],\n",
      "        [0.5305],\n",
      "        [0.3900],\n",
      "        [0.4630],\n",
      "        [0.4161],\n",
      "        [0.4303],\n",
      "        [0.0040],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488],\n",
      "        [0.4488]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0066],\n",
      "        [    0.0078],\n",
      "        [    0.0000],\n",
      "        [    0.0023],\n",
      "        [    0.0158],\n",
      "        [    0.0046],\n",
      "        [    0.0200],\n",
      "        [    0.0058],\n",
      "        [    0.0285],\n",
      "        [    0.0090],\n",
      "        [    0.0204],\n",
      "        [    0.0175],\n",
      "        [    0.0083],\n",
      "        [    0.0073],\n",
      "        [    0.0054],\n",
      "        [    0.0000],\n",
      "        [    0.0123],\n",
      "        [    0.0295],\n",
      "        [    0.0142],\n",
      "        [    0.0035],\n",
      "        [    0.0011],\n",
      "        [    0.0083],\n",
      "        [    0.0004],\n",
      "        [    0.0307],\n",
      "        [    0.0097],\n",
      "        [    0.0253],\n",
      "        [    0.0116],\n",
      "        [    0.0180],\n",
      "        [    0.0475],\n",
      "        [    0.0090],\n",
      "        [    0.0299],\n",
      "        [    0.0776],\n",
      "        [    0.0034],\n",
      "        [    0.0063],\n",
      "        [    0.0822],\n",
      "        [    0.0099],\n",
      "        [    0.0148],\n",
      "        [    0.0235],\n",
      "        [    0.0381],\n",
      "        [    0.0277],\n",
      "        [    0.0360],\n",
      "        [    0.0343],\n",
      "        [    0.0176],\n",
      "        [    0.0125],\n",
      "        [    0.0201],\n",
      "        [    0.0341],\n",
      "        [    0.0242],\n",
      "        [    0.0188],\n",
      "        [    0.0354],\n",
      "        [    0.0491],\n",
      "        [    0.0141],\n",
      "        [    0.0379],\n",
      "        [    0.0536],\n",
      "        [    0.0307],\n",
      "        [    0.0300],\n",
      "        [    0.0366],\n",
      "        [    0.0333],\n",
      "        [    0.0492],\n",
      "        [    0.0723],\n",
      "        [    0.0443],\n",
      "        [    0.0564],\n",
      "        [    0.0180],\n",
      "        [    0.0583],\n",
      "        [    0.0436],\n",
      "        [    0.0398],\n",
      "        [    0.0533],\n",
      "        [    0.0495],\n",
      "        [    0.0612],\n",
      "        [    0.0536],\n",
      "        [    0.0742],\n",
      "        [    0.0341],\n",
      "        [    0.0901],\n",
      "        [    0.0696],\n",
      "        [    0.0634],\n",
      "        [    0.0815],\n",
      "        [    0.0189],\n",
      "        [    0.0642],\n",
      "        [    0.0916],\n",
      "        [    0.0848],\n",
      "        [    0.0298],\n",
      "        [    0.0948],\n",
      "        [    0.0844],\n",
      "        [    0.0078],\n",
      "        [    0.0211],\n",
      "        [    0.0786],\n",
      "        [    0.0833],\n",
      "        [    0.0176],\n",
      "        [    0.0800],\n",
      "        [    0.0893],\n",
      "        [    0.0736],\n",
      "        [    0.0799],\n",
      "        [    0.0062],\n",
      "        [    0.0901],\n",
      "        [    0.0137],\n",
      "        [    0.0889],\n",
      "        [    0.0049],\n",
      "        [    0.0211],\n",
      "        [    0.1059],\n",
      "        [    0.0087],\n",
      "        [    0.0346],\n",
      "        [    0.1340],\n",
      "        [    0.0005],\n",
      "        [    0.0323],\n",
      "        [    0.1002],\n",
      "        [    0.1287],\n",
      "        [    0.0142],\n",
      "        [    0.1188],\n",
      "        [    0.1238],\n",
      "        [    0.0111],\n",
      "        [    0.0189],\n",
      "        [    0.1495],\n",
      "        [    0.1145],\n",
      "        [    0.0489],\n",
      "        [    0.1225],\n",
      "        [    0.1378],\n",
      "        [    0.1232],\n",
      "        [    0.0196],\n",
      "        [    0.0391],\n",
      "        [    0.1626],\n",
      "        [    0.0431],\n",
      "        [    0.0674],\n",
      "        [    0.0700],\n",
      "        [    0.1147],\n",
      "        [    0.2984],\n",
      "        [    0.3124],\n",
      "        [    0.3154],\n",
      "        [    0.3173],\n",
      "        [    0.3183],\n",
      "        [    0.3278],\n",
      "        [    0.3296],\n",
      "        [    0.3308],\n",
      "        [    0.3381],\n",
      "        [    0.3399],\n",
      "        [    0.3420],\n",
      "        [    0.3431],\n",
      "        [    0.3463],\n",
      "        [    0.3654],\n",
      "        [    0.3663],\n",
      "        [    0.3733],\n",
      "        [    0.3777],\n",
      "        [    0.3796]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0064],\n",
      "        [    0.0072],\n",
      "        [    0.0000],\n",
      "        [    0.0018],\n",
      "        [    0.0157],\n",
      "        [    0.0047],\n",
      "        [    0.0195],\n",
      "        [    0.0058],\n",
      "        [    0.0292],\n",
      "        [    0.0088],\n",
      "        [    0.0204],\n",
      "        [    0.0167],\n",
      "        [    0.0079],\n",
      "        [    0.0071],\n",
      "        [    0.0055],\n",
      "        [    0.0002],\n",
      "        [    0.0116],\n",
      "        [    0.0301],\n",
      "        [    0.0128],\n",
      "        [    0.0035],\n",
      "        [    0.0016],\n",
      "        [    0.0077],\n",
      "        [    0.0002],\n",
      "        [    0.0312],\n",
      "        [    0.0091],\n",
      "        [    0.0261],\n",
      "        [    0.0107],\n",
      "        [    0.0192],\n",
      "        [    0.0468],\n",
      "        [    0.0094],\n",
      "        [    0.0288],\n",
      "        [    0.0778],\n",
      "        [    0.0031],\n",
      "        [    0.0064],\n",
      "        [    0.0820],\n",
      "        [    0.0097],\n",
      "        [    0.0151],\n",
      "        [    0.0242],\n",
      "        [    0.0375],\n",
      "        [    0.0270],\n",
      "        [    0.0362],\n",
      "        [    0.0339],\n",
      "        [    0.0178],\n",
      "        [    0.0124],\n",
      "        [    0.0203],\n",
      "        [    0.0342],\n",
      "        [    0.0243],\n",
      "        [    0.0186],\n",
      "        [    0.0357],\n",
      "        [    0.0489],\n",
      "        [    0.0129],\n",
      "        [    0.0378],\n",
      "        [    0.0538],\n",
      "        [    0.0310],\n",
      "        [    0.0298],\n",
      "        [    0.0369],\n",
      "        [    0.0331],\n",
      "        [    0.0498],\n",
      "        [    0.0735],\n",
      "        [    0.0444],\n",
      "        [    0.0553],\n",
      "        [    0.0182],\n",
      "        [    0.0587],\n",
      "        [    0.0431],\n",
      "        [    0.0397],\n",
      "        [    0.0533],\n",
      "        [    0.0497],\n",
      "        [    0.0615],\n",
      "        [    0.0538],\n",
      "        [    0.0738],\n",
      "        [    0.0341],\n",
      "        [    0.0896],\n",
      "        [    0.0698],\n",
      "        [    0.0627],\n",
      "        [    0.0815],\n",
      "        [    0.0191],\n",
      "        [    0.0637],\n",
      "        [    0.0921],\n",
      "        [    0.0846],\n",
      "        [    0.0303],\n",
      "        [    0.0948],\n",
      "        [    0.0841],\n",
      "        [    0.0083],\n",
      "        [    0.0216],\n",
      "        [    0.0783],\n",
      "        [    0.0832],\n",
      "        [    0.0181],\n",
      "        [    0.0800],\n",
      "        [    0.0886],\n",
      "        [    0.0733],\n",
      "        [    0.0790],\n",
      "        [    0.0057],\n",
      "        [    0.0899],\n",
      "        [    0.0144],\n",
      "        [    0.0884],\n",
      "        [    0.0050],\n",
      "        [    0.0208],\n",
      "        [    0.1063],\n",
      "        [    0.0099],\n",
      "        [    0.0346],\n",
      "        [    0.1346],\n",
      "        [    0.0013],\n",
      "        [    0.0320],\n",
      "        [    0.0998],\n",
      "        [    0.1290],\n",
      "        [    0.0138],\n",
      "        [    0.1184],\n",
      "        [    0.1236],\n",
      "        [    0.0105],\n",
      "        [    0.0186],\n",
      "        [    0.1489],\n",
      "        [    0.1140],\n",
      "        [    0.0486],\n",
      "        [    0.1224],\n",
      "        [    0.1370],\n",
      "        [    0.1227],\n",
      "        [    0.0187],\n",
      "        [    0.0384],\n",
      "        [    0.1629],\n",
      "        [    0.0424],\n",
      "        [    0.0668],\n",
      "        [    0.0692],\n",
      "        [    0.1079],\n",
      "        [    0.2980],\n",
      "        [    0.3120],\n",
      "        [    0.3150],\n",
      "        [    0.3169],\n",
      "        [    0.3179],\n",
      "        [    0.3274],\n",
      "        [    0.3292],\n",
      "        [    0.3304],\n",
      "        [    0.3377],\n",
      "        [    0.3395],\n",
      "        [    0.3416],\n",
      "        [    0.3427],\n",
      "        [    0.3459],\n",
      "        [    0.3649],\n",
      "        [    0.3659],\n",
      "        [    0.3729],\n",
      "        [    0.3772],\n",
      "        [    0.3792]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 91.00227999687195\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 142\n",
      "剩餘X 資料 torch.Size([18, 18])\n",
      "剩餘Y 資料 torch.Size([18, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.1444822996854782, 3)\n",
      "The second_loss value of k: (0.14789685606956482, 17)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引3，y= tensor([0.0683])\n",
      "目前模型的Data狀態 torch.Size([142, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1267],\n",
      "        [0.3508],\n",
      "        [0.1086],\n",
      "        [0.6316],\n",
      "        [0.2940],\n",
      "        [0.2669],\n",
      "        [0.6245],\n",
      "        [0.3601],\n",
      "        [0.1886],\n",
      "        [0.6198],\n",
      "        [0.1416],\n",
      "        [0.3412],\n",
      "        [0.6087],\n",
      "        [0.5998],\n",
      "        [0.1271],\n",
      "        [0.1842],\n",
      "        [0.2220],\n",
      "        [0.1620],\n",
      "        [0.6302],\n",
      "        [0.1231],\n",
      "        [0.4420],\n",
      "        [0.5289],\n",
      "        [0.4738],\n",
      "        [0.2987],\n",
      "        [0.6214],\n",
      "        [0.5666],\n",
      "        [0.3527],\n",
      "        [0.6407],\n",
      "        [0.4610],\n",
      "        [0.6372],\n",
      "        [0.6480],\n",
      "        [0.4899],\n",
      "        [0.1414],\n",
      "        [0.5675],\n",
      "        [0.4816],\n",
      "        [0.3548],\n",
      "        [0.6058],\n",
      "        [0.2159],\n",
      "        [0.2696],\n",
      "        [0.3316],\n",
      "        [0.1440],\n",
      "        [0.3248],\n",
      "        [0.1797],\n",
      "        [0.2015],\n",
      "        [0.5762],\n",
      "        [0.3052],\n",
      "        [0.5833],\n",
      "        [0.4533],\n",
      "        [0.5837],\n",
      "        [0.2057],\n",
      "        [0.3782],\n",
      "        [0.4331],\n",
      "        [0.4733],\n",
      "        [0.6383],\n",
      "        [0.1205],\n",
      "        [0.2177],\n",
      "        [0.1262],\n",
      "        [0.5762],\n",
      "        [0.5609],\n",
      "        [0.3697],\n",
      "        [0.6164],\n",
      "        [0.5989],\n",
      "        [0.5179],\n",
      "        [0.4909],\n",
      "        [0.4249],\n",
      "        [0.4339],\n",
      "        [0.2373],\n",
      "        [0.2816],\n",
      "        [0.2128],\n",
      "        [0.1919],\n",
      "        [0.4490],\n",
      "        [0.2182],\n",
      "        [0.1604],\n",
      "        [0.2283],\n",
      "        [0.3277],\n",
      "        [0.4454],\n",
      "        [0.4593],\n",
      "        [0.3570],\n",
      "        [0.3988],\n",
      "        [0.5299],\n",
      "        [0.4063],\n",
      "        [0.3148],\n",
      "        [0.4538],\n",
      "        [0.4783],\n",
      "        [0.1966],\n",
      "        [0.1641],\n",
      "        [0.5107],\n",
      "        [0.2427],\n",
      "        [0.3635],\n",
      "        [0.1003],\n",
      "        [0.3720],\n",
      "        [0.4268],\n",
      "        [0.2655],\n",
      "        [0.5395],\n",
      "        [0.4446],\n",
      "        [0.4933],\n",
      "        [0.4175],\n",
      "        [0.2971],\n",
      "        [0.5016],\n",
      "        [0.4463],\n",
      "        [0.4610],\n",
      "        [0.5426],\n",
      "        [0.3912],\n",
      "        [0.3987],\n",
      "        [0.4158],\n",
      "        [0.4544],\n",
      "        [0.3181],\n",
      "        [0.3436],\n",
      "        [0.4435],\n",
      "        [0.4667],\n",
      "        [0.1703],\n",
      "        [0.1140],\n",
      "        [0.3950],\n",
      "        [0.2668],\n",
      "        [0.1657],\n",
      "        [0.3731],\n",
      "        [0.4511],\n",
      "        [0.5312],\n",
      "        [0.3903],\n",
      "        [0.4638],\n",
      "        [0.4167],\n",
      "        [0.4312],\n",
      "        [0.0108],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484],\n",
      "        [0.4484]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0064],\n",
      "        [    0.0072],\n",
      "        [    0.0000],\n",
      "        [    0.0018],\n",
      "        [    0.0157],\n",
      "        [    0.0047],\n",
      "        [    0.0195],\n",
      "        [    0.0058],\n",
      "        [    0.0292],\n",
      "        [    0.0088],\n",
      "        [    0.0204],\n",
      "        [    0.0167],\n",
      "        [    0.0079],\n",
      "        [    0.0071],\n",
      "        [    0.0055],\n",
      "        [    0.0002],\n",
      "        [    0.0116],\n",
      "        [    0.0301],\n",
      "        [    0.0128],\n",
      "        [    0.0035],\n",
      "        [    0.0016],\n",
      "        [    0.0077],\n",
      "        [    0.0002],\n",
      "        [    0.0312],\n",
      "        [    0.0091],\n",
      "        [    0.0261],\n",
      "        [    0.0107],\n",
      "        [    0.0192],\n",
      "        [    0.0468],\n",
      "        [    0.0094],\n",
      "        [    0.0288],\n",
      "        [    0.0778],\n",
      "        [    0.0031],\n",
      "        [    0.0064],\n",
      "        [    0.0820],\n",
      "        [    0.0097],\n",
      "        [    0.0151],\n",
      "        [    0.0242],\n",
      "        [    0.0375],\n",
      "        [    0.0270],\n",
      "        [    0.0362],\n",
      "        [    0.0339],\n",
      "        [    0.0178],\n",
      "        [    0.0124],\n",
      "        [    0.0203],\n",
      "        [    0.0342],\n",
      "        [    0.0243],\n",
      "        [    0.0186],\n",
      "        [    0.0357],\n",
      "        [    0.0489],\n",
      "        [    0.0129],\n",
      "        [    0.0378],\n",
      "        [    0.0538],\n",
      "        [    0.0310],\n",
      "        [    0.0298],\n",
      "        [    0.0369],\n",
      "        [    0.0331],\n",
      "        [    0.0498],\n",
      "        [    0.0735],\n",
      "        [    0.0444],\n",
      "        [    0.0553],\n",
      "        [    0.0182],\n",
      "        [    0.0587],\n",
      "        [    0.0431],\n",
      "        [    0.0397],\n",
      "        [    0.0533],\n",
      "        [    0.0497],\n",
      "        [    0.0615],\n",
      "        [    0.0538],\n",
      "        [    0.0738],\n",
      "        [    0.0341],\n",
      "        [    0.0896],\n",
      "        [    0.0698],\n",
      "        [    0.0627],\n",
      "        [    0.0815],\n",
      "        [    0.0191],\n",
      "        [    0.0637],\n",
      "        [    0.0921],\n",
      "        [    0.0846],\n",
      "        [    0.0303],\n",
      "        [    0.0948],\n",
      "        [    0.0841],\n",
      "        [    0.0083],\n",
      "        [    0.0216],\n",
      "        [    0.0783],\n",
      "        [    0.0832],\n",
      "        [    0.0181],\n",
      "        [    0.0800],\n",
      "        [    0.0886],\n",
      "        [    0.0733],\n",
      "        [    0.0790],\n",
      "        [    0.0057],\n",
      "        [    0.0899],\n",
      "        [    0.0144],\n",
      "        [    0.0884],\n",
      "        [    0.0050],\n",
      "        [    0.0208],\n",
      "        [    0.1063],\n",
      "        [    0.0099],\n",
      "        [    0.0346],\n",
      "        [    0.1346],\n",
      "        [    0.0013],\n",
      "        [    0.0320],\n",
      "        [    0.0998],\n",
      "        [    0.1290],\n",
      "        [    0.0138],\n",
      "        [    0.1184],\n",
      "        [    0.1236],\n",
      "        [    0.0105],\n",
      "        [    0.0186],\n",
      "        [    0.1489],\n",
      "        [    0.1140],\n",
      "        [    0.0486],\n",
      "        [    0.1224],\n",
      "        [    0.1370],\n",
      "        [    0.1227],\n",
      "        [    0.0187],\n",
      "        [    0.0384],\n",
      "        [    0.1629],\n",
      "        [    0.0424],\n",
      "        [    0.0668],\n",
      "        [    0.0692],\n",
      "        [    0.1079],\n",
      "        [    0.2980],\n",
      "        [    0.3120],\n",
      "        [    0.3150],\n",
      "        [    0.3169],\n",
      "        [    0.3179],\n",
      "        [    0.3274],\n",
      "        [    0.3292],\n",
      "        [    0.3304],\n",
      "        [    0.3377],\n",
      "        [    0.3395],\n",
      "        [    0.3416],\n",
      "        [    0.3427],\n",
      "        [    0.3459],\n",
      "        [    0.3649],\n",
      "        [    0.3659],\n",
      "        [    0.3729],\n",
      "        [    0.3772],\n",
      "        [    0.3792],\n",
      "        [    0.3801]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0066],\n",
      "        [    0.0071],\n",
      "        [    0.0004],\n",
      "        [    0.0022],\n",
      "        [    0.0161],\n",
      "        [    0.0042],\n",
      "        [    0.0200],\n",
      "        [    0.0052],\n",
      "        [    0.0302],\n",
      "        [    0.0097],\n",
      "        [    0.0207],\n",
      "        [    0.0165],\n",
      "        [    0.0085],\n",
      "        [    0.0081],\n",
      "        [    0.0059],\n",
      "        [    0.0000],\n",
      "        [    0.0113],\n",
      "        [    0.0309],\n",
      "        [    0.0125],\n",
      "        [    0.0031],\n",
      "        [    0.0013],\n",
      "        [    0.0081],\n",
      "        [    0.0008],\n",
      "        [    0.0311],\n",
      "        [    0.0096],\n",
      "        [    0.0258],\n",
      "        [    0.0105],\n",
      "        [    0.0193],\n",
      "        [    0.0456],\n",
      "        [    0.0089],\n",
      "        [    0.0287],\n",
      "        [    0.0771],\n",
      "        [    0.0026],\n",
      "        [    0.0055],\n",
      "        [    0.0811],\n",
      "        [    0.0100],\n",
      "        [    0.0145],\n",
      "        [    0.0244],\n",
      "        [    0.0374],\n",
      "        [    0.0269],\n",
      "        [    0.0367],\n",
      "        [    0.0340],\n",
      "        [    0.0183],\n",
      "        [    0.0119],\n",
      "        [    0.0195],\n",
      "        [    0.0337],\n",
      "        [    0.0235],\n",
      "        [    0.0193],\n",
      "        [    0.0351],\n",
      "        [    0.0490],\n",
      "        [    0.0123],\n",
      "        [    0.0386],\n",
      "        [    0.0531],\n",
      "        [    0.0303],\n",
      "        [    0.0292],\n",
      "        [    0.0367],\n",
      "        [    0.0326],\n",
      "        [    0.0493],\n",
      "        [    0.0737],\n",
      "        [    0.0438],\n",
      "        [    0.0552],\n",
      "        [    0.0175],\n",
      "        [    0.0583],\n",
      "        [    0.0436],\n",
      "        [    0.0404],\n",
      "        [    0.0542],\n",
      "        [    0.0494],\n",
      "        [    0.0613],\n",
      "        [    0.0536],\n",
      "        [    0.0732],\n",
      "        [    0.0345],\n",
      "        [    0.0897],\n",
      "        [    0.0696],\n",
      "        [    0.0626],\n",
      "        [    0.0818],\n",
      "        [    0.0184],\n",
      "        [    0.0642],\n",
      "        [    0.0918],\n",
      "        [    0.0838],\n",
      "        [    0.0300],\n",
      "        [    0.0941],\n",
      "        [    0.0845],\n",
      "        [    0.0079],\n",
      "        [    0.0213],\n",
      "        [    0.0777],\n",
      "        [    0.0828],\n",
      "        [    0.0177],\n",
      "        [    0.0795],\n",
      "        [    0.0886],\n",
      "        [    0.0726],\n",
      "        [    0.0788],\n",
      "        [    0.0061],\n",
      "        [    0.0893],\n",
      "        [    0.0142],\n",
      "        [    0.0888],\n",
      "        [    0.0043],\n",
      "        [    0.0212],\n",
      "        [    0.1060],\n",
      "        [    0.0103],\n",
      "        [    0.0353],\n",
      "        [    0.1344],\n",
      "        [    0.0013],\n",
      "        [    0.0325],\n",
      "        [    0.0999],\n",
      "        [    0.1286],\n",
      "        [    0.0141],\n",
      "        [    0.1187],\n",
      "        [    0.1238],\n",
      "        [    0.0108],\n",
      "        [    0.0193],\n",
      "        [    0.1487],\n",
      "        [    0.1134],\n",
      "        [    0.0491],\n",
      "        [    0.1220],\n",
      "        [    0.1366],\n",
      "        [    0.1227],\n",
      "        [    0.0189],\n",
      "        [    0.0384],\n",
      "        [    0.1624],\n",
      "        [    0.0426],\n",
      "        [    0.0671],\n",
      "        [    0.0691],\n",
      "        [    0.1022],\n",
      "        [    0.2976],\n",
      "        [    0.3116],\n",
      "        [    0.3146],\n",
      "        [    0.3165],\n",
      "        [    0.3175],\n",
      "        [    0.3270],\n",
      "        [    0.3288],\n",
      "        [    0.3299],\n",
      "        [    0.3373],\n",
      "        [    0.3391],\n",
      "        [    0.3412],\n",
      "        [    0.3422],\n",
      "        [    0.3455],\n",
      "        [    0.3645],\n",
      "        [    0.3655],\n",
      "        [    0.3724],\n",
      "        [    0.3768],\n",
      "        [    0.3788],\n",
      "        [    0.3797]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 91.23926162719727\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 143\n",
      "剩餘X 資料 torch.Size([17, 18])\n",
      "剩餘Y 資料 torch.Size([17, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.14757893979549408, 16)\n",
      "The second_loss value of k: (0.15335232019424438, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.0638])\n",
      "目前模型的Data狀態 torch.Size([143, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1265],\n",
      "        [0.3510],\n",
      "        [0.1082],\n",
      "        [0.6311],\n",
      "        [0.2936],\n",
      "        [0.2665],\n",
      "        [0.6240],\n",
      "        [0.3595],\n",
      "        [0.1877],\n",
      "        [0.6189],\n",
      "        [0.1413],\n",
      "        [0.3414],\n",
      "        [0.6080],\n",
      "        [0.5989],\n",
      "        [0.1267],\n",
      "        [0.1840],\n",
      "        [0.2223],\n",
      "        [0.1612],\n",
      "        [0.6305],\n",
      "        [0.1227],\n",
      "        [0.4417],\n",
      "        [0.5285],\n",
      "        [0.4731],\n",
      "        [0.2985],\n",
      "        [0.6210],\n",
      "        [0.5664],\n",
      "        [0.3530],\n",
      "        [0.6408],\n",
      "        [0.4598],\n",
      "        [0.6366],\n",
      "        [0.6481],\n",
      "        [0.4892],\n",
      "        [0.1408],\n",
      "        [0.5665],\n",
      "        [0.4807],\n",
      "        [0.3544],\n",
      "        [0.6051],\n",
      "        [0.2161],\n",
      "        [0.2697],\n",
      "        [0.3317],\n",
      "        [0.1435],\n",
      "        [0.3247],\n",
      "        [0.1792],\n",
      "        [0.2011],\n",
      "        [0.5753],\n",
      "        [0.3047],\n",
      "        [0.5825],\n",
      "        [0.4526],\n",
      "        [0.5830],\n",
      "        [0.2056],\n",
      "        [0.3788],\n",
      "        [0.4323],\n",
      "        [0.4727],\n",
      "        [0.6376],\n",
      "        [0.1200],\n",
      "        [0.2175],\n",
      "        [0.1257],\n",
      "        [0.5757],\n",
      "        [0.5612],\n",
      "        [0.3690],\n",
      "        [0.6165],\n",
      "        [0.5982],\n",
      "        [0.5175],\n",
      "        [0.4904],\n",
      "        [0.4242],\n",
      "        [0.4330],\n",
      "        [0.2371],\n",
      "        [0.2814],\n",
      "        [0.2127],\n",
      "        [0.1913],\n",
      "        [0.4486],\n",
      "        [0.2181],\n",
      "        [0.1602],\n",
      "        [0.2284],\n",
      "        [0.3273],\n",
      "        [0.4448],\n",
      "        [0.4589],\n",
      "        [0.3568],\n",
      "        [0.3980],\n",
      "        [0.5295],\n",
      "        [0.4056],\n",
      "        [0.3145],\n",
      "        [0.4534],\n",
      "        [0.4779],\n",
      "        [0.1960],\n",
      "        [0.1637],\n",
      "        [0.5103],\n",
      "        [0.2422],\n",
      "        [0.3636],\n",
      "        [0.0997],\n",
      "        [0.3723],\n",
      "        [0.4265],\n",
      "        [0.2649],\n",
      "        [0.5393],\n",
      "        [0.4442],\n",
      "        [0.4926],\n",
      "        [0.4170],\n",
      "        [0.2969],\n",
      "        [0.5019],\n",
      "        [0.4456],\n",
      "        [0.4607],\n",
      "        [0.5426],\n",
      "        [0.3906],\n",
      "        [0.3986],\n",
      "        [0.4154],\n",
      "        [0.4541],\n",
      "        [0.3179],\n",
      "        [0.3434],\n",
      "        [0.4432],\n",
      "        [0.4661],\n",
      "        [0.1705],\n",
      "        [0.1134],\n",
      "        [0.3945],\n",
      "        [0.2663],\n",
      "        [0.1661],\n",
      "        [0.3731],\n",
      "        [0.4510],\n",
      "        [0.5311],\n",
      "        [0.3898],\n",
      "        [0.4635],\n",
      "        [0.4165],\n",
      "        [0.4312],\n",
      "        [0.0165],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480],\n",
      "        [0.4480]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0066],\n",
      "        [    0.0071],\n",
      "        [    0.0004],\n",
      "        [    0.0022],\n",
      "        [    0.0161],\n",
      "        [    0.0042],\n",
      "        [    0.0200],\n",
      "        [    0.0052],\n",
      "        [    0.0302],\n",
      "        [    0.0097],\n",
      "        [    0.0207],\n",
      "        [    0.0165],\n",
      "        [    0.0085],\n",
      "        [    0.0081],\n",
      "        [    0.0059],\n",
      "        [    0.0000],\n",
      "        [    0.0113],\n",
      "        [    0.0309],\n",
      "        [    0.0125],\n",
      "        [    0.0031],\n",
      "        [    0.0013],\n",
      "        [    0.0081],\n",
      "        [    0.0008],\n",
      "        [    0.0311],\n",
      "        [    0.0096],\n",
      "        [    0.0258],\n",
      "        [    0.0105],\n",
      "        [    0.0193],\n",
      "        [    0.0456],\n",
      "        [    0.0089],\n",
      "        [    0.0287],\n",
      "        [    0.0771],\n",
      "        [    0.0026],\n",
      "        [    0.0055],\n",
      "        [    0.0811],\n",
      "        [    0.0100],\n",
      "        [    0.0145],\n",
      "        [    0.0244],\n",
      "        [    0.0374],\n",
      "        [    0.0269],\n",
      "        [    0.0367],\n",
      "        [    0.0340],\n",
      "        [    0.0183],\n",
      "        [    0.0119],\n",
      "        [    0.0195],\n",
      "        [    0.0337],\n",
      "        [    0.0235],\n",
      "        [    0.0193],\n",
      "        [    0.0351],\n",
      "        [    0.0490],\n",
      "        [    0.0123],\n",
      "        [    0.0386],\n",
      "        [    0.0531],\n",
      "        [    0.0303],\n",
      "        [    0.0292],\n",
      "        [    0.0367],\n",
      "        [    0.0326],\n",
      "        [    0.0493],\n",
      "        [    0.0737],\n",
      "        [    0.0438],\n",
      "        [    0.0552],\n",
      "        [    0.0175],\n",
      "        [    0.0583],\n",
      "        [    0.0436],\n",
      "        [    0.0404],\n",
      "        [    0.0542],\n",
      "        [    0.0494],\n",
      "        [    0.0613],\n",
      "        [    0.0536],\n",
      "        [    0.0732],\n",
      "        [    0.0345],\n",
      "        [    0.0897],\n",
      "        [    0.0696],\n",
      "        [    0.0626],\n",
      "        [    0.0818],\n",
      "        [    0.0184],\n",
      "        [    0.0642],\n",
      "        [    0.0918],\n",
      "        [    0.0838],\n",
      "        [    0.0300],\n",
      "        [    0.0941],\n",
      "        [    0.0845],\n",
      "        [    0.0079],\n",
      "        [    0.0213],\n",
      "        [    0.0777],\n",
      "        [    0.0828],\n",
      "        [    0.0177],\n",
      "        [    0.0795],\n",
      "        [    0.0886],\n",
      "        [    0.0726],\n",
      "        [    0.0788],\n",
      "        [    0.0061],\n",
      "        [    0.0893],\n",
      "        [    0.0142],\n",
      "        [    0.0888],\n",
      "        [    0.0043],\n",
      "        [    0.0212],\n",
      "        [    0.1060],\n",
      "        [    0.0103],\n",
      "        [    0.0353],\n",
      "        [    0.1344],\n",
      "        [    0.0013],\n",
      "        [    0.0325],\n",
      "        [    0.0999],\n",
      "        [    0.1286],\n",
      "        [    0.0141],\n",
      "        [    0.1187],\n",
      "        [    0.1238],\n",
      "        [    0.0108],\n",
      "        [    0.0193],\n",
      "        [    0.1487],\n",
      "        [    0.1134],\n",
      "        [    0.0491],\n",
      "        [    0.1220],\n",
      "        [    0.1366],\n",
      "        [    0.1227],\n",
      "        [    0.0189],\n",
      "        [    0.0384],\n",
      "        [    0.1624],\n",
      "        [    0.0426],\n",
      "        [    0.0671],\n",
      "        [    0.0691],\n",
      "        [    0.1022],\n",
      "        [    0.2976],\n",
      "        [    0.3116],\n",
      "        [    0.3146],\n",
      "        [    0.3165],\n",
      "        [    0.3175],\n",
      "        [    0.3270],\n",
      "        [    0.3288],\n",
      "        [    0.3299],\n",
      "        [    0.3373],\n",
      "        [    0.3391],\n",
      "        [    0.3412],\n",
      "        [    0.3422],\n",
      "        [    0.3455],\n",
      "        [    0.3645],\n",
      "        [    0.3655],\n",
      "        [    0.3724],\n",
      "        [    0.3768],\n",
      "        [    0.3788],\n",
      "        [    0.3797],\n",
      "        [    0.3842]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0065],\n",
      "        [    0.0064],\n",
      "        [    0.0004],\n",
      "        [    0.0017],\n",
      "        [    0.0160],\n",
      "        [    0.0044],\n",
      "        [    0.0195],\n",
      "        [    0.0052],\n",
      "        [    0.0308],\n",
      "        [    0.0096],\n",
      "        [    0.0207],\n",
      "        [    0.0158],\n",
      "        [    0.0082],\n",
      "        [    0.0080],\n",
      "        [    0.0059],\n",
      "        [    0.0003],\n",
      "        [    0.0106],\n",
      "        [    0.0314],\n",
      "        [    0.0111],\n",
      "        [    0.0031],\n",
      "        [    0.0018],\n",
      "        [    0.0076],\n",
      "        [    0.0006],\n",
      "        [    0.0316],\n",
      "        [    0.0091],\n",
      "        [    0.0266],\n",
      "        [    0.0096],\n",
      "        [    0.0203],\n",
      "        [    0.0450],\n",
      "        [    0.0093],\n",
      "        [    0.0276],\n",
      "        [    0.0773],\n",
      "        [    0.0024],\n",
      "        [    0.0055],\n",
      "        [    0.0809],\n",
      "        [    0.0098],\n",
      "        [    0.0148],\n",
      "        [    0.0251],\n",
      "        [    0.0368],\n",
      "        [    0.0262],\n",
      "        [    0.0368],\n",
      "        [    0.0336],\n",
      "        [    0.0187],\n",
      "        [    0.0119],\n",
      "        [    0.0197],\n",
      "        [    0.0338],\n",
      "        [    0.0236],\n",
      "        [    0.0192],\n",
      "        [    0.0354],\n",
      "        [    0.0486],\n",
      "        [    0.0112],\n",
      "        [    0.0385],\n",
      "        [    0.0533],\n",
      "        [    0.0307],\n",
      "        [    0.0291],\n",
      "        [    0.0371],\n",
      "        [    0.0324],\n",
      "        [    0.0498],\n",
      "        [    0.0748],\n",
      "        [    0.0439],\n",
      "        [    0.0542],\n",
      "        [    0.0178],\n",
      "        [    0.0588],\n",
      "        [    0.0431],\n",
      "        [    0.0402],\n",
      "        [    0.0542],\n",
      "        [    0.0497],\n",
      "        [    0.0616],\n",
      "        [    0.0538],\n",
      "        [    0.0726],\n",
      "        [    0.0344],\n",
      "        [    0.0892],\n",
      "        [    0.0698],\n",
      "        [    0.0619],\n",
      "        [    0.0816],\n",
      "        [    0.0185],\n",
      "        [    0.0637],\n",
      "        [    0.0922],\n",
      "        [    0.0836],\n",
      "        [    0.0305],\n",
      "        [    0.0942],\n",
      "        [    0.0842],\n",
      "        [    0.0084],\n",
      "        [    0.0218],\n",
      "        [    0.0776],\n",
      "        [    0.0827],\n",
      "        [    0.0183],\n",
      "        [    0.0795],\n",
      "        [    0.0880],\n",
      "        [    0.0723],\n",
      "        [    0.0779],\n",
      "        [    0.0056],\n",
      "        [    0.0892],\n",
      "        [    0.0149],\n",
      "        [    0.0883],\n",
      "        [    0.0044],\n",
      "        [    0.0209],\n",
      "        [    0.1063],\n",
      "        [    0.0114],\n",
      "        [    0.0353],\n",
      "        [    0.1349],\n",
      "        [    0.0021],\n",
      "        [    0.0323],\n",
      "        [    0.0995],\n",
      "        [    0.1289],\n",
      "        [    0.0137],\n",
      "        [    0.1183],\n",
      "        [    0.1235],\n",
      "        [    0.0102],\n",
      "        [    0.0190],\n",
      "        [    0.1481],\n",
      "        [    0.1130],\n",
      "        [    0.0488],\n",
      "        [    0.1220],\n",
      "        [    0.1358],\n",
      "        [    0.1222],\n",
      "        [    0.0181],\n",
      "        [    0.0377],\n",
      "        [    0.1627],\n",
      "        [    0.0419],\n",
      "        [    0.0666],\n",
      "        [    0.0683],\n",
      "        [    0.0961],\n",
      "        [    0.2972],\n",
      "        [    0.3112],\n",
      "        [    0.3142],\n",
      "        [    0.3161],\n",
      "        [    0.3171],\n",
      "        [    0.3266],\n",
      "        [    0.3284],\n",
      "        [    0.3296],\n",
      "        [    0.3369],\n",
      "        [    0.3387],\n",
      "        [    0.3408],\n",
      "        [    0.3419],\n",
      "        [    0.3451],\n",
      "        [    0.3642],\n",
      "        [    0.3651],\n",
      "        [    0.3721],\n",
      "        [    0.3765],\n",
      "        [    0.3784],\n",
      "        [    0.3793],\n",
      "        [    0.3838]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 91.47816061973572\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 144\n",
      "剩餘X 資料 torch.Size([16, 18])\n",
      "剩餘Y 資料 torch.Size([16, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15305069088935852, 14)\n",
      "The second_loss value of k: (0.15434060990810394, 2)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0564])\n",
      "目前模型的Data狀態 torch.Size([144, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1267],\n",
      "        [0.3516],\n",
      "        [0.1083],\n",
      "        [0.6316],\n",
      "        [0.2937],\n",
      "        [0.2666],\n",
      "        [0.6245],\n",
      "        [0.3595],\n",
      "        [0.1871],\n",
      "        [0.6191],\n",
      "        [0.1414],\n",
      "        [0.3421],\n",
      "        [0.6084],\n",
      "        [0.5990],\n",
      "        [0.1267],\n",
      "        [0.1843],\n",
      "        [0.2230],\n",
      "        [0.1607],\n",
      "        [0.6318],\n",
      "        [0.1227],\n",
      "        [0.4422],\n",
      "        [0.5290],\n",
      "        [0.4734],\n",
      "        [0.2990],\n",
      "        [0.6215],\n",
      "        [0.5671],\n",
      "        [0.3538],\n",
      "        [0.6419],\n",
      "        [0.4591],\n",
      "        [0.6371],\n",
      "        [0.6492],\n",
      "        [0.4894],\n",
      "        [0.1406],\n",
      "        [0.5666],\n",
      "        [0.4805],\n",
      "        [0.3547],\n",
      "        [0.6055],\n",
      "        [0.2168],\n",
      "        [0.2703],\n",
      "        [0.3323],\n",
      "        [0.1434],\n",
      "        [0.3251],\n",
      "        [0.1788],\n",
      "        [0.2011],\n",
      "        [0.5755],\n",
      "        [0.3048],\n",
      "        [0.5826],\n",
      "        [0.4528],\n",
      "        [0.5834],\n",
      "        [0.2059],\n",
      "        [0.3799],\n",
      "        [0.4325],\n",
      "        [0.4729],\n",
      "        [0.6380],\n",
      "        [0.1198],\n",
      "        [0.2179],\n",
      "        [0.1255],\n",
      "        [0.5762],\n",
      "        [0.5622],\n",
      "        [0.3691],\n",
      "        [0.6175],\n",
      "        [0.5984],\n",
      "        [0.5180],\n",
      "        [0.4909],\n",
      "        [0.4243],\n",
      "        [0.4330],\n",
      "        [0.2373],\n",
      "        [0.2817],\n",
      "        [0.2129],\n",
      "        [0.1907],\n",
      "        [0.4486],\n",
      "        [0.2186],\n",
      "        [0.1604],\n",
      "        [0.2290],\n",
      "        [0.3275],\n",
      "        [0.4449],\n",
      "        [0.4593],\n",
      "        [0.3572],\n",
      "        [0.3978],\n",
      "        [0.5301],\n",
      "        [0.4056],\n",
      "        [0.3148],\n",
      "        [0.4538],\n",
      "        [0.4784],\n",
      "        [0.1958],\n",
      "        [0.1636],\n",
      "        [0.5108],\n",
      "        [0.2422],\n",
      "        [0.3642],\n",
      "        [0.0994],\n",
      "        [0.3731],\n",
      "        [0.4269],\n",
      "        [0.2648],\n",
      "        [0.5399],\n",
      "        [0.4446],\n",
      "        [0.4927],\n",
      "        [0.4173],\n",
      "        [0.2972],\n",
      "        [0.5031],\n",
      "        [0.4456],\n",
      "        [0.4613],\n",
      "        [0.5434],\n",
      "        [0.3909],\n",
      "        [0.3991],\n",
      "        [0.4157],\n",
      "        [0.4544],\n",
      "        [0.3182],\n",
      "        [0.3437],\n",
      "        [0.4438],\n",
      "        [0.4663],\n",
      "        [0.1711],\n",
      "        [0.1130],\n",
      "        [0.3948],\n",
      "        [0.2664],\n",
      "        [0.1669],\n",
      "        [0.3736],\n",
      "        [0.4518],\n",
      "        [0.5318],\n",
      "        [0.3901],\n",
      "        [0.4642],\n",
      "        [0.4170],\n",
      "        [0.4320],\n",
      "        [0.0226],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476],\n",
      "        [0.4476]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0065],\n",
      "        [    0.0064],\n",
      "        [    0.0004],\n",
      "        [    0.0017],\n",
      "        [    0.0160],\n",
      "        [    0.0044],\n",
      "        [    0.0195],\n",
      "        [    0.0052],\n",
      "        [    0.0308],\n",
      "        [    0.0096],\n",
      "        [    0.0207],\n",
      "        [    0.0158],\n",
      "        [    0.0082],\n",
      "        [    0.0080],\n",
      "        [    0.0059],\n",
      "        [    0.0003],\n",
      "        [    0.0106],\n",
      "        [    0.0314],\n",
      "        [    0.0111],\n",
      "        [    0.0031],\n",
      "        [    0.0018],\n",
      "        [    0.0076],\n",
      "        [    0.0006],\n",
      "        [    0.0316],\n",
      "        [    0.0091],\n",
      "        [    0.0266],\n",
      "        [    0.0096],\n",
      "        [    0.0203],\n",
      "        [    0.0450],\n",
      "        [    0.0093],\n",
      "        [    0.0276],\n",
      "        [    0.0773],\n",
      "        [    0.0024],\n",
      "        [    0.0055],\n",
      "        [    0.0809],\n",
      "        [    0.0098],\n",
      "        [    0.0148],\n",
      "        [    0.0251],\n",
      "        [    0.0368],\n",
      "        [    0.0262],\n",
      "        [    0.0368],\n",
      "        [    0.0336],\n",
      "        [    0.0187],\n",
      "        [    0.0119],\n",
      "        [    0.0197],\n",
      "        [    0.0338],\n",
      "        [    0.0236],\n",
      "        [    0.0192],\n",
      "        [    0.0354],\n",
      "        [    0.0486],\n",
      "        [    0.0112],\n",
      "        [    0.0385],\n",
      "        [    0.0533],\n",
      "        [    0.0307],\n",
      "        [    0.0291],\n",
      "        [    0.0371],\n",
      "        [    0.0324],\n",
      "        [    0.0498],\n",
      "        [    0.0748],\n",
      "        [    0.0439],\n",
      "        [    0.0542],\n",
      "        [    0.0178],\n",
      "        [    0.0588],\n",
      "        [    0.0431],\n",
      "        [    0.0402],\n",
      "        [    0.0542],\n",
      "        [    0.0497],\n",
      "        [    0.0616],\n",
      "        [    0.0538],\n",
      "        [    0.0726],\n",
      "        [    0.0344],\n",
      "        [    0.0892],\n",
      "        [    0.0698],\n",
      "        [    0.0619],\n",
      "        [    0.0816],\n",
      "        [    0.0185],\n",
      "        [    0.0637],\n",
      "        [    0.0922],\n",
      "        [    0.0836],\n",
      "        [    0.0305],\n",
      "        [    0.0942],\n",
      "        [    0.0842],\n",
      "        [    0.0084],\n",
      "        [    0.0218],\n",
      "        [    0.0776],\n",
      "        [    0.0827],\n",
      "        [    0.0183],\n",
      "        [    0.0795],\n",
      "        [    0.0880],\n",
      "        [    0.0723],\n",
      "        [    0.0779],\n",
      "        [    0.0056],\n",
      "        [    0.0892],\n",
      "        [    0.0149],\n",
      "        [    0.0883],\n",
      "        [    0.0044],\n",
      "        [    0.0209],\n",
      "        [    0.1063],\n",
      "        [    0.0114],\n",
      "        [    0.0353],\n",
      "        [    0.1349],\n",
      "        [    0.0021],\n",
      "        [    0.0323],\n",
      "        [    0.0995],\n",
      "        [    0.1289],\n",
      "        [    0.0137],\n",
      "        [    0.1183],\n",
      "        [    0.1235],\n",
      "        [    0.0102],\n",
      "        [    0.0190],\n",
      "        [    0.1481],\n",
      "        [    0.1130],\n",
      "        [    0.0488],\n",
      "        [    0.1220],\n",
      "        [    0.1358],\n",
      "        [    0.1222],\n",
      "        [    0.0181],\n",
      "        [    0.0377],\n",
      "        [    0.1627],\n",
      "        [    0.0419],\n",
      "        [    0.0666],\n",
      "        [    0.0683],\n",
      "        [    0.0961],\n",
      "        [    0.2972],\n",
      "        [    0.3112],\n",
      "        [    0.3142],\n",
      "        [    0.3161],\n",
      "        [    0.3171],\n",
      "        [    0.3266],\n",
      "        [    0.3284],\n",
      "        [    0.3296],\n",
      "        [    0.3369],\n",
      "        [    0.3387],\n",
      "        [    0.3408],\n",
      "        [    0.3419],\n",
      "        [    0.3451],\n",
      "        [    0.3642],\n",
      "        [    0.3651],\n",
      "        [    0.3721],\n",
      "        [    0.3765],\n",
      "        [    0.3784],\n",
      "        [    0.3793],\n",
      "        [    0.3838],\n",
      "        [    0.3912]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0067],\n",
      "        [    0.0064],\n",
      "        [    0.0007],\n",
      "        [    0.0021],\n",
      "        [    0.0163],\n",
      "        [    0.0039],\n",
      "        [    0.0200],\n",
      "        [    0.0046],\n",
      "        [    0.0315],\n",
      "        [    0.0103],\n",
      "        [    0.0210],\n",
      "        [    0.0157],\n",
      "        [    0.0088],\n",
      "        [    0.0088],\n",
      "        [    0.0063],\n",
      "        [    0.0000],\n",
      "        [    0.0105],\n",
      "        [    0.0321],\n",
      "        [    0.0110],\n",
      "        [    0.0027],\n",
      "        [    0.0014],\n",
      "        [    0.0080],\n",
      "        [    0.0012],\n",
      "        [    0.0313],\n",
      "        [    0.0095],\n",
      "        [    0.0263],\n",
      "        [    0.0095],\n",
      "        [    0.0203],\n",
      "        [    0.0440],\n",
      "        [    0.0088],\n",
      "        [    0.0276],\n",
      "        [    0.0767],\n",
      "        [    0.0019],\n",
      "        [    0.0047],\n",
      "        [    0.0801],\n",
      "        [    0.0102],\n",
      "        [    0.0142],\n",
      "        [    0.0252],\n",
      "        [    0.0368],\n",
      "        [    0.0263],\n",
      "        [    0.0372],\n",
      "        [    0.0337],\n",
      "        [    0.0193],\n",
      "        [    0.0115],\n",
      "        [    0.0189],\n",
      "        [    0.0333],\n",
      "        [    0.0229],\n",
      "        [    0.0198],\n",
      "        [    0.0348],\n",
      "        [    0.0488],\n",
      "        [    0.0108],\n",
      "        [    0.0391],\n",
      "        [    0.0527],\n",
      "        [    0.0301],\n",
      "        [    0.0285],\n",
      "        [    0.0369],\n",
      "        [    0.0319],\n",
      "        [    0.0493],\n",
      "        [    0.0749],\n",
      "        [    0.0433],\n",
      "        [    0.0542],\n",
      "        [    0.0171],\n",
      "        [    0.0584],\n",
      "        [    0.0436],\n",
      "        [    0.0408],\n",
      "        [    0.0549],\n",
      "        [    0.0494],\n",
      "        [    0.0613],\n",
      "        [    0.0536],\n",
      "        [    0.0719],\n",
      "        [    0.0348],\n",
      "        [    0.0894],\n",
      "        [    0.0695],\n",
      "        [    0.0620],\n",
      "        [    0.0819],\n",
      "        [    0.0179],\n",
      "        [    0.0641],\n",
      "        [    0.0919],\n",
      "        [    0.0829],\n",
      "        [    0.0302],\n",
      "        [    0.0935],\n",
      "        [    0.0845],\n",
      "        [    0.0080],\n",
      "        [    0.0214],\n",
      "        [    0.0771],\n",
      "        [    0.0823],\n",
      "        [    0.0179],\n",
      "        [    0.0790],\n",
      "        [    0.0880],\n",
      "        [    0.0718],\n",
      "        [    0.0778],\n",
      "        [    0.0060],\n",
      "        [    0.0888],\n",
      "        [    0.0146],\n",
      "        [    0.0887],\n",
      "        [    0.0038],\n",
      "        [    0.0213],\n",
      "        [    0.1061],\n",
      "        [    0.0116],\n",
      "        [    0.0359],\n",
      "        [    0.1347],\n",
      "        [    0.0020],\n",
      "        [    0.0327],\n",
      "        [    0.0996],\n",
      "        [    0.1285],\n",
      "        [    0.0142],\n",
      "        [    0.1186],\n",
      "        [    0.1236],\n",
      "        [    0.0105],\n",
      "        [    0.0196],\n",
      "        [    0.1481],\n",
      "        [    0.1124],\n",
      "        [    0.0492],\n",
      "        [    0.1217],\n",
      "        [    0.1357],\n",
      "        [    0.1222],\n",
      "        [    0.0183],\n",
      "        [    0.0379],\n",
      "        [    0.1622],\n",
      "        [    0.0422],\n",
      "        [    0.0669],\n",
      "        [    0.0685],\n",
      "        [    0.0919],\n",
      "        [    0.2968],\n",
      "        [    0.3108],\n",
      "        [    0.3138],\n",
      "        [    0.3157],\n",
      "        [    0.3167],\n",
      "        [    0.3262],\n",
      "        [    0.3280],\n",
      "        [    0.3291],\n",
      "        [    0.3365],\n",
      "        [    0.3383],\n",
      "        [    0.3404],\n",
      "        [    0.3414],\n",
      "        [    0.3446],\n",
      "        [    0.3637],\n",
      "        [    0.3647],\n",
      "        [    0.3716],\n",
      "        [    0.3760],\n",
      "        [    0.3780],\n",
      "        [    0.3789],\n",
      "        [    0.3833],\n",
      "        [    0.3908]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 91.71608304977417\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 145\n",
      "剩餘X 資料 torch.Size([15, 18])\n",
      "剩餘Y 資料 torch.Size([15, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15400291979312897, 2)\n",
      "The second_loss value of k: (0.15458756685256958, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引2，y= tensor([0.0547])\n",
      "目前模型的Data狀態 torch.Size([145, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1264],\n",
      "        [0.3516],\n",
      "        [0.1079],\n",
      "        [0.6312],\n",
      "        [0.2934],\n",
      "        [0.2661],\n",
      "        [0.6240],\n",
      "        [0.3588],\n",
      "        [0.1863],\n",
      "        [0.6183],\n",
      "        [0.1411],\n",
      "        [0.3422],\n",
      "        [0.6078],\n",
      "        [0.5982],\n",
      "        [0.1263],\n",
      "        [0.1841],\n",
      "        [0.2231],\n",
      "        [0.1600],\n",
      "        [0.6320],\n",
      "        [0.1223],\n",
      "        [0.4419],\n",
      "        [0.5286],\n",
      "        [0.4728],\n",
      "        [0.2988],\n",
      "        [0.6211],\n",
      "        [0.5669],\n",
      "        [0.3539],\n",
      "        [0.6419],\n",
      "        [0.4581],\n",
      "        [0.6366],\n",
      "        [0.6492],\n",
      "        [0.4888],\n",
      "        [0.1401],\n",
      "        [0.5658],\n",
      "        [0.4797],\n",
      "        [0.3543],\n",
      "        [0.6049],\n",
      "        [0.2169],\n",
      "        [0.2703],\n",
      "        [0.3323],\n",
      "        [0.1430],\n",
      "        [0.3250],\n",
      "        [0.1782],\n",
      "        [0.2007],\n",
      "        [0.5748],\n",
      "        [0.3043],\n",
      "        [0.5819],\n",
      "        [0.4521],\n",
      "        [0.5827],\n",
      "        [0.2058],\n",
      "        [0.3803],\n",
      "        [0.4318],\n",
      "        [0.4723],\n",
      "        [0.6373],\n",
      "        [0.1193],\n",
      "        [0.2177],\n",
      "        [0.1250],\n",
      "        [0.5758],\n",
      "        [0.5624],\n",
      "        [0.3685],\n",
      "        [0.6175],\n",
      "        [0.5978],\n",
      "        [0.5176],\n",
      "        [0.4904],\n",
      "        [0.4237],\n",
      "        [0.4323],\n",
      "        [0.2371],\n",
      "        [0.2815],\n",
      "        [0.2127],\n",
      "        [0.1900],\n",
      "        [0.4482],\n",
      "        [0.2184],\n",
      "        [0.1601],\n",
      "        [0.2290],\n",
      "        [0.3272],\n",
      "        [0.4443],\n",
      "        [0.4589],\n",
      "        [0.3569],\n",
      "        [0.3971],\n",
      "        [0.5297],\n",
      "        [0.4050],\n",
      "        [0.3144],\n",
      "        [0.4534],\n",
      "        [0.4781],\n",
      "        [0.1953],\n",
      "        [0.1632],\n",
      "        [0.5105],\n",
      "        [0.2418],\n",
      "        [0.3641],\n",
      "        [0.0989],\n",
      "        [0.3732],\n",
      "        [0.4266],\n",
      "        [0.2644],\n",
      "        [0.5397],\n",
      "        [0.4442],\n",
      "        [0.4920],\n",
      "        [0.4169],\n",
      "        [0.2969],\n",
      "        [0.5032],\n",
      "        [0.4450],\n",
      "        [0.4610],\n",
      "        [0.5433],\n",
      "        [0.3904],\n",
      "        [0.3990],\n",
      "        [0.4153],\n",
      "        [0.4540],\n",
      "        [0.3179],\n",
      "        [0.3435],\n",
      "        [0.4434],\n",
      "        [0.4657],\n",
      "        [0.1712],\n",
      "        [0.1124],\n",
      "        [0.3944],\n",
      "        [0.2660],\n",
      "        [0.1670],\n",
      "        [0.3735],\n",
      "        [0.4516],\n",
      "        [0.5317],\n",
      "        [0.3896],\n",
      "        [0.4639],\n",
      "        [0.4167],\n",
      "        [0.4319],\n",
      "        [0.0268],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472],\n",
      "        [0.4472]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0067],\n",
      "        [    0.0064],\n",
      "        [    0.0007],\n",
      "        [    0.0021],\n",
      "        [    0.0163],\n",
      "        [    0.0039],\n",
      "        [    0.0200],\n",
      "        [    0.0046],\n",
      "        [    0.0315],\n",
      "        [    0.0103],\n",
      "        [    0.0210],\n",
      "        [    0.0157],\n",
      "        [    0.0088],\n",
      "        [    0.0088],\n",
      "        [    0.0063],\n",
      "        [    0.0000],\n",
      "        [    0.0105],\n",
      "        [    0.0321],\n",
      "        [    0.0110],\n",
      "        [    0.0027],\n",
      "        [    0.0014],\n",
      "        [    0.0080],\n",
      "        [    0.0012],\n",
      "        [    0.0313],\n",
      "        [    0.0095],\n",
      "        [    0.0263],\n",
      "        [    0.0095],\n",
      "        [    0.0203],\n",
      "        [    0.0440],\n",
      "        [    0.0088],\n",
      "        [    0.0276],\n",
      "        [    0.0767],\n",
      "        [    0.0019],\n",
      "        [    0.0047],\n",
      "        [    0.0801],\n",
      "        [    0.0102],\n",
      "        [    0.0142],\n",
      "        [    0.0252],\n",
      "        [    0.0368],\n",
      "        [    0.0263],\n",
      "        [    0.0372],\n",
      "        [    0.0337],\n",
      "        [    0.0193],\n",
      "        [    0.0115],\n",
      "        [    0.0189],\n",
      "        [    0.0333],\n",
      "        [    0.0229],\n",
      "        [    0.0198],\n",
      "        [    0.0348],\n",
      "        [    0.0488],\n",
      "        [    0.0108],\n",
      "        [    0.0391],\n",
      "        [    0.0527],\n",
      "        [    0.0301],\n",
      "        [    0.0285],\n",
      "        [    0.0369],\n",
      "        [    0.0319],\n",
      "        [    0.0493],\n",
      "        [    0.0749],\n",
      "        [    0.0433],\n",
      "        [    0.0542],\n",
      "        [    0.0171],\n",
      "        [    0.0584],\n",
      "        [    0.0436],\n",
      "        [    0.0408],\n",
      "        [    0.0549],\n",
      "        [    0.0494],\n",
      "        [    0.0613],\n",
      "        [    0.0536],\n",
      "        [    0.0719],\n",
      "        [    0.0348],\n",
      "        [    0.0894],\n",
      "        [    0.0695],\n",
      "        [    0.0620],\n",
      "        [    0.0819],\n",
      "        [    0.0179],\n",
      "        [    0.0641],\n",
      "        [    0.0919],\n",
      "        [    0.0829],\n",
      "        [    0.0302],\n",
      "        [    0.0935],\n",
      "        [    0.0845],\n",
      "        [    0.0080],\n",
      "        [    0.0214],\n",
      "        [    0.0771],\n",
      "        [    0.0823],\n",
      "        [    0.0179],\n",
      "        [    0.0790],\n",
      "        [    0.0880],\n",
      "        [    0.0718],\n",
      "        [    0.0778],\n",
      "        [    0.0060],\n",
      "        [    0.0888],\n",
      "        [    0.0146],\n",
      "        [    0.0887],\n",
      "        [    0.0038],\n",
      "        [    0.0213],\n",
      "        [    0.1061],\n",
      "        [    0.0116],\n",
      "        [    0.0359],\n",
      "        [    0.1347],\n",
      "        [    0.0020],\n",
      "        [    0.0327],\n",
      "        [    0.0996],\n",
      "        [    0.1285],\n",
      "        [    0.0142],\n",
      "        [    0.1186],\n",
      "        [    0.1236],\n",
      "        [    0.0105],\n",
      "        [    0.0196],\n",
      "        [    0.1481],\n",
      "        [    0.1124],\n",
      "        [    0.0492],\n",
      "        [    0.1217],\n",
      "        [    0.1357],\n",
      "        [    0.1222],\n",
      "        [    0.0183],\n",
      "        [    0.0379],\n",
      "        [    0.1622],\n",
      "        [    0.0422],\n",
      "        [    0.0669],\n",
      "        [    0.0685],\n",
      "        [    0.0919],\n",
      "        [    0.2968],\n",
      "        [    0.3108],\n",
      "        [    0.3138],\n",
      "        [    0.3157],\n",
      "        [    0.3167],\n",
      "        [    0.3262],\n",
      "        [    0.3280],\n",
      "        [    0.3291],\n",
      "        [    0.3365],\n",
      "        [    0.3383],\n",
      "        [    0.3404],\n",
      "        [    0.3414],\n",
      "        [    0.3446],\n",
      "        [    0.3637],\n",
      "        [    0.3647],\n",
      "        [    0.3716],\n",
      "        [    0.3760],\n",
      "        [    0.3780],\n",
      "        [    0.3789],\n",
      "        [    0.3833],\n",
      "        [    0.3908],\n",
      "        [    0.3924]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0064],\n",
      "        [    0.0058],\n",
      "        [    0.0006],\n",
      "        [    0.0016],\n",
      "        [    0.0161],\n",
      "        [    0.0041],\n",
      "        [    0.0195],\n",
      "        [    0.0046],\n",
      "        [    0.0319],\n",
      "        [    0.0101],\n",
      "        [    0.0208],\n",
      "        [    0.0150],\n",
      "        [    0.0084],\n",
      "        [    0.0087],\n",
      "        [    0.0062],\n",
      "        [    0.0003],\n",
      "        [    0.0098],\n",
      "        [    0.0324],\n",
      "        [    0.0098],\n",
      "        [    0.0028],\n",
      "        [    0.0019],\n",
      "        [    0.0075],\n",
      "        [    0.0009],\n",
      "        [    0.0318],\n",
      "        [    0.0090],\n",
      "        [    0.0269],\n",
      "        [    0.0087],\n",
      "        [    0.0213],\n",
      "        [    0.0435],\n",
      "        [    0.0092],\n",
      "        [    0.0266],\n",
      "        [    0.0769],\n",
      "        [    0.0018],\n",
      "        [    0.0048],\n",
      "        [    0.0800],\n",
      "        [    0.0099],\n",
      "        [    0.0146],\n",
      "        [    0.0259],\n",
      "        [    0.0363],\n",
      "        [    0.0256],\n",
      "        [    0.0372],\n",
      "        [    0.0333],\n",
      "        [    0.0199],\n",
      "        [    0.0116],\n",
      "        [    0.0191],\n",
      "        [    0.0335],\n",
      "        [    0.0231],\n",
      "        [    0.0196],\n",
      "        [    0.0351],\n",
      "        [    0.0484],\n",
      "        [    0.0098],\n",
      "        [    0.0390],\n",
      "        [    0.0529],\n",
      "        [    0.0304],\n",
      "        [    0.0285],\n",
      "        [    0.0373],\n",
      "        [    0.0319],\n",
      "        [    0.0498],\n",
      "        [    0.0758],\n",
      "        [    0.0434],\n",
      "        [    0.0533],\n",
      "        [    0.0174],\n",
      "        [    0.0588],\n",
      "        [    0.0431],\n",
      "        [    0.0407],\n",
      "        [    0.0549],\n",
      "        [    0.0497],\n",
      "        [    0.0617],\n",
      "        [    0.0539],\n",
      "        [    0.0712],\n",
      "        [    0.0346],\n",
      "        [    0.0889],\n",
      "        [    0.0698],\n",
      "        [    0.0613],\n",
      "        [    0.0816],\n",
      "        [    0.0181],\n",
      "        [    0.0637],\n",
      "        [    0.0924],\n",
      "        [    0.0828],\n",
      "        [    0.0307],\n",
      "        [    0.0936],\n",
      "        [    0.0842],\n",
      "        [    0.0084],\n",
      "        [    0.0218],\n",
      "        [    0.0771],\n",
      "        [    0.0824],\n",
      "        [    0.0184],\n",
      "        [    0.0791],\n",
      "        [    0.0875],\n",
      "        [    0.0717],\n",
      "        [    0.0770],\n",
      "        [    0.0056],\n",
      "        [    0.0889],\n",
      "        [    0.0152],\n",
      "        [    0.0883],\n",
      "        [    0.0039],\n",
      "        [    0.0210],\n",
      "        [    0.1064],\n",
      "        [    0.0126],\n",
      "        [    0.0357],\n",
      "        [    0.1352],\n",
      "        [    0.0028],\n",
      "        [    0.0325],\n",
      "        [    0.0990],\n",
      "        [    0.1289],\n",
      "        [    0.0138],\n",
      "        [    0.1182],\n",
      "        [    0.1232],\n",
      "        [    0.0100],\n",
      "        [    0.0194],\n",
      "        [    0.1475],\n",
      "        [    0.1123],\n",
      "        [    0.0489],\n",
      "        [    0.1218],\n",
      "        [    0.1349],\n",
      "        [    0.1217],\n",
      "        [    0.0176],\n",
      "        [    0.0373],\n",
      "        [    0.1625],\n",
      "        [    0.0416],\n",
      "        [    0.0664],\n",
      "        [    0.0678],\n",
      "        [    0.0871],\n",
      "        [    0.2964],\n",
      "        [    0.3104],\n",
      "        [    0.3134],\n",
      "        [    0.3153],\n",
      "        [    0.3163],\n",
      "        [    0.3257],\n",
      "        [    0.3276],\n",
      "        [    0.3287],\n",
      "        [    0.3361],\n",
      "        [    0.3379],\n",
      "        [    0.3400],\n",
      "        [    0.3410],\n",
      "        [    0.3442],\n",
      "        [    0.3633],\n",
      "        [    0.3643],\n",
      "        [    0.3712],\n",
      "        [    0.3756],\n",
      "        [    0.3776],\n",
      "        [    0.3785],\n",
      "        [    0.3829],\n",
      "        [    0.3904],\n",
      "        [    0.3920]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 91.95318007469177\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 146\n",
      "剩餘X 資料 torch.Size([14, 18])\n",
      "剩餘Y 資料 torch.Size([14, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15426644682884216, 8)\n",
      "The second_loss value of k: (0.15429720282554626, 9)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.0540])\n",
      "目前模型的Data狀態 torch.Size([146, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1267],\n",
      "        [0.3522],\n",
      "        [0.1080],\n",
      "        [0.6317],\n",
      "        [0.2936],\n",
      "        [0.2663],\n",
      "        [0.6245],\n",
      "        [0.3589],\n",
      "        [0.1859],\n",
      "        [0.6185],\n",
      "        [0.1413],\n",
      "        [0.3429],\n",
      "        [0.6082],\n",
      "        [0.5983],\n",
      "        [0.1264],\n",
      "        [0.1844],\n",
      "        [0.2238],\n",
      "        [0.1597],\n",
      "        [0.6332],\n",
      "        [0.1224],\n",
      "        [0.4423],\n",
      "        [0.5291],\n",
      "        [0.4730],\n",
      "        [0.2993],\n",
      "        [0.6216],\n",
      "        [0.5675],\n",
      "        [0.3547],\n",
      "        [0.6429],\n",
      "        [0.4577],\n",
      "        [0.6370],\n",
      "        [0.6501],\n",
      "        [0.4890],\n",
      "        [0.1401],\n",
      "        [0.5659],\n",
      "        [0.4796],\n",
      "        [0.3546],\n",
      "        [0.6052],\n",
      "        [0.2176],\n",
      "        [0.2708],\n",
      "        [0.3329],\n",
      "        [0.1430],\n",
      "        [0.3254],\n",
      "        [0.1776],\n",
      "        [0.2008],\n",
      "        [0.5750],\n",
      "        [0.3045],\n",
      "        [0.5821],\n",
      "        [0.4523],\n",
      "        [0.5831],\n",
      "        [0.2062],\n",
      "        [0.3813],\n",
      "        [0.4320],\n",
      "        [0.4725],\n",
      "        [0.6376],\n",
      "        [0.1193],\n",
      "        [0.2181],\n",
      "        [0.1250],\n",
      "        [0.5762],\n",
      "        [0.5633],\n",
      "        [0.3686],\n",
      "        [0.6185],\n",
      "        [0.5981],\n",
      "        [0.5181],\n",
      "        [0.4909],\n",
      "        [0.4239],\n",
      "        [0.4323],\n",
      "        [0.2374],\n",
      "        [0.2818],\n",
      "        [0.2130],\n",
      "        [0.1893],\n",
      "        [0.4484],\n",
      "        [0.2189],\n",
      "        [0.1604],\n",
      "        [0.2296],\n",
      "        [0.3275],\n",
      "        [0.4444],\n",
      "        [0.4593],\n",
      "        [0.3573],\n",
      "        [0.3970],\n",
      "        [0.5302],\n",
      "        [0.4050],\n",
      "        [0.3147],\n",
      "        [0.4539],\n",
      "        [0.4785],\n",
      "        [0.1953],\n",
      "        [0.1633],\n",
      "        [0.5110],\n",
      "        [0.2419],\n",
      "        [0.3647],\n",
      "        [0.0987],\n",
      "        [0.3740],\n",
      "        [0.4270],\n",
      "        [0.2645],\n",
      "        [0.5403],\n",
      "        [0.4447],\n",
      "        [0.4922],\n",
      "        [0.4173],\n",
      "        [0.2973],\n",
      "        [0.5042],\n",
      "        [0.4452],\n",
      "        [0.4615],\n",
      "        [0.5441],\n",
      "        [0.3907],\n",
      "        [0.3995],\n",
      "        [0.4156],\n",
      "        [0.4544],\n",
      "        [0.3183],\n",
      "        [0.3440],\n",
      "        [0.4440],\n",
      "        [0.4659],\n",
      "        [0.1718],\n",
      "        [0.1123],\n",
      "        [0.3947],\n",
      "        [0.2662],\n",
      "        [0.1678],\n",
      "        [0.3741],\n",
      "        [0.4522],\n",
      "        [0.5323],\n",
      "        [0.3899],\n",
      "        [0.4645],\n",
      "        [0.4171],\n",
      "        [0.4326],\n",
      "        [0.0316],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467],\n",
      "        [0.4467]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0064],\n",
      "        [    0.0058],\n",
      "        [    0.0006],\n",
      "        [    0.0016],\n",
      "        [    0.0161],\n",
      "        [    0.0041],\n",
      "        [    0.0195],\n",
      "        [    0.0046],\n",
      "        [    0.0319],\n",
      "        [    0.0101],\n",
      "        [    0.0208],\n",
      "        [    0.0150],\n",
      "        [    0.0084],\n",
      "        [    0.0087],\n",
      "        [    0.0062],\n",
      "        [    0.0003],\n",
      "        [    0.0098],\n",
      "        [    0.0324],\n",
      "        [    0.0098],\n",
      "        [    0.0028],\n",
      "        [    0.0019],\n",
      "        [    0.0075],\n",
      "        [    0.0009],\n",
      "        [    0.0318],\n",
      "        [    0.0090],\n",
      "        [    0.0269],\n",
      "        [    0.0087],\n",
      "        [    0.0213],\n",
      "        [    0.0435],\n",
      "        [    0.0092],\n",
      "        [    0.0266],\n",
      "        [    0.0769],\n",
      "        [    0.0018],\n",
      "        [    0.0048],\n",
      "        [    0.0800],\n",
      "        [    0.0099],\n",
      "        [    0.0146],\n",
      "        [    0.0259],\n",
      "        [    0.0363],\n",
      "        [    0.0256],\n",
      "        [    0.0372],\n",
      "        [    0.0333],\n",
      "        [    0.0199],\n",
      "        [    0.0116],\n",
      "        [    0.0191],\n",
      "        [    0.0335],\n",
      "        [    0.0231],\n",
      "        [    0.0196],\n",
      "        [    0.0351],\n",
      "        [    0.0484],\n",
      "        [    0.0098],\n",
      "        [    0.0390],\n",
      "        [    0.0529],\n",
      "        [    0.0304],\n",
      "        [    0.0285],\n",
      "        [    0.0373],\n",
      "        [    0.0319],\n",
      "        [    0.0498],\n",
      "        [    0.0758],\n",
      "        [    0.0434],\n",
      "        [    0.0533],\n",
      "        [    0.0174],\n",
      "        [    0.0588],\n",
      "        [    0.0431],\n",
      "        [    0.0407],\n",
      "        [    0.0549],\n",
      "        [    0.0497],\n",
      "        [    0.0617],\n",
      "        [    0.0539],\n",
      "        [    0.0712],\n",
      "        [    0.0346],\n",
      "        [    0.0889],\n",
      "        [    0.0698],\n",
      "        [    0.0613],\n",
      "        [    0.0816],\n",
      "        [    0.0181],\n",
      "        [    0.0637],\n",
      "        [    0.0924],\n",
      "        [    0.0828],\n",
      "        [    0.0307],\n",
      "        [    0.0936],\n",
      "        [    0.0842],\n",
      "        [    0.0084],\n",
      "        [    0.0218],\n",
      "        [    0.0771],\n",
      "        [    0.0824],\n",
      "        [    0.0184],\n",
      "        [    0.0791],\n",
      "        [    0.0875],\n",
      "        [    0.0717],\n",
      "        [    0.0770],\n",
      "        [    0.0056],\n",
      "        [    0.0889],\n",
      "        [    0.0152],\n",
      "        [    0.0883],\n",
      "        [    0.0039],\n",
      "        [    0.0210],\n",
      "        [    0.1064],\n",
      "        [    0.0126],\n",
      "        [    0.0357],\n",
      "        [    0.1352],\n",
      "        [    0.0028],\n",
      "        [    0.0325],\n",
      "        [    0.0990],\n",
      "        [    0.1289],\n",
      "        [    0.0138],\n",
      "        [    0.1182],\n",
      "        [    0.1232],\n",
      "        [    0.0100],\n",
      "        [    0.0194],\n",
      "        [    0.1475],\n",
      "        [    0.1123],\n",
      "        [    0.0489],\n",
      "        [    0.1218],\n",
      "        [    0.1349],\n",
      "        [    0.1217],\n",
      "        [    0.0176],\n",
      "        [    0.0373],\n",
      "        [    0.1625],\n",
      "        [    0.0416],\n",
      "        [    0.0664],\n",
      "        [    0.0678],\n",
      "        [    0.0871],\n",
      "        [    0.2964],\n",
      "        [    0.3104],\n",
      "        [    0.3134],\n",
      "        [    0.3153],\n",
      "        [    0.3163],\n",
      "        [    0.3257],\n",
      "        [    0.3276],\n",
      "        [    0.3287],\n",
      "        [    0.3361],\n",
      "        [    0.3379],\n",
      "        [    0.3400],\n",
      "        [    0.3410],\n",
      "        [    0.3442],\n",
      "        [    0.3633],\n",
      "        [    0.3643],\n",
      "        [    0.3712],\n",
      "        [    0.3756],\n",
      "        [    0.3776],\n",
      "        [    0.3785],\n",
      "        [    0.3829],\n",
      "        [    0.3904],\n",
      "        [    0.3920],\n",
      "        [    0.3928]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0066],\n",
      "        [    0.0059],\n",
      "        [    0.0009],\n",
      "        [    0.0021],\n",
      "        [    0.0164],\n",
      "        [    0.0036],\n",
      "        [    0.0199],\n",
      "        [    0.0040],\n",
      "        [    0.0325],\n",
      "        [    0.0108],\n",
      "        [    0.0211],\n",
      "        [    0.0150],\n",
      "        [    0.0090],\n",
      "        [    0.0094],\n",
      "        [    0.0066],\n",
      "        [    0.0001],\n",
      "        [    0.0098],\n",
      "        [    0.0330],\n",
      "        [    0.0097],\n",
      "        [    0.0025],\n",
      "        [    0.0015],\n",
      "        [    0.0080],\n",
      "        [    0.0015],\n",
      "        [    0.0316],\n",
      "        [    0.0093],\n",
      "        [    0.0266],\n",
      "        [    0.0087],\n",
      "        [    0.0212],\n",
      "        [    0.0427],\n",
      "        [    0.0087],\n",
      "        [    0.0266],\n",
      "        [    0.0763],\n",
      "        [    0.0014],\n",
      "        [    0.0041],\n",
      "        [    0.0793],\n",
      "        [    0.0103],\n",
      "        [    0.0140],\n",
      "        [    0.0259],\n",
      "        [    0.0363],\n",
      "        [    0.0257],\n",
      "        [    0.0376],\n",
      "        [    0.0335],\n",
      "        [    0.0206],\n",
      "        [    0.0113],\n",
      "        [    0.0185],\n",
      "        [    0.0329],\n",
      "        [    0.0224],\n",
      "        [    0.0202],\n",
      "        [    0.0346],\n",
      "        [    0.0486],\n",
      "        [    0.0097],\n",
      "        [    0.0396],\n",
      "        [    0.0523],\n",
      "        [    0.0298],\n",
      "        [    0.0281],\n",
      "        [    0.0370],\n",
      "        [    0.0315],\n",
      "        [    0.0494],\n",
      "        [    0.0758],\n",
      "        [    0.0428],\n",
      "        [    0.0533],\n",
      "        [    0.0169],\n",
      "        [    0.0584],\n",
      "        [    0.0436],\n",
      "        [    0.0412],\n",
      "        [    0.0556],\n",
      "        [    0.0494],\n",
      "        [    0.0613],\n",
      "        [    0.0536],\n",
      "        [    0.0704],\n",
      "        [    0.0350],\n",
      "        [    0.0890],\n",
      "        [    0.0696],\n",
      "        [    0.0614],\n",
      "        [    0.0819],\n",
      "        [    0.0176],\n",
      "        [    0.0641],\n",
      "        [    0.0921],\n",
      "        [    0.0821],\n",
      "        [    0.0303],\n",
      "        [    0.0930],\n",
      "        [    0.0846],\n",
      "        [    0.0079],\n",
      "        [    0.0214],\n",
      "        [    0.0767],\n",
      "        [    0.0820],\n",
      "        [    0.0180],\n",
      "        [    0.0787],\n",
      "        [    0.0876],\n",
      "        [    0.0712],\n",
      "        [    0.0771],\n",
      "        [    0.0060],\n",
      "        [    0.0885],\n",
      "        [    0.0150],\n",
      "        [    0.0887],\n",
      "        [    0.0033],\n",
      "        [    0.0215],\n",
      "        [    0.1061],\n",
      "        [    0.0126],\n",
      "        [    0.0363],\n",
      "        [    0.1349],\n",
      "        [    0.0026],\n",
      "        [    0.0330],\n",
      "        [    0.0992],\n",
      "        [    0.1285],\n",
      "        [    0.0142],\n",
      "        [    0.1186],\n",
      "        [    0.1234],\n",
      "        [    0.0104],\n",
      "        [    0.0200],\n",
      "        [    0.1475],\n",
      "        [    0.1117],\n",
      "        [    0.0494],\n",
      "        [    0.1216],\n",
      "        [    0.1349],\n",
      "        [    0.1217],\n",
      "        [    0.0179],\n",
      "        [    0.0375],\n",
      "        [    0.1621],\n",
      "        [    0.0420],\n",
      "        [    0.0668],\n",
      "        [    0.0680],\n",
      "        [    0.0844],\n",
      "        [    0.2959],\n",
      "        [    0.3099],\n",
      "        [    0.3129],\n",
      "        [    0.3148],\n",
      "        [    0.3158],\n",
      "        [    0.3253],\n",
      "        [    0.3271],\n",
      "        [    0.3283],\n",
      "        [    0.3356],\n",
      "        [    0.3374],\n",
      "        [    0.3395],\n",
      "        [    0.3406],\n",
      "        [    0.3438],\n",
      "        [    0.3629],\n",
      "        [    0.3638],\n",
      "        [    0.3708],\n",
      "        [    0.3752],\n",
      "        [    0.3772],\n",
      "        [    0.3780],\n",
      "        [    0.3825],\n",
      "        [    0.3899],\n",
      "        [    0.3916],\n",
      "        [    0.3923]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 92.1919732093811\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 147\n",
      "剩餘X 資料 torch.Size([13, 18])\n",
      "剩餘Y 資料 torch.Size([13, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15394282341003418, 8)\n",
      "The second_loss value of k: (0.15608644485473633, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.0539])\n",
      "目前模型的Data狀態 torch.Size([147, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1265],\n",
      "        [0.3521],\n",
      "        [0.1077],\n",
      "        [0.6313],\n",
      "        [0.2933],\n",
      "        [0.2658],\n",
      "        [0.6240],\n",
      "        [0.3583],\n",
      "        [0.1853],\n",
      "        [0.6178],\n",
      "        [0.1409],\n",
      "        [0.3429],\n",
      "        [0.6076],\n",
      "        [0.5976],\n",
      "        [0.1260],\n",
      "        [0.1841],\n",
      "        [0.2238],\n",
      "        [0.1591],\n",
      "        [0.6332],\n",
      "        [0.1221],\n",
      "        [0.4420],\n",
      "        [0.5287],\n",
      "        [0.4725],\n",
      "        [0.2991],\n",
      "        [0.6213],\n",
      "        [0.5672],\n",
      "        [0.3547],\n",
      "        [0.6428],\n",
      "        [0.4569],\n",
      "        [0.6365],\n",
      "        [0.6501],\n",
      "        [0.4884],\n",
      "        [0.1397],\n",
      "        [0.5652],\n",
      "        [0.4790],\n",
      "        [0.3541],\n",
      "        [0.6047],\n",
      "        [0.2176],\n",
      "        [0.2708],\n",
      "        [0.3328],\n",
      "        [0.1426],\n",
      "        [0.3252],\n",
      "        [0.1770],\n",
      "        [0.2004],\n",
      "        [0.5743],\n",
      "        [0.3039],\n",
      "        [0.5814],\n",
      "        [0.4517],\n",
      "        [0.5825],\n",
      "        [0.2060],\n",
      "        [0.3814],\n",
      "        [0.4314],\n",
      "        [0.4719],\n",
      "        [0.6371],\n",
      "        [0.1189],\n",
      "        [0.2178],\n",
      "        [0.1246],\n",
      "        [0.5758],\n",
      "        [0.5632],\n",
      "        [0.3680],\n",
      "        [0.6184],\n",
      "        [0.5976],\n",
      "        [0.5176],\n",
      "        [0.4904],\n",
      "        [0.4233],\n",
      "        [0.4317],\n",
      "        [0.2370],\n",
      "        [0.2815],\n",
      "        [0.2127],\n",
      "        [0.1885],\n",
      "        [0.4481],\n",
      "        [0.2188],\n",
      "        [0.1602],\n",
      "        [0.2295],\n",
      "        [0.3273],\n",
      "        [0.4439],\n",
      "        [0.4589],\n",
      "        [0.3571],\n",
      "        [0.3963],\n",
      "        [0.5298],\n",
      "        [0.4044],\n",
      "        [0.3143],\n",
      "        [0.4534],\n",
      "        [0.4781],\n",
      "        [0.1949],\n",
      "        [0.1629],\n",
      "        [0.5105],\n",
      "        [0.2414],\n",
      "        [0.3645],\n",
      "        [0.0983],\n",
      "        [0.3740],\n",
      "        [0.4265],\n",
      "        [0.2641],\n",
      "        [0.5400],\n",
      "        [0.4442],\n",
      "        [0.4915],\n",
      "        [0.4168],\n",
      "        [0.2970],\n",
      "        [0.5042],\n",
      "        [0.4446],\n",
      "        [0.4612],\n",
      "        [0.5439],\n",
      "        [0.3902],\n",
      "        [0.3994],\n",
      "        [0.4153],\n",
      "        [0.4540],\n",
      "        [0.3179],\n",
      "        [0.3438],\n",
      "        [0.4436],\n",
      "        [0.4653],\n",
      "        [0.1717],\n",
      "        [0.1117],\n",
      "        [0.3942],\n",
      "        [0.2659],\n",
      "        [0.1678],\n",
      "        [0.3741],\n",
      "        [0.4519],\n",
      "        [0.5321],\n",
      "        [0.3895],\n",
      "        [0.4641],\n",
      "        [0.4168],\n",
      "        [0.4324],\n",
      "        [0.0343],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463],\n",
      "        [0.4463]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0066],\n",
      "        [    0.0059],\n",
      "        [    0.0009],\n",
      "        [    0.0021],\n",
      "        [    0.0164],\n",
      "        [    0.0036],\n",
      "        [    0.0199],\n",
      "        [    0.0040],\n",
      "        [    0.0325],\n",
      "        [    0.0108],\n",
      "        [    0.0211],\n",
      "        [    0.0150],\n",
      "        [    0.0090],\n",
      "        [    0.0094],\n",
      "        [    0.0066],\n",
      "        [    0.0001],\n",
      "        [    0.0098],\n",
      "        [    0.0330],\n",
      "        [    0.0097],\n",
      "        [    0.0025],\n",
      "        [    0.0015],\n",
      "        [    0.0080],\n",
      "        [    0.0015],\n",
      "        [    0.0316],\n",
      "        [    0.0093],\n",
      "        [    0.0266],\n",
      "        [    0.0087],\n",
      "        [    0.0212],\n",
      "        [    0.0427],\n",
      "        [    0.0087],\n",
      "        [    0.0266],\n",
      "        [    0.0763],\n",
      "        [    0.0014],\n",
      "        [    0.0041],\n",
      "        [    0.0793],\n",
      "        [    0.0103],\n",
      "        [    0.0140],\n",
      "        [    0.0259],\n",
      "        [    0.0363],\n",
      "        [    0.0257],\n",
      "        [    0.0376],\n",
      "        [    0.0335],\n",
      "        [    0.0206],\n",
      "        [    0.0113],\n",
      "        [    0.0185],\n",
      "        [    0.0329],\n",
      "        [    0.0224],\n",
      "        [    0.0202],\n",
      "        [    0.0346],\n",
      "        [    0.0486],\n",
      "        [    0.0097],\n",
      "        [    0.0396],\n",
      "        [    0.0523],\n",
      "        [    0.0298],\n",
      "        [    0.0281],\n",
      "        [    0.0370],\n",
      "        [    0.0315],\n",
      "        [    0.0494],\n",
      "        [    0.0758],\n",
      "        [    0.0428],\n",
      "        [    0.0533],\n",
      "        [    0.0169],\n",
      "        [    0.0584],\n",
      "        [    0.0436],\n",
      "        [    0.0412],\n",
      "        [    0.0556],\n",
      "        [    0.0494],\n",
      "        [    0.0613],\n",
      "        [    0.0536],\n",
      "        [    0.0704],\n",
      "        [    0.0350],\n",
      "        [    0.0890],\n",
      "        [    0.0696],\n",
      "        [    0.0614],\n",
      "        [    0.0819],\n",
      "        [    0.0176],\n",
      "        [    0.0641],\n",
      "        [    0.0921],\n",
      "        [    0.0821],\n",
      "        [    0.0303],\n",
      "        [    0.0930],\n",
      "        [    0.0846],\n",
      "        [    0.0079],\n",
      "        [    0.0214],\n",
      "        [    0.0767],\n",
      "        [    0.0820],\n",
      "        [    0.0180],\n",
      "        [    0.0787],\n",
      "        [    0.0876],\n",
      "        [    0.0712],\n",
      "        [    0.0771],\n",
      "        [    0.0060],\n",
      "        [    0.0885],\n",
      "        [    0.0150],\n",
      "        [    0.0887],\n",
      "        [    0.0033],\n",
      "        [    0.0215],\n",
      "        [    0.1061],\n",
      "        [    0.0126],\n",
      "        [    0.0363],\n",
      "        [    0.1349],\n",
      "        [    0.0026],\n",
      "        [    0.0330],\n",
      "        [    0.0992],\n",
      "        [    0.1285],\n",
      "        [    0.0142],\n",
      "        [    0.1186],\n",
      "        [    0.1234],\n",
      "        [    0.0104],\n",
      "        [    0.0200],\n",
      "        [    0.1475],\n",
      "        [    0.1117],\n",
      "        [    0.0494],\n",
      "        [    0.1216],\n",
      "        [    0.1349],\n",
      "        [    0.1217],\n",
      "        [    0.0179],\n",
      "        [    0.0375],\n",
      "        [    0.1621],\n",
      "        [    0.0420],\n",
      "        [    0.0668],\n",
      "        [    0.0680],\n",
      "        [    0.0844],\n",
      "        [    0.2959],\n",
      "        [    0.3099],\n",
      "        [    0.3129],\n",
      "        [    0.3148],\n",
      "        [    0.3158],\n",
      "        [    0.3253],\n",
      "        [    0.3271],\n",
      "        [    0.3283],\n",
      "        [    0.3356],\n",
      "        [    0.3374],\n",
      "        [    0.3395],\n",
      "        [    0.3406],\n",
      "        [    0.3438],\n",
      "        [    0.3629],\n",
      "        [    0.3638],\n",
      "        [    0.3708],\n",
      "        [    0.3752],\n",
      "        [    0.3772],\n",
      "        [    0.3780],\n",
      "        [    0.3825],\n",
      "        [    0.3899],\n",
      "        [    0.3916],\n",
      "        [    0.3923],\n",
      "        [    0.3924]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0059],\n",
      "        [    0.0048],\n",
      "        [    0.0002],\n",
      "        [    0.0009],\n",
      "        [    0.0156],\n",
      "        [    0.0042],\n",
      "        [    0.0187],\n",
      "        [    0.0046],\n",
      "        [    0.0322],\n",
      "        [    0.0098],\n",
      "        [    0.0205],\n",
      "        [    0.0138],\n",
      "        [    0.0079],\n",
      "        [    0.0085],\n",
      "        [    0.0059],\n",
      "        [    0.0008],\n",
      "        [    0.0089],\n",
      "        [    0.0327],\n",
      "        [    0.0081],\n",
      "        [    0.0031],\n",
      "        [    0.0026],\n",
      "        [    0.0068],\n",
      "        [    0.0006],\n",
      "        [    0.0326],\n",
      "        [    0.0080],\n",
      "        [    0.0278],\n",
      "        [    0.0076],\n",
      "        [    0.0227],\n",
      "        [    0.0432],\n",
      "        [    0.0098],\n",
      "        [    0.0250],\n",
      "        [    0.0772],\n",
      "        [    0.0020],\n",
      "        [    0.0050],\n",
      "        [    0.0800],\n",
      "        [    0.0096],\n",
      "        [    0.0150],\n",
      "        [    0.0269],\n",
      "        [    0.0353],\n",
      "        [    0.0247],\n",
      "        [    0.0371],\n",
      "        [    0.0326],\n",
      "        [    0.0211],\n",
      "        [    0.0119],\n",
      "        [    0.0194],\n",
      "        [    0.0336],\n",
      "        [    0.0234],\n",
      "        [    0.0194],\n",
      "        [    0.0356],\n",
      "        [    0.0478],\n",
      "        [    0.0084],\n",
      "        [    0.0387],\n",
      "        [    0.0532],\n",
      "        [    0.0309],\n",
      "        [    0.0287],\n",
      "        [    0.0378],\n",
      "        [    0.0321],\n",
      "        [    0.0505],\n",
      "        [    0.0772],\n",
      "        [    0.0435],\n",
      "        [    0.0518],\n",
      "        [    0.0180],\n",
      "        [    0.0594],\n",
      "        [    0.0425],\n",
      "        [    0.0404],\n",
      "        [    0.0548],\n",
      "        [    0.0502],\n",
      "        [    0.0621],\n",
      "        [    0.0543],\n",
      "        [    0.0699],\n",
      "        [    0.0342],\n",
      "        [    0.0881],\n",
      "        [    0.0704],\n",
      "        [    0.0604],\n",
      "        [    0.0810],\n",
      "        [    0.0184],\n",
      "        [    0.0631],\n",
      "        [    0.0931],\n",
      "        [    0.0826],\n",
      "        [    0.0313],\n",
      "        [    0.0936],\n",
      "        [    0.0839],\n",
      "        [    0.0089],\n",
      "        [    0.0224],\n",
      "        [    0.0773],\n",
      "        [    0.0826],\n",
      "        [    0.0190],\n",
      "        [    0.0793],\n",
      "        [    0.0867],\n",
      "        [    0.0716],\n",
      "        [    0.0760],\n",
      "        [    0.0050],\n",
      "        [    0.0892],\n",
      "        [    0.0161],\n",
      "        [    0.0877],\n",
      "        [    0.0041],\n",
      "        [    0.0205],\n",
      "        [    0.1069],\n",
      "        [    0.0140],\n",
      "        [    0.0355],\n",
      "        [    0.1360],\n",
      "        [    0.0038],\n",
      "        [    0.0321],\n",
      "        [    0.0982],\n",
      "        [    0.1295],\n",
      "        [    0.0131],\n",
      "        [    0.1178],\n",
      "        [    0.1225],\n",
      "        [    0.0093],\n",
      "        [    0.0191],\n",
      "        [    0.1465],\n",
      "        [    0.1121],\n",
      "        [    0.0485],\n",
      "        [    0.1223],\n",
      "        [    0.1337],\n",
      "        [    0.1206],\n",
      "        [    0.0167],\n",
      "        [    0.0363],\n",
      "        [    0.1631],\n",
      "        [    0.0409],\n",
      "        [    0.0657],\n",
      "        [    0.0667],\n",
      "        [    0.0810],\n",
      "        [    0.2955],\n",
      "        [    0.3095],\n",
      "        [    0.3125],\n",
      "        [    0.3143],\n",
      "        [    0.3154],\n",
      "        [    0.3248],\n",
      "        [    0.3266],\n",
      "        [    0.3278],\n",
      "        [    0.3352],\n",
      "        [    0.3370],\n",
      "        [    0.3391],\n",
      "        [    0.3401],\n",
      "        [    0.3433],\n",
      "        [    0.3624],\n",
      "        [    0.3634],\n",
      "        [    0.3703],\n",
      "        [    0.3747],\n",
      "        [    0.3767],\n",
      "        [    0.3776],\n",
      "        [    0.3820],\n",
      "        [    0.3895],\n",
      "        [    0.3911],\n",
      "        [    0.3919],\n",
      "        [    0.3919]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 92.4302818775177\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 148\n",
      "剩餘X 資料 torch.Size([12, 18])\n",
      "剩餘Y 資料 torch.Size([12, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15572676062583923, 6)\n",
      "The second_loss value of k: (0.15834996104240417, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引6，y= tensor([0.0512])\n",
      "目前模型的Data狀態 torch.Size([148, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1273],\n",
      "        [0.3532],\n",
      "        [0.1084],\n",
      "        [0.6324],\n",
      "        [0.2941],\n",
      "        [0.2664],\n",
      "        [0.6252],\n",
      "        [0.3589],\n",
      "        [0.1856],\n",
      "        [0.6188],\n",
      "        [0.1416],\n",
      "        [0.3441],\n",
      "        [0.6087],\n",
      "        [0.5985],\n",
      "        [0.1267],\n",
      "        [0.1848],\n",
      "        [0.2247],\n",
      "        [0.1594],\n",
      "        [0.6349],\n",
      "        [0.1227],\n",
      "        [0.4430],\n",
      "        [0.5298],\n",
      "        [0.4733],\n",
      "        [0.3001],\n",
      "        [0.6225],\n",
      "        [0.5684],\n",
      "        [0.3558],\n",
      "        [0.6443],\n",
      "        [0.4573],\n",
      "        [0.6376],\n",
      "        [0.6517],\n",
      "        [0.4893],\n",
      "        [0.1402],\n",
      "        [0.5661],\n",
      "        [0.4796],\n",
      "        [0.3549],\n",
      "        [0.6057],\n",
      "        [0.2186],\n",
      "        [0.2718],\n",
      "        [0.3339],\n",
      "        [0.1431],\n",
      "        [0.3261],\n",
      "        [0.1764],\n",
      "        [0.2010],\n",
      "        [0.5753],\n",
      "        [0.3046],\n",
      "        [0.5824],\n",
      "        [0.4525],\n",
      "        [0.5836],\n",
      "        [0.2067],\n",
      "        [0.3827],\n",
      "        [0.4322],\n",
      "        [0.4727],\n",
      "        [0.6381],\n",
      "        [0.1194],\n",
      "        [0.2186],\n",
      "        [0.1252],\n",
      "        [0.5770],\n",
      "        [0.5646],\n",
      "        [0.3688],\n",
      "        [0.6200],\n",
      "        [0.5987],\n",
      "        [0.5187],\n",
      "        [0.4915],\n",
      "        [0.4241],\n",
      "        [0.4324],\n",
      "        [0.2378],\n",
      "        [0.2823],\n",
      "        [0.2134],\n",
      "        [0.1880],\n",
      "        [0.4489],\n",
      "        [0.2197],\n",
      "        [0.1610],\n",
      "        [0.2306],\n",
      "        [0.3281],\n",
      "        [0.4448],\n",
      "        [0.4600],\n",
      "        [0.3581],\n",
      "        [0.3968],\n",
      "        [0.5309],\n",
      "        [0.4051],\n",
      "        [0.3151],\n",
      "        [0.4544],\n",
      "        [0.4791],\n",
      "        [0.1955],\n",
      "        [0.1635],\n",
      "        [0.5116],\n",
      "        [0.2421],\n",
      "        [0.3655],\n",
      "        [0.0987],\n",
      "        [0.3751],\n",
      "        [0.4275],\n",
      "        [0.2648],\n",
      "        [0.5412],\n",
      "        [0.4453],\n",
      "        [0.4924],\n",
      "        [0.4177],\n",
      "        [0.2978],\n",
      "        [0.5057],\n",
      "        [0.4454],\n",
      "        [0.4623],\n",
      "        [0.5451],\n",
      "        [0.3910],\n",
      "        [0.4004],\n",
      "        [0.4162],\n",
      "        [0.4551],\n",
      "        [0.3187],\n",
      "        [0.3447],\n",
      "        [0.4446],\n",
      "        [0.4662],\n",
      "        [0.1727],\n",
      "        [0.1121],\n",
      "        [0.3951],\n",
      "        [0.2666],\n",
      "        [0.1690],\n",
      "        [0.3751],\n",
      "        [0.4531],\n",
      "        [0.5332],\n",
      "        [0.3904],\n",
      "        [0.4652],\n",
      "        [0.4179],\n",
      "        [0.4336],\n",
      "        [0.0377],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458],\n",
      "        [0.4458]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0059],\n",
      "        [    0.0048],\n",
      "        [    0.0002],\n",
      "        [    0.0009],\n",
      "        [    0.0156],\n",
      "        [    0.0042],\n",
      "        [    0.0187],\n",
      "        [    0.0046],\n",
      "        [    0.0322],\n",
      "        [    0.0098],\n",
      "        [    0.0205],\n",
      "        [    0.0138],\n",
      "        [    0.0079],\n",
      "        [    0.0085],\n",
      "        [    0.0059],\n",
      "        [    0.0008],\n",
      "        [    0.0089],\n",
      "        [    0.0327],\n",
      "        [    0.0081],\n",
      "        [    0.0031],\n",
      "        [    0.0026],\n",
      "        [    0.0068],\n",
      "        [    0.0006],\n",
      "        [    0.0326],\n",
      "        [    0.0080],\n",
      "        [    0.0278],\n",
      "        [    0.0076],\n",
      "        [    0.0227],\n",
      "        [    0.0432],\n",
      "        [    0.0098],\n",
      "        [    0.0250],\n",
      "        [    0.0772],\n",
      "        [    0.0020],\n",
      "        [    0.0050],\n",
      "        [    0.0800],\n",
      "        [    0.0096],\n",
      "        [    0.0150],\n",
      "        [    0.0269],\n",
      "        [    0.0353],\n",
      "        [    0.0247],\n",
      "        [    0.0371],\n",
      "        [    0.0326],\n",
      "        [    0.0211],\n",
      "        [    0.0119],\n",
      "        [    0.0194],\n",
      "        [    0.0336],\n",
      "        [    0.0234],\n",
      "        [    0.0194],\n",
      "        [    0.0356],\n",
      "        [    0.0478],\n",
      "        [    0.0084],\n",
      "        [    0.0387],\n",
      "        [    0.0532],\n",
      "        [    0.0309],\n",
      "        [    0.0287],\n",
      "        [    0.0378],\n",
      "        [    0.0321],\n",
      "        [    0.0505],\n",
      "        [    0.0772],\n",
      "        [    0.0435],\n",
      "        [    0.0518],\n",
      "        [    0.0180],\n",
      "        [    0.0594],\n",
      "        [    0.0425],\n",
      "        [    0.0404],\n",
      "        [    0.0548],\n",
      "        [    0.0502],\n",
      "        [    0.0621],\n",
      "        [    0.0543],\n",
      "        [    0.0699],\n",
      "        [    0.0342],\n",
      "        [    0.0881],\n",
      "        [    0.0704],\n",
      "        [    0.0604],\n",
      "        [    0.0810],\n",
      "        [    0.0184],\n",
      "        [    0.0631],\n",
      "        [    0.0931],\n",
      "        [    0.0826],\n",
      "        [    0.0313],\n",
      "        [    0.0936],\n",
      "        [    0.0839],\n",
      "        [    0.0089],\n",
      "        [    0.0224],\n",
      "        [    0.0773],\n",
      "        [    0.0826],\n",
      "        [    0.0190],\n",
      "        [    0.0793],\n",
      "        [    0.0867],\n",
      "        [    0.0716],\n",
      "        [    0.0760],\n",
      "        [    0.0050],\n",
      "        [    0.0892],\n",
      "        [    0.0161],\n",
      "        [    0.0877],\n",
      "        [    0.0041],\n",
      "        [    0.0205],\n",
      "        [    0.1069],\n",
      "        [    0.0140],\n",
      "        [    0.0355],\n",
      "        [    0.1360],\n",
      "        [    0.0038],\n",
      "        [    0.0321],\n",
      "        [    0.0982],\n",
      "        [    0.1295],\n",
      "        [    0.0131],\n",
      "        [    0.1178],\n",
      "        [    0.1225],\n",
      "        [    0.0093],\n",
      "        [    0.0191],\n",
      "        [    0.1465],\n",
      "        [    0.1121],\n",
      "        [    0.0485],\n",
      "        [    0.1223],\n",
      "        [    0.1337],\n",
      "        [    0.1206],\n",
      "        [    0.0167],\n",
      "        [    0.0363],\n",
      "        [    0.1631],\n",
      "        [    0.0409],\n",
      "        [    0.0657],\n",
      "        [    0.0667],\n",
      "        [    0.0810],\n",
      "        [    0.2955],\n",
      "        [    0.3095],\n",
      "        [    0.3125],\n",
      "        [    0.3143],\n",
      "        [    0.3154],\n",
      "        [    0.3248],\n",
      "        [    0.3266],\n",
      "        [    0.3278],\n",
      "        [    0.3352],\n",
      "        [    0.3370],\n",
      "        [    0.3391],\n",
      "        [    0.3401],\n",
      "        [    0.3433],\n",
      "        [    0.3624],\n",
      "        [    0.3634],\n",
      "        [    0.3703],\n",
      "        [    0.3747],\n",
      "        [    0.3767],\n",
      "        [    0.3776],\n",
      "        [    0.3820],\n",
      "        [    0.3895],\n",
      "        [    0.3911],\n",
      "        [    0.3919],\n",
      "        [    0.3919],\n",
      "        [    0.3946]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0085],\n",
      "        [    0.0075],\n",
      "        [    0.0029],\n",
      "        [    0.0051],\n",
      "        [    0.0185],\n",
      "        [    0.0012],\n",
      "        [    0.0232],\n",
      "        [    0.0015],\n",
      "        [    0.0351],\n",
      "        [    0.0144],\n",
      "        [    0.0230],\n",
      "        [    0.0164],\n",
      "        [    0.0124],\n",
      "        [    0.0131],\n",
      "        [    0.0087],\n",
      "        [    0.0019],\n",
      "        [    0.0112],\n",
      "        [    0.0354],\n",
      "        [    0.0120],\n",
      "        [    0.0004],\n",
      "        [    0.0010],\n",
      "        [    0.0109],\n",
      "        [    0.0045],\n",
      "        [    0.0295],\n",
      "        [    0.0123],\n",
      "        [    0.0239],\n",
      "        [    0.0102],\n",
      "        [    0.0187],\n",
      "        [    0.0394],\n",
      "        [    0.0053],\n",
      "        [    0.0291],\n",
      "        [    0.0733],\n",
      "        [    0.0008],\n",
      "        [    0.0005],\n",
      "        [    0.0762],\n",
      "        [    0.0126],\n",
      "        [    0.0105],\n",
      "        [    0.0245],\n",
      "        [    0.0378],\n",
      "        [    0.0274],\n",
      "        [    0.0397],\n",
      "        [    0.0353],\n",
      "        [    0.0222],\n",
      "        [    0.0092],\n",
      "        [    0.0150],\n",
      "        [    0.0304],\n",
      "        [    0.0189],\n",
      "        [    0.0232],\n",
      "        [    0.0312],\n",
      "        [    0.0503],\n",
      "        [    0.0108],\n",
      "        [    0.0427],\n",
      "        [    0.0492],\n",
      "        [    0.0263],\n",
      "        [    0.0258],\n",
      "        [    0.0351],\n",
      "        [    0.0293],\n",
      "        [    0.0463],\n",
      "        [    0.0738],\n",
      "        [    0.0401],\n",
      "        [    0.0558],\n",
      "        [    0.0136],\n",
      "        [    0.0556],\n",
      "        [    0.0465],\n",
      "        [    0.0442],\n",
      "        [    0.0588],\n",
      "        [    0.0474],\n",
      "        [    0.0592],\n",
      "        [    0.0518],\n",
      "        [    0.0687],\n",
      "        [    0.0374],\n",
      "        [    0.0907],\n",
      "        [    0.0677],\n",
      "        [    0.0632],\n",
      "        [    0.0840],\n",
      "        [    0.0147],\n",
      "        [    0.0669],\n",
      "        [    0.0899],\n",
      "        [    0.0792],\n",
      "        [    0.0274],\n",
      "        [    0.0902],\n",
      "        [    0.0868],\n",
      "        [    0.0051],\n",
      "        [    0.0187],\n",
      "        [    0.0744],\n",
      "        [    0.0797],\n",
      "        [    0.0152],\n",
      "        [    0.0764],\n",
      "        [    0.0895],\n",
      "        [    0.0690],\n",
      "        [    0.0786],\n",
      "        [    0.0087],\n",
      "        [    0.0862],\n",
      "        [    0.0123],\n",
      "        [    0.0915],\n",
      "        [    0.0001],\n",
      "        [    0.0242],\n",
      "        [    0.1040],\n",
      "        [    0.0107],\n",
      "        [    0.0392],\n",
      "        [    0.1324],\n",
      "        [    0.0002],\n",
      "        [    0.0358],\n",
      "        [    0.1010],\n",
      "        [    0.1260],\n",
      "        [    0.0169],\n",
      "        [    0.1207],\n",
      "        [    0.1254],\n",
      "        [    0.0131],\n",
      "        [    0.0231],\n",
      "        [    0.1490],\n",
      "        [    0.1094],\n",
      "        [    0.0522],\n",
      "        [    0.1194],\n",
      "        [    0.1362],\n",
      "        [    0.1235],\n",
      "        [    0.0204],\n",
      "        [    0.0398],\n",
      "        [    0.1596],\n",
      "        [    0.0447],\n",
      "        [    0.0694],\n",
      "        [    0.0703],\n",
      "        [    0.0807],\n",
      "        [    0.2950],\n",
      "        [    0.3090],\n",
      "        [    0.3120],\n",
      "        [    0.3139],\n",
      "        [    0.3149],\n",
      "        [    0.3244],\n",
      "        [    0.3262],\n",
      "        [    0.3273],\n",
      "        [    0.3347],\n",
      "        [    0.3365],\n",
      "        [    0.3386],\n",
      "        [    0.3396],\n",
      "        [    0.3429],\n",
      "        [    0.3619],\n",
      "        [    0.3629],\n",
      "        [    0.3698],\n",
      "        [    0.3742],\n",
      "        [    0.3762],\n",
      "        [    0.3771],\n",
      "        [    0.3816],\n",
      "        [    0.3890],\n",
      "        [    0.3906],\n",
      "        [    0.3914],\n",
      "        [    0.3914],\n",
      "        [    0.3942]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 92.66724109649658\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 149\n",
      "剩餘X 資料 torch.Size([11, 18])\n",
      "剩餘Y 資料 torch.Size([11, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15797539055347443, 9)\n",
      "The second_loss value of k: (0.15922331809997559, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.0479])\n",
      "目前模型的Data狀態 torch.Size([149, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1246],\n",
      "        [0.3505],\n",
      "        [0.1057],\n",
      "        [0.6282],\n",
      "        [0.2912],\n",
      "        [0.2634],\n",
      "        [0.6207],\n",
      "        [0.3557],\n",
      "        [0.1827],\n",
      "        [0.6143],\n",
      "        [0.1391],\n",
      "        [0.3414],\n",
      "        [0.6041],\n",
      "        [0.5938],\n",
      "        [0.1239],\n",
      "        [0.1822],\n",
      "        [0.2225],\n",
      "        [0.1566],\n",
      "        [0.6310],\n",
      "        [0.1199],\n",
      "        [0.4394],\n",
      "        [0.5258],\n",
      "        [0.4694],\n",
      "        [0.2970],\n",
      "        [0.6182],\n",
      "        [0.5644],\n",
      "        [0.3532],\n",
      "        [0.6403],\n",
      "        [0.4535],\n",
      "        [0.6331],\n",
      "        [0.6476],\n",
      "        [0.4854],\n",
      "        [0.1374],\n",
      "        [0.5616],\n",
      "        [0.4758],\n",
      "        [0.3519],\n",
      "        [0.6012],\n",
      "        [0.2162],\n",
      "        [0.2693],\n",
      "        [0.3312],\n",
      "        [0.1405],\n",
      "        [0.3234],\n",
      "        [0.1753],\n",
      "        [0.1984],\n",
      "        [0.5708],\n",
      "        [0.3014],\n",
      "        [0.5779],\n",
      "        [0.4487],\n",
      "        [0.5792],\n",
      "        [0.2043],\n",
      "        [0.3803],\n",
      "        [0.4283],\n",
      "        [0.4687],\n",
      "        [0.6336],\n",
      "        [0.1166],\n",
      "        [0.2159],\n",
      "        [0.1224],\n",
      "        [0.5728],\n",
      "        [0.5613],\n",
      "        [0.3654],\n",
      "        [0.6159],\n",
      "        [0.5943],\n",
      "        [0.5148],\n",
      "        [0.4875],\n",
      "        [0.4204],\n",
      "        [0.4284],\n",
      "        [0.2350],\n",
      "        [0.2793],\n",
      "        [0.2109],\n",
      "        [0.1868],\n",
      "        [0.4457],\n",
      "        [0.2171],\n",
      "        [0.1583],\n",
      "        [0.2278],\n",
      "        [0.3251],\n",
      "        [0.4410],\n",
      "        [0.4561],\n",
      "        [0.3548],\n",
      "        [0.3934],\n",
      "        [0.5270],\n",
      "        [0.4016],\n",
      "        [0.3121],\n",
      "        [0.4506],\n",
      "        [0.4754],\n",
      "        [0.1927],\n",
      "        [0.1606],\n",
      "        [0.5077],\n",
      "        [0.2391],\n",
      "        [0.3627],\n",
      "        [0.0960],\n",
      "        [0.3725],\n",
      "        [0.4238],\n",
      "        [0.2618],\n",
      "        [0.5374],\n",
      "        [0.4414],\n",
      "        [0.4883],\n",
      "        [0.4141],\n",
      "        [0.2949],\n",
      "        [0.5023],\n",
      "        [0.4417],\n",
      "        [0.4588],\n",
      "        [0.5415],\n",
      "        [0.3873],\n",
      "        [0.3975],\n",
      "        [0.4127],\n",
      "        [0.4513],\n",
      "        [0.3158],\n",
      "        [0.3418],\n",
      "        [0.4409],\n",
      "        [0.4622],\n",
      "        [0.1703],\n",
      "        [0.1094],\n",
      "        [0.3914],\n",
      "        [0.2637],\n",
      "        [0.1665],\n",
      "        [0.3723],\n",
      "        [0.4494],\n",
      "        [0.5297],\n",
      "        [0.3870],\n",
      "        [0.4614],\n",
      "        [0.4142],\n",
      "        [0.4300],\n",
      "        [0.0380],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454],\n",
      "        [0.4454]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0085],\n",
      "        [    0.0075],\n",
      "        [    0.0029],\n",
      "        [    0.0051],\n",
      "        [    0.0185],\n",
      "        [    0.0012],\n",
      "        [    0.0232],\n",
      "        [    0.0015],\n",
      "        [    0.0351],\n",
      "        [    0.0144],\n",
      "        [    0.0230],\n",
      "        [    0.0164],\n",
      "        [    0.0124],\n",
      "        [    0.0131],\n",
      "        [    0.0087],\n",
      "        [    0.0019],\n",
      "        [    0.0112],\n",
      "        [    0.0354],\n",
      "        [    0.0120],\n",
      "        [    0.0004],\n",
      "        [    0.0010],\n",
      "        [    0.0109],\n",
      "        [    0.0045],\n",
      "        [    0.0295],\n",
      "        [    0.0123],\n",
      "        [    0.0239],\n",
      "        [    0.0102],\n",
      "        [    0.0187],\n",
      "        [    0.0394],\n",
      "        [    0.0053],\n",
      "        [    0.0291],\n",
      "        [    0.0733],\n",
      "        [    0.0008],\n",
      "        [    0.0005],\n",
      "        [    0.0762],\n",
      "        [    0.0126],\n",
      "        [    0.0105],\n",
      "        [    0.0245],\n",
      "        [    0.0378],\n",
      "        [    0.0274],\n",
      "        [    0.0397],\n",
      "        [    0.0353],\n",
      "        [    0.0222],\n",
      "        [    0.0092],\n",
      "        [    0.0150],\n",
      "        [    0.0304],\n",
      "        [    0.0189],\n",
      "        [    0.0232],\n",
      "        [    0.0312],\n",
      "        [    0.0503],\n",
      "        [    0.0108],\n",
      "        [    0.0427],\n",
      "        [    0.0492],\n",
      "        [    0.0263],\n",
      "        [    0.0258],\n",
      "        [    0.0351],\n",
      "        [    0.0293],\n",
      "        [    0.0463],\n",
      "        [    0.0738],\n",
      "        [    0.0401],\n",
      "        [    0.0558],\n",
      "        [    0.0136],\n",
      "        [    0.0556],\n",
      "        [    0.0465],\n",
      "        [    0.0442],\n",
      "        [    0.0588],\n",
      "        [    0.0474],\n",
      "        [    0.0592],\n",
      "        [    0.0518],\n",
      "        [    0.0687],\n",
      "        [    0.0374],\n",
      "        [    0.0907],\n",
      "        [    0.0677],\n",
      "        [    0.0632],\n",
      "        [    0.0840],\n",
      "        [    0.0147],\n",
      "        [    0.0669],\n",
      "        [    0.0899],\n",
      "        [    0.0792],\n",
      "        [    0.0274],\n",
      "        [    0.0902],\n",
      "        [    0.0868],\n",
      "        [    0.0051],\n",
      "        [    0.0187],\n",
      "        [    0.0744],\n",
      "        [    0.0797],\n",
      "        [    0.0152],\n",
      "        [    0.0764],\n",
      "        [    0.0895],\n",
      "        [    0.0690],\n",
      "        [    0.0786],\n",
      "        [    0.0087],\n",
      "        [    0.0862],\n",
      "        [    0.0123],\n",
      "        [    0.0915],\n",
      "        [    0.0001],\n",
      "        [    0.0242],\n",
      "        [    0.1040],\n",
      "        [    0.0107],\n",
      "        [    0.0392],\n",
      "        [    0.1324],\n",
      "        [    0.0002],\n",
      "        [    0.0358],\n",
      "        [    0.1010],\n",
      "        [    0.1260],\n",
      "        [    0.0169],\n",
      "        [    0.1207],\n",
      "        [    0.1254],\n",
      "        [    0.0131],\n",
      "        [    0.0231],\n",
      "        [    0.1490],\n",
      "        [    0.1094],\n",
      "        [    0.0522],\n",
      "        [    0.1194],\n",
      "        [    0.1362],\n",
      "        [    0.1235],\n",
      "        [    0.0204],\n",
      "        [    0.0398],\n",
      "        [    0.1596],\n",
      "        [    0.0447],\n",
      "        [    0.0694],\n",
      "        [    0.0703],\n",
      "        [    0.0807],\n",
      "        [    0.2950],\n",
      "        [    0.3090],\n",
      "        [    0.3120],\n",
      "        [    0.3139],\n",
      "        [    0.3149],\n",
      "        [    0.3244],\n",
      "        [    0.3262],\n",
      "        [    0.3273],\n",
      "        [    0.3347],\n",
      "        [    0.3365],\n",
      "        [    0.3386],\n",
      "        [    0.3396],\n",
      "        [    0.3429],\n",
      "        [    0.3619],\n",
      "        [    0.3629],\n",
      "        [    0.3698],\n",
      "        [    0.3742],\n",
      "        [    0.3762],\n",
      "        [    0.3771],\n",
      "        [    0.3816],\n",
      "        [    0.3890],\n",
      "        [    0.3906],\n",
      "        [    0.3914],\n",
      "        [    0.3914],\n",
      "        [    0.3942],\n",
      "        [    0.3975]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0065],\n",
      "        [    0.0051],\n",
      "        [    0.0010],\n",
      "        [    0.0018],\n",
      "        [    0.0163],\n",
      "        [    0.0034],\n",
      "        [    0.0197],\n",
      "        [    0.0037],\n",
      "        [    0.0334],\n",
      "        [    0.0111],\n",
      "        [    0.0212],\n",
      "        [    0.0139],\n",
      "        [    0.0090],\n",
      "        [    0.0099],\n",
      "        [    0.0067],\n",
      "        [    0.0002],\n",
      "        [    0.0090],\n",
      "        [    0.0338],\n",
      "        [    0.0083],\n",
      "        [    0.0023],\n",
      "        [    0.0019],\n",
      "        [    0.0077],\n",
      "        [    0.0016],\n",
      "        [    0.0322],\n",
      "        [    0.0089],\n",
      "        [    0.0272],\n",
      "        [    0.0077],\n",
      "        [    0.0223],\n",
      "        [    0.0417],\n",
      "        [    0.0088],\n",
      "        [    0.0254],\n",
      "        [    0.0762],\n",
      "        [    0.0011],\n",
      "        [    0.0037],\n",
      "        [    0.0788],\n",
      "        [    0.0103],\n",
      "        [    0.0139],\n",
      "        [    0.0267],\n",
      "        [    0.0355],\n",
      "        [    0.0249],\n",
      "        [    0.0379],\n",
      "        [    0.0330],\n",
      "        [    0.0223],\n",
      "        [    0.0111],\n",
      "        [    0.0182],\n",
      "        [    0.0327],\n",
      "        [    0.0221],\n",
      "        [    0.0204],\n",
      "        [    0.0345],\n",
      "        [    0.0483],\n",
      "        [    0.0082],\n",
      "        [    0.0398],\n",
      "        [    0.0522],\n",
      "        [    0.0297],\n",
      "        [    0.0277],\n",
      "        [    0.0373],\n",
      "        [    0.0312],\n",
      "        [    0.0497],\n",
      "        [    0.0770],\n",
      "        [    0.0425],\n",
      "        [    0.0521],\n",
      "        [    0.0169],\n",
      "        [    0.0587],\n",
      "        [    0.0434],\n",
      "        [    0.0414],\n",
      "        [    0.0560],\n",
      "        [    0.0496],\n",
      "        [    0.0615],\n",
      "        [    0.0537],\n",
      "        [    0.0685],\n",
      "        [    0.0349],\n",
      "        [    0.0885],\n",
      "        [    0.0698],\n",
      "        [    0.0606],\n",
      "        [    0.0816],\n",
      "        [    0.0175],\n",
      "        [    0.0639],\n",
      "        [    0.0925],\n",
      "        [    0.0815],\n",
      "        [    0.0306],\n",
      "        [    0.0926],\n",
      "        [    0.0846],\n",
      "        [    0.0081],\n",
      "        [    0.0217],\n",
      "        [    0.0764],\n",
      "        [    0.0817],\n",
      "        [    0.0183],\n",
      "        [    0.0785],\n",
      "        [    0.0870],\n",
      "        [    0.0707],\n",
      "        [    0.0761],\n",
      "        [    0.0058],\n",
      "        [    0.0884],\n",
      "        [    0.0156],\n",
      "        [    0.0885],\n",
      "        [    0.0030],\n",
      "        [    0.0213],\n",
      "        [    0.1063],\n",
      "        [    0.0139],\n",
      "        [    0.0364],\n",
      "        [    0.1354],\n",
      "        [    0.0034],\n",
      "        [    0.0330],\n",
      "        [    0.0985],\n",
      "        [    0.1287],\n",
      "        [    0.0139],\n",
      "        [    0.1184],\n",
      "        [    0.1229],\n",
      "        [    0.0100],\n",
      "        [    0.0201],\n",
      "        [    0.1467],\n",
      "        [    0.1111],\n",
      "        [    0.0493],\n",
      "        [    0.1216],\n",
      "        [    0.1338],\n",
      "        [    0.1209],\n",
      "        [    0.0173],\n",
      "        [    0.0368],\n",
      "        [    0.1623],\n",
      "        [    0.0415],\n",
      "        [    0.0663],\n",
      "        [    0.0672],\n",
      "        [    0.0769],\n",
      "        [    0.2945],\n",
      "        [    0.3086],\n",
      "        [    0.3115],\n",
      "        [    0.3134],\n",
      "        [    0.3144],\n",
      "        [    0.3239],\n",
      "        [    0.3257],\n",
      "        [    0.3269],\n",
      "        [    0.3342],\n",
      "        [    0.3361],\n",
      "        [    0.3381],\n",
      "        [    0.3392],\n",
      "        [    0.3424],\n",
      "        [    0.3615],\n",
      "        [    0.3625],\n",
      "        [    0.3694],\n",
      "        [    0.3738],\n",
      "        [    0.3758],\n",
      "        [    0.3766],\n",
      "        [    0.3811],\n",
      "        [    0.3885],\n",
      "        [    0.3902],\n",
      "        [    0.3909],\n",
      "        [    0.3910],\n",
      "        [    0.3937],\n",
      "        [    0.3970]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 92.90453743934631\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 150\n",
      "剩餘X 資料 torch.Size([10, 18])\n",
      "剩餘Y 資料 torch.Size([10, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15885445475578308, 9)\n",
      "The second_loss value of k: (0.16023123264312744, 8)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引9，y= tensor([0.0463])\n",
      "目前模型的Data狀態 torch.Size([150, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1266],\n",
      "        [0.3529],\n",
      "        [0.1076],\n",
      "        [0.6315],\n",
      "        [0.2935],\n",
      "        [0.2656],\n",
      "        [0.6242],\n",
      "        [0.3580],\n",
      "        [0.1844],\n",
      "        [0.6175],\n",
      "        [0.1409],\n",
      "        [0.3439],\n",
      "        [0.6076],\n",
      "        [0.5971],\n",
      "        [0.1259],\n",
      "        [0.1843],\n",
      "        [0.2246],\n",
      "        [0.1583],\n",
      "        [0.6347],\n",
      "        [0.1219],\n",
      "        [0.4423],\n",
      "        [0.5289],\n",
      "        [0.4723],\n",
      "        [0.2996],\n",
      "        [0.6217],\n",
      "        [0.5678],\n",
      "        [0.3557],\n",
      "        [0.6439],\n",
      "        [0.4558],\n",
      "        [0.6365],\n",
      "        [0.6514],\n",
      "        [0.4883],\n",
      "        [0.1394],\n",
      "        [0.5648],\n",
      "        [0.4784],\n",
      "        [0.3542],\n",
      "        [0.6046],\n",
      "        [0.2184],\n",
      "        [0.2716],\n",
      "        [0.3336],\n",
      "        [0.1423],\n",
      "        [0.3257],\n",
      "        [0.1752],\n",
      "        [0.2002],\n",
      "        [0.5741],\n",
      "        [0.3037],\n",
      "        [0.5811],\n",
      "        [0.4515],\n",
      "        [0.5825],\n",
      "        [0.2062],\n",
      "        [0.3829],\n",
      "        [0.4311],\n",
      "        [0.4717],\n",
      "        [0.6370],\n",
      "        [0.1185],\n",
      "        [0.2181],\n",
      "        [0.1243],\n",
      "        [0.5761],\n",
      "        [0.5645],\n",
      "        [0.3678],\n",
      "        [0.6196],\n",
      "        [0.5976],\n",
      "        [0.5179],\n",
      "        [0.4906],\n",
      "        [0.4231],\n",
      "        [0.4312],\n",
      "        [0.2372],\n",
      "        [0.2816],\n",
      "        [0.2128],\n",
      "        [0.1866],\n",
      "        [0.4482],\n",
      "        [0.2193],\n",
      "        [0.1604],\n",
      "        [0.2303],\n",
      "        [0.3275],\n",
      "        [0.4438],\n",
      "        [0.4591],\n",
      "        [0.3575],\n",
      "        [0.3957],\n",
      "        [0.5301],\n",
      "        [0.4041],\n",
      "        [0.3144],\n",
      "        [0.4536],\n",
      "        [0.4784],\n",
      "        [0.1947],\n",
      "        [0.1626],\n",
      "        [0.5109],\n",
      "        [0.2412],\n",
      "        [0.3652],\n",
      "        [0.0977],\n",
      "        [0.3750],\n",
      "        [0.4268],\n",
      "        [0.2640],\n",
      "        [0.5406],\n",
      "        [0.4444],\n",
      "        [0.4913],\n",
      "        [0.4170],\n",
      "        [0.2972],\n",
      "        [0.5056],\n",
      "        [0.4444],\n",
      "        [0.4617],\n",
      "        [0.5447],\n",
      "        [0.3902],\n",
      "        [0.4000],\n",
      "        [0.4155],\n",
      "        [0.4543],\n",
      "        [0.3181],\n",
      "        [0.3442],\n",
      "        [0.4440],\n",
      "        [0.4652],\n",
      "        [0.1725],\n",
      "        [0.1111],\n",
      "        [0.3943],\n",
      "        [0.2659],\n",
      "        [0.1689],\n",
      "        [0.3748],\n",
      "        [0.4526],\n",
      "        [0.5328],\n",
      "        [0.3897],\n",
      "        [0.4646],\n",
      "        [0.4172],\n",
      "        [0.4332],\n",
      "        [0.0418],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449],\n",
      "        [0.4449]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0065],\n",
      "        [    0.0051],\n",
      "        [    0.0010],\n",
      "        [    0.0018],\n",
      "        [    0.0163],\n",
      "        [    0.0034],\n",
      "        [    0.0197],\n",
      "        [    0.0037],\n",
      "        [    0.0334],\n",
      "        [    0.0111],\n",
      "        [    0.0212],\n",
      "        [    0.0139],\n",
      "        [    0.0090],\n",
      "        [    0.0099],\n",
      "        [    0.0067],\n",
      "        [    0.0002],\n",
      "        [    0.0090],\n",
      "        [    0.0338],\n",
      "        [    0.0083],\n",
      "        [    0.0023],\n",
      "        [    0.0019],\n",
      "        [    0.0077],\n",
      "        [    0.0016],\n",
      "        [    0.0322],\n",
      "        [    0.0089],\n",
      "        [    0.0272],\n",
      "        [    0.0077],\n",
      "        [    0.0223],\n",
      "        [    0.0417],\n",
      "        [    0.0088],\n",
      "        [    0.0254],\n",
      "        [    0.0762],\n",
      "        [    0.0011],\n",
      "        [    0.0037],\n",
      "        [    0.0788],\n",
      "        [    0.0103],\n",
      "        [    0.0139],\n",
      "        [    0.0267],\n",
      "        [    0.0355],\n",
      "        [    0.0249],\n",
      "        [    0.0379],\n",
      "        [    0.0330],\n",
      "        [    0.0223],\n",
      "        [    0.0111],\n",
      "        [    0.0182],\n",
      "        [    0.0327],\n",
      "        [    0.0221],\n",
      "        [    0.0204],\n",
      "        [    0.0345],\n",
      "        [    0.0483],\n",
      "        [    0.0082],\n",
      "        [    0.0398],\n",
      "        [    0.0522],\n",
      "        [    0.0297],\n",
      "        [    0.0277],\n",
      "        [    0.0373],\n",
      "        [    0.0312],\n",
      "        [    0.0497],\n",
      "        [    0.0770],\n",
      "        [    0.0425],\n",
      "        [    0.0521],\n",
      "        [    0.0169],\n",
      "        [    0.0587],\n",
      "        [    0.0434],\n",
      "        [    0.0414],\n",
      "        [    0.0560],\n",
      "        [    0.0496],\n",
      "        [    0.0615],\n",
      "        [    0.0537],\n",
      "        [    0.0685],\n",
      "        [    0.0349],\n",
      "        [    0.0885],\n",
      "        [    0.0698],\n",
      "        [    0.0606],\n",
      "        [    0.0816],\n",
      "        [    0.0175],\n",
      "        [    0.0639],\n",
      "        [    0.0925],\n",
      "        [    0.0815],\n",
      "        [    0.0306],\n",
      "        [    0.0926],\n",
      "        [    0.0846],\n",
      "        [    0.0081],\n",
      "        [    0.0217],\n",
      "        [    0.0764],\n",
      "        [    0.0817],\n",
      "        [    0.0183],\n",
      "        [    0.0785],\n",
      "        [    0.0870],\n",
      "        [    0.0707],\n",
      "        [    0.0761],\n",
      "        [    0.0058],\n",
      "        [    0.0884],\n",
      "        [    0.0156],\n",
      "        [    0.0885],\n",
      "        [    0.0030],\n",
      "        [    0.0213],\n",
      "        [    0.1063],\n",
      "        [    0.0139],\n",
      "        [    0.0364],\n",
      "        [    0.1354],\n",
      "        [    0.0034],\n",
      "        [    0.0330],\n",
      "        [    0.0985],\n",
      "        [    0.1287],\n",
      "        [    0.0139],\n",
      "        [    0.1184],\n",
      "        [    0.1229],\n",
      "        [    0.0100],\n",
      "        [    0.0201],\n",
      "        [    0.1467],\n",
      "        [    0.1111],\n",
      "        [    0.0493],\n",
      "        [    0.1216],\n",
      "        [    0.1338],\n",
      "        [    0.1209],\n",
      "        [    0.0173],\n",
      "        [    0.0368],\n",
      "        [    0.1623],\n",
      "        [    0.0415],\n",
      "        [    0.0663],\n",
      "        [    0.0672],\n",
      "        [    0.0769],\n",
      "        [    0.2945],\n",
      "        [    0.3086],\n",
      "        [    0.3115],\n",
      "        [    0.3134],\n",
      "        [    0.3144],\n",
      "        [    0.3239],\n",
      "        [    0.3257],\n",
      "        [    0.3269],\n",
      "        [    0.3342],\n",
      "        [    0.3361],\n",
      "        [    0.3381],\n",
      "        [    0.3392],\n",
      "        [    0.3424],\n",
      "        [    0.3615],\n",
      "        [    0.3625],\n",
      "        [    0.3694],\n",
      "        [    0.3738],\n",
      "        [    0.3758],\n",
      "        [    0.3766],\n",
      "        [    0.3811],\n",
      "        [    0.3885],\n",
      "        [    0.3902],\n",
      "        [    0.3909],\n",
      "        [    0.3910],\n",
      "        [    0.3937],\n",
      "        [    0.3970],\n",
      "        [    0.3986]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0067],\n",
      "        [    0.0051],\n",
      "        [    0.0013],\n",
      "        [    0.0021],\n",
      "        [    0.0165],\n",
      "        [    0.0030],\n",
      "        [    0.0201],\n",
      "        [    0.0032],\n",
      "        [    0.0339],\n",
      "        [    0.0117],\n",
      "        [    0.0214],\n",
      "        [    0.0139],\n",
      "        [    0.0095],\n",
      "        [    0.0105],\n",
      "        [    0.0070],\n",
      "        [    0.0000],\n",
      "        [    0.0090],\n",
      "        [    0.0342],\n",
      "        [    0.0082],\n",
      "        [    0.0021],\n",
      "        [    0.0016],\n",
      "        [    0.0080],\n",
      "        [    0.0021],\n",
      "        [    0.0320],\n",
      "        [    0.0091],\n",
      "        [    0.0269],\n",
      "        [    0.0077],\n",
      "        [    0.0223],\n",
      "        [    0.0410],\n",
      "        [    0.0083],\n",
      "        [    0.0254],\n",
      "        [    0.0757],\n",
      "        [    0.0008],\n",
      "        [    0.0031],\n",
      "        [    0.0782],\n",
      "        [    0.0107],\n",
      "        [    0.0134],\n",
      "        [    0.0267],\n",
      "        [    0.0355],\n",
      "        [    0.0250],\n",
      "        [    0.0382],\n",
      "        [    0.0331],\n",
      "        [    0.0230],\n",
      "        [    0.0108],\n",
      "        [    0.0177],\n",
      "        [    0.0323],\n",
      "        [    0.0215],\n",
      "        [    0.0209],\n",
      "        [    0.0340],\n",
      "        [    0.0484],\n",
      "        [    0.0081],\n",
      "        [    0.0403],\n",
      "        [    0.0516],\n",
      "        [    0.0292],\n",
      "        [    0.0274],\n",
      "        [    0.0371],\n",
      "        [    0.0308],\n",
      "        [    0.0493],\n",
      "        [    0.0770],\n",
      "        [    0.0421],\n",
      "        [    0.0522],\n",
      "        [    0.0165],\n",
      "        [    0.0583],\n",
      "        [    0.0438],\n",
      "        [    0.0419],\n",
      "        [    0.0566],\n",
      "        [    0.0493],\n",
      "        [    0.0612],\n",
      "        [    0.0536],\n",
      "        [    0.0677],\n",
      "        [    0.0352],\n",
      "        [    0.0886],\n",
      "        [    0.0696],\n",
      "        [    0.0607],\n",
      "        [    0.0818],\n",
      "        [    0.0170],\n",
      "        [    0.0642],\n",
      "        [    0.0923],\n",
      "        [    0.0809],\n",
      "        [    0.0302],\n",
      "        [    0.0921],\n",
      "        [    0.0849],\n",
      "        [    0.0077],\n",
      "        [    0.0214],\n",
      "        [    0.0761],\n",
      "        [    0.0814],\n",
      "        [    0.0180],\n",
      "        [    0.0781],\n",
      "        [    0.0872],\n",
      "        [    0.0703],\n",
      "        [    0.0761],\n",
      "        [    0.0062],\n",
      "        [    0.0881],\n",
      "        [    0.0153],\n",
      "        [    0.0888],\n",
      "        [    0.0025],\n",
      "        [    0.0217],\n",
      "        [    0.1061],\n",
      "        [    0.0139],\n",
      "        [    0.0369],\n",
      "        [    0.1351],\n",
      "        [    0.0033],\n",
      "        [    0.0334],\n",
      "        [    0.0986],\n",
      "        [    0.1284],\n",
      "        [    0.0142],\n",
      "        [    0.1187],\n",
      "        [    0.1230],\n",
      "        [    0.0103],\n",
      "        [    0.0206],\n",
      "        [    0.1467],\n",
      "        [    0.1107],\n",
      "        [    0.0497],\n",
      "        [    0.1214],\n",
      "        [    0.1337],\n",
      "        [    0.1210],\n",
      "        [    0.0175],\n",
      "        [    0.0370],\n",
      "        [    0.1620],\n",
      "        [    0.0418],\n",
      "        [    0.0666],\n",
      "        [    0.0673],\n",
      "        [    0.0749],\n",
      "        [    0.2940],\n",
      "        [    0.3081],\n",
      "        [    0.3110],\n",
      "        [    0.3129],\n",
      "        [    0.3139],\n",
      "        [    0.3234],\n",
      "        [    0.3252],\n",
      "        [    0.3264],\n",
      "        [    0.3338],\n",
      "        [    0.3356],\n",
      "        [    0.3376],\n",
      "        [    0.3387],\n",
      "        [    0.3419],\n",
      "        [    0.3610],\n",
      "        [    0.3620],\n",
      "        [    0.3689],\n",
      "        [    0.3733],\n",
      "        [    0.3753],\n",
      "        [    0.3761],\n",
      "        [    0.3806],\n",
      "        [    0.3881],\n",
      "        [    0.3897],\n",
      "        [    0.3904],\n",
      "        [    0.3905],\n",
      "        [    0.3932],\n",
      "        [    0.3965],\n",
      "        [    0.3981]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 93.14119148254395\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 151\n",
      "剩餘X 資料 torch.Size([9, 18])\n",
      "剩餘Y 資料 torch.Size([9, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15984228253364563, 8)\n",
      "The second_loss value of k: (0.16034379601478577, 7)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引8，y= tensor([0.0446])\n",
      "目前模型的Data狀態 torch.Size([151, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1264],\n",
      "        [0.3529],\n",
      "        [0.1074],\n",
      "        [0.6312],\n",
      "        [0.2932],\n",
      "        [0.2652],\n",
      "        [0.6238],\n",
      "        [0.3575],\n",
      "        [0.1839],\n",
      "        [0.6170],\n",
      "        [0.1407],\n",
      "        [0.3439],\n",
      "        [0.6071],\n",
      "        [0.5965],\n",
      "        [0.1256],\n",
      "        [0.1841],\n",
      "        [0.2246],\n",
      "        [0.1579],\n",
      "        [0.6348],\n",
      "        [0.1216],\n",
      "        [0.4420],\n",
      "        [0.5286],\n",
      "        [0.4719],\n",
      "        [0.2994],\n",
      "        [0.6214],\n",
      "        [0.5675],\n",
      "        [0.3557],\n",
      "        [0.6439],\n",
      "        [0.4551],\n",
      "        [0.6361],\n",
      "        [0.6513],\n",
      "        [0.4878],\n",
      "        [0.1390],\n",
      "        [0.5642],\n",
      "        [0.4779],\n",
      "        [0.3538],\n",
      "        [0.6041],\n",
      "        [0.2184],\n",
      "        [0.2716],\n",
      "        [0.3336],\n",
      "        [0.1420],\n",
      "        [0.3256],\n",
      "        [0.1745],\n",
      "        [0.2000],\n",
      "        [0.5735],\n",
      "        [0.3033],\n",
      "        [0.5805],\n",
      "        [0.4510],\n",
      "        [0.5820],\n",
      "        [0.2061],\n",
      "        [0.3830],\n",
      "        [0.4306],\n",
      "        [0.4712],\n",
      "        [0.6365],\n",
      "        [0.1181],\n",
      "        [0.2179],\n",
      "        [0.1239],\n",
      "        [0.5757],\n",
      "        [0.5645],\n",
      "        [0.3673],\n",
      "        [0.6195],\n",
      "        [0.5971],\n",
      "        [0.5175],\n",
      "        [0.4902],\n",
      "        [0.4227],\n",
      "        [0.4307],\n",
      "        [0.2369],\n",
      "        [0.2814],\n",
      "        [0.2127],\n",
      "        [0.1858],\n",
      "        [0.4479],\n",
      "        [0.2192],\n",
      "        [0.1602],\n",
      "        [0.2302],\n",
      "        [0.3273],\n",
      "        [0.4434],\n",
      "        [0.4588],\n",
      "        [0.3573],\n",
      "        [0.3951],\n",
      "        [0.5297],\n",
      "        [0.4036],\n",
      "        [0.3141],\n",
      "        [0.4532],\n",
      "        [0.4781],\n",
      "        [0.1943],\n",
      "        [0.1623],\n",
      "        [0.5105],\n",
      "        [0.2409],\n",
      "        [0.3650],\n",
      "        [0.0974],\n",
      "        [0.3749],\n",
      "        [0.4264],\n",
      "        [0.2637],\n",
      "        [0.5404],\n",
      "        [0.4441],\n",
      "        [0.4907],\n",
      "        [0.4166],\n",
      "        [0.2970],\n",
      "        [0.5056],\n",
      "        [0.4440],\n",
      "        [0.4615],\n",
      "        [0.5446],\n",
      "        [0.3897],\n",
      "        [0.3999],\n",
      "        [0.4152],\n",
      "        [0.4540],\n",
      "        [0.3178],\n",
      "        [0.3441],\n",
      "        [0.4436],\n",
      "        [0.4647],\n",
      "        [0.1725],\n",
      "        [0.1107],\n",
      "        [0.3939],\n",
      "        [0.2657],\n",
      "        [0.1689],\n",
      "        [0.3748],\n",
      "        [0.4523],\n",
      "        [0.5326],\n",
      "        [0.3893],\n",
      "        [0.4643],\n",
      "        [0.4169],\n",
      "        [0.4330],\n",
      "        [0.0438],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444],\n",
      "        [0.4444]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0067],\n",
      "        [    0.0051],\n",
      "        [    0.0013],\n",
      "        [    0.0021],\n",
      "        [    0.0165],\n",
      "        [    0.0030],\n",
      "        [    0.0201],\n",
      "        [    0.0032],\n",
      "        [    0.0339],\n",
      "        [    0.0117],\n",
      "        [    0.0214],\n",
      "        [    0.0139],\n",
      "        [    0.0095],\n",
      "        [    0.0105],\n",
      "        [    0.0070],\n",
      "        [    0.0000],\n",
      "        [    0.0090],\n",
      "        [    0.0342],\n",
      "        [    0.0082],\n",
      "        [    0.0021],\n",
      "        [    0.0016],\n",
      "        [    0.0080],\n",
      "        [    0.0021],\n",
      "        [    0.0320],\n",
      "        [    0.0091],\n",
      "        [    0.0269],\n",
      "        [    0.0077],\n",
      "        [    0.0223],\n",
      "        [    0.0410],\n",
      "        [    0.0083],\n",
      "        [    0.0254],\n",
      "        [    0.0757],\n",
      "        [    0.0008],\n",
      "        [    0.0031],\n",
      "        [    0.0782],\n",
      "        [    0.0107],\n",
      "        [    0.0134],\n",
      "        [    0.0267],\n",
      "        [    0.0355],\n",
      "        [    0.0250],\n",
      "        [    0.0382],\n",
      "        [    0.0331],\n",
      "        [    0.0230],\n",
      "        [    0.0108],\n",
      "        [    0.0177],\n",
      "        [    0.0323],\n",
      "        [    0.0215],\n",
      "        [    0.0209],\n",
      "        [    0.0340],\n",
      "        [    0.0484],\n",
      "        [    0.0081],\n",
      "        [    0.0403],\n",
      "        [    0.0516],\n",
      "        [    0.0292],\n",
      "        [    0.0274],\n",
      "        [    0.0371],\n",
      "        [    0.0308],\n",
      "        [    0.0493],\n",
      "        [    0.0770],\n",
      "        [    0.0421],\n",
      "        [    0.0522],\n",
      "        [    0.0165],\n",
      "        [    0.0583],\n",
      "        [    0.0438],\n",
      "        [    0.0419],\n",
      "        [    0.0566],\n",
      "        [    0.0493],\n",
      "        [    0.0612],\n",
      "        [    0.0536],\n",
      "        [    0.0677],\n",
      "        [    0.0352],\n",
      "        [    0.0886],\n",
      "        [    0.0696],\n",
      "        [    0.0607],\n",
      "        [    0.0818],\n",
      "        [    0.0170],\n",
      "        [    0.0642],\n",
      "        [    0.0923],\n",
      "        [    0.0809],\n",
      "        [    0.0302],\n",
      "        [    0.0921],\n",
      "        [    0.0849],\n",
      "        [    0.0077],\n",
      "        [    0.0214],\n",
      "        [    0.0761],\n",
      "        [    0.0814],\n",
      "        [    0.0180],\n",
      "        [    0.0781],\n",
      "        [    0.0872],\n",
      "        [    0.0703],\n",
      "        [    0.0761],\n",
      "        [    0.0062],\n",
      "        [    0.0881],\n",
      "        [    0.0153],\n",
      "        [    0.0888],\n",
      "        [    0.0025],\n",
      "        [    0.0217],\n",
      "        [    0.1061],\n",
      "        [    0.0139],\n",
      "        [    0.0369],\n",
      "        [    0.1351],\n",
      "        [    0.0033],\n",
      "        [    0.0334],\n",
      "        [    0.0986],\n",
      "        [    0.1284],\n",
      "        [    0.0142],\n",
      "        [    0.1187],\n",
      "        [    0.1230],\n",
      "        [    0.0103],\n",
      "        [    0.0206],\n",
      "        [    0.1467],\n",
      "        [    0.1107],\n",
      "        [    0.0497],\n",
      "        [    0.1214],\n",
      "        [    0.1337],\n",
      "        [    0.1210],\n",
      "        [    0.0175],\n",
      "        [    0.0370],\n",
      "        [    0.1620],\n",
      "        [    0.0418],\n",
      "        [    0.0666],\n",
      "        [    0.0673],\n",
      "        [    0.0749],\n",
      "        [    0.2940],\n",
      "        [    0.3081],\n",
      "        [    0.3110],\n",
      "        [    0.3129],\n",
      "        [    0.3139],\n",
      "        [    0.3234],\n",
      "        [    0.3252],\n",
      "        [    0.3264],\n",
      "        [    0.3338],\n",
      "        [    0.3356],\n",
      "        [    0.3376],\n",
      "        [    0.3387],\n",
      "        [    0.3419],\n",
      "        [    0.3610],\n",
      "        [    0.3620],\n",
      "        [    0.3689],\n",
      "        [    0.3733],\n",
      "        [    0.3753],\n",
      "        [    0.3761],\n",
      "        [    0.3806],\n",
      "        [    0.3881],\n",
      "        [    0.3897],\n",
      "        [    0.3904],\n",
      "        [    0.3905],\n",
      "        [    0.3932],\n",
      "        [    0.3965],\n",
      "        [    0.3981],\n",
      "        [    0.3998]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0065],\n",
      "        [    0.0047],\n",
      "        [    0.0011],\n",
      "        [    0.0018],\n",
      "        [    0.0162],\n",
      "        [    0.0031],\n",
      "        [    0.0198],\n",
      "        [    0.0033],\n",
      "        [    0.0340],\n",
      "        [    0.0115],\n",
      "        [    0.0212],\n",
      "        [    0.0134],\n",
      "        [    0.0092],\n",
      "        [    0.0103],\n",
      "        [    0.0069],\n",
      "        [    0.0003],\n",
      "        [    0.0086],\n",
      "        [    0.0343],\n",
      "        [    0.0075],\n",
      "        [    0.0022],\n",
      "        [    0.0020],\n",
      "        [    0.0077],\n",
      "        [    0.0019],\n",
      "        [    0.0324],\n",
      "        [    0.0087],\n",
      "        [    0.0274],\n",
      "        [    0.0072],\n",
      "        [    0.0229],\n",
      "        [    0.0409],\n",
      "        [    0.0086],\n",
      "        [    0.0247],\n",
      "        [    0.0759],\n",
      "        [    0.0009],\n",
      "        [    0.0033],\n",
      "        [    0.0783],\n",
      "        [    0.0105],\n",
      "        [    0.0136],\n",
      "        [    0.0272],\n",
      "        [    0.0351],\n",
      "        [    0.0246],\n",
      "        [    0.0381],\n",
      "        [    0.0328],\n",
      "        [    0.0236],\n",
      "        [    0.0110],\n",
      "        [    0.0179],\n",
      "        [    0.0325],\n",
      "        [    0.0217],\n",
      "        [    0.0207],\n",
      "        [    0.0343],\n",
      "        [    0.0482],\n",
      "        [    0.0074],\n",
      "        [    0.0401],\n",
      "        [    0.0518],\n",
      "        [    0.0295],\n",
      "        [    0.0274],\n",
      "        [    0.0374],\n",
      "        [    0.0309],\n",
      "        [    0.0497],\n",
      "        [    0.0776],\n",
      "        [    0.0422],\n",
      "        [    0.0515],\n",
      "        [    0.0168],\n",
      "        [    0.0587],\n",
      "        [    0.0434],\n",
      "        [    0.0417],\n",
      "        [    0.0564],\n",
      "        [    0.0495],\n",
      "        [    0.0615],\n",
      "        [    0.0538],\n",
      "        [    0.0671],\n",
      "        [    0.0350],\n",
      "        [    0.0883],\n",
      "        [    0.0698],\n",
      "        [    0.0603],\n",
      "        [    0.0815],\n",
      "        [    0.0173],\n",
      "        [    0.0639],\n",
      "        [    0.0927],\n",
      "        [    0.0809],\n",
      "        [    0.0306],\n",
      "        [    0.0923],\n",
      "        [    0.0847],\n",
      "        [    0.0080],\n",
      "        [    0.0217],\n",
      "        [    0.0762],\n",
      "        [    0.0815],\n",
      "        [    0.0183],\n",
      "        [    0.0783],\n",
      "        [    0.0868],\n",
      "        [    0.0703],\n",
      "        [    0.0756],\n",
      "        [    0.0058],\n",
      "        [    0.0883],\n",
      "        [    0.0158],\n",
      "        [    0.0885],\n",
      "        [    0.0027],\n",
      "        [    0.0214],\n",
      "        [    0.1064],\n",
      "        [    0.0146],\n",
      "        [    0.0367],\n",
      "        [    0.1355],\n",
      "        [    0.0038],\n",
      "        [    0.0332],\n",
      "        [    0.0982],\n",
      "        [    0.1287],\n",
      "        [    0.0139],\n",
      "        [    0.1184],\n",
      "        [    0.1227],\n",
      "        [    0.0100],\n",
      "        [    0.0204],\n",
      "        [    0.1463],\n",
      "        [    0.1106],\n",
      "        [    0.0495],\n",
      "        [    0.1216],\n",
      "        [    0.1332],\n",
      "        [    0.1205],\n",
      "        [    0.0171],\n",
      "        [    0.0365],\n",
      "        [    0.1623],\n",
      "        [    0.0414],\n",
      "        [    0.0662],\n",
      "        [    0.0669],\n",
      "        [    0.0727],\n",
      "        [    0.2936],\n",
      "        [    0.3076],\n",
      "        [    0.3106],\n",
      "        [    0.3125],\n",
      "        [    0.3135],\n",
      "        [    0.3230],\n",
      "        [    0.3248],\n",
      "        [    0.3259],\n",
      "        [    0.3333],\n",
      "        [    0.3351],\n",
      "        [    0.3372],\n",
      "        [    0.3382],\n",
      "        [    0.3414],\n",
      "        [    0.3605],\n",
      "        [    0.3615],\n",
      "        [    0.3684],\n",
      "        [    0.3728],\n",
      "        [    0.3748],\n",
      "        [    0.3757],\n",
      "        [    0.3801],\n",
      "        [    0.3876],\n",
      "        [    0.3892],\n",
      "        [    0.3900],\n",
      "        [    0.3900],\n",
      "        [    0.3927],\n",
      "        [    0.3960],\n",
      "        [    0.3976],\n",
      "        [    0.3993]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 93.37766456604004\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 152\n",
      "剩餘X 資料 torch.Size([8, 18])\n",
      "剩餘Y 資料 torch.Size([8, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.15996836125850677, 7)\n",
      "The second_loss value of k: (0.1612241119146347, 5)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引7，y= tensor([0.0440])\n",
      "目前模型的Data狀態 torch.Size([152, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1267],\n",
      "        [0.3533],\n",
      "        [0.1075],\n",
      "        [0.6316],\n",
      "        [0.2935],\n",
      "        [0.2653],\n",
      "        [0.6242],\n",
      "        [0.3576],\n",
      "        [0.1838],\n",
      "        [0.6172],\n",
      "        [0.1408],\n",
      "        [0.3445],\n",
      "        [0.6074],\n",
      "        [0.5967],\n",
      "        [0.1257],\n",
      "        [0.1843],\n",
      "        [0.2250],\n",
      "        [0.1578],\n",
      "        [0.6355],\n",
      "        [0.1218],\n",
      "        [0.4424],\n",
      "        [0.5289],\n",
      "        [0.4721],\n",
      "        [0.2998],\n",
      "        [0.6218],\n",
      "        [0.5680],\n",
      "        [0.3562],\n",
      "        [0.6445],\n",
      "        [0.4551],\n",
      "        [0.6364],\n",
      "        [0.6520],\n",
      "        [0.4880],\n",
      "        [0.1391],\n",
      "        [0.5643],\n",
      "        [0.4780],\n",
      "        [0.3540],\n",
      "        [0.6043],\n",
      "        [0.2189],\n",
      "        [0.2720],\n",
      "        [0.3340],\n",
      "        [0.1421],\n",
      "        [0.3259],\n",
      "        [0.1740],\n",
      "        [0.2001],\n",
      "        [0.5737],\n",
      "        [0.3034],\n",
      "        [0.5807],\n",
      "        [0.4512],\n",
      "        [0.5823],\n",
      "        [0.2064],\n",
      "        [0.3837],\n",
      "        [0.4308],\n",
      "        [0.4714],\n",
      "        [0.6368],\n",
      "        [0.1182],\n",
      "        [0.2182],\n",
      "        [0.1240],\n",
      "        [0.5761],\n",
      "        [0.5651],\n",
      "        [0.3675],\n",
      "        [0.6202],\n",
      "        [0.5975],\n",
      "        [0.5179],\n",
      "        [0.4906],\n",
      "        [0.4229],\n",
      "        [0.4308],\n",
      "        [0.2372],\n",
      "        [0.2816],\n",
      "        [0.2129],\n",
      "        [0.1852],\n",
      "        [0.4481],\n",
      "        [0.2195],\n",
      "        [0.1604],\n",
      "        [0.2307],\n",
      "        [0.3276],\n",
      "        [0.4436],\n",
      "        [0.4592],\n",
      "        [0.3576],\n",
      "        [0.3951],\n",
      "        [0.5301],\n",
      "        [0.4037],\n",
      "        [0.3143],\n",
      "        [0.4535],\n",
      "        [0.4784],\n",
      "        [0.1945],\n",
      "        [0.1624],\n",
      "        [0.5109],\n",
      "        [0.2410],\n",
      "        [0.3654],\n",
      "        [0.0974],\n",
      "        [0.3754],\n",
      "        [0.4267],\n",
      "        [0.2639],\n",
      "        [0.5408],\n",
      "        [0.4444],\n",
      "        [0.4909],\n",
      "        [0.4169],\n",
      "        [0.2972],\n",
      "        [0.5062],\n",
      "        [0.4442],\n",
      "        [0.4619],\n",
      "        [0.5451],\n",
      "        [0.3900],\n",
      "        [0.4003],\n",
      "        [0.4155],\n",
      "        [0.4543],\n",
      "        [0.3181],\n",
      "        [0.3445],\n",
      "        [0.4440],\n",
      "        [0.4649],\n",
      "        [0.1729],\n",
      "        [0.1106],\n",
      "        [0.3941],\n",
      "        [0.2659],\n",
      "        [0.1695],\n",
      "        [0.3753],\n",
      "        [0.4528],\n",
      "        [0.5330],\n",
      "        [0.3896],\n",
      "        [0.4647],\n",
      "        [0.4173],\n",
      "        [0.4335],\n",
      "        [0.0460],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439],\n",
      "        [0.4439]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0065],\n",
      "        [    0.0047],\n",
      "        [    0.0011],\n",
      "        [    0.0018],\n",
      "        [    0.0162],\n",
      "        [    0.0031],\n",
      "        [    0.0198],\n",
      "        [    0.0033],\n",
      "        [    0.0340],\n",
      "        [    0.0115],\n",
      "        [    0.0212],\n",
      "        [    0.0134],\n",
      "        [    0.0092],\n",
      "        [    0.0103],\n",
      "        [    0.0069],\n",
      "        [    0.0003],\n",
      "        [    0.0086],\n",
      "        [    0.0343],\n",
      "        [    0.0075],\n",
      "        [    0.0022],\n",
      "        [    0.0020],\n",
      "        [    0.0077],\n",
      "        [    0.0019],\n",
      "        [    0.0324],\n",
      "        [    0.0087],\n",
      "        [    0.0274],\n",
      "        [    0.0072],\n",
      "        [    0.0229],\n",
      "        [    0.0409],\n",
      "        [    0.0086],\n",
      "        [    0.0247],\n",
      "        [    0.0759],\n",
      "        [    0.0009],\n",
      "        [    0.0033],\n",
      "        [    0.0783],\n",
      "        [    0.0105],\n",
      "        [    0.0136],\n",
      "        [    0.0272],\n",
      "        [    0.0351],\n",
      "        [    0.0246],\n",
      "        [    0.0381],\n",
      "        [    0.0328],\n",
      "        [    0.0236],\n",
      "        [    0.0110],\n",
      "        [    0.0179],\n",
      "        [    0.0325],\n",
      "        [    0.0217],\n",
      "        [    0.0207],\n",
      "        [    0.0343],\n",
      "        [    0.0482],\n",
      "        [    0.0074],\n",
      "        [    0.0401],\n",
      "        [    0.0518],\n",
      "        [    0.0295],\n",
      "        [    0.0274],\n",
      "        [    0.0374],\n",
      "        [    0.0309],\n",
      "        [    0.0497],\n",
      "        [    0.0776],\n",
      "        [    0.0422],\n",
      "        [    0.0515],\n",
      "        [    0.0168],\n",
      "        [    0.0587],\n",
      "        [    0.0434],\n",
      "        [    0.0417],\n",
      "        [    0.0564],\n",
      "        [    0.0495],\n",
      "        [    0.0615],\n",
      "        [    0.0538],\n",
      "        [    0.0671],\n",
      "        [    0.0350],\n",
      "        [    0.0883],\n",
      "        [    0.0698],\n",
      "        [    0.0603],\n",
      "        [    0.0815],\n",
      "        [    0.0173],\n",
      "        [    0.0639],\n",
      "        [    0.0927],\n",
      "        [    0.0809],\n",
      "        [    0.0306],\n",
      "        [    0.0923],\n",
      "        [    0.0847],\n",
      "        [    0.0080],\n",
      "        [    0.0217],\n",
      "        [    0.0762],\n",
      "        [    0.0815],\n",
      "        [    0.0183],\n",
      "        [    0.0783],\n",
      "        [    0.0868],\n",
      "        [    0.0703],\n",
      "        [    0.0756],\n",
      "        [    0.0058],\n",
      "        [    0.0883],\n",
      "        [    0.0158],\n",
      "        [    0.0885],\n",
      "        [    0.0027],\n",
      "        [    0.0214],\n",
      "        [    0.1064],\n",
      "        [    0.0146],\n",
      "        [    0.0367],\n",
      "        [    0.1355],\n",
      "        [    0.0038],\n",
      "        [    0.0332],\n",
      "        [    0.0982],\n",
      "        [    0.1287],\n",
      "        [    0.0139],\n",
      "        [    0.1184],\n",
      "        [    0.1227],\n",
      "        [    0.0100],\n",
      "        [    0.0204],\n",
      "        [    0.1463],\n",
      "        [    0.1106],\n",
      "        [    0.0495],\n",
      "        [    0.1216],\n",
      "        [    0.1332],\n",
      "        [    0.1205],\n",
      "        [    0.0171],\n",
      "        [    0.0365],\n",
      "        [    0.1623],\n",
      "        [    0.0414],\n",
      "        [    0.0662],\n",
      "        [    0.0669],\n",
      "        [    0.0727],\n",
      "        [    0.2936],\n",
      "        [    0.3076],\n",
      "        [    0.3106],\n",
      "        [    0.3125],\n",
      "        [    0.3135],\n",
      "        [    0.3230],\n",
      "        [    0.3248],\n",
      "        [    0.3259],\n",
      "        [    0.3333],\n",
      "        [    0.3351],\n",
      "        [    0.3372],\n",
      "        [    0.3382],\n",
      "        [    0.3414],\n",
      "        [    0.3605],\n",
      "        [    0.3615],\n",
      "        [    0.3684],\n",
      "        [    0.3728],\n",
      "        [    0.3748],\n",
      "        [    0.3757],\n",
      "        [    0.3801],\n",
      "        [    0.3876],\n",
      "        [    0.3892],\n",
      "        [    0.3900],\n",
      "        [    0.3900],\n",
      "        [    0.3927],\n",
      "        [    0.3960],\n",
      "        [    0.3976],\n",
      "        [    0.3993],\n",
      "        [    0.4000]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 67\n",
      "Number of shrink: 33\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0066],\n",
      "        [    0.0047],\n",
      "        [    0.0014],\n",
      "        [    0.0021],\n",
      "        [    0.0164],\n",
      "        [    0.0027],\n",
      "        [    0.0201],\n",
      "        [    0.0029],\n",
      "        [    0.0345],\n",
      "        [    0.0120],\n",
      "        [    0.0214],\n",
      "        [    0.0134],\n",
      "        [    0.0096],\n",
      "        [    0.0109],\n",
      "        [    0.0071],\n",
      "        [    0.0000],\n",
      "        [    0.0086],\n",
      "        [    0.0347],\n",
      "        [    0.0074],\n",
      "        [    0.0020],\n",
      "        [    0.0017],\n",
      "        [    0.0080],\n",
      "        [    0.0023],\n",
      "        [    0.0322],\n",
      "        [    0.0089],\n",
      "        [    0.0272],\n",
      "        [    0.0072],\n",
      "        [    0.0229],\n",
      "        [    0.0403],\n",
      "        [    0.0082],\n",
      "        [    0.0247],\n",
      "        [    0.0755],\n",
      "        [    0.0006],\n",
      "        [    0.0027],\n",
      "        [    0.0778],\n",
      "        [    0.0108],\n",
      "        [    0.0132],\n",
      "        [    0.0271],\n",
      "        [    0.0351],\n",
      "        [    0.0246],\n",
      "        [    0.0385],\n",
      "        [    0.0329],\n",
      "        [    0.0242],\n",
      "        [    0.0107],\n",
      "        [    0.0174],\n",
      "        [    0.0321],\n",
      "        [    0.0212],\n",
      "        [    0.0212],\n",
      "        [    0.0339],\n",
      "        [    0.0483],\n",
      "        [    0.0073],\n",
      "        [    0.0405],\n",
      "        [    0.0513],\n",
      "        [    0.0290],\n",
      "        [    0.0271],\n",
      "        [    0.0372],\n",
      "        [    0.0306],\n",
      "        [    0.0493],\n",
      "        [    0.0776],\n",
      "        [    0.0418],\n",
      "        [    0.0515],\n",
      "        [    0.0164],\n",
      "        [    0.0583],\n",
      "        [    0.0438],\n",
      "        [    0.0421],\n",
      "        [    0.0569],\n",
      "        [    0.0493],\n",
      "        [    0.0612],\n",
      "        [    0.0536],\n",
      "        [    0.0664],\n",
      "        [    0.0352],\n",
      "        [    0.0884],\n",
      "        [    0.0696],\n",
      "        [    0.0603],\n",
      "        [    0.0817],\n",
      "        [    0.0169],\n",
      "        [    0.0642],\n",
      "        [    0.0925],\n",
      "        [    0.0804],\n",
      "        [    0.0302],\n",
      "        [    0.0918],\n",
      "        [    0.0850],\n",
      "        [    0.0076],\n",
      "        [    0.0214],\n",
      "        [    0.0759],\n",
      "        [    0.0812],\n",
      "        [    0.0180],\n",
      "        [    0.0780],\n",
      "        [    0.0869],\n",
      "        [    0.0700],\n",
      "        [    0.0756],\n",
      "        [    0.0062],\n",
      "        [    0.0880],\n",
      "        [    0.0155],\n",
      "        [    0.0888],\n",
      "        [    0.0022],\n",
      "        [    0.0217],\n",
      "        [    0.1061],\n",
      "        [    0.0145],\n",
      "        [    0.0372],\n",
      "        [    0.1353],\n",
      "        [    0.0036],\n",
      "        [    0.0336],\n",
      "        [    0.0983],\n",
      "        [    0.1284],\n",
      "        [    0.0142],\n",
      "        [    0.1187],\n",
      "        [    0.1228],\n",
      "        [    0.0103],\n",
      "        [    0.0209],\n",
      "        [    0.1463],\n",
      "        [    0.1102],\n",
      "        [    0.0498],\n",
      "        [    0.1214],\n",
      "        [    0.1332],\n",
      "        [    0.1205],\n",
      "        [    0.0173],\n",
      "        [    0.0367],\n",
      "        [    0.1620],\n",
      "        [    0.0417],\n",
      "        [    0.0665],\n",
      "        [    0.0670],\n",
      "        [    0.0710],\n",
      "        [    0.2931],\n",
      "        [    0.3071],\n",
      "        [    0.3101],\n",
      "        [    0.3120],\n",
      "        [    0.3130],\n",
      "        [    0.3225],\n",
      "        [    0.3243],\n",
      "        [    0.3254],\n",
      "        [    0.3328],\n",
      "        [    0.3346],\n",
      "        [    0.3367],\n",
      "        [    0.3377],\n",
      "        [    0.3409],\n",
      "        [    0.3600],\n",
      "        [    0.3610],\n",
      "        [    0.3679],\n",
      "        [    0.3723],\n",
      "        [    0.3743],\n",
      "        [    0.3752],\n",
      "        [    0.3796],\n",
      "        [    0.3871],\n",
      "        [    0.3887],\n",
      "        [    0.3895],\n",
      "        [    0.3895],\n",
      "        [    0.3922],\n",
      "        [    0.3955],\n",
      "        [    0.3971],\n",
      "        [    0.3988],\n",
      "        [    0.3995]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 93.61412072181702\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 153\n",
      "剩餘X 資料 torch.Size([7, 18])\n",
      "剩餘Y 資料 torch.Size([7, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.16082434356212616, 5)\n",
      "The second_loss value of k: (0.16309428215026855, 6)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引5，y= tensor([0.0424])\n",
      "目前模型的Data狀態 torch.Size([153, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1265],\n",
      "        [0.3533],\n",
      "        [0.1072],\n",
      "        [0.6312],\n",
      "        [0.2933],\n",
      "        [0.2650],\n",
      "        [0.6238],\n",
      "        [0.3571],\n",
      "        [0.1834],\n",
      "        [0.6167],\n",
      "        [0.1406],\n",
      "        [0.3445],\n",
      "        [0.6070],\n",
      "        [0.5961],\n",
      "        [0.1255],\n",
      "        [0.1841],\n",
      "        [0.2250],\n",
      "        [0.1573],\n",
      "        [0.6356],\n",
      "        [0.1215],\n",
      "        [0.4421],\n",
      "        [0.5286],\n",
      "        [0.4717],\n",
      "        [0.2997],\n",
      "        [0.6216],\n",
      "        [0.5677],\n",
      "        [0.3562],\n",
      "        [0.6444],\n",
      "        [0.4544],\n",
      "        [0.6360],\n",
      "        [0.6520],\n",
      "        [0.4876],\n",
      "        [0.1388],\n",
      "        [0.5638],\n",
      "        [0.4775],\n",
      "        [0.3536],\n",
      "        [0.6039],\n",
      "        [0.2188],\n",
      "        [0.2719],\n",
      "        [0.3339],\n",
      "        [0.1418],\n",
      "        [0.3258],\n",
      "        [0.1733],\n",
      "        [0.1998],\n",
      "        [0.5732],\n",
      "        [0.3030],\n",
      "        [0.5802],\n",
      "        [0.4507],\n",
      "        [0.5819],\n",
      "        [0.2062],\n",
      "        [0.3838],\n",
      "        [0.4304],\n",
      "        [0.4709],\n",
      "        [0.6363],\n",
      "        [0.1178],\n",
      "        [0.2180],\n",
      "        [0.1237],\n",
      "        [0.5758],\n",
      "        [0.5650],\n",
      "        [0.3670],\n",
      "        [0.6202],\n",
      "        [0.5971],\n",
      "        [0.5176],\n",
      "        [0.4902],\n",
      "        [0.4224],\n",
      "        [0.4303],\n",
      "        [0.2369],\n",
      "        [0.2814],\n",
      "        [0.2127],\n",
      "        [0.1845],\n",
      "        [0.4478],\n",
      "        [0.2194],\n",
      "        [0.1602],\n",
      "        [0.2306],\n",
      "        [0.3275],\n",
      "        [0.4432],\n",
      "        [0.4588],\n",
      "        [0.3574],\n",
      "        [0.3945],\n",
      "        [0.5298],\n",
      "        [0.4032],\n",
      "        [0.3139],\n",
      "        [0.4531],\n",
      "        [0.4781],\n",
      "        [0.1941],\n",
      "        [0.1620],\n",
      "        [0.5106],\n",
      "        [0.2407],\n",
      "        [0.3652],\n",
      "        [0.0971],\n",
      "        [0.3754],\n",
      "        [0.4264],\n",
      "        [0.2636],\n",
      "        [0.5406],\n",
      "        [0.4441],\n",
      "        [0.4904],\n",
      "        [0.4165],\n",
      "        [0.2970],\n",
      "        [0.5062],\n",
      "        [0.4437],\n",
      "        [0.4617],\n",
      "        [0.5449],\n",
      "        [0.3896],\n",
      "        [0.4002],\n",
      "        [0.4152],\n",
      "        [0.4540],\n",
      "        [0.3178],\n",
      "        [0.3444],\n",
      "        [0.4437],\n",
      "        [0.4645],\n",
      "        [0.1729],\n",
      "        [0.1102],\n",
      "        [0.3937],\n",
      "        [0.2657],\n",
      "        [0.1695],\n",
      "        [0.3752],\n",
      "        [0.4525],\n",
      "        [0.5328],\n",
      "        [0.3893],\n",
      "        [0.4644],\n",
      "        [0.4170],\n",
      "        [0.4333],\n",
      "        [0.0478],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434],\n",
      "        [0.4434]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0066],\n",
      "        [    0.0047],\n",
      "        [    0.0014],\n",
      "        [    0.0021],\n",
      "        [    0.0164],\n",
      "        [    0.0027],\n",
      "        [    0.0201],\n",
      "        [    0.0029],\n",
      "        [    0.0345],\n",
      "        [    0.0120],\n",
      "        [    0.0214],\n",
      "        [    0.0134],\n",
      "        [    0.0096],\n",
      "        [    0.0109],\n",
      "        [    0.0071],\n",
      "        [    0.0000],\n",
      "        [    0.0086],\n",
      "        [    0.0347],\n",
      "        [    0.0074],\n",
      "        [    0.0020],\n",
      "        [    0.0017],\n",
      "        [    0.0080],\n",
      "        [    0.0023],\n",
      "        [    0.0322],\n",
      "        [    0.0089],\n",
      "        [    0.0272],\n",
      "        [    0.0072],\n",
      "        [    0.0229],\n",
      "        [    0.0403],\n",
      "        [    0.0082],\n",
      "        [    0.0247],\n",
      "        [    0.0755],\n",
      "        [    0.0006],\n",
      "        [    0.0027],\n",
      "        [    0.0778],\n",
      "        [    0.0108],\n",
      "        [    0.0132],\n",
      "        [    0.0271],\n",
      "        [    0.0351],\n",
      "        [    0.0246],\n",
      "        [    0.0385],\n",
      "        [    0.0329],\n",
      "        [    0.0242],\n",
      "        [    0.0107],\n",
      "        [    0.0174],\n",
      "        [    0.0321],\n",
      "        [    0.0212],\n",
      "        [    0.0212],\n",
      "        [    0.0339],\n",
      "        [    0.0483],\n",
      "        [    0.0073],\n",
      "        [    0.0405],\n",
      "        [    0.0513],\n",
      "        [    0.0290],\n",
      "        [    0.0271],\n",
      "        [    0.0372],\n",
      "        [    0.0306],\n",
      "        [    0.0493],\n",
      "        [    0.0776],\n",
      "        [    0.0418],\n",
      "        [    0.0515],\n",
      "        [    0.0164],\n",
      "        [    0.0583],\n",
      "        [    0.0438],\n",
      "        [    0.0421],\n",
      "        [    0.0569],\n",
      "        [    0.0493],\n",
      "        [    0.0612],\n",
      "        [    0.0536],\n",
      "        [    0.0664],\n",
      "        [    0.0352],\n",
      "        [    0.0884],\n",
      "        [    0.0696],\n",
      "        [    0.0603],\n",
      "        [    0.0817],\n",
      "        [    0.0169],\n",
      "        [    0.0642],\n",
      "        [    0.0925],\n",
      "        [    0.0804],\n",
      "        [    0.0302],\n",
      "        [    0.0918],\n",
      "        [    0.0850],\n",
      "        [    0.0076],\n",
      "        [    0.0214],\n",
      "        [    0.0759],\n",
      "        [    0.0812],\n",
      "        [    0.0180],\n",
      "        [    0.0780],\n",
      "        [    0.0869],\n",
      "        [    0.0700],\n",
      "        [    0.0756],\n",
      "        [    0.0062],\n",
      "        [    0.0880],\n",
      "        [    0.0155],\n",
      "        [    0.0888],\n",
      "        [    0.0022],\n",
      "        [    0.0217],\n",
      "        [    0.1061],\n",
      "        [    0.0145],\n",
      "        [    0.0372],\n",
      "        [    0.1353],\n",
      "        [    0.0036],\n",
      "        [    0.0336],\n",
      "        [    0.0983],\n",
      "        [    0.1284],\n",
      "        [    0.0142],\n",
      "        [    0.1187],\n",
      "        [    0.1228],\n",
      "        [    0.0103],\n",
      "        [    0.0209],\n",
      "        [    0.1463],\n",
      "        [    0.1102],\n",
      "        [    0.0498],\n",
      "        [    0.1214],\n",
      "        [    0.1332],\n",
      "        [    0.1205],\n",
      "        [    0.0173],\n",
      "        [    0.0367],\n",
      "        [    0.1620],\n",
      "        [    0.0417],\n",
      "        [    0.0665],\n",
      "        [    0.0670],\n",
      "        [    0.0710],\n",
      "        [    0.2931],\n",
      "        [    0.3071],\n",
      "        [    0.3101],\n",
      "        [    0.3120],\n",
      "        [    0.3130],\n",
      "        [    0.3225],\n",
      "        [    0.3243],\n",
      "        [    0.3254],\n",
      "        [    0.3328],\n",
      "        [    0.3346],\n",
      "        [    0.3367],\n",
      "        [    0.3377],\n",
      "        [    0.3409],\n",
      "        [    0.3600],\n",
      "        [    0.3610],\n",
      "        [    0.3679],\n",
      "        [    0.3723],\n",
      "        [    0.3743],\n",
      "        [    0.3752],\n",
      "        [    0.3796],\n",
      "        [    0.3871],\n",
      "        [    0.3887],\n",
      "        [    0.3895],\n",
      "        [    0.3895],\n",
      "        [    0.3922],\n",
      "        [    0.3955],\n",
      "        [    0.3971],\n",
      "        [    0.3988],\n",
      "        [    0.3995],\n",
      "        [    0.4010]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0087],\n",
      "        [    0.0068],\n",
      "        [    0.0034],\n",
      "        [    0.0054],\n",
      "        [    0.0186],\n",
      "        [    0.0004],\n",
      "        [    0.0236],\n",
      "        [    0.0004],\n",
      "        [    0.0367],\n",
      "        [    0.0155],\n",
      "        [    0.0233],\n",
      "        [    0.0154],\n",
      "        [    0.0131],\n",
      "        [    0.0145],\n",
      "        [    0.0093],\n",
      "        [    0.0020],\n",
      "        [    0.0103],\n",
      "        [    0.0369],\n",
      "        [    0.0104],\n",
      "        [    0.0002],\n",
      "        [    0.0011],\n",
      "        [    0.0111],\n",
      "        [    0.0053],\n",
      "        [    0.0299],\n",
      "        [    0.0122],\n",
      "        [    0.0241],\n",
      "        [    0.0092],\n",
      "        [    0.0198],\n",
      "        [    0.0373],\n",
      "        [    0.0047],\n",
      "        [    0.0278],\n",
      "        [    0.0724],\n",
      "        [    0.0016],\n",
      "        [    0.0008],\n",
      "        [    0.0749],\n",
      "        [    0.0132],\n",
      "        [    0.0097],\n",
      "        [    0.0254],\n",
      "        [    0.0370],\n",
      "        [    0.0267],\n",
      "        [    0.0405],\n",
      "        [    0.0350],\n",
      "        [    0.0253],\n",
      "        [    0.0087],\n",
      "        [    0.0139],\n",
      "        [    0.0296],\n",
      "        [    0.0177],\n",
      "        [    0.0242],\n",
      "        [    0.0305],\n",
      "        [    0.0502],\n",
      "        [    0.0091],\n",
      "        [    0.0436],\n",
      "        [    0.0483],\n",
      "        [    0.0255],\n",
      "        [    0.0249],\n",
      "        [    0.0351],\n",
      "        [    0.0284],\n",
      "        [    0.0461],\n",
      "        [    0.0751],\n",
      "        [    0.0391],\n",
      "        [    0.0546],\n",
      "        [    0.0130],\n",
      "        [    0.0554],\n",
      "        [    0.0468],\n",
      "        [    0.0451],\n",
      "        [    0.0600],\n",
      "        [    0.0471],\n",
      "        [    0.0590],\n",
      "        [    0.0516],\n",
      "        [    0.0652],\n",
      "        [    0.0377],\n",
      "        [    0.0903],\n",
      "        [    0.0675],\n",
      "        [    0.0625],\n",
      "        [    0.0839],\n",
      "        [    0.0140],\n",
      "        [    0.0672],\n",
      "        [    0.0900],\n",
      "        [    0.0777],\n",
      "        [    0.0272],\n",
      "        [    0.0891],\n",
      "        [    0.0873],\n",
      "        [    0.0047],\n",
      "        [    0.0185],\n",
      "        [    0.0737],\n",
      "        [    0.0789],\n",
      "        [    0.0151],\n",
      "        [    0.0757],\n",
      "        [    0.0891],\n",
      "        [    0.0680],\n",
      "        [    0.0776],\n",
      "        [    0.0090],\n",
      "        [    0.0858],\n",
      "        [    0.0127],\n",
      "        [    0.0918],\n",
      "        [    0.0010],\n",
      "        [    0.0246],\n",
      "        [    0.1039],\n",
      "        [    0.0120],\n",
      "        [    0.0401],\n",
      "        [    0.1326],\n",
      "        [    0.0009],\n",
      "        [    0.0364],\n",
      "        [    0.1005],\n",
      "        [    0.1257],\n",
      "        [    0.0170],\n",
      "        [    0.1210],\n",
      "        [    0.1250],\n",
      "        [    0.0132],\n",
      "        [    0.0240],\n",
      "        [    0.1482],\n",
      "        [    0.1082],\n",
      "        [    0.0527],\n",
      "        [    0.1192],\n",
      "        [    0.1351],\n",
      "        [    0.1226],\n",
      "        [    0.0201],\n",
      "        [    0.0394],\n",
      "        [    0.1593],\n",
      "        [    0.0446],\n",
      "        [    0.0693],\n",
      "        [    0.0697],\n",
      "        [    0.0707],\n",
      "        [    0.2926],\n",
      "        [    0.3066],\n",
      "        [    0.3096],\n",
      "        [    0.3115],\n",
      "        [    0.3125],\n",
      "        [    0.3220],\n",
      "        [    0.3238],\n",
      "        [    0.3249],\n",
      "        [    0.3323],\n",
      "        [    0.3341],\n",
      "        [    0.3362],\n",
      "        [    0.3372],\n",
      "        [    0.3404],\n",
      "        [    0.3595],\n",
      "        [    0.3605],\n",
      "        [    0.3674],\n",
      "        [    0.3718],\n",
      "        [    0.3738],\n",
      "        [    0.3747],\n",
      "        [    0.3791],\n",
      "        [    0.3866],\n",
      "        [    0.3882],\n",
      "        [    0.3890],\n",
      "        [    0.3890],\n",
      "        [    0.3917],\n",
      "        [    0.3950],\n",
      "        [    0.3966],\n",
      "        [    0.3983],\n",
      "        [    0.3990],\n",
      "        [    0.4005]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 93.84884285926819\n",
      "------------------------------------------------------------------------------------------\n",
      "到第 6 個區塊累積花費時間(s) 6.290612459182739\n",
      "<<The performance of 6 block>>\n",
      "<<Training step>>\n",
      "The training time(s): 6.290612459182739\n",
      "<<The percentage of each step>>\n",
      "Step 4: 100.00%\n",
      "Step 6.1: 0.00%\n",
      "Step 6.2: 0.00%\n",
      "------------------------------------------------------------\n",
      "Total frequency of cramming occurrences: 0\n",
      "------------------------------------------------------------\n",
      "The amount of hidden node that be pruned: 0\n",
      "------------------------------------------------------------\n",
      "The amount of adopted hidden nodes: 4\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in training step>>\n",
      "The MAE for l = 1: 2683.73\n",
      "The MAPE for l = 1: 0.07%\n",
      "The RMSE for l = 1: 4259.68\n",
      "The accuracy(2000) for l = 1: 66.67%\n",
      "The accuracy(3000) for l = 1: 74.51%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in toutlier>>\n",
      "The MAE for l = 1: 10542.70\n",
      "The MAPE for l = 1: 0.30%\n",
      "The RMSE for l = 1: 10543.85\n",
      "The accuracy(2000) for l = 1: 0.00%\n",
      "The accuracy(3000) for l = 1: 0.00%\n",
      "------------------------------------------------------------\n",
      "<<Accuracy in inferencing step>>\n",
      "The MAE for l = 1: 5590.2\n",
      "The MAPE for l = 1: 0.1%\n",
      "The RMSE for l = 1: 5948.0\n",
      "The accuracy(2000) for l = 1: 7.7%\n",
      "The accuracy(3000) for l = 1: 7.7%\n",
      "------------------------------------------------------------\n",
      "0.6666666666666666\n",
      "<class 'float'>\n",
      "0.0\n",
      "<class 'float'>\n",
      "0.07692307692307693\n",
      "<class 'float'>\n",
      "The <<7>> Block\n",
      "<<Selecting module>>\n",
      "The loss value of k: (2.5459915065084715e-08, 92)\n",
      "The second_loss value of k: (1.3809039955958724e-07, 25)\n",
      "Selecting module finish!\n",
      "其他區塊剛開始選的資料索引： [92, 25, 30, 114, 118, 3, 95, 36, 87, 127, 0, 15, 93, 42, 128, 19, 18, 94, 38, 111, 115, 122, 24, 107, 116, 14, 108, 123, 99, 121, 46, 119, 1, 131, 88, 40, 37, 17, 117, 91, 29, 8, 16, 35, 129, 43, 44, 13, 105, 104, 28, 113, 132, 45, 120, 2, 34, 126, 39, 41, 130, 124, 134, 133, 31, 9, 47, 90, 50, 109, 110, 86, 125, 96, 106, 112, 33, 20, 26, 89, 100, 98, 23, 21, 27, 7, 12, 103, 32, 49, 97, 22, 102, 101, 5, 135, 4, 10, 145, 156, 155, 11, 6, 157, 144, 146, 152, 153, 154, 151, 136, 158, 143, 147, 140, 137, 141, 139, 150, 138, 83, 84, 85, 82, 66, 142, 71]\n",
      "<<其他區塊剛開始時看一下差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0009],\n",
      "        [    0.0010],\n",
      "        [    0.0011],\n",
      "        [    0.0016],\n",
      "        [    0.0020],\n",
      "        [    0.0034],\n",
      "        [    0.0047],\n",
      "        [    0.0053],\n",
      "        [    0.0068],\n",
      "        [    0.0087],\n",
      "        [    0.0087],\n",
      "        [    0.0090],\n",
      "        [    0.0091],\n",
      "        [    0.0092],\n",
      "        [    0.0093],\n",
      "        [    0.0103],\n",
      "        [    0.0120],\n",
      "        [    0.0127],\n",
      "        [    0.0132],\n",
      "        [    0.0132],\n",
      "        [    0.0140],\n",
      "        [    0.0151],\n",
      "        [    0.0154],\n",
      "        [    0.0170],\n",
      "        [    0.0185],\n",
      "        [    0.0186],\n",
      "        [    0.0201],\n",
      "        [    0.0233],\n",
      "        [    0.0240],\n",
      "        [    0.0242],\n",
      "        [    0.0246],\n",
      "        [    0.0249],\n",
      "        [    0.0253],\n",
      "        [    0.0254],\n",
      "        [    0.0267],\n",
      "        [    0.0272],\n",
      "        [    0.0284],\n",
      "        [    0.0296],\n",
      "        [    0.0299],\n",
      "        [    0.0350],\n",
      "        [    0.0351],\n",
      "        [    0.0364],\n",
      "        [    0.0367],\n",
      "        [    0.0369],\n",
      "        [    0.0370],\n",
      "        [    0.0373],\n",
      "        [    0.0377],\n",
      "        [    0.0391],\n",
      "        [    0.0394],\n",
      "        [    0.0401],\n",
      "        [    0.0405],\n",
      "        [    0.0446],\n",
      "        [    0.0451],\n",
      "        [    0.0471],\n",
      "        [    0.0483],\n",
      "        [    0.0502],\n",
      "        [    0.0516],\n",
      "        [    0.0527],\n",
      "        [    0.0554],\n",
      "        [    0.0554],\n",
      "        [    0.0557],\n",
      "        [    0.0590],\n",
      "        [    0.0625],\n",
      "        [    0.0652],\n",
      "        [    0.0675],\n",
      "        [    0.0680],\n",
      "        [    0.0693],\n",
      "        [    0.0697],\n",
      "        [    0.0707],\n",
      "        [    0.0724],\n",
      "        [    0.0737],\n",
      "        [    0.0749],\n",
      "        [    0.0751],\n",
      "        [    0.0757],\n",
      "        [    0.0776],\n",
      "        [    0.0777],\n",
      "        [    0.0789],\n",
      "        [    0.0839],\n",
      "        [    0.0858],\n",
      "        [    0.0873],\n",
      "        [    0.0891],\n",
      "        [    0.0891],\n",
      "        [    0.0900],\n",
      "        [    0.0903],\n",
      "        [    0.1005],\n",
      "        [    0.1039],\n",
      "        [    0.1082],\n",
      "        [    0.1192],\n",
      "        [    0.1210],\n",
      "        [    0.1226],\n",
      "        [    0.1250],\n",
      "        [    0.1257],\n",
      "        [    0.1266],\n",
      "        [    0.1326],\n",
      "        [    0.1351],\n",
      "        [    0.1424],\n",
      "        [    0.1432],\n",
      "        [    0.1456],\n",
      "        [    0.1482],\n",
      "        [    0.1593],\n",
      "        [    0.1676],\n",
      "        [    0.1856],\n",
      "        [    0.1874],\n",
      "        [    0.1899],\n",
      "        [    0.1967],\n",
      "        [    0.2064],\n",
      "        [    0.2187],\n",
      "        [    0.2198],\n",
      "        [    0.2259],\n",
      "        [    0.2316],\n",
      "        [    0.2494],\n",
      "        [    0.2614],\n",
      "        [    0.2810],\n",
      "        [    0.2845],\n",
      "        [    0.2879],\n",
      "        [    0.2901],\n",
      "        [    0.2908],\n",
      "        [    0.2926],\n",
      "        [    0.3066],\n",
      "        [    0.3096],\n",
      "        [    0.3115],\n",
      "        [    0.3125],\n",
      "        [    0.3135],\n",
      "        [    0.3220]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "現在訓練到第幾筆資料: 128\n",
      "剩餘X 資料 torch.Size([32, 18])\n",
      "剩餘Y 資料 torch.Size([32, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10481911152601242, 15)\n",
      "The second_loss value of k: (0.1055813729763031, 29)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.1192])\n",
      "目前模型的Data狀態 torch.Size([128, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1194],\n",
      "        [0.3547],\n",
      "        [0.2626],\n",
      "        [0.5422],\n",
      "        [0.4873],\n",
      "        [0.4393],\n",
      "        [0.1366],\n",
      "        [0.1821],\n",
      "        [0.1052],\n",
      "        [0.4502],\n",
      "        [0.4687],\n",
      "        [0.3512],\n",
      "        [0.1245],\n",
      "        [0.1978],\n",
      "        [0.4235],\n",
      "        [0.3820],\n",
      "        [0.3542],\n",
      "        [0.1233],\n",
      "        [0.2233],\n",
      "        [0.5037],\n",
      "        [0.5377],\n",
      "        [0.4408],\n",
      "        [0.3513],\n",
      "        [0.4403],\n",
      "        [0.5076],\n",
      "        [0.3425],\n",
      "        [0.4512],\n",
      "        [0.4752],\n",
      "        [0.2911],\n",
      "        [0.4497],\n",
      "        [0.1387],\n",
      "        [0.4614],\n",
      "        [0.4477],\n",
      "        [0.4137],\n",
      "        [0.1156],\n",
      "        [0.1722],\n",
      "        [0.2171],\n",
      "        [0.3319],\n",
      "        [0.5268],\n",
      "        [0.1215],\n",
      "        [0.3006],\n",
      "        [0.2973],\n",
      "        [0.3237],\n",
      "        [0.2159],\n",
      "        [0.3867],\n",
      "        [0.1811],\n",
      "        [0.1552],\n",
      "        [0.2700],\n",
      "        [0.4515],\n",
      "        [0.4454],\n",
      "        [0.3644],\n",
      "        [0.5302],\n",
      "        [0.4408],\n",
      "        [0.1397],\n",
      "        [0.4615],\n",
      "        [0.4195],\n",
      "        [0.2348],\n",
      "        [0.4678],\n",
      "        [0.2044],\n",
      "        [0.2107],\n",
      "        [0.3909],\n",
      "        [0.5146],\n",
      "        [0.4328],\n",
      "        [0.4349],\n",
      "        [0.2791],\n",
      "        [0.2284],\n",
      "        [0.1833],\n",
      "        [0.1581],\n",
      "        [0.0950],\n",
      "        [0.4142],\n",
      "        [0.4306],\n",
      "        [0.0481],\n",
      "        [0.4845],\n",
      "        [0.1919],\n",
      "        [0.4745],\n",
      "        [0.5625],\n",
      "        [0.2384],\n",
      "        [0.3735],\n",
      "        [0.3918],\n",
      "        [0.1598],\n",
      "        [0.3252],\n",
      "        [0.2614],\n",
      "        [0.3117],\n",
      "        [0.3631],\n",
      "        [0.4005],\n",
      "        [0.3549],\n",
      "        [0.2175],\n",
      "        [0.3980],\n",
      "        [0.2947],\n",
      "        [0.1082],\n",
      "        [0.2635],\n",
      "        [0.3156],\n",
      "        [0.3732],\n",
      "        [0.3422],\n",
      "        [0.4125],\n",
      "        [0.3874],\n",
      "        [0.4590],\n",
      "        [0.1676],\n",
      "        [0.4970],\n",
      "        [0.5497],\n",
      "        [0.5497],\n",
      "        [0.1710],\n",
      "        [0.3867],\n",
      "        [0.5829],\n",
      "        [0.4427],\n",
      "        [0.5002],\n",
      "        [0.5462],\n",
      "        [0.5517],\n",
      "        [0.5312],\n",
      "        [0.5470],\n",
      "        [0.3518],\n",
      "        [0.5700],\n",
      "        [0.4178],\n",
      "        [0.4784],\n",
      "        [0.4046],\n",
      "        [0.3360],\n",
      "        [0.4267],\n",
      "        [0.3356],\n",
      "        [0.4835],\n",
      "        [0.3329],\n",
      "        [0.4430],\n",
      "        [0.4430],\n",
      "        [0.4430],\n",
      "        [0.4430],\n",
      "        [0.4430],\n",
      "        [0.4160],\n",
      "        [0.4430],\n",
      "        [0.4430]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0002],\n",
      "        [    0.0004],\n",
      "        [    0.0004],\n",
      "        [    0.0009],\n",
      "        [    0.0010],\n",
      "        [    0.0011],\n",
      "        [    0.0016],\n",
      "        [    0.0020],\n",
      "        [    0.0034],\n",
      "        [    0.0047],\n",
      "        [    0.0053],\n",
      "        [    0.0068],\n",
      "        [    0.0087],\n",
      "        [    0.0087],\n",
      "        [    0.0090],\n",
      "        [    0.0091],\n",
      "        [    0.0092],\n",
      "        [    0.0093],\n",
      "        [    0.0103],\n",
      "        [    0.0120],\n",
      "        [    0.0127],\n",
      "        [    0.0132],\n",
      "        [    0.0132],\n",
      "        [    0.0140],\n",
      "        [    0.0151],\n",
      "        [    0.0154],\n",
      "        [    0.0170],\n",
      "        [    0.0185],\n",
      "        [    0.0186],\n",
      "        [    0.0201],\n",
      "        [    0.0233],\n",
      "        [    0.0240],\n",
      "        [    0.0242],\n",
      "        [    0.0246],\n",
      "        [    0.0249],\n",
      "        [    0.0253],\n",
      "        [    0.0254],\n",
      "        [    0.0267],\n",
      "        [    0.0272],\n",
      "        [    0.0284],\n",
      "        [    0.0296],\n",
      "        [    0.0299],\n",
      "        [    0.0350],\n",
      "        [    0.0351],\n",
      "        [    0.0364],\n",
      "        [    0.0367],\n",
      "        [    0.0369],\n",
      "        [    0.0370],\n",
      "        [    0.0373],\n",
      "        [    0.0377],\n",
      "        [    0.0391],\n",
      "        [    0.0394],\n",
      "        [    0.0401],\n",
      "        [    0.0405],\n",
      "        [    0.0446],\n",
      "        [    0.0451],\n",
      "        [    0.0471],\n",
      "        [    0.0483],\n",
      "        [    0.0502],\n",
      "        [    0.0516],\n",
      "        [    0.0527],\n",
      "        [    0.0554],\n",
      "        [    0.0554],\n",
      "        [    0.0557],\n",
      "        [    0.0590],\n",
      "        [    0.0625],\n",
      "        [    0.0652],\n",
      "        [    0.0675],\n",
      "        [    0.0680],\n",
      "        [    0.0693],\n",
      "        [    0.0697],\n",
      "        [    0.0707],\n",
      "        [    0.0724],\n",
      "        [    0.0737],\n",
      "        [    0.0749],\n",
      "        [    0.0751],\n",
      "        [    0.0757],\n",
      "        [    0.0776],\n",
      "        [    0.0777],\n",
      "        [    0.0789],\n",
      "        [    0.0839],\n",
      "        [    0.0858],\n",
      "        [    0.0873],\n",
      "        [    0.0891],\n",
      "        [    0.0891],\n",
      "        [    0.0900],\n",
      "        [    0.0903],\n",
      "        [    0.1005],\n",
      "        [    0.1039],\n",
      "        [    0.1082],\n",
      "        [    0.1192],\n",
      "        [    0.1210],\n",
      "        [    0.1226],\n",
      "        [    0.1250],\n",
      "        [    0.1257],\n",
      "        [    0.1266],\n",
      "        [    0.1326],\n",
      "        [    0.1351],\n",
      "        [    0.1424],\n",
      "        [    0.1432],\n",
      "        [    0.1456],\n",
      "        [    0.1482],\n",
      "        [    0.1593],\n",
      "        [    0.1676],\n",
      "        [    0.1856],\n",
      "        [    0.1874],\n",
      "        [    0.1899],\n",
      "        [    0.1967],\n",
      "        [    0.2064],\n",
      "        [    0.2187],\n",
      "        [    0.2198],\n",
      "        [    0.2259],\n",
      "        [    0.2316],\n",
      "        [    0.2494],\n",
      "        [    0.2614],\n",
      "        [    0.2810],\n",
      "        [    0.2845],\n",
      "        [    0.2879],\n",
      "        [    0.2901],\n",
      "        [    0.2908],\n",
      "        [    0.2926],\n",
      "        [    0.3066],\n",
      "        [    0.3096],\n",
      "        [    0.3115],\n",
      "        [    0.3125],\n",
      "        [    0.3135],\n",
      "        [    0.3220],\n",
      "        [    0.3238]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 71\n",
      "Number of shrink: 29\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0241],\n",
      "        [0.0115],\n",
      "        [0.0606],\n",
      "        [0.0854],\n",
      "        [0.0477],\n",
      "        [0.0037],\n",
      "        [0.0075],\n",
      "        [0.0050],\n",
      "        [0.0831],\n",
      "        [0.0507],\n",
      "        [0.0035],\n",
      "        [0.0088],\n",
      "        [0.0053],\n",
      "        [0.0635],\n",
      "        [0.0157],\n",
      "        [0.0066],\n",
      "        [0.0040],\n",
      "        [0.0304],\n",
      "        [0.0643],\n",
      "        [0.0816],\n",
      "        [0.0641],\n",
      "        [0.0068],\n",
      "        [0.0775],\n",
      "        [0.0909],\n",
      "        [0.0117],\n",
      "        [0.0492],\n",
      "        [0.0935],\n",
      "        [0.0147],\n",
      "        [0.0607],\n",
      "        [0.0132],\n",
      "        [0.0615],\n",
      "        [0.0334],\n",
      "        [0.0601],\n",
      "        [0.0346],\n",
      "        [0.0318],\n",
      "        [0.0124],\n",
      "        [0.0209],\n",
      "        [0.1123],\n",
      "        [0.0324],\n",
      "        [0.0468],\n",
      "        [0.0682],\n",
      "        [0.0261],\n",
      "        [0.0315],\n",
      "        [0.0390],\n",
      "        [0.0261],\n",
      "        [0.0275],\n",
      "        [0.0332],\n",
      "        [0.0849],\n",
      "        [0.0098],\n",
      "        [0.0636],\n",
      "        [0.0182],\n",
      "        [0.0401],\n",
      "        [0.0314],\n",
      "        [0.0380],\n",
      "        [0.0097],\n",
      "        [0.0453],\n",
      "        [0.1324],\n",
      "        [0.0682],\n",
      "        [0.0400],\n",
      "        [0.0303],\n",
      "        [0.1379],\n",
      "        [0.0341],\n",
      "        [0.0288],\n",
      "        [0.0637],\n",
      "        [0.0283],\n",
      "        [0.0644],\n",
      "        [0.0728],\n",
      "        [0.0730],\n",
      "        [0.0072],\n",
      "        [0.0127],\n",
      "        [0.0604],\n",
      "        [0.1585],\n",
      "        [0.0793],\n",
      "        [0.1294],\n",
      "        [0.1254],\n",
      "        [0.0765],\n",
      "        [0.0781],\n",
      "        [0.1076],\n",
      "        [0.0880],\n",
      "        [0.0784],\n",
      "        [0.0899],\n",
      "        [0.0700],\n",
      "        [0.0824],\n",
      "        [0.1191],\n",
      "        [0.1341],\n",
      "        [0.0813],\n",
      "        [0.0878],\n",
      "        [0.1075],\n",
      "        [0.1103],\n",
      "        [0.1215],\n",
      "        [0.1078],\n",
      "        [0.1164],\n",
      "        [0.1170],\n",
      "        [0.1680],\n",
      "        [0.0383],\n",
      "        [0.1786],\n",
      "        [0.1163],\n",
      "        [0.0106],\n",
      "        [0.0074],\n",
      "        [0.0087],\n",
      "        [0.1351],\n",
      "        [0.2029],\n",
      "        [0.0271],\n",
      "        [0.0609],\n",
      "        [0.0460],\n",
      "        [0.0401],\n",
      "        [0.0368],\n",
      "        [0.0479],\n",
      "        [0.0790],\n",
      "        [0.1279],\n",
      "        [0.0911],\n",
      "        [0.1081],\n",
      "        [0.1116],\n",
      "        [0.1506],\n",
      "        [0.1874],\n",
      "        [0.1702],\n",
      "        [0.1844],\n",
      "        [0.1620],\n",
      "        [0.1921],\n",
      "        [0.2905],\n",
      "        [0.3045],\n",
      "        [0.3075],\n",
      "        [0.3094],\n",
      "        [0.3104],\n",
      "        [0.1947],\n",
      "        [0.3199],\n",
      "        [0.3217]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 94.34538292884827\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 129\n",
      "剩餘X 資料 torch.Size([31, 18])\n",
      "剩餘Y 資料 torch.Size([31, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.05231093242764473, 29)\n",
      "The second_loss value of k: (0.061816249042749405, 30)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引29，y= tensor([0.8033])\n",
      "目前模型的Data狀態 torch.Size([129, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1209],\n",
      "        [0.3784],\n",
      "        [0.2737],\n",
      "        [0.6019],\n",
      "        [0.5736],\n",
      "        [0.4881],\n",
      "        [0.1419],\n",
      "        [0.1766],\n",
      "        [0.1136],\n",
      "        [0.5285],\n",
      "        [0.5247],\n",
      "        [0.3545],\n",
      "        [0.1244],\n",
      "        [0.1945],\n",
      "        [0.4960],\n",
      "        [0.3754],\n",
      "        [0.3569],\n",
      "        [0.1286],\n",
      "        [0.2032],\n",
      "        [0.5560],\n",
      "        [0.6066],\n",
      "        [0.5180],\n",
      "        [0.3713],\n",
      "        [0.5038],\n",
      "        [0.5835],\n",
      "        [0.3462],\n",
      "        [0.5174],\n",
      "        [0.5502],\n",
      "        [0.2950],\n",
      "        [0.5306],\n",
      "        [0.1488],\n",
      "        [0.5468],\n",
      "        [0.5053],\n",
      "        [0.4983],\n",
      "        [0.1253],\n",
      "        [0.1658],\n",
      "        [0.2041],\n",
      "        [0.3377],\n",
      "        [0.6119],\n",
      "        [0.1255],\n",
      "        [0.3177],\n",
      "        [0.3357],\n",
      "        [0.3326],\n",
      "        [0.2123],\n",
      "        [0.4621],\n",
      "        [0.1917],\n",
      "        [0.1646],\n",
      "        [0.2739],\n",
      "        [0.4991],\n",
      "        [0.4733],\n",
      "        [0.3889],\n",
      "        [0.5878],\n",
      "        [0.5210],\n",
      "        [0.1488],\n",
      "        [0.5441],\n",
      "        [0.4742],\n",
      "        [0.2329],\n",
      "        [0.5520],\n",
      "        [0.1863],\n",
      "        [0.1991],\n",
      "        [0.4739],\n",
      "        [0.5971],\n",
      "        [0.5223],\n",
      "        [0.5194],\n",
      "        [0.2838],\n",
      "        [0.2627],\n",
      "        [0.1824],\n",
      "        [0.1634],\n",
      "        [0.1000],\n",
      "        [0.4764],\n",
      "        [0.4876],\n",
      "        [0.0583],\n",
      "        [0.5706],\n",
      "        [0.1976],\n",
      "        [0.5290],\n",
      "        [0.6129],\n",
      "        [0.2392],\n",
      "        [0.3729],\n",
      "        [0.4218],\n",
      "        [0.1689],\n",
      "        [0.3307],\n",
      "        [0.2655],\n",
      "        [0.3290],\n",
      "        [0.3698],\n",
      "        [0.4305],\n",
      "        [0.3991],\n",
      "        [0.2265],\n",
      "        [0.4107],\n",
      "        [0.2984],\n",
      "        [0.1103],\n",
      "        [0.2658],\n",
      "        [0.3287],\n",
      "        [0.3793],\n",
      "        [0.3502],\n",
      "        [0.4548],\n",
      "        [0.4757],\n",
      "        [0.5050],\n",
      "        [0.1863],\n",
      "        [0.6287],\n",
      "        [0.7004],\n",
      "        [0.7039],\n",
      "        [0.1842],\n",
      "        [0.4303],\n",
      "        [0.7235],\n",
      "        [0.5674],\n",
      "        [0.6416],\n",
      "        [0.6959],\n",
      "        [0.7116],\n",
      "        [0.6896],\n",
      "        [0.6867],\n",
      "        [0.4437],\n",
      "        [0.7047],\n",
      "        [0.5413],\n",
      "        [0.6161],\n",
      "        [0.5154],\n",
      "        [0.4297],\n",
      "        [0.5410],\n",
      "        [0.4391],\n",
      "        [0.6116],\n",
      "        [0.4316],\n",
      "        [0.4409],\n",
      "        [0.4409],\n",
      "        [0.4409],\n",
      "        [0.4409],\n",
      "        [0.4409],\n",
      "        [0.5348],\n",
      "        [0.4409],\n",
      "        [0.4409],\n",
      "        [0.5746]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0013],\n",
      "        [0.0241],\n",
      "        [0.0115],\n",
      "        [0.0606],\n",
      "        [0.0854],\n",
      "        [0.0477],\n",
      "        [0.0037],\n",
      "        [0.0075],\n",
      "        [0.0050],\n",
      "        [0.0831],\n",
      "        [0.0507],\n",
      "        [0.0035],\n",
      "        [0.0088],\n",
      "        [0.0053],\n",
      "        [0.0635],\n",
      "        [0.0157],\n",
      "        [0.0066],\n",
      "        [0.0040],\n",
      "        [0.0304],\n",
      "        [0.0643],\n",
      "        [0.0816],\n",
      "        [0.0641],\n",
      "        [0.0068],\n",
      "        [0.0775],\n",
      "        [0.0909],\n",
      "        [0.0117],\n",
      "        [0.0492],\n",
      "        [0.0935],\n",
      "        [0.0147],\n",
      "        [0.0607],\n",
      "        [0.0132],\n",
      "        [0.0615],\n",
      "        [0.0334],\n",
      "        [0.0601],\n",
      "        [0.0346],\n",
      "        [0.0318],\n",
      "        [0.0124],\n",
      "        [0.0209],\n",
      "        [0.1123],\n",
      "        [0.0324],\n",
      "        [0.0468],\n",
      "        [0.0682],\n",
      "        [0.0261],\n",
      "        [0.0315],\n",
      "        [0.0390],\n",
      "        [0.0261],\n",
      "        [0.0275],\n",
      "        [0.0332],\n",
      "        [0.0849],\n",
      "        [0.0098],\n",
      "        [0.0636],\n",
      "        [0.0182],\n",
      "        [0.0401],\n",
      "        [0.0314],\n",
      "        [0.0380],\n",
      "        [0.0097],\n",
      "        [0.0453],\n",
      "        [0.1324],\n",
      "        [0.0682],\n",
      "        [0.0400],\n",
      "        [0.0303],\n",
      "        [0.1379],\n",
      "        [0.0341],\n",
      "        [0.0288],\n",
      "        [0.0637],\n",
      "        [0.0283],\n",
      "        [0.0644],\n",
      "        [0.0728],\n",
      "        [0.0730],\n",
      "        [0.0072],\n",
      "        [0.0127],\n",
      "        [0.0604],\n",
      "        [0.1585],\n",
      "        [0.0793],\n",
      "        [0.1294],\n",
      "        [0.1254],\n",
      "        [0.0765],\n",
      "        [0.0781],\n",
      "        [0.1076],\n",
      "        [0.0880],\n",
      "        [0.0784],\n",
      "        [0.0899],\n",
      "        [0.0700],\n",
      "        [0.0824],\n",
      "        [0.1191],\n",
      "        [0.1341],\n",
      "        [0.0813],\n",
      "        [0.0878],\n",
      "        [0.1075],\n",
      "        [0.1103],\n",
      "        [0.1215],\n",
      "        [0.1078],\n",
      "        [0.1164],\n",
      "        [0.1170],\n",
      "        [0.1680],\n",
      "        [0.0383],\n",
      "        [0.1786],\n",
      "        [0.1163],\n",
      "        [0.0106],\n",
      "        [0.0074],\n",
      "        [0.0087],\n",
      "        [0.1351],\n",
      "        [0.2029],\n",
      "        [0.0271],\n",
      "        [0.0609],\n",
      "        [0.0460],\n",
      "        [0.0401],\n",
      "        [0.0368],\n",
      "        [0.0479],\n",
      "        [0.0790],\n",
      "        [0.1279],\n",
      "        [0.0911],\n",
      "        [0.1081],\n",
      "        [0.1116],\n",
      "        [0.1506],\n",
      "        [0.1874],\n",
      "        [0.1702],\n",
      "        [0.1844],\n",
      "        [0.1620],\n",
      "        [0.1921],\n",
      "        [0.2905],\n",
      "        [0.3045],\n",
      "        [0.3075],\n",
      "        [0.3094],\n",
      "        [0.3104],\n",
      "        [0.1947],\n",
      "        [0.3199],\n",
      "        [0.3217],\n",
      "        [0.2287]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0130],\n",
      "        [0.0222],\n",
      "        [0.0040],\n",
      "        [0.0306],\n",
      "        [0.0642],\n",
      "        [0.0281],\n",
      "        [0.0058],\n",
      "        [0.0166],\n",
      "        [0.0029],\n",
      "        [0.0653],\n",
      "        [0.0289],\n",
      "        [0.0143],\n",
      "        [0.0240],\n",
      "        [0.0023],\n",
      "        [0.0441],\n",
      "        [0.0342],\n",
      "        [0.0183],\n",
      "        [0.0151],\n",
      "        [0.0465],\n",
      "        [0.0343],\n",
      "        [0.0551],\n",
      "        [0.0364],\n",
      "        [0.0042],\n",
      "        [0.0625],\n",
      "        [0.0689],\n",
      "        [0.0223],\n",
      "        [0.0316],\n",
      "        [0.0694],\n",
      "        [0.0309],\n",
      "        [0.0324],\n",
      "        [0.0068],\n",
      "        [0.0369],\n",
      "        [0.0170],\n",
      "        [0.0549],\n",
      "        [0.0292],\n",
      "        [0.0356],\n",
      "        [0.0011],\n",
      "        [0.0308],\n",
      "        [0.0939],\n",
      "        [0.0239],\n",
      "        [0.0406],\n",
      "        [0.0564],\n",
      "        [0.0332],\n",
      "        [0.0205],\n",
      "        [0.0234],\n",
      "        [0.0202],\n",
      "        [0.0219],\n",
      "        [0.0411],\n",
      "        [0.0748],\n",
      "        [0.0260],\n",
      "        [0.0582],\n",
      "        [0.0066],\n",
      "        [0.0342],\n",
      "        [0.0267],\n",
      "        [0.0099],\n",
      "        [0.0070],\n",
      "        [0.0341],\n",
      "        [0.1152],\n",
      "        [0.0811],\n",
      "        [0.0315],\n",
      "        [0.0191],\n",
      "        [0.1187],\n",
      "        [0.0334],\n",
      "        [0.0257],\n",
      "        [0.0524],\n",
      "        [0.0342],\n",
      "        [0.0643],\n",
      "        [0.0627],\n",
      "        [0.0749],\n",
      "        [0.0289],\n",
      "        [0.0401],\n",
      "        [0.0617],\n",
      "        [0.1423],\n",
      "        [0.0684],\n",
      "        [0.1148],\n",
      "        [0.0963],\n",
      "        [0.0665],\n",
      "        [0.0925],\n",
      "        [0.1063],\n",
      "        [0.0809],\n",
      "        [0.0946],\n",
      "        [0.0757],\n",
      "        [0.0720],\n",
      "        [0.0926],\n",
      "        [0.1140],\n",
      "        [0.1234],\n",
      "        [0.0851],\n",
      "        [0.1069],\n",
      "        [0.0958],\n",
      "        [0.1105],\n",
      "        [0.1061],\n",
      "        [0.1125],\n",
      "        [0.1369],\n",
      "        [0.1341],\n",
      "        [0.1539],\n",
      "        [0.0362],\n",
      "        [0.1575],\n",
      "        [0.1206],\n",
      "        [0.0015],\n",
      "        [0.0180],\n",
      "        [0.0223],\n",
      "        [0.1376],\n",
      "        [0.1908],\n",
      "        [0.0241],\n",
      "        [0.0498],\n",
      "        [0.0307],\n",
      "        [0.0257],\n",
      "        [0.0175],\n",
      "        [0.0302],\n",
      "        [0.0670],\n",
      "        [0.1203],\n",
      "        [0.0920],\n",
      "        [0.0948],\n",
      "        [0.1007],\n",
      "        [0.1404],\n",
      "        [0.1805],\n",
      "        [0.1590],\n",
      "        [0.1761],\n",
      "        [0.1559],\n",
      "        [0.1847],\n",
      "        [0.2890],\n",
      "        [0.3030],\n",
      "        [0.3060],\n",
      "        [0.3079],\n",
      "        [0.3089],\n",
      "        [0.1821],\n",
      "        [0.3184],\n",
      "        [0.3202],\n",
      "        [0.2213]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 94.5886173248291\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 130\n",
      "剩餘X 資料 torch.Size([30, 18])\n",
      "剩餘Y 資料 torch.Size([30, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.06010686606168747, 29)\n",
      "The second_loss value of k: (0.10326527059078217, 28)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引29，y= tensor([0.8186])\n",
      "目前模型的Data狀態 torch.Size([130, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1066],\n",
      "        [0.3765],\n",
      "        [0.2662],\n",
      "        [0.5719],\n",
      "        [0.5524],\n",
      "        [0.4685],\n",
      "        [0.1324],\n",
      "        [0.1675],\n",
      "        [0.1057],\n",
      "        [0.5107],\n",
      "        [0.5029],\n",
      "        [0.3437],\n",
      "        [0.1092],\n",
      "        [0.1914],\n",
      "        [0.4766],\n",
      "        [0.3569],\n",
      "        [0.3451],\n",
      "        [0.1175],\n",
      "        [0.1871],\n",
      "        [0.5260],\n",
      "        [0.5802],\n",
      "        [0.4903],\n",
      "        [0.3686],\n",
      "        [0.4888],\n",
      "        [0.5615],\n",
      "        [0.3356],\n",
      "        [0.4998],\n",
      "        [0.5261],\n",
      "        [0.2788],\n",
      "        [0.5022],\n",
      "        [0.1553],\n",
      "        [0.5222],\n",
      "        [0.4889],\n",
      "        [0.4931],\n",
      "        [0.1199],\n",
      "        [0.1619],\n",
      "        [0.1906],\n",
      "        [0.3277],\n",
      "        [0.5934],\n",
      "        [0.1170],\n",
      "        [0.3116],\n",
      "        [0.3239],\n",
      "        [0.3255],\n",
      "        [0.2013],\n",
      "        [0.4465],\n",
      "        [0.1976],\n",
      "        [0.1702],\n",
      "        [0.2660],\n",
      "        [0.4889],\n",
      "        [0.4570],\n",
      "        [0.3834],\n",
      "        [0.5629],\n",
      "        [0.5151],\n",
      "        [0.1535],\n",
      "        [0.5160],\n",
      "        [0.4576],\n",
      "        [0.2218],\n",
      "        [0.5348],\n",
      "        [0.1734],\n",
      "        [0.1906],\n",
      "        [0.4627],\n",
      "        [0.5780],\n",
      "        [0.5216],\n",
      "        [0.5163],\n",
      "        [0.2725],\n",
      "        [0.2567],\n",
      "        [0.1824],\n",
      "        [0.1533],\n",
      "        [0.1020],\n",
      "        [0.4546],\n",
      "        [0.4602],\n",
      "        [0.0570],\n",
      "        [0.5544],\n",
      "        [0.1866],\n",
      "        [0.5144],\n",
      "        [0.5838],\n",
      "        [0.2293],\n",
      "        [0.3586],\n",
      "        [0.4204],\n",
      "        [0.1618],\n",
      "        [0.3145],\n",
      "        [0.2513],\n",
      "        [0.3270],\n",
      "        [0.3596],\n",
      "        [0.4254],\n",
      "        [0.3883],\n",
      "        [0.2227],\n",
      "        [0.3916],\n",
      "        [0.2867],\n",
      "        [0.1105],\n",
      "        [0.2504],\n",
      "        [0.3240],\n",
      "        [0.3589],\n",
      "        [0.3331],\n",
      "        [0.4406],\n",
      "        [0.4778],\n",
      "        [0.4838],\n",
      "        [0.1820],\n",
      "        [0.6409],\n",
      "        [0.7109],\n",
      "        [0.7175],\n",
      "        [0.1816],\n",
      "        [0.4182],\n",
      "        [0.7264],\n",
      "        [0.5785],\n",
      "        [0.6569],\n",
      "        [0.7103],\n",
      "        [0.7309],\n",
      "        [0.7073],\n",
      "        [0.6986],\n",
      "        [0.4513],\n",
      "        [0.7039],\n",
      "        [0.5546],\n",
      "        [0.6271],\n",
      "        [0.5256],\n",
      "        [0.4365],\n",
      "        [0.5522],\n",
      "        [0.4475],\n",
      "        [0.6177],\n",
      "        [0.4391],\n",
      "        [0.4394],\n",
      "        [0.4394],\n",
      "        [0.4394],\n",
      "        [0.4394],\n",
      "        [0.4394],\n",
      "        [0.5474],\n",
      "        [0.4394],\n",
      "        [0.4394],\n",
      "        [0.5821],\n",
      "        [0.5734]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0130],\n",
      "        [0.0222],\n",
      "        [0.0040],\n",
      "        [0.0306],\n",
      "        [0.0642],\n",
      "        [0.0281],\n",
      "        [0.0058],\n",
      "        [0.0166],\n",
      "        [0.0029],\n",
      "        [0.0653],\n",
      "        [0.0289],\n",
      "        [0.0143],\n",
      "        [0.0240],\n",
      "        [0.0023],\n",
      "        [0.0441],\n",
      "        [0.0342],\n",
      "        [0.0183],\n",
      "        [0.0151],\n",
      "        [0.0465],\n",
      "        [0.0343],\n",
      "        [0.0551],\n",
      "        [0.0364],\n",
      "        [0.0042],\n",
      "        [0.0625],\n",
      "        [0.0689],\n",
      "        [0.0223],\n",
      "        [0.0316],\n",
      "        [0.0694],\n",
      "        [0.0309],\n",
      "        [0.0324],\n",
      "        [0.0068],\n",
      "        [0.0369],\n",
      "        [0.0170],\n",
      "        [0.0549],\n",
      "        [0.0292],\n",
      "        [0.0356],\n",
      "        [0.0011],\n",
      "        [0.0308],\n",
      "        [0.0939],\n",
      "        [0.0239],\n",
      "        [0.0406],\n",
      "        [0.0564],\n",
      "        [0.0332],\n",
      "        [0.0205],\n",
      "        [0.0234],\n",
      "        [0.0202],\n",
      "        [0.0219],\n",
      "        [0.0411],\n",
      "        [0.0748],\n",
      "        [0.0260],\n",
      "        [0.0582],\n",
      "        [0.0066],\n",
      "        [0.0342],\n",
      "        [0.0267],\n",
      "        [0.0099],\n",
      "        [0.0070],\n",
      "        [0.0341],\n",
      "        [0.1152],\n",
      "        [0.0811],\n",
      "        [0.0315],\n",
      "        [0.0191],\n",
      "        [0.1187],\n",
      "        [0.0334],\n",
      "        [0.0257],\n",
      "        [0.0524],\n",
      "        [0.0342],\n",
      "        [0.0643],\n",
      "        [0.0627],\n",
      "        [0.0749],\n",
      "        [0.0289],\n",
      "        [0.0401],\n",
      "        [0.0617],\n",
      "        [0.1423],\n",
      "        [0.0684],\n",
      "        [0.1148],\n",
      "        [0.0963],\n",
      "        [0.0665],\n",
      "        [0.0925],\n",
      "        [0.1063],\n",
      "        [0.0809],\n",
      "        [0.0946],\n",
      "        [0.0757],\n",
      "        [0.0720],\n",
      "        [0.0926],\n",
      "        [0.1140],\n",
      "        [0.1234],\n",
      "        [0.0851],\n",
      "        [0.1069],\n",
      "        [0.0958],\n",
      "        [0.1105],\n",
      "        [0.1061],\n",
      "        [0.1125],\n",
      "        [0.1369],\n",
      "        [0.1341],\n",
      "        [0.1539],\n",
      "        [0.0362],\n",
      "        [0.1575],\n",
      "        [0.1206],\n",
      "        [0.0015],\n",
      "        [0.0180],\n",
      "        [0.0223],\n",
      "        [0.1376],\n",
      "        [0.1908],\n",
      "        [0.0241],\n",
      "        [0.0498],\n",
      "        [0.0307],\n",
      "        [0.0257],\n",
      "        [0.0175],\n",
      "        [0.0302],\n",
      "        [0.0670],\n",
      "        [0.1203],\n",
      "        [0.0920],\n",
      "        [0.0948],\n",
      "        [0.1007],\n",
      "        [0.1404],\n",
      "        [0.1805],\n",
      "        [0.1590],\n",
      "        [0.1761],\n",
      "        [0.1559],\n",
      "        [0.1847],\n",
      "        [0.2890],\n",
      "        [0.3030],\n",
      "        [0.3060],\n",
      "        [0.3079],\n",
      "        [0.3089],\n",
      "        [0.1821],\n",
      "        [0.3184],\n",
      "        [0.3202],\n",
      "        [0.2213],\n",
      "        [0.2452]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0112],\n",
      "        [0.0307],\n",
      "        [0.0080],\n",
      "        [0.0234],\n",
      "        [0.0661],\n",
      "        [0.0239],\n",
      "        [0.0008],\n",
      "        [0.0162],\n",
      "        [0.0019],\n",
      "        [0.0699],\n",
      "        [0.0245],\n",
      "        [0.0141],\n",
      "        [0.0234],\n",
      "        [0.0054],\n",
      "        [0.0472],\n",
      "        [0.0413],\n",
      "        [0.0194],\n",
      "        [0.0112],\n",
      "        [0.0533],\n",
      "        [0.0264],\n",
      "        [0.0517],\n",
      "        [0.0339],\n",
      "        [0.0121],\n",
      "        [0.0662],\n",
      "        [0.0695],\n",
      "        [0.0217],\n",
      "        [0.0334],\n",
      "        [0.0687],\n",
      "        [0.0300],\n",
      "        [0.0297],\n",
      "        [0.0030],\n",
      "        [0.0372],\n",
      "        [0.0152],\n",
      "        [0.0675],\n",
      "        [0.0368],\n",
      "        [0.0377],\n",
      "        [0.0043],\n",
      "        [0.0304],\n",
      "        [0.0974],\n",
      "        [0.0291],\n",
      "        [0.0465],\n",
      "        [0.0559],\n",
      "        [0.0304],\n",
      "        [0.0200],\n",
      "        [0.0294],\n",
      "        [0.0100],\n",
      "        [0.0119],\n",
      "        [0.0384],\n",
      "        [0.0824],\n",
      "        [0.0248],\n",
      "        [0.0651],\n",
      "        [0.0111],\n",
      "        [0.0460],\n",
      "        [0.0178],\n",
      "        [0.0076],\n",
      "        [0.0090],\n",
      "        [0.0337],\n",
      "        [0.1211],\n",
      "        [0.0857],\n",
      "        [0.0302],\n",
      "        [0.0283],\n",
      "        [0.1223],\n",
      "        [0.0504],\n",
      "        [0.0401],\n",
      "        [0.0528],\n",
      "        [0.0307],\n",
      "        [0.0651],\n",
      "        [0.0666],\n",
      "        [0.0827],\n",
      "        [0.0302],\n",
      "        [0.0458],\n",
      "        [0.0548],\n",
      "        [0.1489],\n",
      "        [0.0727],\n",
      "        [0.1191],\n",
      "        [0.0885],\n",
      "        [0.0673],\n",
      "        [0.0959],\n",
      "        [0.1161],\n",
      "        [0.0873],\n",
      "        [0.0932],\n",
      "        [0.0778],\n",
      "        [0.0639],\n",
      "        [0.0917],\n",
      "        [0.1214],\n",
      "        [0.1240],\n",
      "        [0.0792],\n",
      "        [0.1089],\n",
      "        [0.0955],\n",
      "        [0.1160],\n",
      "        [0.1077],\n",
      "        [0.1069],\n",
      "        [0.1404],\n",
      "        [0.1342],\n",
      "        [0.1527],\n",
      "        [0.0174],\n",
      "        [0.1519],\n",
      "        [0.1167],\n",
      "        [0.0309],\n",
      "        [0.0490],\n",
      "        [0.0557],\n",
      "        [0.1321],\n",
      "        [0.1915],\n",
      "        [0.0011],\n",
      "        [0.0222],\n",
      "        [0.0021],\n",
      "        [0.0084],\n",
      "        [0.0206],\n",
      "        [0.0071],\n",
      "        [0.0353],\n",
      "        [0.0978],\n",
      "        [0.0699],\n",
      "        [0.0658],\n",
      "        [0.0709],\n",
      "        [0.1152],\n",
      "        [0.1590],\n",
      "        [0.1329],\n",
      "        [0.1527],\n",
      "        [0.1291],\n",
      "        [0.1625],\n",
      "        [0.2878],\n",
      "        [0.3018],\n",
      "        [0.3048],\n",
      "        [0.3066],\n",
      "        [0.3077],\n",
      "        [0.1543],\n",
      "        [0.3171],\n",
      "        [0.3189],\n",
      "        [0.1936],\n",
      "        [0.2199]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 94.82985973358154\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 131\n",
      "剩餘X 資料 torch.Size([29, 18])\n",
      "剩餘Y 資料 torch.Size([29, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10247065871953964, 28)\n",
      "The second_loss value of k: (0.10723937302827835, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引28，y= tensor([0.1180])\n",
      "目前模型的Data狀態 torch.Size([131, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1084],\n",
      "        [0.3850],\n",
      "        [0.2702],\n",
      "        [0.5647],\n",
      "        [0.5544],\n",
      "        [0.4643],\n",
      "        [0.1374],\n",
      "        [0.1679],\n",
      "        [0.1105],\n",
      "        [0.5153],\n",
      "        [0.4985],\n",
      "        [0.3439],\n",
      "        [0.1097],\n",
      "        [0.1946],\n",
      "        [0.4798],\n",
      "        [0.3498],\n",
      "        [0.3440],\n",
      "        [0.1214],\n",
      "        [0.1803],\n",
      "        [0.5181],\n",
      "        [0.5767],\n",
      "        [0.4878],\n",
      "        [0.3766],\n",
      "        [0.4925],\n",
      "        [0.5621],\n",
      "        [0.3362],\n",
      "        [0.5015],\n",
      "        [0.5253],\n",
      "        [0.2797],\n",
      "        [0.4995],\n",
      "        [0.1651],\n",
      "        [0.5226],\n",
      "        [0.4871],\n",
      "        [0.5058],\n",
      "        [0.1275],\n",
      "        [0.1598],\n",
      "        [0.1874],\n",
      "        [0.3282],\n",
      "        [0.5969],\n",
      "        [0.1222],\n",
      "        [0.3175],\n",
      "        [0.3233],\n",
      "        [0.3283],\n",
      "        [0.2008],\n",
      "        [0.4526],\n",
      "        [0.2078],\n",
      "        [0.1802],\n",
      "        [0.2687],\n",
      "        [0.4966],\n",
      "        [0.4583],\n",
      "        [0.3904],\n",
      "        [0.5585],\n",
      "        [0.5269],\n",
      "        [0.1624],\n",
      "        [0.5137],\n",
      "        [0.4555],\n",
      "        [0.2213],\n",
      "        [0.5406],\n",
      "        [0.1688],\n",
      "        [0.1893],\n",
      "        [0.4719],\n",
      "        [0.5815],\n",
      "        [0.5386],\n",
      "        [0.5307],\n",
      "        [0.2729],\n",
      "        [0.2602],\n",
      "        [0.1832],\n",
      "        [0.1572],\n",
      "        [0.1097],\n",
      "        [0.4533],\n",
      "        [0.4545],\n",
      "        [0.0640],\n",
      "        [0.5610],\n",
      "        [0.1909],\n",
      "        [0.5187],\n",
      "        [0.5759],\n",
      "        [0.2301],\n",
      "        [0.3552],\n",
      "        [0.4303],\n",
      "        [0.1682],\n",
      "        [0.3160],\n",
      "        [0.2534],\n",
      "        [0.3350],\n",
      "        [0.3605],\n",
      "        [0.4329],\n",
      "        [0.3890],\n",
      "        [0.2286],\n",
      "        [0.3896],\n",
      "        [0.2863],\n",
      "        [0.1160],\n",
      "        [0.2520],\n",
      "        [0.3296],\n",
      "        [0.3554],\n",
      "        [0.3330],\n",
      "        [0.4395],\n",
      "        [0.4967],\n",
      "        [0.4783],\n",
      "        [0.1860],\n",
      "        [0.6703],\n",
      "        [0.7419],\n",
      "        [0.7509],\n",
      "        [0.1872],\n",
      "        [0.4188],\n",
      "        [0.7517],\n",
      "        [0.6061],\n",
      "        [0.6897],\n",
      "        [0.7444],\n",
      "        [0.7690],\n",
      "        [0.7446],\n",
      "        [0.7303],\n",
      "        [0.4738],\n",
      "        [0.7260],\n",
      "        [0.5836],\n",
      "        [0.6568],\n",
      "        [0.5508],\n",
      "        [0.4580],\n",
      "        [0.5783],\n",
      "        [0.4709],\n",
      "        [0.6445],\n",
      "        [0.4613],\n",
      "        [0.4381],\n",
      "        [0.4381],\n",
      "        [0.4381],\n",
      "        [0.4381],\n",
      "        [0.4381],\n",
      "        [0.5752],\n",
      "        [0.4381],\n",
      "        [0.4381],\n",
      "        [0.6098],\n",
      "        [0.5987],\n",
      "        [0.4381]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0112],\n",
      "        [0.0307],\n",
      "        [0.0080],\n",
      "        [0.0234],\n",
      "        [0.0661],\n",
      "        [0.0239],\n",
      "        [0.0008],\n",
      "        [0.0162],\n",
      "        [0.0019],\n",
      "        [0.0699],\n",
      "        [0.0245],\n",
      "        [0.0141],\n",
      "        [0.0234],\n",
      "        [0.0054],\n",
      "        [0.0472],\n",
      "        [0.0413],\n",
      "        [0.0194],\n",
      "        [0.0112],\n",
      "        [0.0533],\n",
      "        [0.0264],\n",
      "        [0.0517],\n",
      "        [0.0339],\n",
      "        [0.0121],\n",
      "        [0.0662],\n",
      "        [0.0695],\n",
      "        [0.0217],\n",
      "        [0.0334],\n",
      "        [0.0687],\n",
      "        [0.0300],\n",
      "        [0.0297],\n",
      "        [0.0030],\n",
      "        [0.0372],\n",
      "        [0.0152],\n",
      "        [0.0675],\n",
      "        [0.0368],\n",
      "        [0.0377],\n",
      "        [0.0043],\n",
      "        [0.0304],\n",
      "        [0.0974],\n",
      "        [0.0291],\n",
      "        [0.0465],\n",
      "        [0.0559],\n",
      "        [0.0304],\n",
      "        [0.0200],\n",
      "        [0.0294],\n",
      "        [0.0100],\n",
      "        [0.0119],\n",
      "        [0.0384],\n",
      "        [0.0824],\n",
      "        [0.0248],\n",
      "        [0.0651],\n",
      "        [0.0111],\n",
      "        [0.0460],\n",
      "        [0.0178],\n",
      "        [0.0076],\n",
      "        [0.0090],\n",
      "        [0.0337],\n",
      "        [0.1211],\n",
      "        [0.0857],\n",
      "        [0.0302],\n",
      "        [0.0283],\n",
      "        [0.1223],\n",
      "        [0.0504],\n",
      "        [0.0401],\n",
      "        [0.0528],\n",
      "        [0.0307],\n",
      "        [0.0651],\n",
      "        [0.0666],\n",
      "        [0.0827],\n",
      "        [0.0302],\n",
      "        [0.0458],\n",
      "        [0.0548],\n",
      "        [0.1489],\n",
      "        [0.0727],\n",
      "        [0.1191],\n",
      "        [0.0885],\n",
      "        [0.0673],\n",
      "        [0.0959],\n",
      "        [0.1161],\n",
      "        [0.0873],\n",
      "        [0.0932],\n",
      "        [0.0778],\n",
      "        [0.0639],\n",
      "        [0.0917],\n",
      "        [0.1214],\n",
      "        [0.1240],\n",
      "        [0.0792],\n",
      "        [0.1089],\n",
      "        [0.0955],\n",
      "        [0.1160],\n",
      "        [0.1077],\n",
      "        [0.1069],\n",
      "        [0.1404],\n",
      "        [0.1342],\n",
      "        [0.1527],\n",
      "        [0.0174],\n",
      "        [0.1519],\n",
      "        [0.1167],\n",
      "        [0.0309],\n",
      "        [0.0490],\n",
      "        [0.0557],\n",
      "        [0.1321],\n",
      "        [0.1915],\n",
      "        [0.0011],\n",
      "        [0.0222],\n",
      "        [0.0021],\n",
      "        [0.0084],\n",
      "        [0.0206],\n",
      "        [0.0071],\n",
      "        [0.0353],\n",
      "        [0.0978],\n",
      "        [0.0699],\n",
      "        [0.0658],\n",
      "        [0.0709],\n",
      "        [0.1152],\n",
      "        [0.1590],\n",
      "        [0.1329],\n",
      "        [0.1527],\n",
      "        [0.1291],\n",
      "        [0.1625],\n",
      "        [0.2878],\n",
      "        [0.3018],\n",
      "        [0.3048],\n",
      "        [0.3066],\n",
      "        [0.3077],\n",
      "        [0.1543],\n",
      "        [0.3171],\n",
      "        [0.3189],\n",
      "        [0.1936],\n",
      "        [0.2199],\n",
      "        [0.3201]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0135],\n",
      "        [    0.0284],\n",
      "        [    0.0036],\n",
      "        [    0.0059],\n",
      "        [    0.0534],\n",
      "        [    0.0077],\n",
      "        [    0.0001],\n",
      "        [    0.0198],\n",
      "        [    0.0032],\n",
      "        [    0.0603],\n",
      "        [    0.0074],\n",
      "        [    0.0215],\n",
      "        [    0.0262],\n",
      "        [    0.0048],\n",
      "        [    0.0373],\n",
      "        [    0.0536],\n",
      "        [    0.0273],\n",
      "        [    0.0112],\n",
      "        [    0.0613],\n",
      "        [    0.0090],\n",
      "        [    0.0362],\n",
      "        [    0.0179],\n",
      "        [    0.0098],\n",
      "        [    0.0570],\n",
      "        [    0.0569],\n",
      "        [    0.0286],\n",
      "        [    0.0223],\n",
      "        [    0.0545],\n",
      "        [    0.0342],\n",
      "        [    0.0131],\n",
      "        [    0.0070],\n",
      "        [    0.0226],\n",
      "        [    0.0003],\n",
      "        [    0.0653],\n",
      "        [    0.0394],\n",
      "        [    0.0399],\n",
      "        [    0.0103],\n",
      "        [    0.0374],\n",
      "        [    0.0867],\n",
      "        [    0.0297],\n",
      "        [    0.0423],\n",
      "        [    0.0448],\n",
      "        [    0.0358],\n",
      "        [    0.0147],\n",
      "        [    0.0219],\n",
      "        [    0.0059],\n",
      "        [    0.0081],\n",
      "        [    0.0435],\n",
      "        [    0.0787],\n",
      "        [    0.0311],\n",
      "        [    0.0605],\n",
      "        [    0.0257],\n",
      "        [    0.0433],\n",
      "        [    0.0149],\n",
      "        [    0.0092],\n",
      "        [    0.0237],\n",
      "        [    0.0277],\n",
      "        [    0.1116],\n",
      "        [    0.0919],\n",
      "        [    0.0265],\n",
      "        [    0.0230],\n",
      "        [    0.1109],\n",
      "        [    0.0510],\n",
      "        [    0.0390],\n",
      "        [    0.0461],\n",
      "        [    0.0375],\n",
      "        [    0.0650],\n",
      "        [    0.0661],\n",
      "        [    0.0849],\n",
      "        [    0.0432],\n",
      "        [    0.0618],\n",
      "        [    0.0506],\n",
      "        [    0.1397],\n",
      "        [    0.0725],\n",
      "        [    0.1108],\n",
      "        [    0.0713],\n",
      "        [    0.0620],\n",
      "        [    0.1061],\n",
      "        [    0.1135],\n",
      "        [    0.0885],\n",
      "        [    0.0973],\n",
      "        [    0.0752],\n",
      "        [    0.0657],\n",
      "        [    0.0992],\n",
      "        [    0.1165],\n",
      "        [    0.1128],\n",
      "        [    0.0818],\n",
      "        [    0.1158],\n",
      "        [    0.0886],\n",
      "        [    0.1168],\n",
      "        [    0.1047],\n",
      "        [    0.1106],\n",
      "        [    0.1480],\n",
      "        [    0.1391],\n",
      "        [    0.1396],\n",
      "        [    0.0150],\n",
      "        [    0.1347],\n",
      "        [    0.1215],\n",
      "        [    0.0356],\n",
      "        [    0.0515],\n",
      "        [    0.0595],\n",
      "        [    0.1347],\n",
      "        [    0.1796],\n",
      "        [    0.0005],\n",
      "        [    0.0177],\n",
      "        [    0.0085],\n",
      "        [    0.0145],\n",
      "        [    0.0288],\n",
      "        [    0.0140],\n",
      "        [    0.0305],\n",
      "        [    0.0926],\n",
      "        [    0.0735],\n",
      "        [    0.0598],\n",
      "        [    0.0672],\n",
      "        [    0.1097],\n",
      "        [    0.1546],\n",
      "        [    0.1274],\n",
      "        [    0.1476],\n",
      "        [    0.1271],\n",
      "        [    0.1579],\n",
      "        [    0.2867],\n",
      "        [    0.3008],\n",
      "        [    0.3037],\n",
      "        [    0.3056],\n",
      "        [    0.3066],\n",
      "        [    0.1484],\n",
      "        [    0.3161],\n",
      "        [    0.3179],\n",
      "        [    0.1909],\n",
      "        [    0.2193],\n",
      "        [    0.3191]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 95.07187604904175\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 132\n",
      "剩餘X 資料 torch.Size([28, 18])\n",
      "剩餘Y 資料 torch.Size([28, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10657718777656555, 15)\n",
      "The second_loss value of k: (0.10775686055421829, 18)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引15，y= tensor([0.1107])\n",
      "目前模型的Data狀態 torch.Size([132, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1061],\n",
      "        [0.3826],\n",
      "        [0.2658],\n",
      "        [0.5472],\n",
      "        [0.5416],\n",
      "        [0.4482],\n",
      "        [0.1384],\n",
      "        [0.1643],\n",
      "        [0.1118],\n",
      "        [0.5058],\n",
      "        [0.4814],\n",
      "        [0.3365],\n",
      "        [0.1069],\n",
      "        [0.1940],\n",
      "        [0.4698],\n",
      "        [0.3375],\n",
      "        [0.3362],\n",
      "        [0.1213],\n",
      "        [0.1723],\n",
      "        [0.5006],\n",
      "        [0.5613],\n",
      "        [0.4719],\n",
      "        [0.3743],\n",
      "        [0.4833],\n",
      "        [0.5494],\n",
      "        [0.3293],\n",
      "        [0.4905],\n",
      "        [0.5111],\n",
      "        [0.2755],\n",
      "        [0.4830],\n",
      "        [0.1690],\n",
      "        [0.5079],\n",
      "        [0.4722],\n",
      "        [0.5035],\n",
      "        [0.1302],\n",
      "        [0.1576],\n",
      "        [0.1814],\n",
      "        [0.3211],\n",
      "        [0.5862],\n",
      "        [0.1228],\n",
      "        [0.3133],\n",
      "        [0.3122],\n",
      "        [0.3229],\n",
      "        [0.1956],\n",
      "        [0.4450],\n",
      "        [0.2120],\n",
      "        [0.1840],\n",
      "        [0.2636],\n",
      "        [0.4928],\n",
      "        [0.4520],\n",
      "        [0.3857],\n",
      "        [0.5438],\n",
      "        [0.5241],\n",
      "        [0.1653],\n",
      "        [0.4969],\n",
      "        [0.4408],\n",
      "        [0.2154],\n",
      "        [0.5312],\n",
      "        [0.1626],\n",
      "        [0.1856],\n",
      "        [0.4666],\n",
      "        [0.5701],\n",
      "        [0.5393],\n",
      "        [0.5296],\n",
      "        [0.2662],\n",
      "        [0.2535],\n",
      "        [0.1831],\n",
      "        [0.1567],\n",
      "        [0.1120],\n",
      "        [0.4404],\n",
      "        [0.4385],\n",
      "        [0.0681],\n",
      "        [0.5518],\n",
      "        [0.1908],\n",
      "        [0.5105],\n",
      "        [0.5587],\n",
      "        [0.2248],\n",
      "        [0.3450],\n",
      "        [0.4277],\n",
      "        [0.1694],\n",
      "        [0.3119],\n",
      "        [0.2508],\n",
      "        [0.3333],\n",
      "        [0.3530],\n",
      "        [0.4279],\n",
      "        [0.3778],\n",
      "        [0.2260],\n",
      "        [0.3827],\n",
      "        [0.2794],\n",
      "        [0.1168],\n",
      "        [0.2491],\n",
      "        [0.3260],\n",
      "        [0.3478],\n",
      "        [0.3280],\n",
      "        [0.4263],\n",
      "        [0.4991],\n",
      "        [0.4610],\n",
      "        [0.1812],\n",
      "        [0.6750],\n",
      "        [0.7444],\n",
      "        [0.7547],\n",
      "        [0.1846],\n",
      "        [0.4070],\n",
      "        [0.7500],\n",
      "        [0.6106],\n",
      "        [0.6961],\n",
      "        [0.7505],\n",
      "        [0.7772],\n",
      "        [0.7515],\n",
      "        [0.7352],\n",
      "        [0.4790],\n",
      "        [0.7224],\n",
      "        [0.5896],\n",
      "        [0.6605],\n",
      "        [0.5563],\n",
      "        [0.4624],\n",
      "        [0.5838],\n",
      "        [0.4759],\n",
      "        [0.6465],\n",
      "        [0.4659],\n",
      "        [0.4371],\n",
      "        [0.4371],\n",
      "        [0.4371],\n",
      "        [0.4371],\n",
      "        [0.4371],\n",
      "        [0.5812],\n",
      "        [0.4371],\n",
      "        [0.4371],\n",
      "        [0.6125],\n",
      "        [0.5993],\n",
      "        [0.4371],\n",
      "        [0.4371]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0135],\n",
      "        [    0.0284],\n",
      "        [    0.0036],\n",
      "        [    0.0059],\n",
      "        [    0.0534],\n",
      "        [    0.0077],\n",
      "        [    0.0001],\n",
      "        [    0.0198],\n",
      "        [    0.0032],\n",
      "        [    0.0603],\n",
      "        [    0.0074],\n",
      "        [    0.0215],\n",
      "        [    0.0262],\n",
      "        [    0.0048],\n",
      "        [    0.0373],\n",
      "        [    0.0536],\n",
      "        [    0.0273],\n",
      "        [    0.0112],\n",
      "        [    0.0613],\n",
      "        [    0.0090],\n",
      "        [    0.0362],\n",
      "        [    0.0179],\n",
      "        [    0.0098],\n",
      "        [    0.0570],\n",
      "        [    0.0569],\n",
      "        [    0.0286],\n",
      "        [    0.0223],\n",
      "        [    0.0545],\n",
      "        [    0.0342],\n",
      "        [    0.0131],\n",
      "        [    0.0070],\n",
      "        [    0.0226],\n",
      "        [    0.0003],\n",
      "        [    0.0653],\n",
      "        [    0.0394],\n",
      "        [    0.0399],\n",
      "        [    0.0103],\n",
      "        [    0.0374],\n",
      "        [    0.0867],\n",
      "        [    0.0297],\n",
      "        [    0.0423],\n",
      "        [    0.0448],\n",
      "        [    0.0358],\n",
      "        [    0.0147],\n",
      "        [    0.0219],\n",
      "        [    0.0059],\n",
      "        [    0.0081],\n",
      "        [    0.0435],\n",
      "        [    0.0787],\n",
      "        [    0.0311],\n",
      "        [    0.0605],\n",
      "        [    0.0257],\n",
      "        [    0.0433],\n",
      "        [    0.0149],\n",
      "        [    0.0092],\n",
      "        [    0.0237],\n",
      "        [    0.0277],\n",
      "        [    0.1116],\n",
      "        [    0.0919],\n",
      "        [    0.0265],\n",
      "        [    0.0230],\n",
      "        [    0.1109],\n",
      "        [    0.0510],\n",
      "        [    0.0390],\n",
      "        [    0.0461],\n",
      "        [    0.0375],\n",
      "        [    0.0650],\n",
      "        [    0.0661],\n",
      "        [    0.0849],\n",
      "        [    0.0432],\n",
      "        [    0.0618],\n",
      "        [    0.0506],\n",
      "        [    0.1397],\n",
      "        [    0.0725],\n",
      "        [    0.1108],\n",
      "        [    0.0713],\n",
      "        [    0.0620],\n",
      "        [    0.1061],\n",
      "        [    0.1135],\n",
      "        [    0.0885],\n",
      "        [    0.0973],\n",
      "        [    0.0752],\n",
      "        [    0.0657],\n",
      "        [    0.0992],\n",
      "        [    0.1165],\n",
      "        [    0.1128],\n",
      "        [    0.0818],\n",
      "        [    0.1158],\n",
      "        [    0.0886],\n",
      "        [    0.1168],\n",
      "        [    0.1047],\n",
      "        [    0.1106],\n",
      "        [    0.1480],\n",
      "        [    0.1391],\n",
      "        [    0.1396],\n",
      "        [    0.0150],\n",
      "        [    0.1347],\n",
      "        [    0.1215],\n",
      "        [    0.0356],\n",
      "        [    0.0515],\n",
      "        [    0.0595],\n",
      "        [    0.1347],\n",
      "        [    0.1796],\n",
      "        [    0.0005],\n",
      "        [    0.0177],\n",
      "        [    0.0085],\n",
      "        [    0.0145],\n",
      "        [    0.0288],\n",
      "        [    0.0140],\n",
      "        [    0.0305],\n",
      "        [    0.0926],\n",
      "        [    0.0735],\n",
      "        [    0.0598],\n",
      "        [    0.0672],\n",
      "        [    0.1097],\n",
      "        [    0.1546],\n",
      "        [    0.1274],\n",
      "        [    0.1476],\n",
      "        [    0.1271],\n",
      "        [    0.1579],\n",
      "        [    0.2867],\n",
      "        [    0.3008],\n",
      "        [    0.3037],\n",
      "        [    0.3056],\n",
      "        [    0.3066],\n",
      "        [    0.1484],\n",
      "        [    0.3161],\n",
      "        [    0.3179],\n",
      "        [    0.1909],\n",
      "        [    0.2193],\n",
      "        [    0.3191],\n",
      "        [    0.3265]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 69\n",
      "Number of shrink: 31\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0106],\n",
      "        [0.0340],\n",
      "        [0.0070],\n",
      "        [0.0004],\n",
      "        [0.0509],\n",
      "        [0.0026],\n",
      "        [0.0059],\n",
      "        [0.0163],\n",
      "        [0.0091],\n",
      "        [0.0604],\n",
      "        [0.0015],\n",
      "        [0.0210],\n",
      "        [0.0239],\n",
      "        [0.0098],\n",
      "        [0.0367],\n",
      "        [0.0567],\n",
      "        [0.0268],\n",
      "        [0.0065],\n",
      "        [0.0615],\n",
      "        [0.0028],\n",
      "        [0.0318],\n",
      "        [0.0129],\n",
      "        [0.0154],\n",
      "        [0.0569],\n",
      "        [0.0547],\n",
      "        [0.0278],\n",
      "        [0.0214],\n",
      "        [0.0508],\n",
      "        [0.0324],\n",
      "        [0.0075],\n",
      "        [0.0154],\n",
      "        [0.0186],\n",
      "        [0.0037],\n",
      "        [0.0711],\n",
      "        [0.0461],\n",
      "        [0.0404],\n",
      "        [0.0087],\n",
      "        [0.0366],\n",
      "        [0.0861],\n",
      "        [0.0350],\n",
      "        [0.0463],\n",
      "        [0.0426],\n",
      "        [0.0338],\n",
      "        [0.0171],\n",
      "        [0.0230],\n",
      "        [0.0029],\n",
      "        [0.0004],\n",
      "        [0.0417],\n",
      "        [0.0820],\n",
      "        [0.0302],\n",
      "        [0.0645],\n",
      "        [0.0298],\n",
      "        [0.0485],\n",
      "        [0.0070],\n",
      "        [0.0148],\n",
      "        [0.0276],\n",
      "        [0.0296],\n",
      "        [0.1119],\n",
      "        [0.0909],\n",
      "        [0.0291],\n",
      "        [0.0262],\n",
      "        [0.1097],\n",
      "        [0.0592],\n",
      "        [0.0457],\n",
      "        [0.0478],\n",
      "        [0.0363],\n",
      "        [0.0660],\n",
      "        [0.0706],\n",
      "        [0.0916],\n",
      "        [0.0457],\n",
      "        [0.0669],\n",
      "        [0.0433],\n",
      "        [0.1402],\n",
      "        [0.0775],\n",
      "        [0.1110],\n",
      "        [0.0652],\n",
      "        [0.0645],\n",
      "        [0.1074],\n",
      "        [0.1194],\n",
      "        [0.0942],\n",
      "        [0.0951],\n",
      "        [0.0781],\n",
      "        [0.0600],\n",
      "        [0.0982],\n",
      "        [0.1206],\n",
      "        [0.1108],\n",
      "        [0.0781],\n",
      "        [0.1157],\n",
      "        [0.0901],\n",
      "        [0.1223],\n",
      "        [0.1072],\n",
      "        [0.1065],\n",
      "        [0.1484],\n",
      "        [0.1374],\n",
      "        [0.1365],\n",
      "        [0.0056],\n",
      "        [0.1286],\n",
      "        [0.1191],\n",
      "        [0.0484],\n",
      "        [0.0638],\n",
      "        [0.0727],\n",
      "        [0.1311],\n",
      "        [0.1772],\n",
      "        [0.0086],\n",
      "        [0.0055],\n",
      "        [0.0229],\n",
      "        [0.0290],\n",
      "        [0.0455],\n",
      "        [0.0296],\n",
      "        [0.0172],\n",
      "        [0.0813],\n",
      "        [0.0662],\n",
      "        [0.0466],\n",
      "        [0.0548],\n",
      "        [0.0972],\n",
      "        [0.1439],\n",
      "        [0.1148],\n",
      "        [0.1358],\n",
      "        [0.1164],\n",
      "        [0.1467],\n",
      "        [0.2859],\n",
      "        [0.2999],\n",
      "        [0.3029],\n",
      "        [0.3048],\n",
      "        [0.3058],\n",
      "        [0.1354],\n",
      "        [0.3153],\n",
      "        [0.3171],\n",
      "        [0.1790],\n",
      "        [0.2094],\n",
      "        [0.3183],\n",
      "        [0.3256]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 95.31836247444153\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 133\n",
      "剩餘X 資料 torch.Size([27, 18])\n",
      "剩餘Y 資料 torch.Size([27, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10721218585968018, 17)\n",
      "The second_loss value of k: (0.10857599973678589, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引17，y= tensor([0.1089])\n",
      "目前模型的Data狀態 torch.Size([133, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1090],\n",
      "        [0.3883],\n",
      "        [0.2692],\n",
      "        [0.5409],\n",
      "        [0.5391],\n",
      "        [0.4430],\n",
      "        [0.1442],\n",
      "        [0.1678],\n",
      "        [0.1177],\n",
      "        [0.5059],\n",
      "        [0.4755],\n",
      "        [0.3370],\n",
      "        [0.1093],\n",
      "        [0.1989],\n",
      "        [0.4693],\n",
      "        [0.3344],\n",
      "        [0.3367],\n",
      "        [0.1261],\n",
      "        [0.1721],\n",
      "        [0.4945],\n",
      "        [0.5569],\n",
      "        [0.4668],\n",
      "        [0.3798],\n",
      "        [0.4833],\n",
      "        [0.5473],\n",
      "        [0.3301],\n",
      "        [0.4896],\n",
      "        [0.5075],\n",
      "        [0.2773],\n",
      "        [0.4774],\n",
      "        [0.1774],\n",
      "        [0.5039],\n",
      "        [0.4683],\n",
      "        [0.5093],\n",
      "        [0.1369],\n",
      "        [0.1572],\n",
      "        [0.1830],\n",
      "        [0.3220],\n",
      "        [0.5857],\n",
      "        [0.1281],\n",
      "        [0.3173],\n",
      "        [0.3100],\n",
      "        [0.3250],\n",
      "        [0.1979],\n",
      "        [0.4462],\n",
      "        [0.2207],\n",
      "        [0.1925],\n",
      "        [0.2654],\n",
      "        [0.4961],\n",
      "        [0.4529],\n",
      "        [0.3898],\n",
      "        [0.5397],\n",
      "        [0.5294],\n",
      "        [0.1732],\n",
      "        [0.4913],\n",
      "        [0.4370],\n",
      "        [0.2172],\n",
      "        [0.5314],\n",
      "        [0.1637],\n",
      "        [0.1882],\n",
      "        [0.4698],\n",
      "        [0.5689],\n",
      "        [0.5475],\n",
      "        [0.5363],\n",
      "        [0.2679],\n",
      "        [0.2546],\n",
      "        [0.1841],\n",
      "        [0.1612],\n",
      "        [0.1187],\n",
      "        [0.4379],\n",
      "        [0.4335],\n",
      "        [0.0754],\n",
      "        [0.5523],\n",
      "        [0.1957],\n",
      "        [0.5106],\n",
      "        [0.5527],\n",
      "        [0.2272],\n",
      "        [0.3436],\n",
      "        [0.4335],\n",
      "        [0.1751],\n",
      "        [0.3140],\n",
      "        [0.2537],\n",
      "        [0.3389],\n",
      "        [0.3540],\n",
      "        [0.4320],\n",
      "        [0.3758],\n",
      "        [0.2297],\n",
      "        [0.3828],\n",
      "        [0.2810],\n",
      "        [0.1223],\n",
      "        [0.2516],\n",
      "        [0.3300],\n",
      "        [0.3474],\n",
      "        [0.3298],\n",
      "        [0.4233],\n",
      "        [0.5084],\n",
      "        [0.4550],\n",
      "        [0.1836],\n",
      "        [0.6877],\n",
      "        [0.7568],\n",
      "        [0.7679],\n",
      "        [0.1882],\n",
      "        [0.4046],\n",
      "        [0.7591],\n",
      "        [0.6228],\n",
      "        [0.7105],\n",
      "        [0.7650],\n",
      "        [0.7938],\n",
      "        [0.7671],\n",
      "        [0.7485],\n",
      "        [0.4903],\n",
      "        [0.7297],\n",
      "        [0.6028],\n",
      "        [0.6729],\n",
      "        [0.5688],\n",
      "        [0.4731],\n",
      "        [0.5964],\n",
      "        [0.4877],\n",
      "        [0.6573],\n",
      "        [0.4770],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.5941],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.6243],\n",
      "        [0.6092],\n",
      "        [0.4363],\n",
      "        [0.4363],\n",
      "        [0.4363]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0106],\n",
      "        [0.0340],\n",
      "        [0.0070],\n",
      "        [0.0004],\n",
      "        [0.0509],\n",
      "        [0.0026],\n",
      "        [0.0059],\n",
      "        [0.0163],\n",
      "        [0.0091],\n",
      "        [0.0604],\n",
      "        [0.0015],\n",
      "        [0.0210],\n",
      "        [0.0239],\n",
      "        [0.0098],\n",
      "        [0.0367],\n",
      "        [0.0567],\n",
      "        [0.0268],\n",
      "        [0.0065],\n",
      "        [0.0615],\n",
      "        [0.0028],\n",
      "        [0.0318],\n",
      "        [0.0129],\n",
      "        [0.0154],\n",
      "        [0.0569],\n",
      "        [0.0547],\n",
      "        [0.0278],\n",
      "        [0.0214],\n",
      "        [0.0508],\n",
      "        [0.0324],\n",
      "        [0.0075],\n",
      "        [0.0154],\n",
      "        [0.0186],\n",
      "        [0.0037],\n",
      "        [0.0711],\n",
      "        [0.0461],\n",
      "        [0.0404],\n",
      "        [0.0087],\n",
      "        [0.0366],\n",
      "        [0.0861],\n",
      "        [0.0350],\n",
      "        [0.0463],\n",
      "        [0.0426],\n",
      "        [0.0338],\n",
      "        [0.0171],\n",
      "        [0.0230],\n",
      "        [0.0029],\n",
      "        [0.0004],\n",
      "        [0.0417],\n",
      "        [0.0820],\n",
      "        [0.0302],\n",
      "        [0.0645],\n",
      "        [0.0298],\n",
      "        [0.0485],\n",
      "        [0.0070],\n",
      "        [0.0148],\n",
      "        [0.0276],\n",
      "        [0.0296],\n",
      "        [0.1119],\n",
      "        [0.0909],\n",
      "        [0.0291],\n",
      "        [0.0262],\n",
      "        [0.1097],\n",
      "        [0.0592],\n",
      "        [0.0457],\n",
      "        [0.0478],\n",
      "        [0.0363],\n",
      "        [0.0660],\n",
      "        [0.0706],\n",
      "        [0.0916],\n",
      "        [0.0457],\n",
      "        [0.0669],\n",
      "        [0.0433],\n",
      "        [0.1402],\n",
      "        [0.0775],\n",
      "        [0.1110],\n",
      "        [0.0652],\n",
      "        [0.0645],\n",
      "        [0.1074],\n",
      "        [0.1194],\n",
      "        [0.0942],\n",
      "        [0.0951],\n",
      "        [0.0781],\n",
      "        [0.0600],\n",
      "        [0.0982],\n",
      "        [0.1206],\n",
      "        [0.1108],\n",
      "        [0.0781],\n",
      "        [0.1157],\n",
      "        [0.0901],\n",
      "        [0.1223],\n",
      "        [0.1072],\n",
      "        [0.1065],\n",
      "        [0.1484],\n",
      "        [0.1374],\n",
      "        [0.1365],\n",
      "        [0.0056],\n",
      "        [0.1286],\n",
      "        [0.1191],\n",
      "        [0.0484],\n",
      "        [0.0638],\n",
      "        [0.0727],\n",
      "        [0.1311],\n",
      "        [0.1772],\n",
      "        [0.0086],\n",
      "        [0.0055],\n",
      "        [0.0229],\n",
      "        [0.0290],\n",
      "        [0.0455],\n",
      "        [0.0296],\n",
      "        [0.0172],\n",
      "        [0.0813],\n",
      "        [0.0662],\n",
      "        [0.0466],\n",
      "        [0.0548],\n",
      "        [0.0972],\n",
      "        [0.1439],\n",
      "        [0.1148],\n",
      "        [0.1358],\n",
      "        [0.1164],\n",
      "        [0.1467],\n",
      "        [0.2859],\n",
      "        [0.2999],\n",
      "        [0.3029],\n",
      "        [0.3048],\n",
      "        [0.3058],\n",
      "        [0.1354],\n",
      "        [0.3153],\n",
      "        [0.3171],\n",
      "        [0.1790],\n",
      "        [0.2094],\n",
      "        [0.3183],\n",
      "        [0.3256],\n",
      "        [0.3274]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0140],\n",
      "        [0.0335],\n",
      "        [0.0047],\n",
      "        [0.0121],\n",
      "        [0.0416],\n",
      "        [0.0087],\n",
      "        [0.0050],\n",
      "        [0.0181],\n",
      "        [0.0083],\n",
      "        [0.0533],\n",
      "        [0.0109],\n",
      "        [0.0255],\n",
      "        [0.0273],\n",
      "        [0.0089],\n",
      "        [0.0292],\n",
      "        [0.0631],\n",
      "        [0.0311],\n",
      "        [0.0084],\n",
      "        [0.0654],\n",
      "        [0.0081],\n",
      "        [0.0214],\n",
      "        [0.0020],\n",
      "        [0.0149],\n",
      "        [0.0502],\n",
      "        [0.0458],\n",
      "        [0.0321],\n",
      "        [0.0141],\n",
      "        [0.0410],\n",
      "        [0.0371],\n",
      "        [0.0039],\n",
      "        [0.0173],\n",
      "        [0.0080],\n",
      "        [0.0140],\n",
      "        [0.0686],\n",
      "        [0.0456],\n",
      "        [0.0416],\n",
      "        [0.0115],\n",
      "        [0.0408],\n",
      "        [0.0784],\n",
      "        [0.0335],\n",
      "        [0.0440],\n",
      "        [0.0345],\n",
      "        [0.0372],\n",
      "        [0.0144],\n",
      "        [0.0168],\n",
      "        [0.0046],\n",
      "        [0.0024],\n",
      "        [0.0450],\n",
      "        [0.0777],\n",
      "        [0.0354],\n",
      "        [0.0620],\n",
      "        [0.0394],\n",
      "        [0.0457],\n",
      "        [0.0055],\n",
      "        [0.0264],\n",
      "        [0.0379],\n",
      "        [0.0263],\n",
      "        [0.1046],\n",
      "        [0.0940],\n",
      "        [0.0267],\n",
      "        [0.0214],\n",
      "        [0.1016],\n",
      "        [0.0580],\n",
      "        [0.0436],\n",
      "        [0.0441],\n",
      "        [0.0412],\n",
      "        [0.0656],\n",
      "        [0.0687],\n",
      "        [0.0920],\n",
      "        [0.0541],\n",
      "        [0.0773],\n",
      "        [0.0416],\n",
      "        [0.1333],\n",
      "        [0.0757],\n",
      "        [0.1042],\n",
      "        [0.0544],\n",
      "        [0.0614],\n",
      "        [0.1129],\n",
      "        [0.1186],\n",
      "        [0.0929],\n",
      "        [0.0994],\n",
      "        [0.0744],\n",
      "        [0.0604],\n",
      "        [0.1024],\n",
      "        [0.1183],\n",
      "        [0.1026],\n",
      "        [0.0803],\n",
      "        [0.1213],\n",
      "        [0.0863],\n",
      "        [0.1217],\n",
      "        [0.1033],\n",
      "        [0.1082],\n",
      "        [0.1543],\n",
      "        [0.1417],\n",
      "        [0.1273],\n",
      "        [0.0058],\n",
      "        [0.1168],\n",
      "        [0.1223],\n",
      "        [0.0493],\n",
      "        [0.0641],\n",
      "        [0.0733],\n",
      "        [0.1332],\n",
      "        [0.1684],\n",
      "        [0.0066],\n",
      "        [0.0048],\n",
      "        [0.0249],\n",
      "        [0.0310],\n",
      "        [0.0491],\n",
      "        [0.0321],\n",
      "        [0.0161],\n",
      "        [0.0802],\n",
      "        [0.0695],\n",
      "        [0.0448],\n",
      "        [0.0542],\n",
      "        [0.0951],\n",
      "        [0.1430],\n",
      "        [0.1127],\n",
      "        [0.1341],\n",
      "        [0.1171],\n",
      "        [0.1454],\n",
      "        [0.2852],\n",
      "        [0.2992],\n",
      "        [0.3022],\n",
      "        [0.3041],\n",
      "        [0.3051],\n",
      "        [0.1336],\n",
      "        [0.3146],\n",
      "        [0.3164],\n",
      "        [0.1789],\n",
      "        [0.2107],\n",
      "        [0.3176],\n",
      "        [0.3249],\n",
      "        [0.3267]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 95.55986166000366\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 134\n",
      "剩餘X 資料 torch.Size([26, 18])\n",
      "剩餘Y 資料 torch.Size([26, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.10812074691057205, 14)\n",
      "The second_loss value of k: (0.10881735384464264, 25)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.1068])\n",
      "目前模型的Data狀態 torch.Size([134, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1056],\n",
      "        [0.3878],\n",
      "        [0.2669],\n",
      "        [0.5291],\n",
      "        [0.5298],\n",
      "        [0.4318],\n",
      "        [0.1432],\n",
      "        [0.1660],\n",
      "        [0.1169],\n",
      "        [0.4988],\n",
      "        [0.4631],\n",
      "        [0.3325],\n",
      "        [0.1058],\n",
      "        [0.1980],\n",
      "        [0.4617],\n",
      "        [0.3280],\n",
      "        [0.3324],\n",
      "        [0.1242],\n",
      "        [0.1682],\n",
      "        [0.4836],\n",
      "        [0.5464],\n",
      "        [0.4559],\n",
      "        [0.3794],\n",
      "        [0.4765],\n",
      "        [0.5384],\n",
      "        [0.3258],\n",
      "        [0.4823],\n",
      "        [0.4977],\n",
      "        [0.2726],\n",
      "        [0.4659],\n",
      "        [0.1794],\n",
      "        [0.4934],\n",
      "        [0.4579],\n",
      "        [0.5069],\n",
      "        [0.1363],\n",
      "        [0.1559],\n",
      "        [0.1802],\n",
      "        [0.3178],\n",
      "        [0.5779],\n",
      "        [0.1266],\n",
      "        [0.3150],\n",
      "        [0.3020],\n",
      "        [0.3215],\n",
      "        [0.1952],\n",
      "        [0.4399],\n",
      "        [0.2224],\n",
      "        [0.1945],\n",
      "        [0.2620],\n",
      "        [0.4918],\n",
      "        [0.4477],\n",
      "        [0.3873],\n",
      "        [0.5301],\n",
      "        [0.5266],\n",
      "        [0.1748],\n",
      "        [0.4797],\n",
      "        [0.4266],\n",
      "        [0.2139],\n",
      "        [0.5242],\n",
      "        [0.1606],\n",
      "        [0.1858],\n",
      "        [0.4650],\n",
      "        [0.5608],\n",
      "        [0.5463],\n",
      "        [0.5342],\n",
      "        [0.2642],\n",
      "        [0.2497],\n",
      "        [0.1837],\n",
      "        [0.1593],\n",
      "        [0.1191],\n",
      "        [0.4294],\n",
      "        [0.4230],\n",
      "        [0.0771],\n",
      "        [0.5454],\n",
      "        [0.1940],\n",
      "        [0.5038],\n",
      "        [0.5419],\n",
      "        [0.2241],\n",
      "        [0.3382],\n",
      "        [0.4327],\n",
      "        [0.1738],\n",
      "        [0.3097],\n",
      "        [0.2500],\n",
      "        [0.3385],\n",
      "        [0.3498],\n",
      "        [0.4297],\n",
      "        [0.3676],\n",
      "        [0.2275],\n",
      "        [0.3772],\n",
      "        [0.2772],\n",
      "        [0.1217],\n",
      "        [0.2477],\n",
      "        [0.3283],\n",
      "        [0.3415],\n",
      "        [0.3255],\n",
      "        [0.4140],\n",
      "        [0.5082],\n",
      "        [0.4431],\n",
      "        [0.1804],\n",
      "        [0.6887],\n",
      "        [0.7571],\n",
      "        [0.7686],\n",
      "        [0.1861],\n",
      "        [0.3958],\n",
      "        [0.7571],\n",
      "        [0.6235],\n",
      "        [0.7125],\n",
      "        [0.7670],\n",
      "        [0.7974],\n",
      "        [0.7697],\n",
      "        [0.7495],\n",
      "        [0.4914],\n",
      "        [0.7264],\n",
      "        [0.6046],\n",
      "        [0.6736],\n",
      "        [0.5709],\n",
      "        [0.4741],\n",
      "        [0.5985],\n",
      "        [0.4894],\n",
      "        [0.6565],\n",
      "        [0.4783],\n",
      "        [0.4356],\n",
      "        [0.4356],\n",
      "        [0.4356],\n",
      "        [0.4356],\n",
      "        [0.4356],\n",
      "        [0.5959],\n",
      "        [0.4356],\n",
      "        [0.4356],\n",
      "        [0.6244],\n",
      "        [0.6079],\n",
      "        [0.4356],\n",
      "        [0.4356],\n",
      "        [0.4356],\n",
      "        [0.4356]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0140],\n",
      "        [0.0335],\n",
      "        [0.0047],\n",
      "        [0.0121],\n",
      "        [0.0416],\n",
      "        [0.0087],\n",
      "        [0.0050],\n",
      "        [0.0181],\n",
      "        [0.0083],\n",
      "        [0.0533],\n",
      "        [0.0109],\n",
      "        [0.0255],\n",
      "        [0.0273],\n",
      "        [0.0089],\n",
      "        [0.0292],\n",
      "        [0.0631],\n",
      "        [0.0311],\n",
      "        [0.0084],\n",
      "        [0.0654],\n",
      "        [0.0081],\n",
      "        [0.0214],\n",
      "        [0.0020],\n",
      "        [0.0149],\n",
      "        [0.0502],\n",
      "        [0.0458],\n",
      "        [0.0321],\n",
      "        [0.0141],\n",
      "        [0.0410],\n",
      "        [0.0371],\n",
      "        [0.0039],\n",
      "        [0.0173],\n",
      "        [0.0080],\n",
      "        [0.0140],\n",
      "        [0.0686],\n",
      "        [0.0456],\n",
      "        [0.0416],\n",
      "        [0.0115],\n",
      "        [0.0408],\n",
      "        [0.0784],\n",
      "        [0.0335],\n",
      "        [0.0440],\n",
      "        [0.0345],\n",
      "        [0.0372],\n",
      "        [0.0144],\n",
      "        [0.0168],\n",
      "        [0.0046],\n",
      "        [0.0024],\n",
      "        [0.0450],\n",
      "        [0.0777],\n",
      "        [0.0354],\n",
      "        [0.0620],\n",
      "        [0.0394],\n",
      "        [0.0457],\n",
      "        [0.0055],\n",
      "        [0.0264],\n",
      "        [0.0379],\n",
      "        [0.0263],\n",
      "        [0.1046],\n",
      "        [0.0940],\n",
      "        [0.0267],\n",
      "        [0.0214],\n",
      "        [0.1016],\n",
      "        [0.0580],\n",
      "        [0.0436],\n",
      "        [0.0441],\n",
      "        [0.0412],\n",
      "        [0.0656],\n",
      "        [0.0687],\n",
      "        [0.0920],\n",
      "        [0.0541],\n",
      "        [0.0773],\n",
      "        [0.0416],\n",
      "        [0.1333],\n",
      "        [0.0757],\n",
      "        [0.1042],\n",
      "        [0.0544],\n",
      "        [0.0614],\n",
      "        [0.1129],\n",
      "        [0.1186],\n",
      "        [0.0929],\n",
      "        [0.0994],\n",
      "        [0.0744],\n",
      "        [0.0604],\n",
      "        [0.1024],\n",
      "        [0.1183],\n",
      "        [0.1026],\n",
      "        [0.0803],\n",
      "        [0.1213],\n",
      "        [0.0863],\n",
      "        [0.1217],\n",
      "        [0.1033],\n",
      "        [0.1082],\n",
      "        [0.1543],\n",
      "        [0.1417],\n",
      "        [0.1273],\n",
      "        [0.0058],\n",
      "        [0.1168],\n",
      "        [0.1223],\n",
      "        [0.0493],\n",
      "        [0.0641],\n",
      "        [0.0733],\n",
      "        [0.1332],\n",
      "        [0.1684],\n",
      "        [0.0066],\n",
      "        [0.0048],\n",
      "        [0.0249],\n",
      "        [0.0310],\n",
      "        [0.0491],\n",
      "        [0.0321],\n",
      "        [0.0161],\n",
      "        [0.0802],\n",
      "        [0.0695],\n",
      "        [0.0448],\n",
      "        [0.0542],\n",
      "        [0.0951],\n",
      "        [0.1430],\n",
      "        [0.1127],\n",
      "        [0.1341],\n",
      "        [0.1171],\n",
      "        [0.1454],\n",
      "        [0.2852],\n",
      "        [0.2992],\n",
      "        [0.3022],\n",
      "        [0.3041],\n",
      "        [0.3051],\n",
      "        [0.1336],\n",
      "        [0.3146],\n",
      "        [0.3164],\n",
      "        [0.1789],\n",
      "        [0.2107],\n",
      "        [0.3176],\n",
      "        [0.3249],\n",
      "        [0.3267],\n",
      "        [0.3288]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0118],\n",
      "        [0.0374],\n",
      "        [0.0071],\n",
      "        [0.0155],\n",
      "        [0.0408],\n",
      "        [0.0103],\n",
      "        [0.0090],\n",
      "        [0.0162],\n",
      "        [0.0113],\n",
      "        [0.0538],\n",
      "        [0.0136],\n",
      "        [0.0235],\n",
      "        [0.0252],\n",
      "        [0.0106],\n",
      "        [0.0289],\n",
      "        [0.0637],\n",
      "        [0.0299],\n",
      "        [0.0051],\n",
      "        [0.0648],\n",
      "        [0.0098],\n",
      "        [0.0196],\n",
      "        [0.0004],\n",
      "        [0.0188],\n",
      "        [0.0508],\n",
      "        [0.0453],\n",
      "        [0.0296],\n",
      "        [0.0148],\n",
      "        [0.0396],\n",
      "        [0.0358],\n",
      "        [0.0057],\n",
      "        [0.0210],\n",
      "        [0.0067],\n",
      "        [0.0155],\n",
      "        [0.0722],\n",
      "        [0.0488],\n",
      "        [0.0422],\n",
      "        [0.0097],\n",
      "        [0.0392],\n",
      "        [0.0786],\n",
      "        [0.0367],\n",
      "        [0.0466],\n",
      "        [0.0345],\n",
      "        [0.0349],\n",
      "        [0.0159],\n",
      "        [0.0172],\n",
      "        [0.0080],\n",
      "        [0.0060],\n",
      "        [0.0421],\n",
      "        [0.0796],\n",
      "        [0.0345],\n",
      "        [0.0654],\n",
      "        [0.0414],\n",
      "        [0.0489],\n",
      "        [0.0019],\n",
      "        [0.0281],\n",
      "        [0.0392],\n",
      "        [0.0276],\n",
      "        [0.1053],\n",
      "        [0.0931],\n",
      "        [0.0275],\n",
      "        [0.0234],\n",
      "        [0.1018],\n",
      "        [0.0625],\n",
      "        [0.0471],\n",
      "        [0.0456],\n",
      "        [0.0390],\n",
      "        [0.0654],\n",
      "        [0.0715],\n",
      "        [0.0950],\n",
      "        [0.0542],\n",
      "        [0.0787],\n",
      "        [0.0373],\n",
      "        [0.1344],\n",
      "        [0.0792],\n",
      "        [0.1046],\n",
      "        [0.0521],\n",
      "        [0.0629],\n",
      "        [0.1128],\n",
      "        [0.1228],\n",
      "        [0.0959],\n",
      "        [0.0975],\n",
      "        [0.0761],\n",
      "        [0.0567],\n",
      "        [0.1012],\n",
      "        [0.1220],\n",
      "        [0.1027],\n",
      "        [0.0765],\n",
      "        [0.1207],\n",
      "        [0.0879],\n",
      "        [0.1232],\n",
      "        [0.1053],\n",
      "        [0.1055],\n",
      "        [0.1537],\n",
      "        [0.1398],\n",
      "        [0.1270],\n",
      "        [0.0010],\n",
      "        [0.1149],\n",
      "        [0.1189],\n",
      "        [0.0568],\n",
      "        [0.0726],\n",
      "        [0.0818],\n",
      "        [0.1294],\n",
      "        [0.1683],\n",
      "        [0.0139],\n",
      "        [0.0018],\n",
      "        [0.0335],\n",
      "        [0.0401],\n",
      "        [0.0597],\n",
      "        [0.0422],\n",
      "        [0.0074],\n",
      "        [0.0747],\n",
      "        [0.0634],\n",
      "        [0.0376],\n",
      "        [0.0461],\n",
      "        [0.0878],\n",
      "        [0.1376],\n",
      "        [0.1055],\n",
      "        [0.1273],\n",
      "        [0.1098],\n",
      "        [0.1394],\n",
      "        [0.2847],\n",
      "        [0.2987],\n",
      "        [0.3017],\n",
      "        [0.3035],\n",
      "        [0.3046],\n",
      "        [0.1267],\n",
      "        [0.3140],\n",
      "        [0.3158],\n",
      "        [0.1710],\n",
      "        [0.2033],\n",
      "        [0.3170],\n",
      "        [0.3244],\n",
      "        [0.3262],\n",
      "        [0.3283]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 95.81925058364868\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 135\n",
      "剩餘X 資料 torch.Size([25, 18])\n",
      "剩餘Y 資料 torch.Size([25, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.108449786901474, 24)\n",
      "The second_loss value of k: (0.11057557910680771, 16)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引24，y= tensor([0.1057])\n",
      "目前模型的Data狀態 torch.Size([135, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1078],\n",
      "        [0.3917],\n",
      "        [0.2693],\n",
      "        [0.5258],\n",
      "        [0.5290],\n",
      "        [0.4301],\n",
      "        [0.1472],\n",
      "        [0.1679],\n",
      "        [0.1199],\n",
      "        [0.4992],\n",
      "        [0.4604],\n",
      "        [0.3345],\n",
      "        [0.1080],\n",
      "        [0.1998],\n",
      "        [0.4614],\n",
      "        [0.3274],\n",
      "        [0.3335],\n",
      "        [0.1275],\n",
      "        [0.1688],\n",
      "        [0.4818],\n",
      "        [0.5447],\n",
      "        [0.4544],\n",
      "        [0.3833],\n",
      "        [0.4772],\n",
      "        [0.5379],\n",
      "        [0.3283],\n",
      "        [0.4830],\n",
      "        [0.4963],\n",
      "        [0.2739],\n",
      "        [0.4642],\n",
      "        [0.1830],\n",
      "        [0.4920],\n",
      "        [0.4564],\n",
      "        [0.5105],\n",
      "        [0.1395],\n",
      "        [0.1554],\n",
      "        [0.1820],\n",
      "        [0.3193],\n",
      "        [0.5782],\n",
      "        [0.1298],\n",
      "        [0.3176],\n",
      "        [0.3019],\n",
      "        [0.3238],\n",
      "        [0.1967],\n",
      "        [0.4403],\n",
      "        [0.2259],\n",
      "        [0.1981],\n",
      "        [0.2650],\n",
      "        [0.4937],\n",
      "        [0.4486],\n",
      "        [0.3906],\n",
      "        [0.5281],\n",
      "        [0.5297],\n",
      "        [0.1783],\n",
      "        [0.4780],\n",
      "        [0.4253],\n",
      "        [0.2153],\n",
      "        [0.5249],\n",
      "        [0.1614],\n",
      "        [0.1866],\n",
      "        [0.4670],\n",
      "        [0.5610],\n",
      "        [0.5508],\n",
      "        [0.5377],\n",
      "        [0.2657],\n",
      "        [0.2519],\n",
      "        [0.1835],\n",
      "        [0.1621],\n",
      "        [0.1221],\n",
      "        [0.4293],\n",
      "        [0.4216],\n",
      "        [0.0814],\n",
      "        [0.5465],\n",
      "        [0.1974],\n",
      "        [0.5042],\n",
      "        [0.5396],\n",
      "        [0.2257],\n",
      "        [0.3382],\n",
      "        [0.4370],\n",
      "        [0.1768],\n",
      "        [0.3116],\n",
      "        [0.2517],\n",
      "        [0.3423],\n",
      "        [0.3510],\n",
      "        [0.4335],\n",
      "        [0.3676],\n",
      "        [0.2313],\n",
      "        [0.3779],\n",
      "        [0.2788],\n",
      "        [0.1232],\n",
      "        [0.2496],\n",
      "        [0.3310],\n",
      "        [0.3421],\n",
      "        [0.3274],\n",
      "        [0.4138],\n",
      "        [0.5131],\n",
      "        [0.4412],\n",
      "        [0.1838],\n",
      "        [0.6962],\n",
      "        [0.7656],\n",
      "        [0.7770],\n",
      "        [0.1898],\n",
      "        [0.3957],\n",
      "        [0.7644],\n",
      "        [0.6301],\n",
      "        [0.7211],\n",
      "        [0.7761],\n",
      "        [0.8081],\n",
      "        [0.7797],\n",
      "        [0.7582],\n",
      "        [0.4969],\n",
      "        [0.7325],\n",
      "        [0.6118],\n",
      "        [0.6817],\n",
      "        [0.5782],\n",
      "        [0.4794],\n",
      "        [0.6057],\n",
      "        [0.4962],\n",
      "        [0.6639],\n",
      "        [0.4844],\n",
      "        [0.4350],\n",
      "        [0.4350],\n",
      "        [0.4350],\n",
      "        [0.4350],\n",
      "        [0.4350],\n",
      "        [0.6028],\n",
      "        [0.4350],\n",
      "        [0.4350],\n",
      "        [0.6323],\n",
      "        [0.6153],\n",
      "        [0.4350],\n",
      "        [0.4350],\n",
      "        [0.4350],\n",
      "        [0.4350],\n",
      "        [0.4350]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0118],\n",
      "        [0.0374],\n",
      "        [0.0071],\n",
      "        [0.0155],\n",
      "        [0.0408],\n",
      "        [0.0103],\n",
      "        [0.0090],\n",
      "        [0.0162],\n",
      "        [0.0113],\n",
      "        [0.0538],\n",
      "        [0.0136],\n",
      "        [0.0235],\n",
      "        [0.0252],\n",
      "        [0.0106],\n",
      "        [0.0289],\n",
      "        [0.0637],\n",
      "        [0.0299],\n",
      "        [0.0051],\n",
      "        [0.0648],\n",
      "        [0.0098],\n",
      "        [0.0196],\n",
      "        [0.0004],\n",
      "        [0.0188],\n",
      "        [0.0508],\n",
      "        [0.0453],\n",
      "        [0.0296],\n",
      "        [0.0148],\n",
      "        [0.0396],\n",
      "        [0.0358],\n",
      "        [0.0057],\n",
      "        [0.0210],\n",
      "        [0.0067],\n",
      "        [0.0155],\n",
      "        [0.0722],\n",
      "        [0.0488],\n",
      "        [0.0422],\n",
      "        [0.0097],\n",
      "        [0.0392],\n",
      "        [0.0786],\n",
      "        [0.0367],\n",
      "        [0.0466],\n",
      "        [0.0345],\n",
      "        [0.0349],\n",
      "        [0.0159],\n",
      "        [0.0172],\n",
      "        [0.0080],\n",
      "        [0.0060],\n",
      "        [0.0421],\n",
      "        [0.0796],\n",
      "        [0.0345],\n",
      "        [0.0654],\n",
      "        [0.0414],\n",
      "        [0.0489],\n",
      "        [0.0019],\n",
      "        [0.0281],\n",
      "        [0.0392],\n",
      "        [0.0276],\n",
      "        [0.1053],\n",
      "        [0.0931],\n",
      "        [0.0275],\n",
      "        [0.0234],\n",
      "        [0.1018],\n",
      "        [0.0625],\n",
      "        [0.0471],\n",
      "        [0.0456],\n",
      "        [0.0390],\n",
      "        [0.0654],\n",
      "        [0.0715],\n",
      "        [0.0950],\n",
      "        [0.0542],\n",
      "        [0.0787],\n",
      "        [0.0373],\n",
      "        [0.1344],\n",
      "        [0.0792],\n",
      "        [0.1046],\n",
      "        [0.0521],\n",
      "        [0.0629],\n",
      "        [0.1128],\n",
      "        [0.1228],\n",
      "        [0.0959],\n",
      "        [0.0975],\n",
      "        [0.0761],\n",
      "        [0.0567],\n",
      "        [0.1012],\n",
      "        [0.1220],\n",
      "        [0.1027],\n",
      "        [0.0765],\n",
      "        [0.1207],\n",
      "        [0.0879],\n",
      "        [0.1232],\n",
      "        [0.1053],\n",
      "        [0.1055],\n",
      "        [0.1537],\n",
      "        [0.1398],\n",
      "        [0.1270],\n",
      "        [0.0010],\n",
      "        [0.1149],\n",
      "        [0.1189],\n",
      "        [0.0568],\n",
      "        [0.0726],\n",
      "        [0.0818],\n",
      "        [0.1294],\n",
      "        [0.1683],\n",
      "        [0.0139],\n",
      "        [0.0018],\n",
      "        [0.0335],\n",
      "        [0.0401],\n",
      "        [0.0597],\n",
      "        [0.0422],\n",
      "        [0.0074],\n",
      "        [0.0747],\n",
      "        [0.0634],\n",
      "        [0.0376],\n",
      "        [0.0461],\n",
      "        [0.0878],\n",
      "        [0.1376],\n",
      "        [0.1055],\n",
      "        [0.1273],\n",
      "        [0.1098],\n",
      "        [0.1394],\n",
      "        [0.2847],\n",
      "        [0.2987],\n",
      "        [0.3017],\n",
      "        [0.3035],\n",
      "        [0.3046],\n",
      "        [0.1267],\n",
      "        [0.3140],\n",
      "        [0.3158],\n",
      "        [0.1710],\n",
      "        [0.2033],\n",
      "        [0.3170],\n",
      "        [0.3244],\n",
      "        [0.3262],\n",
      "        [0.3283],\n",
      "        [0.3293]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0108],\n",
      "        [    0.0381],\n",
      "        [    0.0068],\n",
      "        [    0.0202],\n",
      "        [    0.0380],\n",
      "        [    0.0137],\n",
      "        [    0.0115],\n",
      "        [    0.0164],\n",
      "        [    0.0123],\n",
      "        [    0.0520],\n",
      "        [    0.0183],\n",
      "        [    0.0232],\n",
      "        [    0.0241],\n",
      "        [    0.0101],\n",
      "        [    0.0263],\n",
      "        [    0.0657],\n",
      "        [    0.0308],\n",
      "        [    0.0033],\n",
      "        [    0.0654],\n",
      "        [    0.0124],\n",
      "        [    0.0162],\n",
      "        [    0.0026],\n",
      "        [    0.0198],\n",
      "        [    0.0493],\n",
      "        [    0.0429],\n",
      "        [    0.0286],\n",
      "        [    0.0135],\n",
      "        [    0.0365],\n",
      "        [    0.0357],\n",
      "        [    0.0088],\n",
      "        [    0.0219],\n",
      "        [    0.0035],\n",
      "        [    0.0192],\n",
      "        [    0.0730],\n",
      "        [    0.0498],\n",
      "        [    0.0429],\n",
      "        [    0.0096],\n",
      "        [    0.0395],\n",
      "        [    0.0767],\n",
      "        [    0.0381],\n",
      "        [    0.0465],\n",
      "        [    0.0326],\n",
      "        [    0.0346],\n",
      "        [    0.0154],\n",
      "        [    0.0151],\n",
      "        [    0.0085],\n",
      "        [    0.0067],\n",
      "        [    0.0407],\n",
      "        [    0.0790],\n",
      "        [    0.0352],\n",
      "        [    0.0659],\n",
      "        [    0.0451],\n",
      "        [    0.0491],\n",
      "        [    0.0011],\n",
      "        [    0.0313],\n",
      "        [    0.0427],\n",
      "        [    0.0268],\n",
      "        [    0.1038],\n",
      "        [    0.0937],\n",
      "        [    0.0264],\n",
      "        [    0.0228],\n",
      "        [    0.0999],\n",
      "        [    0.0637],\n",
      "        [    0.0474],\n",
      "        [    0.0449],\n",
      "        [    0.0387],\n",
      "        [    0.0648],\n",
      "        [    0.0728],\n",
      "        [    0.0956],\n",
      "        [    0.0560],\n",
      "        [    0.0812],\n",
      "        [    0.0348],\n",
      "        [    0.1332],\n",
      "        [    0.0812],\n",
      "        [    0.1028],\n",
      "        [    0.0486],\n",
      "        [    0.0621],\n",
      "        [    0.1145],\n",
      "        [    0.1240],\n",
      "        [    0.0969],\n",
      "        [    0.0969],\n",
      "        [    0.0765],\n",
      "        [    0.0557],\n",
      "        [    0.1023],\n",
      "        [    0.1229],\n",
      "        [    0.1007],\n",
      "        [    0.0745],\n",
      "        [    0.1212],\n",
      "        [    0.0873],\n",
      "        [    0.1223],\n",
      "        [    0.1060],\n",
      "        [    0.1055],\n",
      "        [    0.1540],\n",
      "        [    0.1389],\n",
      "        [    0.1248],\n",
      "        [    0.0003],\n",
      "        [    0.1114],\n",
      "        [    0.1173],\n",
      "        [    0.0595],\n",
      "        [    0.0761],\n",
      "        [    0.0850],\n",
      "        [    0.1275],\n",
      "        [    0.1661],\n",
      "        [    0.0169],\n",
      "        [    0.0036],\n",
      "        [    0.0370],\n",
      "        [    0.0441],\n",
      "        [    0.0648],\n",
      "        [    0.0467],\n",
      "        [    0.0036],\n",
      "        [    0.0733],\n",
      "        [    0.0614],\n",
      "        [    0.0353],\n",
      "        [    0.0428],\n",
      "        [    0.0847],\n",
      "        [    0.1363],\n",
      "        [    0.1026],\n",
      "        [    0.1244],\n",
      "        [    0.1068],\n",
      "        [    0.1372],\n",
      "        [    0.2842],\n",
      "        [    0.2982],\n",
      "        [    0.3012],\n",
      "        [    0.3031],\n",
      "        [    0.3041],\n",
      "        [    0.1245],\n",
      "        [    0.3136],\n",
      "        [    0.3154],\n",
      "        [    0.1677],\n",
      "        [    0.2000],\n",
      "        [    0.3165],\n",
      "        [    0.3239],\n",
      "        [    0.3257],\n",
      "        [    0.3278],\n",
      "        [    0.3288]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 96.13286995887756\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 136\n",
      "剩餘X 資料 torch.Size([24, 18])\n",
      "剩餘Y 資料 torch.Size([24, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.11025874316692352, 16)\n",
      "The second_loss value of k: (0.12329081445932388, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引16，y= tensor([0.1025])\n",
      "目前模型的Data狀態 torch.Size([136, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1088],\n",
      "        [0.3924],\n",
      "        [0.2690],\n",
      "        [0.5211],\n",
      "        [0.5262],\n",
      "        [0.4267],\n",
      "        [0.1497],\n",
      "        [0.1677],\n",
      "        [0.1209],\n",
      "        [0.4974],\n",
      "        [0.4557],\n",
      "        [0.3348],\n",
      "        [0.1091],\n",
      "        [0.1992],\n",
      "        [0.4588],\n",
      "        [0.3254],\n",
      "        [0.3327],\n",
      "        [0.1293],\n",
      "        [0.1682],\n",
      "        [0.4792],\n",
      "        [0.5413],\n",
      "        [0.4514],\n",
      "        [0.3843],\n",
      "        [0.4757],\n",
      "        [0.5354],\n",
      "        [0.3293],\n",
      "        [0.4817],\n",
      "        [0.4932],\n",
      "        [0.2740],\n",
      "        [0.4611],\n",
      "        [0.1839],\n",
      "        [0.4889],\n",
      "        [0.4527],\n",
      "        [0.5112],\n",
      "        [0.1406],\n",
      "        [0.1546],\n",
      "        [0.1821],\n",
      "        [0.3190],\n",
      "        [0.5762],\n",
      "        [0.1312],\n",
      "        [0.3174],\n",
      "        [0.3000],\n",
      "        [0.3241],\n",
      "        [0.1962],\n",
      "        [0.4383],\n",
      "        [0.2263],\n",
      "        [0.1988],\n",
      "        [0.2664],\n",
      "        [0.4932],\n",
      "        [0.4479],\n",
      "        [0.3912],\n",
      "        [0.5245],\n",
      "        [0.5300],\n",
      "        [0.1791],\n",
      "        [0.4748],\n",
      "        [0.4218],\n",
      "        [0.2145],\n",
      "        [0.5233],\n",
      "        [0.1608],\n",
      "        [0.1854],\n",
      "        [0.4663],\n",
      "        [0.5591],\n",
      "        [0.5519],\n",
      "        [0.5380],\n",
      "        [0.2650],\n",
      "        [0.2522],\n",
      "        [0.1829],\n",
      "        [0.1634],\n",
      "        [0.1227],\n",
      "        [0.4276],\n",
      "        [0.4191],\n",
      "        [0.0840],\n",
      "        [0.5453],\n",
      "        [0.1995],\n",
      "        [0.5024],\n",
      "        [0.5361],\n",
      "        [0.2249],\n",
      "        [0.3365],\n",
      "        [0.4381],\n",
      "        [0.1778],\n",
      "        [0.3122],\n",
      "        [0.2521],\n",
      "        [0.3432],\n",
      "        [0.3499],\n",
      "        [0.4343],\n",
      "        [0.3656],\n",
      "        [0.2333],\n",
      "        [0.3773],\n",
      "        [0.2782],\n",
      "        [0.1223],\n",
      "        [0.2504],\n",
      "        [0.3310],\n",
      "        [0.3418],\n",
      "        [0.3283],\n",
      "        [0.4116],\n",
      "        [0.5143],\n",
      "        [0.4377],\n",
      "        [0.1854],\n",
      "        [0.6988],\n",
      "        [0.7691],\n",
      "        [0.7803],\n",
      "        [0.1917],\n",
      "        [0.3935],\n",
      "        [0.7674],\n",
      "        [0.6319],\n",
      "        [0.7246],\n",
      "        [0.7801],\n",
      "        [0.8132],\n",
      "        [0.7843],\n",
      "        [0.7621],\n",
      "        [0.4983],\n",
      "        [0.7345],\n",
      "        [0.6141],\n",
      "        [0.6850],\n",
      "        [0.5813],\n",
      "        [0.4808],\n",
      "        [0.6086],\n",
      "        [0.4991],\n",
      "        [0.6669],\n",
      "        [0.4865],\n",
      "        [0.4346],\n",
      "        [0.4346],\n",
      "        [0.4346],\n",
      "        [0.4346],\n",
      "        [0.4346],\n",
      "        [0.6050],\n",
      "        [0.4346],\n",
      "        [0.4346],\n",
      "        [0.6357],\n",
      "        [0.6186],\n",
      "        [0.4346],\n",
      "        [0.4346],\n",
      "        [0.4346],\n",
      "        [0.4346],\n",
      "        [0.4346],\n",
      "        [0.4346]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0108],\n",
      "        [    0.0381],\n",
      "        [    0.0068],\n",
      "        [    0.0202],\n",
      "        [    0.0380],\n",
      "        [    0.0137],\n",
      "        [    0.0115],\n",
      "        [    0.0164],\n",
      "        [    0.0123],\n",
      "        [    0.0520],\n",
      "        [    0.0183],\n",
      "        [    0.0232],\n",
      "        [    0.0241],\n",
      "        [    0.0101],\n",
      "        [    0.0263],\n",
      "        [    0.0657],\n",
      "        [    0.0308],\n",
      "        [    0.0033],\n",
      "        [    0.0654],\n",
      "        [    0.0124],\n",
      "        [    0.0162],\n",
      "        [    0.0026],\n",
      "        [    0.0198],\n",
      "        [    0.0493],\n",
      "        [    0.0429],\n",
      "        [    0.0286],\n",
      "        [    0.0135],\n",
      "        [    0.0365],\n",
      "        [    0.0357],\n",
      "        [    0.0088],\n",
      "        [    0.0219],\n",
      "        [    0.0035],\n",
      "        [    0.0192],\n",
      "        [    0.0730],\n",
      "        [    0.0498],\n",
      "        [    0.0429],\n",
      "        [    0.0096],\n",
      "        [    0.0395],\n",
      "        [    0.0767],\n",
      "        [    0.0381],\n",
      "        [    0.0465],\n",
      "        [    0.0326],\n",
      "        [    0.0346],\n",
      "        [    0.0154],\n",
      "        [    0.0151],\n",
      "        [    0.0085],\n",
      "        [    0.0067],\n",
      "        [    0.0407],\n",
      "        [    0.0790],\n",
      "        [    0.0352],\n",
      "        [    0.0659],\n",
      "        [    0.0451],\n",
      "        [    0.0491],\n",
      "        [    0.0011],\n",
      "        [    0.0313],\n",
      "        [    0.0427],\n",
      "        [    0.0268],\n",
      "        [    0.1038],\n",
      "        [    0.0937],\n",
      "        [    0.0264],\n",
      "        [    0.0228],\n",
      "        [    0.0999],\n",
      "        [    0.0637],\n",
      "        [    0.0474],\n",
      "        [    0.0449],\n",
      "        [    0.0387],\n",
      "        [    0.0648],\n",
      "        [    0.0728],\n",
      "        [    0.0956],\n",
      "        [    0.0560],\n",
      "        [    0.0812],\n",
      "        [    0.0348],\n",
      "        [    0.1332],\n",
      "        [    0.0812],\n",
      "        [    0.1028],\n",
      "        [    0.0486],\n",
      "        [    0.0621],\n",
      "        [    0.1145],\n",
      "        [    0.1240],\n",
      "        [    0.0969],\n",
      "        [    0.0969],\n",
      "        [    0.0765],\n",
      "        [    0.0557],\n",
      "        [    0.1023],\n",
      "        [    0.1229],\n",
      "        [    0.1007],\n",
      "        [    0.0745],\n",
      "        [    0.1212],\n",
      "        [    0.0873],\n",
      "        [    0.1223],\n",
      "        [    0.1060],\n",
      "        [    0.1055],\n",
      "        [    0.1540],\n",
      "        [    0.1389],\n",
      "        [    0.1248],\n",
      "        [    0.0003],\n",
      "        [    0.1114],\n",
      "        [    0.1173],\n",
      "        [    0.0595],\n",
      "        [    0.0761],\n",
      "        [    0.0850],\n",
      "        [    0.1275],\n",
      "        [    0.1661],\n",
      "        [    0.0169],\n",
      "        [    0.0036],\n",
      "        [    0.0370],\n",
      "        [    0.0441],\n",
      "        [    0.0648],\n",
      "        [    0.0467],\n",
      "        [    0.0036],\n",
      "        [    0.0733],\n",
      "        [    0.0614],\n",
      "        [    0.0353],\n",
      "        [    0.0428],\n",
      "        [    0.0847],\n",
      "        [    0.1363],\n",
      "        [    0.1026],\n",
      "        [    0.1244],\n",
      "        [    0.1068],\n",
      "        [    0.1372],\n",
      "        [    0.2842],\n",
      "        [    0.2982],\n",
      "        [    0.3012],\n",
      "        [    0.3031],\n",
      "        [    0.3041],\n",
      "        [    0.1245],\n",
      "        [    0.3136],\n",
      "        [    0.3154],\n",
      "        [    0.1677],\n",
      "        [    0.2000],\n",
      "        [    0.3165],\n",
      "        [    0.3239],\n",
      "        [    0.3257],\n",
      "        [    0.3278],\n",
      "        [    0.3288],\n",
      "        [    0.3321]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0101],\n",
      "        [0.0389],\n",
      "        [0.0069],\n",
      "        [0.0230],\n",
      "        [0.0359],\n",
      "        [0.0162],\n",
      "        [0.0134],\n",
      "        [0.0158],\n",
      "        [0.0132],\n",
      "        [0.0507],\n",
      "        [0.0221],\n",
      "        [0.0225],\n",
      "        [0.0232],\n",
      "        [0.0104],\n",
      "        [0.0244],\n",
      "        [0.0664],\n",
      "        [0.0308],\n",
      "        [0.0019],\n",
      "        [0.0648],\n",
      "        [0.0133],\n",
      "        [0.0143],\n",
      "        [0.0046],\n",
      "        [0.0208],\n",
      "        [0.0486],\n",
      "        [0.0415],\n",
      "        [0.0275],\n",
      "        [0.0132],\n",
      "        [0.0345],\n",
      "        [0.0356],\n",
      "        [0.0109],\n",
      "        [0.0234],\n",
      "        [0.0012],\n",
      "        [0.0219],\n",
      "        [0.0738],\n",
      "        [0.0503],\n",
      "        [0.0434],\n",
      "        [0.0084],\n",
      "        [0.0393],\n",
      "        [0.0756],\n",
      "        [0.0392],\n",
      "        [0.0465],\n",
      "        [0.0313],\n",
      "        [0.0340],\n",
      "        [0.0157],\n",
      "        [0.0135],\n",
      "        [0.0095],\n",
      "        [0.0079],\n",
      "        [0.0394],\n",
      "        [0.0785],\n",
      "        [0.0354],\n",
      "        [0.0665],\n",
      "        [0.0470],\n",
      "        [0.0495],\n",
      "        [0.0003],\n",
      "        [0.0335],\n",
      "        [0.0453],\n",
      "        [0.0268],\n",
      "        [0.1026],\n",
      "        [0.0932],\n",
      "        [0.0264],\n",
      "        [0.0224],\n",
      "        [0.0988],\n",
      "        [0.0645],\n",
      "        [0.0478],\n",
      "        [0.0448],\n",
      "        [0.0382],\n",
      "        [0.0643],\n",
      "        [0.0737],\n",
      "        [0.0965],\n",
      "        [0.0566],\n",
      "        [0.0823],\n",
      "        [0.0324],\n",
      "        [0.1323],\n",
      "        [0.0830],\n",
      "        [0.1015],\n",
      "        [0.0471],\n",
      "        [0.0619],\n",
      "        [0.1151],\n",
      "        [0.1249],\n",
      "        [0.0974],\n",
      "        [0.0962],\n",
      "        [0.0769],\n",
      "        [0.0548],\n",
      "        [0.1027],\n",
      "        [0.1238],\n",
      "        [0.0993],\n",
      "        [0.0730],\n",
      "        [0.1210],\n",
      "        [0.0874],\n",
      "        [0.1221],\n",
      "        [0.1067],\n",
      "        [0.1051],\n",
      "        [0.1534],\n",
      "        [0.1377],\n",
      "        [0.1232],\n",
      "        [0.0010],\n",
      "        [0.1089],\n",
      "        [0.1157],\n",
      "        [0.0609],\n",
      "        [0.0782],\n",
      "        [0.0867],\n",
      "        [0.1258],\n",
      "        [0.1645],\n",
      "        [0.0187],\n",
      "        [0.0045],\n",
      "        [0.0389],\n",
      "        [0.0463],\n",
      "        [0.0678],\n",
      "        [0.0492],\n",
      "        [0.0015],\n",
      "        [0.0726],\n",
      "        [0.0603],\n",
      "        [0.0341],\n",
      "        [0.0410],\n",
      "        [0.0824],\n",
      "        [0.1354],\n",
      "        [0.1007],\n",
      "        [0.1225],\n",
      "        [0.1052],\n",
      "        [0.1358],\n",
      "        [0.2837],\n",
      "        [0.2977],\n",
      "        [0.3007],\n",
      "        [0.3026],\n",
      "        [0.3036],\n",
      "        [0.1233],\n",
      "        [0.3131],\n",
      "        [0.3149],\n",
      "        [0.1658],\n",
      "        [0.1981],\n",
      "        [0.3161],\n",
      "        [0.3234],\n",
      "        [0.3252],\n",
      "        [0.3273],\n",
      "        [0.3284],\n",
      "        [0.3316]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 96.37691640853882\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 137\n",
      "剩餘X 資料 torch.Size([23, 18])\n",
      "剩餘Y 資料 torch.Size([23, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12296776473522186, 14)\n",
      "The second_loss value of k: (0.12365549802780151, 15)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0834])\n",
      "目前模型的Data狀態 torch.Size([137, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1094],\n",
      "        [0.3932],\n",
      "        [0.2691],\n",
      "        [0.5183],\n",
      "        [0.5242],\n",
      "        [0.4242],\n",
      "        [0.1517],\n",
      "        [0.1683],\n",
      "        [0.1218],\n",
      "        [0.4962],\n",
      "        [0.4519],\n",
      "        [0.3355],\n",
      "        [0.1099],\n",
      "        [0.1996],\n",
      "        [0.4569],\n",
      "        [0.3247],\n",
      "        [0.3326],\n",
      "        [0.1307],\n",
      "        [0.1689],\n",
      "        [0.4783],\n",
      "        [0.5394],\n",
      "        [0.4493],\n",
      "        [0.3853],\n",
      "        [0.4750],\n",
      "        [0.5340],\n",
      "        [0.3303],\n",
      "        [0.4814],\n",
      "        [0.4911],\n",
      "        [0.2741],\n",
      "        [0.4590],\n",
      "        [0.1854],\n",
      "        [0.4865],\n",
      "        [0.4500],\n",
      "        [0.5121],\n",
      "        [0.1411],\n",
      "        [0.1541],\n",
      "        [0.1833],\n",
      "        [0.3192],\n",
      "        [0.5752],\n",
      "        [0.1323],\n",
      "        [0.3175],\n",
      "        [0.2988],\n",
      "        [0.3247],\n",
      "        [0.1965],\n",
      "        [0.4366],\n",
      "        [0.2274],\n",
      "        [0.2000],\n",
      "        [0.2677],\n",
      "        [0.4926],\n",
      "        [0.4476],\n",
      "        [0.3918],\n",
      "        [0.5225],\n",
      "        [0.5304],\n",
      "        [0.1806],\n",
      "        [0.4726],\n",
      "        [0.4192],\n",
      "        [0.2144],\n",
      "        [0.5222],\n",
      "        [0.1614],\n",
      "        [0.1854],\n",
      "        [0.4659],\n",
      "        [0.5580],\n",
      "        [0.5528],\n",
      "        [0.5384],\n",
      "        [0.2650],\n",
      "        [0.2528],\n",
      "        [0.1824],\n",
      "        [0.1643],\n",
      "        [0.1236],\n",
      "        [0.4269],\n",
      "        [0.4180],\n",
      "        [0.0864],\n",
      "        [0.5444],\n",
      "        [0.2012],\n",
      "        [0.5012],\n",
      "        [0.5346],\n",
      "        [0.2246],\n",
      "        [0.3359],\n",
      "        [0.4391],\n",
      "        [0.1783],\n",
      "        [0.3129],\n",
      "        [0.2525],\n",
      "        [0.3442],\n",
      "        [0.3495],\n",
      "        [0.4353],\n",
      "        [0.3642],\n",
      "        [0.2348],\n",
      "        [0.3775],\n",
      "        [0.2783],\n",
      "        [0.1221],\n",
      "        [0.2510],\n",
      "        [0.3314],\n",
      "        [0.3423],\n",
      "        [0.3295],\n",
      "        [0.4100],\n",
      "        [0.5151],\n",
      "        [0.4352],\n",
      "        [0.1870],\n",
      "        [0.7002],\n",
      "        [0.7711],\n",
      "        [0.7819],\n",
      "        [0.1934],\n",
      "        [0.3918],\n",
      "        [0.7693],\n",
      "        [0.6328],\n",
      "        [0.7265],\n",
      "        [0.7823],\n",
      "        [0.8161],\n",
      "        [0.7867],\n",
      "        [0.7642],\n",
      "        [0.4990],\n",
      "        [0.7356],\n",
      "        [0.6153],\n",
      "        [0.6868],\n",
      "        [0.5836],\n",
      "        [0.4816],\n",
      "        [0.6105],\n",
      "        [0.5010],\n",
      "        [0.6684],\n",
      "        [0.4879],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.6062],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.6375],\n",
      "        [0.6205],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.4341],\n",
      "        [0.4341]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0101],\n",
      "        [    0.0389],\n",
      "        [    0.0069],\n",
      "        [    0.0230],\n",
      "        [    0.0359],\n",
      "        [    0.0162],\n",
      "        [    0.0134],\n",
      "        [    0.0158],\n",
      "        [    0.0132],\n",
      "        [    0.0507],\n",
      "        [    0.0221],\n",
      "        [    0.0225],\n",
      "        [    0.0232],\n",
      "        [    0.0104],\n",
      "        [    0.0244],\n",
      "        [    0.0664],\n",
      "        [    0.0308],\n",
      "        [    0.0019],\n",
      "        [    0.0648],\n",
      "        [    0.0133],\n",
      "        [    0.0143],\n",
      "        [    0.0046],\n",
      "        [    0.0208],\n",
      "        [    0.0486],\n",
      "        [    0.0415],\n",
      "        [    0.0275],\n",
      "        [    0.0132],\n",
      "        [    0.0345],\n",
      "        [    0.0356],\n",
      "        [    0.0109],\n",
      "        [    0.0234],\n",
      "        [    0.0012],\n",
      "        [    0.0219],\n",
      "        [    0.0738],\n",
      "        [    0.0503],\n",
      "        [    0.0434],\n",
      "        [    0.0084],\n",
      "        [    0.0393],\n",
      "        [    0.0756],\n",
      "        [    0.0392],\n",
      "        [    0.0465],\n",
      "        [    0.0313],\n",
      "        [    0.0340],\n",
      "        [    0.0157],\n",
      "        [    0.0135],\n",
      "        [    0.0095],\n",
      "        [    0.0079],\n",
      "        [    0.0394],\n",
      "        [    0.0785],\n",
      "        [    0.0354],\n",
      "        [    0.0665],\n",
      "        [    0.0470],\n",
      "        [    0.0495],\n",
      "        [    0.0003],\n",
      "        [    0.0335],\n",
      "        [    0.0453],\n",
      "        [    0.0268],\n",
      "        [    0.1026],\n",
      "        [    0.0932],\n",
      "        [    0.0264],\n",
      "        [    0.0224],\n",
      "        [    0.0988],\n",
      "        [    0.0645],\n",
      "        [    0.0478],\n",
      "        [    0.0448],\n",
      "        [    0.0382],\n",
      "        [    0.0643],\n",
      "        [    0.0737],\n",
      "        [    0.0965],\n",
      "        [    0.0566],\n",
      "        [    0.0823],\n",
      "        [    0.0324],\n",
      "        [    0.1323],\n",
      "        [    0.0830],\n",
      "        [    0.1015],\n",
      "        [    0.0471],\n",
      "        [    0.0619],\n",
      "        [    0.1151],\n",
      "        [    0.1249],\n",
      "        [    0.0974],\n",
      "        [    0.0962],\n",
      "        [    0.0769],\n",
      "        [    0.0548],\n",
      "        [    0.1027],\n",
      "        [    0.1238],\n",
      "        [    0.0993],\n",
      "        [    0.0730],\n",
      "        [    0.1210],\n",
      "        [    0.0874],\n",
      "        [    0.1221],\n",
      "        [    0.1067],\n",
      "        [    0.1051],\n",
      "        [    0.1534],\n",
      "        [    0.1377],\n",
      "        [    0.1232],\n",
      "        [    0.0010],\n",
      "        [    0.1089],\n",
      "        [    0.1157],\n",
      "        [    0.0609],\n",
      "        [    0.0782],\n",
      "        [    0.0867],\n",
      "        [    0.1258],\n",
      "        [    0.1645],\n",
      "        [    0.0187],\n",
      "        [    0.0045],\n",
      "        [    0.0389],\n",
      "        [    0.0463],\n",
      "        [    0.0678],\n",
      "        [    0.0492],\n",
      "        [    0.0015],\n",
      "        [    0.0726],\n",
      "        [    0.0603],\n",
      "        [    0.0341],\n",
      "        [    0.0410],\n",
      "        [    0.0824],\n",
      "        [    0.1354],\n",
      "        [    0.1007],\n",
      "        [    0.1225],\n",
      "        [    0.1052],\n",
      "        [    0.1358],\n",
      "        [    0.2837],\n",
      "        [    0.2977],\n",
      "        [    0.3007],\n",
      "        [    0.3026],\n",
      "        [    0.3036],\n",
      "        [    0.1233],\n",
      "        [    0.3131],\n",
      "        [    0.3149],\n",
      "        [    0.1658],\n",
      "        [    0.1981],\n",
      "        [    0.3161],\n",
      "        [    0.3234],\n",
      "        [    0.3252],\n",
      "        [    0.3273],\n",
      "        [    0.3284],\n",
      "        [    0.3316],\n",
      "        [    0.3507]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 68\n",
      "Number of shrink: 32\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0098],\n",
      "        [    0.0392],\n",
      "        [    0.0066],\n",
      "        [    0.0253],\n",
      "        [    0.0342],\n",
      "        [    0.0181],\n",
      "        [    0.0151],\n",
      "        [    0.0153],\n",
      "        [    0.0139],\n",
      "        [    0.0496],\n",
      "        [    0.0253],\n",
      "        [    0.0222],\n",
      "        [    0.0226],\n",
      "        [    0.0107],\n",
      "        [    0.0226],\n",
      "        [    0.0672],\n",
      "        [    0.0311],\n",
      "        [    0.0008],\n",
      "        [    0.0638],\n",
      "        [    0.0138],\n",
      "        [    0.0128],\n",
      "        [    0.0062],\n",
      "        [    0.0215],\n",
      "        [    0.0479],\n",
      "        [    0.0404],\n",
      "        [    0.0268],\n",
      "        [    0.0130],\n",
      "        [    0.0328],\n",
      "        [    0.0357],\n",
      "        [    0.0125],\n",
      "        [    0.0250],\n",
      "        [    0.0009],\n",
      "        [    0.0242],\n",
      "        [    0.0747],\n",
      "        [    0.0503],\n",
      "        [    0.0440],\n",
      "        [    0.0073],\n",
      "        [    0.0393],\n",
      "        [    0.0750],\n",
      "        [    0.0398],\n",
      "        [    0.0461],\n",
      "        [    0.0303],\n",
      "        [    0.0338],\n",
      "        [    0.0159],\n",
      "        [    0.0120],\n",
      "        [    0.0105],\n",
      "        [    0.0088],\n",
      "        [    0.0386],\n",
      "        [    0.0776],\n",
      "        [    0.0358],\n",
      "        [    0.0666],\n",
      "        [    0.0487],\n",
      "        [    0.0500],\n",
      "        [    0.0017],\n",
      "        [    0.0353],\n",
      "        [    0.0474],\n",
      "        [    0.0266],\n",
      "        [    0.1016],\n",
      "        [    0.0924],\n",
      "        [    0.0264],\n",
      "        [    0.0221],\n",
      "        [    0.0979],\n",
      "        [    0.0652],\n",
      "        [    0.0480],\n",
      "        [    0.0445],\n",
      "        [    0.0375],\n",
      "        [    0.0636],\n",
      "        [    0.0742],\n",
      "        [    0.0974],\n",
      "        [    0.0571],\n",
      "        [    0.0830],\n",
      "        [    0.0298],\n",
      "        [    0.1315],\n",
      "        [    0.0844],\n",
      "        [    0.1001],\n",
      "        [    0.0460],\n",
      "        [    0.0613],\n",
      "        [    0.1159],\n",
      "        [    0.1253],\n",
      "        [    0.0975],\n",
      "        [    0.0957],\n",
      "        [    0.0771],\n",
      "        [    0.0543],\n",
      "        [    0.1035],\n",
      "        [    0.1243],\n",
      "        [    0.0982],\n",
      "        [    0.0719],\n",
      "        [    0.1208],\n",
      "        [    0.0873],\n",
      "        [    0.1220],\n",
      "        [    0.1070],\n",
      "        [    0.1052],\n",
      "        [    0.1527],\n",
      "        [    0.1365],\n",
      "        [    0.1218],\n",
      "        [    0.0016],\n",
      "        [    0.1069],\n",
      "        [    0.1143],\n",
      "        [    0.0619],\n",
      "        [    0.0798],\n",
      "        [    0.0880],\n",
      "        [    0.1243],\n",
      "        [    0.1629],\n",
      "        [    0.0203],\n",
      "        [    0.0049],\n",
      "        [    0.0402],\n",
      "        [    0.0480],\n",
      "        [    0.0702],\n",
      "        [    0.0509],\n",
      "        [    0.0003],\n",
      "        [    0.0721],\n",
      "        [    0.0595],\n",
      "        [    0.0335],\n",
      "        [    0.0397],\n",
      "        [    0.0804],\n",
      "        [    0.1348],\n",
      "        [    0.0992],\n",
      "        [    0.1209],\n",
      "        [    0.1039],\n",
      "        [    0.1347],\n",
      "        [    0.2833],\n",
      "        [    0.2973],\n",
      "        [    0.3003],\n",
      "        [    0.3022],\n",
      "        [    0.3032],\n",
      "        [    0.1227],\n",
      "        [    0.3127],\n",
      "        [    0.3145],\n",
      "        [    0.1644],\n",
      "        [    0.1963],\n",
      "        [    0.3156],\n",
      "        [    0.3230],\n",
      "        [    0.3248],\n",
      "        [    0.3269],\n",
      "        [    0.3279],\n",
      "        [    0.3311],\n",
      "        [    0.3502]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 96.6147403717041\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 138\n",
      "剩餘X 資料 torch.Size([22, 18])\n",
      "剩餘Y 資料 torch.Size([22, 1])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.12333766371011734, 14)\n",
      "The second_loss value of k: (0.12825539708137512, 4)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.0825])\n",
      "目前模型的Data狀態 torch.Size([138, 1])\n",
      "<<預測值>>\n",
      "tensor([[0.1098],\n",
      "        [0.3935],\n",
      "        [0.2688],\n",
      "        [0.5160],\n",
      "        [0.5224],\n",
      "        [0.4223],\n",
      "        [0.1534],\n",
      "        [0.1688],\n",
      "        [0.1225],\n",
      "        [0.4950],\n",
      "        [0.4487],\n",
      "        [0.3358],\n",
      "        [0.1106],\n",
      "        [0.1998],\n",
      "        [0.4551],\n",
      "        [0.3239],\n",
      "        [0.3323],\n",
      "        [0.1318],\n",
      "        [0.1698],\n",
      "        [0.4779],\n",
      "        [0.5379],\n",
      "        [0.4477],\n",
      "        [0.3860],\n",
      "        [0.4743],\n",
      "        [0.5330],\n",
      "        [0.3311],\n",
      "        [0.4812],\n",
      "        [0.4895],\n",
      "        [0.2740],\n",
      "        [0.4574],\n",
      "        [0.1870],\n",
      "        [0.4845],\n",
      "        [0.4478],\n",
      "        [0.5130],\n",
      "        [0.1411],\n",
      "        [0.1535],\n",
      "        [0.1844],\n",
      "        [0.3192],\n",
      "        [0.5745],\n",
      "        [0.1329],\n",
      "        [0.3170],\n",
      "        [0.2978],\n",
      "        [0.3249],\n",
      "        [0.1967],\n",
      "        [0.4351],\n",
      "        [0.2283],\n",
      "        [0.2009],\n",
      "        [0.2685],\n",
      "        [0.4918],\n",
      "        [0.4473],\n",
      "        [0.3918],\n",
      "        [0.5209],\n",
      "        [0.5308],\n",
      "        [0.1820],\n",
      "        [0.4708],\n",
      "        [0.4171],\n",
      "        [0.2142],\n",
      "        [0.5211],\n",
      "        [0.1622],\n",
      "        [0.1855],\n",
      "        [0.4657],\n",
      "        [0.5571],\n",
      "        [0.5535],\n",
      "        [0.5386],\n",
      "        [0.2646],\n",
      "        [0.2535],\n",
      "        [0.1817],\n",
      "        [0.1648],\n",
      "        [0.1245],\n",
      "        [0.4265],\n",
      "        [0.4174],\n",
      "        [0.0890],\n",
      "        [0.5437],\n",
      "        [0.2027],\n",
      "        [0.4998],\n",
      "        [0.5335],\n",
      "        [0.2240],\n",
      "        [0.3351],\n",
      "        [0.4395],\n",
      "        [0.1784],\n",
      "        [0.3134],\n",
      "        [0.2526],\n",
      "        [0.3447],\n",
      "        [0.3487],\n",
      "        [0.4357],\n",
      "        [0.3631],\n",
      "        [0.2358],\n",
      "        [0.3777],\n",
      "        [0.2782],\n",
      "        [0.1220],\n",
      "        [0.2514],\n",
      "        [0.3313],\n",
      "        [0.3430],\n",
      "        [0.3307],\n",
      "        [0.4086],\n",
      "        [0.5156],\n",
      "        [0.4333],\n",
      "        [0.1884],\n",
      "        [0.7013],\n",
      "        [0.7728],\n",
      "        [0.7832],\n",
      "        [0.1949],\n",
      "        [0.3903],\n",
      "        [0.7708],\n",
      "        [0.6332],\n",
      "        [0.7279],\n",
      "        [0.7841],\n",
      "        [0.8185],\n",
      "        [0.7884],\n",
      "        [0.7660],\n",
      "        [0.4996],\n",
      "        [0.7364],\n",
      "        [0.6159],\n",
      "        [0.6880],\n",
      "        [0.5856],\n",
      "        [0.4823],\n",
      "        [0.6120],\n",
      "        [0.5027],\n",
      "        [0.6698],\n",
      "        [0.4891],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.6068],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.6389],\n",
      "        [0.6223],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.4336],\n",
      "        [0.4336]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0098],\n",
      "        [    0.0392],\n",
      "        [    0.0066],\n",
      "        [    0.0253],\n",
      "        [    0.0342],\n",
      "        [    0.0181],\n",
      "        [    0.0151],\n",
      "        [    0.0153],\n",
      "        [    0.0139],\n",
      "        [    0.0496],\n",
      "        [    0.0253],\n",
      "        [    0.0222],\n",
      "        [    0.0226],\n",
      "        [    0.0107],\n",
      "        [    0.0226],\n",
      "        [    0.0672],\n",
      "        [    0.0311],\n",
      "        [    0.0008],\n",
      "        [    0.0638],\n",
      "        [    0.0138],\n",
      "        [    0.0128],\n",
      "        [    0.0062],\n",
      "        [    0.0215],\n",
      "        [    0.0479],\n",
      "        [    0.0404],\n",
      "        [    0.0268],\n",
      "        [    0.0130],\n",
      "        [    0.0328],\n",
      "        [    0.0357],\n",
      "        [    0.0125],\n",
      "        [    0.0250],\n",
      "        [    0.0009],\n",
      "        [    0.0242],\n",
      "        [    0.0747],\n",
      "        [    0.0503],\n",
      "        [    0.0440],\n",
      "        [    0.0073],\n",
      "        [    0.0393],\n",
      "        [    0.0750],\n",
      "        [    0.0398],\n",
      "        [    0.0461],\n",
      "        [    0.0303],\n",
      "        [    0.0338],\n",
      "        [    0.0159],\n",
      "        [    0.0120],\n",
      "        [    0.0105],\n",
      "        [    0.0088],\n",
      "        [    0.0386],\n",
      "        [    0.0776],\n",
      "        [    0.0358],\n",
      "        [    0.0666],\n",
      "        [    0.0487],\n",
      "        [    0.0500],\n",
      "        [    0.0017],\n",
      "        [    0.0353],\n",
      "        [    0.0474],\n",
      "        [    0.0266],\n",
      "        [    0.1016],\n",
      "        [    0.0924],\n",
      "        [    0.0264],\n",
      "        [    0.0221],\n",
      "        [    0.0979],\n",
      "        [    0.0652],\n",
      "        [    0.0480],\n",
      "        [    0.0445],\n",
      "        [    0.0375],\n",
      "        [    0.0636],\n",
      "        [    0.0742],\n",
      "        [    0.0974],\n",
      "        [    0.0571],\n",
      "        [    0.0830],\n",
      "        [    0.0298],\n",
      "        [    0.1315],\n",
      "        [    0.0844],\n",
      "        [    0.1001],\n",
      "        [    0.0460],\n",
      "        [    0.0613],\n",
      "        [    0.1159],\n",
      "        [    0.1253],\n",
      "        [    0.0975],\n",
      "        [    0.0957],\n",
      "        [    0.0771],\n",
      "        [    0.0543],\n",
      "        [    0.1035],\n",
      "        [    0.1243],\n",
      "        [    0.0982],\n",
      "        [    0.0719],\n",
      "        [    0.1208],\n",
      "        [    0.0873],\n",
      "        [    0.1220],\n",
      "        [    0.1070],\n",
      "        [    0.1052],\n",
      "        [    0.1527],\n",
      "        [    0.1365],\n",
      "        [    0.1218],\n",
      "        [    0.0016],\n",
      "        [    0.1069],\n",
      "        [    0.1143],\n",
      "        [    0.0619],\n",
      "        [    0.0798],\n",
      "        [    0.0880],\n",
      "        [    0.1243],\n",
      "        [    0.1629],\n",
      "        [    0.0203],\n",
      "        [    0.0049],\n",
      "        [    0.0402],\n",
      "        [    0.0480],\n",
      "        [    0.0702],\n",
      "        [    0.0509],\n",
      "        [    0.0003],\n",
      "        [    0.0721],\n",
      "        [    0.0595],\n",
      "        [    0.0335],\n",
      "        [    0.0397],\n",
      "        [    0.0804],\n",
      "        [    0.1348],\n",
      "        [    0.0992],\n",
      "        [    0.1209],\n",
      "        [    0.1039],\n",
      "        [    0.1347],\n",
      "        [    0.2833],\n",
      "        [    0.2973],\n",
      "        [    0.3003],\n",
      "        [    0.3022],\n",
      "        [    0.3032],\n",
      "        [    0.1227],\n",
      "        [    0.3127],\n",
      "        [    0.3145],\n",
      "        [    0.1644],\n",
      "        [    0.1963],\n",
      "        [    0.3156],\n",
      "        [    0.3230],\n",
      "        [    0.3248],\n",
      "        [    0.3269],\n",
      "        [    0.3279],\n",
      "        [    0.3311],\n",
      "        [    0.3502],\n",
      "        [    0.3512]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.43\n",
      "Loss值\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.43\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.43\n"
     ]
    }
   ],
   "source": [
    "evaluation_table_train = pd.DataFrame(columns=[\"Window_index\",\"Stage\",\"MAE\",\"MAPE\",\"RMSE\",\"Accuracy(2000)\",\"Accuracy(3000)\",\"Step4\",\"Step6.1\",\"Step6.2\",\"Time\",\"Adopted_hidden_node\"])\n",
    "evaluation_table_outlier = pd.DataFrame(columns=[\"Window_index\",\"Stage\",\"MAE\",\"MAPE\",\"RMSE\",\"Accuracy(2000)\",\"Accuracy(3000)\",\"Step4\",\"Step6.1\",\"Step6.2\",\"Time\",\"Adopted_hidden_node\"])\n",
    "evaluation_table_test = pd.DataFrame(columns=[\"Window_index\",\"Stage\",\"MAE\",\"MAPE\",\"RMSE\",\"Accuracy(2000)\",\"Accuracy(3000)\",\"Step4\",\"Step6.1\",\"Step6.2\",\"Time\",\"Adopted_hidden_node\"])\n",
    "    \n",
    "x_data, y_data= get_data(4)\n",
    "x_data = sc.fit_transform(x_data)\n",
    "y_data = sc.fit_transform(y_data[:,3].reshape(-1,1))\n",
    "threshold_for_error = 11000/(sc.data_max_-sc.data_min_)\n",
    "\n",
    "data = range(x_data.shape[0])\n",
    "# window_size => the length of training block\n",
    "window_size = 159\n",
    "# step_window => step size of each window\n",
    "step_window = 26\n",
    "# the split data\n",
    "splits = []\n",
    "\n",
    "## Moving window mechnism\n",
    "for i in range(window_size, len(data), step_window):\n",
    "    train = np.array(data[i-window_size:i])\n",
    "    test = np.array(data[i:i+step_window])\n",
    "#     test = np.array(data[i-window_size:i+step_window])\n",
    "    splits.append(('TRAIN:', train, 'TEST:', test))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i_block in range(len(splits)):\n",
    "# for i_block in range(-2,0,1):\n",
    "# for i_block in range(2):\n",
    "    block_start = time.time()\n",
    "    ## Record the number of each step\n",
    "    nb_step4 = 0\n",
    "    nb_step6_1 = 0\n",
    "    nb_step6_2 = 0\n",
    "    \n",
    "    print(\"The <<%d>> Block\" %(i_block+1))\n",
    "#     print(\"The <<%d>> Block\" %(len(splits)+i_block+1))\n",
    "#     print(\"The training block\\n\", y_data[splits[i_block][1]])\n",
    "#     print(\"The testing block\\n\", y_data[splits[i_block][3]])\n",
    "    \n",
    "    x_train = x_data[splits[i_block][1]]\n",
    "    x_test = x_data[splits[i_block][3]]\n",
    "    y_train = y_data[splits[i_block][1]]\n",
    "    y_test = y_data[splits[i_block][3]]\n",
    "    \n",
    "    x_train_scaled = torch.FloatTensor(x_train)\n",
    "    x_test_scaled = torch.FloatTensor(x_test)\n",
    "    y_train_scaled = torch.FloatTensor(y_train)\n",
    "    y_test = sc.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "#     if i_block == -2:\n",
    "    if i_block == 0:\n",
    "        lower = torch.mean(y_train_scaled)-torch.std(y_train_scaled)\n",
    "        upper = torch.mean(y_train_scaled)+torch.std(y_train_scaled)\n",
    "        nonoutlier_index = torch.nonzero((y_train_scaled[:,0]>lower)&(y_train_scaled[:,0]<upper)).reshape([-1])\n",
    "        \n",
    "        initial_x = x_train_scaled[nonoutlier_index[:19]]\n",
    "        initial_y = y_train_scaled[nonoutlier_index[:19]]\n",
    "        \n",
    "        x_train_scaled = np.delete(x_train_scaled, nonoutlier_index[:19], 0)\n",
    "        y_train_scaled = np.delete(y_train_scaled, nonoutlier_index[:19], 0)\n",
    "#         print(initial_x.shape[0])\n",
    "        \n",
    "        network = Network(1,initial_x,initial_y)\n",
    "        \n",
    "        network.nb_node_acceptable = torch.IntTensor([1 for _ in range(initial_x.shape[0])])\n",
    "        network.threshold_for_error = round(threshold_for_error[0],2)\n",
    "        \n",
    "        initializing(network, initial_x, initial_y)\n",
    "        \n",
    "        print(\"<<Initializing後看一下差異>>\")\n",
    "        yo,loss = network.forward()\n",
    "        print(torch.abs(network.y-yo))\n",
    "        print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "        remainder = int(window_size*0.9624) - initial_x.shape[0]\n",
    "    \n",
    "    else:\n",
    "#         print(\"新的Code待驗證\")\n",
    "#         print(network.state_dict())\n",
    "        sorted_index = selecting(network, x_train_scaled, y_train_scaled)\n",
    "        restart_index = int(x_train_scaled.shape[0]*0.9624)-step_window\n",
    "        print(\"其他區塊剛開始選的資料索引：\",sorted_index[:restart_index])\n",
    "        init_x = x_train_scaled[sorted_index[:int(x_train_scaled.shape[0]*0.9624)-step_window]].reshape(-1,x_train_scaled.shape[1])\n",
    "        init_y = y_train_scaled[sorted_index[:int(x_train_scaled.shape[0]*0.9624)-step_window]].reshape(-1,1)\n",
    "#         print(\"取得的x\",init_x.shape)\n",
    "#         print(\"取得的y\",init_y.shape)\n",
    "#         print(\"前\")\n",
    "#         print(network.y.shape)\n",
    "        network.setData(init_x, init_y)\n",
    "#         print(\"後\")\n",
    "#         print(network.y.shape)\n",
    "        network.nb_node_acceptable = torch.IntTensor([network.linear1.bias.data.shape[0] for _ in range(init_x.shape[0])])\n",
    "        network.nb_node_pruned = 0\n",
    "        \n",
    "        print(\"<<其他區塊剛開始時看一下差異>>\")\n",
    "        yo,loss = network.forward()\n",
    "        print(torch.abs(network.y-yo))\n",
    "        \n",
    "        remainder = int(window_size*0.9624) - init_x.shape[0]\n",
    "        x_train_scaled = np.delete(x_train_scaled, sorted_index[:restart_index], 0)\n",
    "        y_train_scaled = np.delete(y_train_scaled, sorted_index[:restart_index], 0)\n",
    "        \n",
    "#         print(\"X 資料\",x_train_scaled.shape)\n",
    "#         print(\"Y 資料\",y_train_scaled.shape)\n",
    "#     network.limit = network.linear1.bias.data.shape[0]\n",
    "#     print(\"Limit for node pruned:\",network.limit)\n",
    "    \n",
    "#     for i in range(int(x_train_scaled.shape[0]*0.9624)):\n",
    "#     for i in range(2):\n",
    "    for i in range(remainder):\n",
    "#         if i_block == -2:\n",
    "        if i_block == 0:\n",
    "            print(\"現在訓練到第幾筆資料: %d\"%(i+x_train_scaled.shape[1]+2))\n",
    "        \n",
    "        else:\n",
    "            print(\"現在訓練到第幾筆資料: %d\"%(restart_index+i+1))\n",
    "        \n",
    "        print(\"剩餘X 資料\",x_train_scaled.shape)\n",
    "        print(\"剩餘Y 資料\",y_train_scaled.shape)\n",
    "        \n",
    "        sorted_index = selecting(network, x_train_scaled, y_train_scaled)\n",
    "        \n",
    "\n",
    "        ## Add new data for training\n",
    "        \n",
    "        print(\"現在要進去模型的數據，索引%d，y=\"%(sorted_index[0]),y_train_scaled[sorted_index[0]].data)        \n",
    "        network.addData(x_train_scaled[sorted_index[0]], y_train_scaled[sorted_index[0]])\n",
    "        print(\"目前模型的Data狀態\",network.y.shape)\n",
    "        x_train_scaled = np.delete(x_train_scaled, sorted_index[0], 0)\n",
    "        y_train_scaled = np.delete(y_train_scaled, sorted_index[0], 0)\n",
    "        \n",
    "        yo,loss = network.forward()\n",
    "        print(\"<<預測值>>\")\n",
    "        print(yo)\n",
    "        print(\"<<差異>>\")\n",
    "        print(torch.abs(yo-network.y))\n",
    "        print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "        print(\"Loss值\")\n",
    "        print(loss)\n",
    "\n",
    "        pre_network = copy.deepcopy(network)\n",
    "        \n",
    "        if not torch.all(torch.abs(network.y-yo)<=network.threshold_for_error):\n",
    "\n",
    "            network.acceptable = False\n",
    "            network = matching(network)\n",
    "            \n",
    "            print(\"<<Matching後看一下差異>>\")\n",
    "            yo,loss = network.forward()\n",
    "            print(torch.abs(yo-network.y))\n",
    "            print(\"threshold_for_error:\",threshold_for_error)\n",
    "            \n",
    "            if network.acceptable == False:\n",
    "                \n",
    "                network = copy.deepcopy(pre_network)\n",
    "                cramming(network)\n",
    "\n",
    "                if network.acceptable == False:\n",
    "                    sys.exit(0)  \n",
    "                \n",
    "                print(\"<<Cramming後看一下差異>>\")\n",
    "                yo,loss = network.forward()\n",
    "                print(torch.abs(yo-network.y))\n",
    "                print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "                nb_step6_2 += 1\n",
    "\n",
    "            else:\n",
    "                nb_step6_1 += 1\n",
    "\n",
    "        else:\n",
    "            nb_step4 += 1\n",
    "\n",
    "        network = reorganizing(network)\n",
    "        print(\"<<Reorganizing後看一下差異>>\")\n",
    "        yo,loss = network.forward()\n",
    "        print(torch.abs(yo-network.y))\n",
    "        print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "        \n",
    "        network.nb_node_acceptable = torch.cat([network.nb_node_acceptable, torch.IntTensor([network.linear1.bias.data.shape[0]])],0)\n",
    "        print(\"看一下 hidden node\")\n",
    "        print(network.nb_node_acceptable)\n",
    "       \n",
    "        print(\"使用裝置\",(list(network.parameters())[0].device))\n",
    "        print(\"累計時間(s)\",time.time()-start)\n",
    "#         print(network.state_dict())\n",
    "        print(\"-\"*90)\n",
    "\n",
    "    \n",
    "    block_end = time.time()\n",
    "    print(\"到第 %d 個區塊累積花費時間(s)\"%(i_block+1),block_end-block_start)\n",
    "#     print(\"到第 %d 個區塊累積花費時間(s)\"%(len(splits)+i_block+1),block_end-block_start)\n",
    "    print(\"<<The performance of %d block>>\"%(i_block+1))\n",
    "#     print(\"<<The performance of %d block>>\"%(len(splits)+i_block+1))\n",
    "    \n",
    "    evaluation_table_train, evaluation_table_outlier, evaluation_table_test = validation(network, nb_step4, nb_step6_1, nb_step6_2, x_train_scaled, y_train_scaled,x_test_scaled, y_test, block_start, block_end,i_block+1,evaluation_table_train,evaluation_table_outlier,evaluation_table_test)\n",
    "\n",
    "    evaluation_table_train.to_csv(\"evaluation_table_train.csv\",index=False)\n",
    "    evaluation_table_outlier.to_csv(\"evaluation_table_outlier.csv\",index=False)\n",
    "    evaluation_table_test.to_csv(\"evaluation_table_inferencing.csv\",index=False)\n",
    "#     validation(network, nb_step4, nb_step6_1, nb_step6_2, x_test_scaled, y_test, block_start, block_end,(len(splits)+i_block+1))\n",
    "end = time.time()\n",
    "print(\"總計時間(s)\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
